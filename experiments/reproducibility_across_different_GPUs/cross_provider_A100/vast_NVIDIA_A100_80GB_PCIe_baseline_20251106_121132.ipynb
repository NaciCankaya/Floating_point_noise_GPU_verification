{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02062431-9d8f-407d-92b5-01b29f5122a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter notebook\n",
      "Enter provider name (e.g., 'runpod', 'vast', or press Enter for 'unknown'):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " vast\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYSTEM ATTESTATION\n",
      "======================================================================\n",
      "Provider: vast\n",
      "Hostname: eb6c05545f33\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "PyTorch: 2.8.0+cu129\n",
      "CUDA: 12.9\n",
      "Driver: 580.65.06\n",
      "Compute Mode: Default\n",
      "Persistence Mode: unknown\n",
      "\n",
      "Firmware Info:\n",
      "  VBIOS Version: 92.00.9A.00.01\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL\n",
      "======================================================================\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Precision: BF16 (bfloat16)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5355b250a794d65953612668977b181",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5857c3febd744619a85ec46f67f101b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35962092f9a44b84a3c3ecee5a2fa7b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bcdfdfc1754a9abb555d709c6ffb1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a4d3865cc274a53a3d384ce3ab125b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ceba47c9aa4cce91eaffa6bcac3bae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41faa8e2dee74c68be574e8a112e93f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13739a10e5e74777bb5faeeee341e9ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c495000dad14db2946ad53085b6d3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "933b94b59d4445d48439ee05d2115ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97539ecaf5234a19b3b06a40f05e6227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d9df8396ff40b58887ab5883052174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c556ab360c4f9da2da8dde1286af05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after load: 14.19 GB\n",
      "\n",
      "======================================================================\n",
      "RUNNING BASELINE EXPERIMENT\n",
      "======================================================================\n",
      "Layers to extract: [0, 1, 2, 3, 4, 7, 10, 14, 18, 22, 26, 28]\n",
      "Repetitions: 5\n",
      "Operation: Prefill only (no generation)\n",
      "Extraction: Last valid token position only\n",
      "\n",
      "Repetition 1/5...\n",
      "  Hidden state dim: 3584\n",
      "  Sequence length: 129\n",
      "  Last valid pos: 128\n",
      "  Layers extracted: 12\n",
      "Repetition 2/5...\n",
      "Repetition 3/5...\n",
      "Repetition 4/5...\n",
      "Repetition 5/5...\n",
      "\n",
      "======================================================================\n",
      "REPRODUCIBILITY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Hidden States:\n",
      "  layer_0: [EXACT] Bit-exact across all repetitions\n",
      "  layer_1: [EXACT] Bit-exact across all repetitions\n",
      "  layer_2: [EXACT] Bit-exact across all repetitions\n",
      "  layer_3: [EXACT] Bit-exact across all repetitions\n",
      "  layer_4: [EXACT] Bit-exact across all repetitions\n",
      "  layer_7: [EXACT] Bit-exact across all repetitions\n",
      "  layer_10: [EXACT] Bit-exact across all repetitions\n",
      "  layer_14: [EXACT] Bit-exact across all repetitions\n",
      "  layer_18: [EXACT] Bit-exact across all repetitions\n",
      "  layer_22: [EXACT] Bit-exact across all repetitions\n",
      "  layer_26: [EXACT] Bit-exact across all repetitions\n",
      "  layer_28: [EXACT] Bit-exact across all repetitions\n",
      "\n",
      "Key Vectors:\n",
      "  layer_1: [EXACT] Bit-exact across all repetitions\n",
      "  layer_2: [EXACT] Bit-exact across all repetitions\n",
      "  layer_3: [EXACT] Bit-exact across all repetitions\n",
      "  layer_4: [EXACT] Bit-exact across all repetitions\n",
      "  layer_7: [EXACT] Bit-exact across all repetitions\n",
      "  layer_10: [EXACT] Bit-exact across all repetitions\n",
      "  layer_14: [EXACT] Bit-exact across all repetitions\n",
      "  layer_18: [EXACT] Bit-exact across all repetitions\n",
      "  layer_22: [EXACT] Bit-exact across all repetitions\n",
      "  layer_26: [EXACT] Bit-exact across all repetitions\n",
      "  layer_28: [EXACT] Bit-exact across all repetitions\n",
      "\n",
      "======================================================================\n",
      "VERDICT\n",
      "======================================================================\n",
      "[SUCCESS] FULLY REPRODUCIBLE: All activations bit-exact across repetitions\n",
      "  This configuration provides a clean baseline for cross-provider comparison\n",
      "\n",
      "======================================================================\n",
      "SAVING RESULTS\n",
      "======================================================================\n",
      "[SUCCESS] Results saved to: /workspace/vast_NVIDIA_A100_80GB_PCIe_baseline_20251106_121132.json\n",
      "[INFO] File size: 3126.7 KB\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Next steps:\n",
      "1. Run this same script on different provider\n",
      "2. Compare attestation sections (especially firmware)\n",
      "3. Compare hidden_states and key_vectors for bit-exactness\n",
      "4. If differences found, binary search on configuration variables\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Provider A100 Baseline Experiment\n",
    "\n",
    "Tests FP reproducibility across different cloud providers (RunPod vs Vast.ai)\n",
    "Measures both hidden states and key vectors across multiple layers.\n",
    "Only extracts from last valid token position (prefill workload).\n",
    "\n",
    "Usage:\n",
    "    Command-line:\n",
    "        python cross_provider_a100_baseline.py --provider runpod\n",
    "    \n",
    "    Jupyter notebook:\n",
    "        # Run all cells, then:\n",
    "        main('runpod')  # or main('vast'), etc.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM ATTESTATION\n",
    "# ============================================================================\n",
    "\n",
    "def get_gpu_firmware_info():\n",
    "    \"\"\"Extract detailed GPU firmware/VBIOS information\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        output = result.stdout\n",
    "        \n",
    "        firmware_info = {}\n",
    "        for line in output.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if 'VBIOS Version' in line or 'GPU Board Serial Number' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    key = parts[0].strip()\n",
    "                    value = parts[1].strip()\n",
    "                    firmware_info[key] = value\n",
    "        \n",
    "        return firmware_info\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def get_gpu_detailed_info():\n",
    "    \"\"\"Get comprehensive GPU configuration\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=name,memory.total,driver_version,compute_cap,pci.bus_id,pcie.link.gen.max,pcie.link.width.max,power.limit',\n",
    "             '--format=csv,noheader,nounits'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            parts = result.stdout.strip().split(', ')\n",
    "            return {\n",
    "                \"name\": parts[0],\n",
    "                \"memory_total_mb\": parts[1],\n",
    "                \"driver_version\": parts[2],\n",
    "                \"compute_capability\": parts[3],\n",
    "                \"pci_bus_id\": parts[4],\n",
    "                \"pcie_gen_max\": parts[5],\n",
    "                \"pcie_width_max\": parts[6],\n",
    "                \"power_limit_w\": parts[7]\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def get_compute_mode():\n",
    "    \"\"\"Check if GPU is in exclusive compute mode\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q', '-d', 'COMPUTE'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'Compute Mode' in line:\n",
    "                return line.split(':', 1)[1].strip()\n",
    "        \n",
    "        return \"unknown\"\n",
    "    except Exception as e:\n",
    "        return f\"error: {e}\"\n",
    "\n",
    "def get_persistence_mode():\n",
    "    \"\"\"Check persistence mode status\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q', '-d', 'PERSISTENCE_MODE'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'Persistence Mode' in line:\n",
    "                return line.split(':', 1)[1].strip()\n",
    "        \n",
    "        return \"unknown\"\n",
    "    except Exception as e:\n",
    "        return f\"error: {e}\"\n",
    "\n",
    "def get_ecc_status():\n",
    "    \"\"\"Check ECC memory status\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q', '-d', 'ECC'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        ecc_info = {}\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'ECC Mode' in line or 'Current' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    ecc_info[parts[0].strip()] = parts[1].strip()\n",
    "        \n",
    "        return ecc_info\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def get_clock_info():\n",
    "    \"\"\"Get GPU clock speeds\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q', '-d', 'CLOCK'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        clocks = {}\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if 'Graphics' in line or 'SM' in line or 'Memory' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    clocks[parts[0].strip()] = parts[1].strip()\n",
    "        \n",
    "        return clocks\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def check_mig_mode():\n",
    "    \"\"\"Check if GPU is in MIG mode or partitioned\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-L'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        output = result.stdout\n",
    "        is_mig = 'MIG' in output\n",
    "        \n",
    "        return {\n",
    "            \"is_mig\": is_mig,\n",
    "            \"devices\": output.strip()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def attest_system(provider_name=\"unknown\"):\n",
    "    \"\"\"Comprehensive system attestation\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"SYSTEM ATTESTATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    attestation = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"provider\": provider_name,\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"container_id\": os.environ.get('HOSTNAME', 'unknown'),\n",
    "    }\n",
    "    \n",
    "    # PyTorch and CUDA\n",
    "    attestation[\"pytorch\"] = {\n",
    "        \"version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda,\n",
    "        \"cudnn_version\": torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else None,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        attestation[\"gpu\"] = {\n",
    "            \"name\": torch.cuda.get_device_name(0),\n",
    "            \"capability\": f\"{torch.cuda.get_device_capability(0)[0]}.{torch.cuda.get_device_capability(0)[1]}\",\n",
    "            \"memory_allocated_gb\": torch.cuda.memory_allocated(0) / 1024**3,\n",
    "            \"memory_reserved_gb\": torch.cuda.memory_reserved(0) / 1024**3,\n",
    "        }\n",
    "        \n",
    "        # Detailed info from nvidia-smi\n",
    "        attestation[\"gpu_detailed\"] = get_gpu_detailed_info()\n",
    "        attestation[\"gpu_firmware\"] = get_gpu_firmware_info()\n",
    "        attestation[\"compute_mode\"] = get_compute_mode()\n",
    "        attestation[\"persistence_mode\"] = get_persistence_mode()\n",
    "        attestation[\"ecc_status\"] = get_ecc_status()\n",
    "        attestation[\"clock_info\"] = get_clock_info()\n",
    "        attestation[\"mig_status\"] = check_mig_mode()\n",
    "    \n",
    "    # Environment variables\n",
    "    env_vars = {}\n",
    "    for key in sorted(os.environ.keys()):\n",
    "        if any(x in key.upper() for x in ['CUDA', 'TORCH', 'NCCL', 'CUDNN', 'PYTORCH', 'NVIDIA']):\n",
    "            env_vars[key] = os.environ[key]\n",
    "    attestation[\"environment\"] = env_vars\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Provider: {provider_name}\")\n",
    "    print(f\"Hostname: {attestation['hostname']}\")\n",
    "    print(f\"GPU: {attestation['gpu']['name']}\")\n",
    "    print(f\"PyTorch: {attestation['pytorch']['version']}\")\n",
    "    print(f\"CUDA: {attestation['pytorch']['cuda_version']}\")\n",
    "    print(f\"Driver: {attestation['gpu_detailed'].get('driver_version', 'unknown')}\")\n",
    "    print(f\"Compute Mode: {attestation['compute_mode']}\")\n",
    "    print(f\"Persistence Mode: {attestation['persistence_mode']}\")\n",
    "    \n",
    "    if attestation['gpu_firmware']:\n",
    "        print(f\"\\nFirmware Info:\")\n",
    "        for key, value in attestation['gpu_firmware'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    if attestation['mig_status'].get('is_mig'):\n",
    "        print(f\"\\n[WARNING] MIG mode detected!\")\n",
    "        print(f\"  {attestation['mig_status']['devices']}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return attestation\n",
    "\n",
    "# ============================================================================\n",
    "# DATA COLLECTION\n",
    "# ============================================================================\n",
    "\n",
    "def collect_multilayer_activations(model, tokenizer, prompt, layer_indices, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Extract hidden states and key vectors from multiple layers.\n",
    "    Only extract from last valid token position to save memory.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"hidden_states\": {f\"layer_{idx}\": tensor},\n",
    "            \"key_vectors\": {f\"layer_{idx}\": tensor},\n",
    "            \"metadata\": {...}\n",
    "        }\n",
    "    \"\"\"\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get both hidden states and key-value cache\n",
    "        outputs = model(**inputs, output_hidden_states=True, use_cache=True, return_dict=True)\n",
    "    \n",
    "    last_valid_pos = inputs['attention_mask'][0].sum() - 1\n",
    "    \n",
    "    # Extract hidden states from selected layers\n",
    "    hidden_states = {}\n",
    "    for idx in layer_indices:\n",
    "        if idx < len(outputs.hidden_states):\n",
    "            # Extract last valid token position, move to CPU immediately\n",
    "            hidden = outputs.hidden_states[idx][0, last_valid_pos, :].cpu().clone()\n",
    "            hidden_states[f\"layer_{idx}\"] = hidden\n",
    "    \n",
    "    # Extract key vectors from selected layers\n",
    "    # past_key_values is tuple of (key, value) pairs per layer\n",
    "    # Each key: [batch_size, num_key_value_heads, seq_len, head_dim]\n",
    "    key_vectors = {}\n",
    "    for i, layer_idx in enumerate(layer_indices):\n",
    "        # Skip embedding layer (idx=0)\n",
    "        if layer_idx > 0 and i < len(outputs.past_key_values):\n",
    "            layer_keys = outputs.past_key_values[i][0]  # [0] for keys\n",
    "            # Extract last valid token position, all heads\n",
    "            key_vec = layer_keys[0, :, last_valid_pos, :]\n",
    "            # Flatten: [num_key_value_heads * head_dim]\n",
    "            key_vec_flat = key_vec.reshape(-1).cpu().clone()\n",
    "            key_vectors[f\"layer_{layer_idx}\"] = key_vec_flat\n",
    "    \n",
    "    metadata = {\n",
    "        \"seq_len\": seq_len,\n",
    "        \"last_valid_pos\": int(last_valid_pos),\n",
    "        \"num_layers_extracted\": len(layer_indices),\n",
    "        \"hidden_dim\": hidden_states[f\"layer_{layer_indices[0]}\"].shape[0] if hidden_states else 0,\n",
    "    }\n",
    "    \n",
    "    # Aggressive cleanup\n",
    "    del outputs.hidden_states\n",
    "    del outputs.past_key_values\n",
    "    del outputs\n",
    "    del inputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return {\n",
    "        \"hidden_states\": hidden_states,\n",
    "        \"key_vectors\": key_vectors,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "def main(provider_name='unknown'):\n",
    "    \"\"\"\n",
    "    Main experiment function.\n",
    "    \n",
    "    Args:\n",
    "        provider_name: Name of the cloud provider (e.g., 'runpod', 'vast')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Attest system first\n",
    "    attestation = attest_system(provider_name)\n",
    "    \n",
    "    # Check for concerning configurations\n",
    "    if attestation.get('mig_status', {}).get('is_mig'):\n",
    "        print(\"칙코  WARNING: MIG mode detected - results may not be comparable!\")\n",
    "        response = input(\"Continue anyway? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            sys.exit(1)\n",
    "    \n",
    "    if attestation.get('compute_mode') not in ['Default', 'Exclusive Process']:\n",
    "        print(f\"칙코  WARNING: Compute mode is '{attestation.get('compute_mode')}'\")\n",
    "    \n",
    "    # Configuration\n",
    "    CACHE_DIR = '/workspace/huggingface_cache'\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    num_repetitions = 5\n",
    "    \n",
    "    # Select layers to sample (dense at beginning, sparse at end)\n",
    "    # Layer 0 is embedding, so we start from 1\n",
    "    total_layers = 28  # Qwen2.5-7B has 28 transformer layers\n",
    "    layer_indices = [0, 1, 2, 3, 4, 7, 10, 14, 18, 22, 26, 28]  # 0 for embedding\n",
    "    \n",
    "    # Test prompt - technical content\n",
    "    prompt = \"\"\"The study investigates the quantum decoherence effects on a multi-qubit superconducting system when subjected to controlled microwave pulses. We utilized a novel cryogenic amplification chain to minimize thermal noise and achieve a signal-to-noise ratio previously unattainable in similar setups. The experimental protocol involved preparing the qubits in a Greenberger-Horne-Zeilinger (GHZ) state and then measuring the decay of quantum entanglement over time by performing state tomography. Our results demonstrate a non-linear relationship between pulse amplitude and coherence time, suggesting that higher-order coupling terms, often neglected in theoretical models, play a significant role in system dynamics.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Precision: BF16 (bfloat16)\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    prompt_tokens = len(tokenizer.encode(prompt))\n",
    "    print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    mem_after_load = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"GPU memory after load: {mem_after_load:.2f} GB\")\n",
    "    print()\n",
    "    \n",
    "    # Run experiment\n",
    "    print(\"=\"*70)\n",
    "    print(\"RUNNING BASELINE EXPERIMENT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Layers to extract: {layer_indices}\")\n",
    "    print(f\"Repetitions: {num_repetitions}\")\n",
    "    print(f\"Operation: Prefill only (no generation)\")\n",
    "    print(f\"Extraction: Last valid token position only\")\n",
    "    print()\n",
    "    \n",
    "    all_runs = []\n",
    "    \n",
    "    for rep in range(num_repetitions):\n",
    "        print(f\"Repetition {rep+1}/{num_repetitions}...\")\n",
    "        \n",
    "        run_data = collect_multilayer_activations(\n",
    "            model, tokenizer, prompt, layer_indices, device=\"cuda\"\n",
    "        )\n",
    "        \n",
    "        all_runs.append(run_data)\n",
    "        \n",
    "        if rep == 0:\n",
    "            print(f\"  Hidden state dim: {run_data['metadata']['hidden_dim']}\")\n",
    "            print(f\"  Sequence length: {run_data['metadata']['seq_len']}\")\n",
    "            print(f\"  Last valid pos: {run_data['metadata']['last_valid_pos']}\")\n",
    "            print(f\"  Layers extracted: {run_data['metadata']['num_layers_extracted']}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Analyze reproducibility\n",
    "    print(\"=\"*70)\n",
    "    print(\"REPRODUCIBILITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    reproducibility = {\n",
    "        \"hidden_states\": {},\n",
    "        \"key_vectors\": {}\n",
    "    }\n",
    "    \n",
    "    # Check hidden states\n",
    "    print(\"\\nHidden States:\")\n",
    "    for layer_name in all_runs[0][\"hidden_states\"].keys():\n",
    "        first_rep = all_runs[0][\"hidden_states\"][layer_name]\n",
    "        \n",
    "        # Check if all repetitions are identical\n",
    "        all_identical = all(\n",
    "            torch.equal(first_rep, all_runs[i][\"hidden_states\"][layer_name])\n",
    "            for i in range(1, num_repetitions)\n",
    "        )\n",
    "        \n",
    "        if all_identical:\n",
    "            print(f\"  {layer_name}: [EXACT] Bit-exact across all repetitions\")\n",
    "            max_dev = 0.0\n",
    "        else:\n",
    "            # Compute max deviation\n",
    "            deviations = [\n",
    "                torch.norm(first_rep - all_runs[i][\"hidden_states\"][layer_name]).item()\n",
    "                for i in range(1, num_repetitions)\n",
    "            ]\n",
    "            max_dev = max(deviations)\n",
    "            print(f\"  {layer_name}: [VARIES] max L2 deviation: {max_dev:.6f}\")\n",
    "        \n",
    "        reproducibility[\"hidden_states\"][layer_name] = {\n",
    "            \"bit_exact\": all_identical,\n",
    "            \"max_deviation\": max_dev\n",
    "        }\n",
    "    \n",
    "    # Check key vectors\n",
    "    print(\"\\nKey Vectors:\")\n",
    "    for layer_name in all_runs[0][\"key_vectors\"].keys():\n",
    "        first_rep = all_runs[0][\"key_vectors\"][layer_name]\n",
    "        \n",
    "        all_identical = all(\n",
    "            torch.equal(first_rep, all_runs[i][\"key_vectors\"][layer_name])\n",
    "            for i in range(1, num_repetitions)\n",
    "        )\n",
    "        \n",
    "        if all_identical:\n",
    "            print(f\"  {layer_name}: [EXACT] Bit-exact across all repetitions\")\n",
    "            max_dev = 0.0\n",
    "        else:\n",
    "            deviations = [\n",
    "                torch.norm(first_rep - all_runs[i][\"key_vectors\"][layer_name]).item()\n",
    "                for i in range(1, num_repetitions)\n",
    "            ]\n",
    "            max_dev = max(deviations)\n",
    "            print(f\"  {layer_name}: [VARIES] max L2 deviation: {max_dev:.6f}\")\n",
    "        \n",
    "        reproducibility[\"key_vectors\"][layer_name] = {\n",
    "            \"bit_exact\": all_identical,\n",
    "            \"max_deviation\": max_dev\n",
    "        }\n",
    "    \n",
    "    # Overall verdict\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VERDICT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_hidden_exact = all(v[\"bit_exact\"] for v in reproducibility[\"hidden_states\"].values())\n",
    "    all_keys_exact = all(v[\"bit_exact\"] for v in reproducibility[\"key_vectors\"].values())\n",
    "    \n",
    "    if all_hidden_exact and all_keys_exact:\n",
    "        print(\"[SUCCESS] FULLY REPRODUCIBLE: All activations bit-exact across repetitions\")\n",
    "        print(\"  This configuration provides a clean baseline for cross-provider comparison\")\n",
    "    else:\n",
    "        print(\"[WARNING] NON-DETERMINISM DETECTED\")\n",
    "        if not all_hidden_exact:\n",
    "            print(\"  Hidden states show variation\")\n",
    "        if not all_keys_exact:\n",
    "            print(\"  Key vectors show variation\")\n",
    "        print(\"  May indicate:\")\n",
    "        print(\"    - Non-deterministic CUDA kernels\")\n",
    "        print(\"    - Asynchronous operations\")\n",
    "        print(\"    - Thermal/power variability (unlikely)\")\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Convert tensors to lists for JSON serialization\n",
    "    serializable_runs = []\n",
    "    for run in all_runs:\n",
    "        serializable_run = {\n",
    "            \"hidden_states\": {\n",
    "                k: v.float().numpy().tolist()\n",
    "                for k, v in run[\"hidden_states\"].items()\n",
    "            },\n",
    "            \"key_vectors\": {\n",
    "                k: v.float().numpy().tolist()\n",
    "                for k, v in run[\"key_vectors\"].items()\n",
    "            },\n",
    "            \"metadata\": run[\"metadata\"]\n",
    "        }\n",
    "        serializable_runs.append(serializable_run)\n",
    "    \n",
    "    output = {\n",
    "        \"experiment\": \"cross_provider_a100_baseline\",\n",
    "        \"attestation\": attestation,\n",
    "        \"config\": {\n",
    "            \"model\": model_name,\n",
    "            \"precision\": \"bfloat16\",\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"layer_indices\": layer_indices,\n",
    "            \"repetitions\": num_repetitions,\n",
    "            \"operation\": \"prefill_only\",\n",
    "            \"extraction\": \"last_valid_token_only\"\n",
    "        },\n",
    "        \"reproducibility\": reproducibility,\n",
    "        \"runs\": serializable_runs\n",
    "    }\n",
    "    \n",
    "    provider_clean = provider_name.replace(' ', '_').replace('.', '_')\n",
    "    gpu_name = torch.cuda.get_device_name(0).replace(' ', '_')\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    output_file = f\"{provider_clean}_{gpu_name}_baseline_{timestamp}.json\"\n",
    "    output_path = f\"/workspace/{output_file}\"\n",
    "    \n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    file_size_kb = len(json.dumps(output)) / 1024\n",
    "    print(f\"[SUCCESS] Results saved to: {output_path}\")\n",
    "    print(f\"[INFO] File size: {file_size_kb:.1f} KB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"1. Run this same script on different provider\")\n",
    "    print(f\"2. Compare attestation sections (especially firmware)\")\n",
    "    print(f\"3. Compare hidden_states and key_vectors for bit-exactness\")\n",
    "    print(f\"4. If differences found, binary search on configuration variables\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Detect if running in notebook or command-line\n",
    "    try:\n",
    "        get_ipython()  # This will exist in Jupyter/IPython\n",
    "        in_notebook = True\n",
    "    except NameError:\n",
    "        in_notebook = False\n",
    "    \n",
    "    if in_notebook:\n",
    "        # Running in notebook - prompt for provider name\n",
    "        print(\"Running in Jupyter notebook\")\n",
    "        print(\"Enter provider name (e.g., 'runpod', 'vast', or press Enter for 'unknown'):\")\n",
    "        provider = input().strip() or 'unknown'\n",
    "        main(provider)\n",
    "    else:\n",
    "        # Running from command line - use argparse\n",
    "        parser = argparse.ArgumentParser(description='Cross-provider A100 baseline experiment')\n",
    "        parser.add_argument('--provider', type=str, default='unknown',\n",
    "                           help='Provider name (e.g., runpod, vast)')\n",
    "        args = parser.parse_args()\n",
    "        main(args.provider)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
