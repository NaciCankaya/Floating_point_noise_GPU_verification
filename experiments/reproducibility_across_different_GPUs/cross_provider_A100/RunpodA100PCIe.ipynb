{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ec8bfd6-22a7-461f-a191-d8a0a2e7195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.5/803.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2025.11.3 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.3)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.11.0\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (25.0)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, ninja\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ninja]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ninja-1.13.0 wheel-0.45.1\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu128)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: flash-attn\n",
      "\u001b[33m  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
      "  Stored in directory: /workspace/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [flash-attn]2\u001b[0m [flash-attn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed einops-0.8.1 flash-attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install hf_transfer\n",
    "!pip install accelerate\n",
    "!pip install ninja packaging wheel\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02fcd448-4f2f-465e-b9d2-035939351805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter notebook\n",
      "Enter provider name (e.g., 'runpod', 'vast', or press Enter for 'unknown'):\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " runpod\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYSTEM ATTESTATION\n",
      "======================================================================\n",
      "Provider: runpod\n",
      "Hostname: 1e14ec78f3ec\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA: 12.8\n",
      "Driver: 550.127.05\n",
      "Compute Mode: Default\n",
      "Persistence Mode: unknown\n",
      "\n",
      "Firmware Info:\n",
      "  VBIOS Version: 92.00.A0.00.05\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL\n",
      "======================================================================\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Precision: BF16 (bfloat16)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01baeae3ff40444ead9d546295d53917",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128b69e995bd4b5ea20f0c15109984ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f921730a86154c76a2c0777dede6882d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcb92b634944c9382b2cfbe692ebe13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt tokens: 129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "909c40d8ed6a499a9b8706cb939f0cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9a59c27475f4cb597593d6379a09dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7444f58bceea42aa9f492e75c086d69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd31993b598a4035b968be8c8282c97d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf66f7c33eb4896b88010f08e75b32d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36393a84480444dc8eedb599a0805dfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280bca1411fe4930abc10d0bf620958b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c15262afe2ab48699b1098f2a874a982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU memory after load: 14.19 GB\n",
      "\n",
      "======================================================================\n",
      "RUNNING BASELINE EXPERIMENT\n",
      "======================================================================\n",
      "Layers to extract: [0, 1, 2, 3, 4, 7, 10, 14, 18, 22, 26, 28]\n",
      "Repetitions: 5\n",
      "Operation: Prefill only (no generation)\n",
      "Extraction: Last valid token position only\n",
      "\n",
      "Repetition 1/5...\n",
      "  Hidden state dim: 3584\n",
      "  Sequence length: 129\n",
      "  Last valid pos: 128\n",
      "  Layers extracted: 12\n",
      "Repetition 2/5...\n",
      "Repetition 3/5...\n",
      "Repetition 4/5...\n",
      "Repetition 5/5...\n",
      "\n",
      "======================================================================\n",
      "REPRODUCIBILITY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Hidden States:\n",
      "  layer_0: [EXACT] Bit-exact across all repetitions\n",
      "  layer_1: [EXACT] Bit-exact across all repetitions\n",
      "  layer_2: [EXACT] Bit-exact across all repetitions\n",
      "  layer_3: [EXACT] Bit-exact across all repetitions\n",
      "  layer_4: [EXACT] Bit-exact across all repetitions\n",
      "  layer_7: [EXACT] Bit-exact across all repetitions\n",
      "  layer_10: [EXACT] Bit-exact across all repetitions\n",
      "  layer_14: [EXACT] Bit-exact across all repetitions\n",
      "  layer_18: [EXACT] Bit-exact across all repetitions\n",
      "  layer_22: [EXACT] Bit-exact across all repetitions\n",
      "  layer_26: [EXACT] Bit-exact across all repetitions\n",
      "  layer_28: [EXACT] Bit-exact across all repetitions\n",
      "\n",
      "Key Vectors:\n",
      "  layer_1: [EXACT] Bit-exact across all repetitions\n",
      "  layer_2: [EXACT] Bit-exact across all repetitions\n",
      "  layer_3: [EXACT] Bit-exact across all repetitions\n",
      "  layer_4: [EXACT] Bit-exact across all repetitions\n",
      "  layer_7: [EXACT] Bit-exact across all repetitions\n",
      "  layer_10: [EXACT] Bit-exact across all repetitions\n",
      "  layer_14: [EXACT] Bit-exact across all repetitions\n",
      "  layer_18: [EXACT] Bit-exact across all repetitions\n",
      "  layer_22: [EXACT] Bit-exact across all repetitions\n",
      "  layer_26: [EXACT] Bit-exact across all repetitions\n",
      "  layer_28: [EXACT] Bit-exact across all repetitions\n",
      "\n",
      "======================================================================\n",
      "VERDICT\n",
      "======================================================================\n",
      "[SUCCESS] FULLY REPRODUCIBLE: All activations bit-exact across repetitions\n",
      "  This configuration provides a clean baseline for cross-provider comparison\n",
      "\n",
      "======================================================================\n",
      "SAVING RESULTS\n",
      "======================================================================\n",
      "[SUCCESS] Results saved to: /workspace/runpod_NVIDIA_A100_80GB_PCIe_baseline_20251106_113440.json\n",
      "[INFO] File size: 3126.6 KB\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Next steps:\n",
      "1. Run this same script on different provider\n",
      "2. Compare attestation sections (especially firmware)\n",
      "3. Compare hidden_states and key_vectors for bit-exactness\n",
      "4. If differences found, binary search on configuration variables\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Provider A100 Baseline Experiment\n",
    "\n",
    "Tests FP reproducibility across different cloud providers (RunPod vs Vast.ai)\n",
    "Measures both hidden states and key vectors across multiple layers.\n",
    "Only extracts from last valid token position (prefill workload).\n",
    "\n",
    "Usage:\n",
    "    Command-line:\n",
    "        python cross_provider_a100_baseline.py --provider runpod\n",
    "    \n",
    "    Jupyter notebook:\n",
    "        # Run all cells, then:\n",
    "        main('runpod')  # or main('vast'), etc.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import subprocess\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM ATTESTATION\n",
    "# ============================================================================\n",
    "\n",
    "def get_gpu_firmware_info():\n",
    "    \"\"\"Extract detailed GPU firmware/VBIOS information\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        output = result.stdout\n",
    "        \n",
    "        firmware_info = {}\n",
    "        for line in output.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if 'VBIOS Version' in line or 'GPU Board Serial Number' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    key = parts[0].strip()\n",
    "                    value = parts[1].strip()\n",
    "                    firmware_info[key] = value\n",
    "        \n",
    "        return firmware_info\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def get_gpu_detailed_info():\n",
    "    \"\"\"Get comprehensive GPU configuration\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=name,memory.total,driver_version,compute_cap,pci.bus_id,pcie.link.gen.max,pcie.link.width.max,power.limit',\n",
    "             '--format=csv,noheader,nounits'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            parts = result.stdout.strip().split(', ')\n",
    "            return {\n",
    "                \"name\": parts[0],\n",
    "                \"memory_total_mb\": parts[1],\n",
    "                \"driver_version\": parts[2],\n",
    "                \"compute_capability\": parts[3],\n",
    "                \"pci_bus_id\": parts[4],\n",
    "                \"pcie_gen_max\": parts[5],\n",
    "                \"pcie_width_max\": parts[6],\n",
    "                \"power_limit_w\": parts[7]\n",
    "            }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def get_compute_mode():\n",
    "    \"\"\"Check if GPU is in exclusive compute mode\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q', '-d', 'COMPUTE'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'Compute Mode' in line:\n",
    "                return line.split(':', 1)[1].strip()\n",
    "        \n",
    "        return \"unknown\"\n",
    "    except Exception as e:\n",
    "        return f\"error: {e}\"\n",
    "\n",
    "def get_persistence_mode():\n",
    "    \"\"\"Check persistence mode status\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q', '-d', 'PERSISTENCE_MODE'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'Persistence Mode' in line:\n",
    "                return line.split(':', 1)[1].strip()\n",
    "        \n",
    "        return \"unknown\"\n",
    "    except Exception as e:\n",
    "        return f\"error: {e}\"\n",
    "\n",
    "def get_ecc_status():\n",
    "    \"\"\"Check ECC memory status\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q', '-d', 'ECC'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        ecc_info = {}\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if 'ECC Mode' in line or 'Current' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    ecc_info[parts[0].strip()] = parts[1].strip()\n",
    "        \n",
    "        return ecc_info\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def get_clock_info():\n",
    "    \"\"\"Get GPU clock speeds\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-q', '-d', 'CLOCK'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        clocks = {}\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            line = line.strip()\n",
    "            if 'Graphics' in line or 'SM' in line or 'Memory' in line:\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) == 2:\n",
    "                    clocks[parts[0].strip()] = parts[1].strip()\n",
    "        \n",
    "        return clocks\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def check_mig_mode():\n",
    "    \"\"\"Check if GPU is in MIG mode or partitioned\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '-L'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        output = result.stdout\n",
    "        is_mig = 'MIG' in output\n",
    "        \n",
    "        return {\n",
    "            \"is_mig\": is_mig,\n",
    "            \"devices\": output.strip()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def attest_system(provider_name=\"unknown\"):\n",
    "    \"\"\"Comprehensive system attestation\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"SYSTEM ATTESTATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    attestation = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"provider\": provider_name,\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"container_id\": os.environ.get('HOSTNAME', 'unknown'),\n",
    "    }\n",
    "    \n",
    "    # PyTorch and CUDA\n",
    "    attestation[\"pytorch\"] = {\n",
    "        \"version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda,\n",
    "        \"cudnn_version\": torch.backends.cudnn.version() if torch.backends.cudnn.is_available() else None,\n",
    "        \"cuda_available\": torch.cuda.is_available(),\n",
    "    }\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        attestation[\"gpu\"] = {\n",
    "            \"name\": torch.cuda.get_device_name(0),\n",
    "            \"capability\": f\"{torch.cuda.get_device_capability(0)[0]}.{torch.cuda.get_device_capability(0)[1]}\",\n",
    "            \"memory_allocated_gb\": torch.cuda.memory_allocated(0) / 1024**3,\n",
    "            \"memory_reserved_gb\": torch.cuda.memory_reserved(0) / 1024**3,\n",
    "        }\n",
    "        \n",
    "        # Detailed info from nvidia-smi\n",
    "        attestation[\"gpu_detailed\"] = get_gpu_detailed_info()\n",
    "        attestation[\"gpu_firmware\"] = get_gpu_firmware_info()\n",
    "        attestation[\"compute_mode\"] = get_compute_mode()\n",
    "        attestation[\"persistence_mode\"] = get_persistence_mode()\n",
    "        attestation[\"ecc_status\"] = get_ecc_status()\n",
    "        attestation[\"clock_info\"] = get_clock_info()\n",
    "        attestation[\"mig_status\"] = check_mig_mode()\n",
    "    \n",
    "    # Environment variables\n",
    "    env_vars = {}\n",
    "    for key in sorted(os.environ.keys()):\n",
    "        if any(x in key.upper() for x in ['CUDA', 'TORCH', 'NCCL', 'CUDNN', 'PYTORCH', 'NVIDIA']):\n",
    "            env_vars[key] = os.environ[key]\n",
    "    attestation[\"environment\"] = env_vars\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Provider: {provider_name}\")\n",
    "    print(f\"Hostname: {attestation['hostname']}\")\n",
    "    print(f\"GPU: {attestation['gpu']['name']}\")\n",
    "    print(f\"PyTorch: {attestation['pytorch']['version']}\")\n",
    "    print(f\"CUDA: {attestation['pytorch']['cuda_version']}\")\n",
    "    print(f\"Driver: {attestation['gpu_detailed'].get('driver_version', 'unknown')}\")\n",
    "    print(f\"Compute Mode: {attestation['compute_mode']}\")\n",
    "    print(f\"Persistence Mode: {attestation['persistence_mode']}\")\n",
    "    \n",
    "    if attestation['gpu_firmware']:\n",
    "        print(f\"\\nFirmware Info:\")\n",
    "        for key, value in attestation['gpu_firmware'].items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "    \n",
    "    if attestation['mig_status'].get('is_mig'):\n",
    "        print(f\"\\n[WARNING] MIG mode detected!\")\n",
    "        print(f\"  {attestation['mig_status']['devices']}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return attestation\n",
    "\n",
    "# ============================================================================\n",
    "# DATA COLLECTION\n",
    "# ============================================================================\n",
    "\n",
    "def collect_multilayer_activations(model, tokenizer, prompt, layer_indices, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Extract hidden states and key vectors from multiple layers.\n",
    "    Only extract from last valid token position to save memory.\n",
    "    \n",
    "    Returns:\n",
    "        dict: {\n",
    "            \"hidden_states\": {f\"layer_{idx}\": tensor},\n",
    "            \"key_vectors\": {f\"layer_{idx}\": tensor},\n",
    "            \"metadata\": {...}\n",
    "        }\n",
    "    \"\"\"\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get both hidden states and key-value cache\n",
    "        outputs = model(**inputs, output_hidden_states=True, use_cache=True, return_dict=True)\n",
    "    \n",
    "    last_valid_pos = inputs['attention_mask'][0].sum() - 1\n",
    "    \n",
    "    # Extract hidden states from selected layers\n",
    "    hidden_states = {}\n",
    "    for idx in layer_indices:\n",
    "        if idx < len(outputs.hidden_states):\n",
    "            # Extract last valid token position, move to CPU immediately\n",
    "            hidden = outputs.hidden_states[idx][0, last_valid_pos, :].cpu().clone()\n",
    "            hidden_states[f\"layer_{idx}\"] = hidden\n",
    "    \n",
    "    # Extract key vectors from selected layers\n",
    "    # past_key_values is tuple of (key, value) pairs per layer\n",
    "    # Each key: [batch_size, num_key_value_heads, seq_len, head_dim]\n",
    "    key_vectors = {}\n",
    "    for i, layer_idx in enumerate(layer_indices):\n",
    "        # Skip embedding layer (idx=0)\n",
    "        if layer_idx > 0 and i < len(outputs.past_key_values):\n",
    "            layer_keys = outputs.past_key_values[i][0]  # [0] for keys\n",
    "            # Extract last valid token position, all heads\n",
    "            key_vec = layer_keys[0, :, last_valid_pos, :]\n",
    "            # Flatten: [num_key_value_heads * head_dim]\n",
    "            key_vec_flat = key_vec.reshape(-1).cpu().clone()\n",
    "            key_vectors[f\"layer_{layer_idx}\"] = key_vec_flat\n",
    "    \n",
    "    metadata = {\n",
    "        \"seq_len\": seq_len,\n",
    "        \"last_valid_pos\": int(last_valid_pos),\n",
    "        \"num_layers_extracted\": len(layer_indices),\n",
    "        \"hidden_dim\": hidden_states[f\"layer_{layer_indices[0]}\"].shape[0] if hidden_states else 0,\n",
    "    }\n",
    "    \n",
    "    # Aggressive cleanup\n",
    "    del outputs.hidden_states\n",
    "    del outputs.past_key_values\n",
    "    del outputs\n",
    "    del inputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return {\n",
    "        \"hidden_states\": hidden_states,\n",
    "        \"key_vectors\": key_vectors,\n",
    "        \"metadata\": metadata\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "def main(provider_name='unknown'):\n",
    "    \"\"\"\n",
    "    Main experiment function.\n",
    "    \n",
    "    Args:\n",
    "        provider_name: Name of the cloud provider (e.g., 'runpod', 'vast')\n",
    "    \"\"\"\n",
    "    \n",
    "    # Attest system first\n",
    "    attestation = attest_system(provider_name)\n",
    "    \n",
    "    # Check for concerning configurations\n",
    "    if attestation.get('mig_status', {}).get('is_mig'):\n",
    "        print(\"âš  WARNING: MIG mode detected - results may not be comparable!\")\n",
    "        response = input(\"Continue anyway? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            sys.exit(1)\n",
    "    \n",
    "    if attestation.get('compute_mode') not in ['Default', 'Exclusive Process']:\n",
    "        print(f\"âš  WARNING: Compute mode is '{attestation.get('compute_mode')}'\")\n",
    "    \n",
    "    # Configuration\n",
    "    CACHE_DIR = '/workspace/huggingface_cache'\n",
    "    model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    num_repetitions = 5\n",
    "    \n",
    "    # Select layers to sample (dense at beginning, sparse at end)\n",
    "    # Layer 0 is embedding, so we start from 1\n",
    "    total_layers = 28  # Qwen2.5-7B has 28 transformer layers\n",
    "    layer_indices = [0, 1, 2, 3, 4, 7, 10, 14, 18, 22, 26, 28]  # 0 for embedding\n",
    "    \n",
    "    # Test prompt - technical content\n",
    "    prompt = \"\"\"The study investigates the quantum decoherence effects on a multi-qubit superconducting system when subjected to controlled microwave pulses. We utilized a novel cryogenic amplification chain to minimize thermal noise and achieve a signal-to-noise ratio previously unattainable in similar setups. The experimental protocol involved preparing the qubits in a Greenberger-Horne-Zeilinger (GHZ) state and then measuring the decay of quantum entanglement over time by performing state tomography. Our results demonstrate a non-linear relationship between pulse amplitude and coherence time, suggesting that higher-order coupling terms, often neglected in theoretical models, play a significant role in system dynamics.\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"LOADING MODEL\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Precision: BF16 (bfloat16)\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "    prompt_tokens = len(tokenizer.encode(prompt))\n",
    "    print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    mem_after_load = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"GPU memory after load: {mem_after_load:.2f} GB\")\n",
    "    print()\n",
    "    \n",
    "    # Run experiment\n",
    "    print(\"=\"*70)\n",
    "    print(\"RUNNING BASELINE EXPERIMENT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Layers to extract: {layer_indices}\")\n",
    "    print(f\"Repetitions: {num_repetitions}\")\n",
    "    print(f\"Operation: Prefill only (no generation)\")\n",
    "    print(f\"Extraction: Last valid token position only\")\n",
    "    print()\n",
    "    \n",
    "    all_runs = []\n",
    "    \n",
    "    for rep in range(num_repetitions):\n",
    "        print(f\"Repetition {rep+1}/{num_repetitions}...\")\n",
    "        \n",
    "        run_data = collect_multilayer_activations(\n",
    "            model, tokenizer, prompt, layer_indices, device=\"cuda\"\n",
    "        )\n",
    "        \n",
    "        all_runs.append(run_data)\n",
    "        \n",
    "        if rep == 0:\n",
    "            print(f\"  Hidden state dim: {run_data['metadata']['hidden_dim']}\")\n",
    "            print(f\"  Sequence length: {run_data['metadata']['seq_len']}\")\n",
    "            print(f\"  Last valid pos: {run_data['metadata']['last_valid_pos']}\")\n",
    "            print(f\"  Layers extracted: {run_data['metadata']['num_layers_extracted']}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Analyze reproducibility\n",
    "    print(\"=\"*70)\n",
    "    print(\"REPRODUCIBILITY ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    reproducibility = {\n",
    "        \"hidden_states\": {},\n",
    "        \"key_vectors\": {}\n",
    "    }\n",
    "    \n",
    "    # Check hidden states\n",
    "    print(\"\\nHidden States:\")\n",
    "    for layer_name in all_runs[0][\"hidden_states\"].keys():\n",
    "        first_rep = all_runs[0][\"hidden_states\"][layer_name]\n",
    "        \n",
    "        # Check if all repetitions are identical\n",
    "        all_identical = all(\n",
    "            torch.equal(first_rep, all_runs[i][\"hidden_states\"][layer_name])\n",
    "            for i in range(1, num_repetitions)\n",
    "        )\n",
    "        \n",
    "        if all_identical:\n",
    "            print(f\"  {layer_name}: [EXACT] Bit-exact across all repetitions\")\n",
    "            max_dev = 0.0\n",
    "        else:\n",
    "            # Compute max deviation\n",
    "            deviations = [\n",
    "                torch.norm(first_rep - all_runs[i][\"hidden_states\"][layer_name]).item()\n",
    "                for i in range(1, num_repetitions)\n",
    "            ]\n",
    "            max_dev = max(deviations)\n",
    "            print(f\"  {layer_name}: [VARIES] max L2 deviation: {max_dev:.6f}\")\n",
    "        \n",
    "        reproducibility[\"hidden_states\"][layer_name] = {\n",
    "            \"bit_exact\": all_identical,\n",
    "            \"max_deviation\": max_dev\n",
    "        }\n",
    "    \n",
    "    # Check key vectors\n",
    "    print(\"\\nKey Vectors:\")\n",
    "    for layer_name in all_runs[0][\"key_vectors\"].keys():\n",
    "        first_rep = all_runs[0][\"key_vectors\"][layer_name]\n",
    "        \n",
    "        all_identical = all(\n",
    "            torch.equal(first_rep, all_runs[i][\"key_vectors\"][layer_name])\n",
    "            for i in range(1, num_repetitions)\n",
    "        )\n",
    "        \n",
    "        if all_identical:\n",
    "            print(f\"  {layer_name}: [EXACT] Bit-exact across all repetitions\")\n",
    "            max_dev = 0.0\n",
    "        else:\n",
    "            deviations = [\n",
    "                torch.norm(first_rep - all_runs[i][\"key_vectors\"][layer_name]).item()\n",
    "                for i in range(1, num_repetitions)\n",
    "            ]\n",
    "            max_dev = max(deviations)\n",
    "            print(f\"  {layer_name}: [VARIES] max L2 deviation: {max_dev:.6f}\")\n",
    "        \n",
    "        reproducibility[\"key_vectors\"][layer_name] = {\n",
    "            \"bit_exact\": all_identical,\n",
    "            \"max_deviation\": max_dev\n",
    "        }\n",
    "    \n",
    "    # Overall verdict\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VERDICT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    all_hidden_exact = all(v[\"bit_exact\"] for v in reproducibility[\"hidden_states\"].values())\n",
    "    all_keys_exact = all(v[\"bit_exact\"] for v in reproducibility[\"key_vectors\"].values())\n",
    "    \n",
    "    if all_hidden_exact and all_keys_exact:\n",
    "        print(\"[SUCCESS] FULLY REPRODUCIBLE: All activations bit-exact across repetitions\")\n",
    "        print(\"  This configuration provides a clean baseline for cross-provider comparison\")\n",
    "    else:\n",
    "        print(\"[WARNING] NON-DETERMINISM DETECTED\")\n",
    "        if not all_hidden_exact:\n",
    "            print(\"  Hidden states show variation\")\n",
    "        if not all_keys_exact:\n",
    "            print(\"  Key vectors show variation\")\n",
    "        print(\"  May indicate:\")\n",
    "        print(\"    - Non-deterministic CUDA kernels\")\n",
    "        print(\"    - Asynchronous operations\")\n",
    "        print(\"    - Thermal/power variability (unlikely)\")\n",
    "    \n",
    "    # Save results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Convert tensors to lists for JSON serialization\n",
    "    serializable_runs = []\n",
    "    for run in all_runs:\n",
    "        serializable_run = {\n",
    "            \"hidden_states\": {\n",
    "                k: v.float().numpy().tolist()\n",
    "                for k, v in run[\"hidden_states\"].items()\n",
    "            },\n",
    "            \"key_vectors\": {\n",
    "                k: v.float().numpy().tolist()\n",
    "                for k, v in run[\"key_vectors\"].items()\n",
    "            },\n",
    "            \"metadata\": run[\"metadata\"]\n",
    "        }\n",
    "        serializable_runs.append(serializable_run)\n",
    "    \n",
    "    output = {\n",
    "        \"experiment\": \"cross_provider_a100_baseline\",\n",
    "        \"attestation\": attestation,\n",
    "        \"config\": {\n",
    "            \"model\": model_name,\n",
    "            \"precision\": \"bfloat16\",\n",
    "            \"prompt_tokens\": prompt_tokens,\n",
    "            \"layer_indices\": layer_indices,\n",
    "            \"repetitions\": num_repetitions,\n",
    "            \"operation\": \"prefill_only\",\n",
    "            \"extraction\": \"last_valid_token_only\"\n",
    "        },\n",
    "        \"reproducibility\": reproducibility,\n",
    "        \"runs\": serializable_runs\n",
    "    }\n",
    "    \n",
    "    provider_clean = provider_name.replace(' ', '_').replace('.', '_')\n",
    "    gpu_name = torch.cuda.get_device_name(0).replace(' ', '_')\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    output_file = f\"{provider_clean}_{gpu_name}_baseline_{timestamp}.json\"\n",
    "    output_path = f\"/workspace/{output_file}\"\n",
    "    \n",
    "    with open(output_path, \"w\") as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    file_size_kb = len(json.dumps(output)) / 1024\n",
    "    print(f\"[SUCCESS] Results saved to: {output_path}\")\n",
    "    print(f\"[INFO] File size: {file_size_kb:.1f} KB\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EXPERIMENT COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nNext steps:\")\n",
    "    print(f\"1. Run this same script on different provider\")\n",
    "    print(f\"2. Compare attestation sections (especially firmware)\")\n",
    "    print(f\"3. Compare hidden_states and key_vectors for bit-exactness\")\n",
    "    print(f\"4. If differences found, binary search on configuration variables\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Detect if running in notebook or command-line\n",
    "    try:\n",
    "        get_ipython()  # This will exist in Jupyter/IPython\n",
    "        in_notebook = True\n",
    "    except NameError:\n",
    "        in_notebook = False\n",
    "    \n",
    "    if in_notebook:\n",
    "        # Running in notebook - prompt for provider name\n",
    "        print(\"Running in Jupyter notebook\")\n",
    "        print(\"Enter provider name (e.g., 'runpod', 'vast', or press Enter for 'unknown'):\")\n",
    "        provider = input().strip() or 'unknown'\n",
    "        main(provider)\n",
    "    else:\n",
    "        # Running from command line - use argparse\n",
    "        parser = argparse.ArgumentParser(description='Cross-provider A100 baseline experiment')\n",
    "        parser.add_argument('--provider', type=str, default='unknown',\n",
    "                           help='Provider name (e.g., runpod, vast)')\n",
    "        args = parser.parse_args()\n",
    "        main(args.provider)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
