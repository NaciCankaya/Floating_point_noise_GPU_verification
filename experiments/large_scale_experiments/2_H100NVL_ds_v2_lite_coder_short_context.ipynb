{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02bffb65-9bf6-4a52-9199-c617de49acc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 'tokenizer_config.json' to '/tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/685b4b63ad1aedad8f98824ab21d2ff422d9e1b1.incomplete'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "vLLM LOGPROBS EXTRACTION TEST\n",
      "================================================================================\n",
      "\n",
      "No txt/pdf files found in current directory\n",
      "Using hardcoded content\n",
      "\n",
      "Using standard OpenAI-compatible message format\n",
      "Message content length: 401 characters\n",
      "Note: Chat template will be applied to convert messages to string prompt\n",
      "\n",
      "Loading tokenizer to validate prompt length...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32d3a9522de943d690b3745ce0803098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/685b4b63ad1aedad8f98824ab21d2ff422d9e1b1\n",
      "Downloading 'tokenizer.json' to '/tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/ae8c6f0b4e0cdb2102a045267678e8f3d3d54ceb.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "943d3d580632454dbd793e9cbc0658b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/ae8c6f0b4e0cdb2102a045267678e8f3d3d54ceb\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Downloading 'config.json' to '/tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/3b1ccb3ddc847f2475a8e1b86e18ab1505fd87da.incomplete'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat template applied successfully\n",
      "\n",
      "Prompt statistics:\n",
      "  Characters: 440\n",
      "  Tokens: 73\n",
      "  Max model length: 128,000\n",
      "  Generation tokens: 40\n",
      "  Total required: 113\n",
      "\n",
      "✓ Prompt length validation passed\n",
      "  Remaining capacity: 127,887 tokens\n",
      "\n",
      "Configuration:\n",
      "  Model: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\n",
      "  Tensor parallel: 2\n",
      "  Max model len: 128,000\n",
      "  Max tokens: 40\n",
      "  Temperature: 0.0\n",
      "  Seed: 42\n",
      "  Repetitions: 20\n",
      "\n",
      "Loading model...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b440053ed3904d6484fb2b51b6d9cdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/3b1ccb3ddc847f2475a8e1b86e18ab1505fd87da\n",
      "Downloading 'configuration_deepseek.py' to '/tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/82e0f5d9d33620a66e328fdeae0b8dc12e2cff7c.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d80f5de16334519933cb15b6c98c8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_deepseek.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/82e0f5d9d33620a66e328fdeae0b8dc12e2cff7c\n",
      "A new version of the following files was downloaded from https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct:\n",
      "- configuration_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Downloading 'generation_config.json' to '/tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/458e1d985ba3fbaaf62a4d1a9dd6ff795a451f7e.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504c8d84733445999266a71adf8ad2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/458e1d985ba3fbaaf62a4d1a9dd6ff795a451f7e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 1 is connected to 1 peer ranks. [Gloo] Rank Expected number of connected peer ranks is : 1\n",
      "0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank [Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 10\n",
      " is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank [Gloo] Rank 10 is connected to  is connected to 11 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 11\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Downloading 'model-00001-of-000004.safetensors' to '/tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/75d08ddaf92b68f751c95e1b4a51dbf5c011d5692f97cc0d71bd32587a3ea8d9.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5587b547986c471aa032e1d39eafe85f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-000004.safetensors:   0%|          | 0.00/8.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Download complete. Moving file to /tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/75d08ddaf92b68f751c95e1b4a51dbf5c011d5692f97cc0d71bd32587a3ea8d9\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Downloading 'model-00002-of-000004.safetensors' to '/tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/7bf22dfa271527f7a0b8dbd56592722cd8fdcfeb6aad32ebb1110d21882eb1d8.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364554c1a07e43f882fd666edcf1e200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-000004.safetensors:   0%|          | 0.00/8.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Download complete. Moving file to /tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/7bf22dfa271527f7a0b8dbd56592722cd8fdcfeb6aad32ebb1110d21882eb1d8\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Downloading 'model-00003-of-000004.safetensors' to '/tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/18f5a20f4d737b496e03ff8761834dfa9754ceedd56f54a336d0eab5e0e20968.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2864c8cfe5a045c4ad2ee472cbc7bdfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-000004.safetensors:   0%|          | 0.00/8.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Download complete. Moving file to /tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/18f5a20f4d737b496e03ff8761834dfa9754ceedd56f54a336d0eab5e0e20968\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Downloading 'model-00004-of-000004.safetensors' to '/tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/1365ca25494e6592b6cb11f62f4a63cbdcdd9853e01d67f274d0b282732cc5cd.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69098a72f6b24664abe6787e07545280",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-000004.safetensors:   0%|          | 0.00/5.64G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Download complete. Moving file to /tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/1365ca25494e6592b6cb11f62f4a63cbdcdd9853e01d67f274d0b282732cc5cd\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Downloading 'model.safetensors.index.json' to '/tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/5a821356160292c668d01f8e7fdf9abba4a7b72d.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e65c85f243f14d5f95939de00e4b6bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m Download complete. Moving file to /tmp/hf_cache/hub/models--deepseek-ai--DeepSeek-Coder-V2-Lite-Instruct/blobs/5a821356160292c668d01f8e7fdf9abba4a7b72d\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=1340)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m 2025-11-10 15:27:41,451 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2025-11-10 15:27:41,451 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=1334)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1342)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=1340)\u001b[0;0m 2025-11-10 15:27:41,777 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "2025-11-10 15:27:41,777 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Sampling parameters:\n",
      "  Temperature: 0.0\n",
      "  Max tokens: 40\n",
      "  Seed: 42\n",
      "  Top logprobs: 10\n",
      "\n",
      "Running warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defd7a5cc7794d42966f8c4053a9e4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a98abfaba2d4e3dad4cc748227f63eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup complete - generated 40 tokens\n",
      "\n",
      "================================================================================\n",
      "RUNNING EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Repetition 1/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3486f5831094a57a1ad5c5c4ac8c5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f521f27d7b0f430fb5b00a427433cfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 2/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e939dc14be4ba88fcafa6850926e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1c01ff64f34487b95bb4e9980e81fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 3/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a18a1a715464360b9931c92c7094a83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "985f92cda5cb42aeabb855c660647da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 4/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a3849ce2cb42838e070895a54321b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20db481598a74061af9df2f31b2b85a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 5/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb33ef105ec04df7957e174c4e8e7aa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1360f02acee4c55b8b8ed8bcefbc7c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 6/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd3490291ac749fea1d2ca7ae745af97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11f2e00976644d98be63d06a2c2d7ba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 7/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8fbd264f1e4da9889f9d1446ba6949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cdd3690dc040f4b852eeb66b5e57ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 8/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719a7330303849bbb4164e49be492612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913b8f447dc843e9a4654e21f625deab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 9/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b20b8394b243db8729532ecb3c0c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "009ec82bf1464686bacf186a2f5a5aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 10/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f34e9dca9f405386a5ee14b05d6559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136574fdc4e5402fa580733338da6c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 11/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5beed380bf74904a6406ac66464409b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c67eedef59496baae5f1881df7bc27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 12/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7b609fccfae40b38101ba46493558b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded770c98b134db99d493b07dc6a04b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 13/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5160407e720d449e8c2aaf37770f9009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f0510922884628a0f669d74d423ecc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 14/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "490badfc3d28499083e204faab68e76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8590720c53154dae8af8f87437253355",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 15/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc2f0ead79d483daf1cb405deb7ee2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc8b34989f24bb0a430c7c46a86b3b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 16/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c59b878360c4aba94cef03a323eaf08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec784955d4fe4eb7a167c762c5c15f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 17/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8708d5b074147e8ba880c39a636efb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99226a993359475d8678fcfbff1a5856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 18/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfd2b240ab64d1abad8121939dafd24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa169a2271e04f9aab8ccee0aefed091",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 19/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2147a8b190e04275bba5a89080fc3cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f777eb0b294b098f6bddb859d94901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "Repetition 20/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "784d73398cef44bbbaa0d73670780009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d1032cc3584db293134541a36b3f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 40 tokens\n",
      "\n",
      "All repetitions complete!\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Checking token sequences...\n",
      "Token sequences identical: True\n",
      "\n",
      "Checking selected token logprobs...\n",
      "Selected token logprobs bit-exact: True\n",
      "\n",
      "Checking full top-k distributions...\n",
      "Top-k distributions bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "\n",
      "✓ PERFECT REPRODUCIBILITY\n",
      "  - Token sequences: bit-exact\n",
      "  - Selected token logprobs: bit-exact\n",
      "  - Full top-k distributions: bit-exact\n",
      "  - DeepSeek-Coder-V2-Lite-Instruct with vLLM is deterministic for this config\n",
      "\n",
      "Results saved to: vllm_deepseek_coder_v2_lite_test_20251110_152840.json\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM Logprobs Extraction Test - DeepSeek-Coder-V2-Lite-Instruct\n",
    "Tests bit-exact reproducibility across multiple runs\n",
    "Automatically finds txt/pdf files in current directory\n",
    "Uses standard OpenAI-compatible message format\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SUPPRESS VERBOSE LOGGING\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "\n",
    "# Suppress vLLM verbose output\n",
    "os.environ['VLLM_LOGGING_LEVEL'] = 'WARNING'\n",
    "os.environ['VLLM_CONFIGURE_LOGGING'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "# Suppress vLLM and related libraries\n",
    "logging.getLogger('vllm').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.engine').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.worker').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.executor').setLevel(logging.ERROR)\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "logging.getLogger('torch').setLevel(logging.ERROR)\n",
    "\n",
    "# Keep download progress - only allow INFO for huggingface_hub downloads\n",
    "logging.getLogger('huggingface_hub').setLevel(logging.INFO)\n",
    "logging.getLogger('huggingface_hub.file_download').setLevel(logging.INFO)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\"\n",
    "TENSOR_PARALLEL_SIZE = 2\n",
    "MAX_MODEL_LEN = 128000\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 40\n",
    "NUM_REPETITIONS = 20\n",
    "TEMPERATURE = 0.0  # Greedy decoding\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Prompt source - finds first txt or pdf in current directory\n",
    "AUTO_FIND_FILE = True  # Set to False to use hardcoded content\n",
    "\n",
    "# User task\n",
    "USER_TASK = \"Please provide a detailed summary of the following text.\"\n",
    "\n",
    "# Hardcoded content (used if AUTO_FIND_FILE=False or no files found)\n",
    "HARDCODED_CONTENT = \"\"\"The development of large language models has fundamentally transformed natural language processing and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FILE LOADING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def find_prompt_file():\n",
    "    \"\"\"Find first txt or pdf file in current directory\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # Look for txt files first, then pdf\n",
    "    txt_files = glob.glob(os.path.join(cwd, \"*.txt\"))\n",
    "    pdf_files = glob.glob(os.path.join(cwd, \"*.pdf\"))\n",
    "    \n",
    "    if txt_files:\n",
    "        return txt_files[0]\n",
    "    elif pdf_files:\n",
    "        return pdf_files[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_text_from_file(filepath):\n",
    "    \"\"\"Load text from txt or pdf file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    if filepath.endswith('.txt'):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print(f\"Loaded {len(text)} characters from txt file\")\n",
    "        return text\n",
    "    \n",
    "    elif filepath.endswith('.pdf'):\n",
    "        try:\n",
    "            import PyPDF2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"PyPDF2 required for PDF loading. Install with: pip install PyPDF2\")\n",
    "        \n",
    "        text = []\n",
    "        with open(filepath, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Loading {num_pages} pages from PDF...\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text.append(page_text)\n",
    "                if page_num % 10 == 0:\n",
    "                    print(f\"  Processed {page_num}/{num_pages} pages\")\n",
    "        \n",
    "        full_text = '\\n'.join(text)\n",
    "        print(f\"Loaded {len(full_text)} characters from PDF ({num_pages} pages)\")\n",
    "        return full_text\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath}. Use .txt or .pdf\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"vLLM LOGPROBS EXTRACTION TEST\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load document content\n",
    "prompt_file = None\n",
    "if AUTO_FIND_FILE:\n",
    "    prompt_file = find_prompt_file()\n",
    "    if prompt_file:\n",
    "        print(f\"Found file: {os.path.basename(prompt_file)}\")\n",
    "        DOCUMENT_CONTENT = load_text_from_file(prompt_file)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No txt/pdf files found in current directory\")\n",
    "        print(\"Using hardcoded content\")\n",
    "        DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "        print()\n",
    "else:\n",
    "    DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "    print(\"Using hardcoded content\")\n",
    "    print()\n",
    "\n",
    "# Prepare messages in standard OpenAI format\n",
    "# vLLM's tokenizer will apply DeepSeek-Coder-V2-Lite-Instruct's chat template\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{USER_TASK}\\n\\n{DOCUMENT_CONTENT}\"}\n",
    "]\n",
    "\n",
    "print(\"Using standard OpenAI-compatible message format\")\n",
    "print(f\"Message content length: {len(messages[0]['content'])} characters\")\n",
    "print(\"Note: Chat template will be applied to convert messages to string prompt\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER PRECHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading tokenizer to validate prompt length...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir='/tmp/hf_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply chat template to convert messages to string prompt\n",
    "# This is the actual prompt that will be used for generation\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Chat template applied successfully\")\n",
    "print()\n",
    "\n",
    "# Tokenize prompt\n",
    "prompt_tokens = tokenizer.encode(prompt_text)\n",
    "prompt_length = len(prompt_tokens)\n",
    "\n",
    "print(f\"Prompt statistics:\")\n",
    "print(f\"  Characters: {len(prompt_text):,}\")\n",
    "print(f\"  Tokens: {prompt_length:,}\")\n",
    "print(f\"  Max model length: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Generation tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Total required: {prompt_length + MAX_TOKENS:,}\")\n",
    "print()\n",
    "\n",
    "# Validate length\n",
    "if prompt_length > MAX_MODEL_LEN:\n",
    "    print(f\"❌ ERROR: Prompt is too long!\")\n",
    "    print(f\"  Prompt has {prompt_length:,} tokens\")\n",
    "    print(f\"  Model max is {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Exceeds by {prompt_length - MAX_MODEL_LEN:,} tokens\")\n",
    "    print()\n",
    "    print(\"Solutions:\")\n",
    "    print(f\"  1. Increase MAX_MODEL_LEN to at least {prompt_length + MAX_TOKENS}\")\n",
    "    print(f\"  2. Truncate/reduce the prompt\")\n",
    "    exit(1)\n",
    "\n",
    "if prompt_length + MAX_TOKENS > MAX_MODEL_LEN:\n",
    "    print(f\"⚠️  WARNING: Prompt + generation may exceed context\")\n",
    "    print(f\"  Prompt: {prompt_length:,} tokens\")\n",
    "    print(f\"  Generation: {MAX_TOKENS} tokens\")\n",
    "    print(f\"  Total: {prompt_length + MAX_TOKENS:,} tokens\")\n",
    "    print(f\"  Model max: {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Consider increasing MAX_MODEL_LEN to {prompt_length + MAX_TOKENS + 100}\")\n",
    "    print()\n",
    "else:\n",
    "    print(f\"✓ Prompt length validation passed\")\n",
    "    print(f\"  Remaining capacity: {MAX_MODEL_LEN - prompt_length - MAX_TOKENS:,} tokens\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print()\n",
    "\n",
    "print(\"Loading model...\")\n",
    "print()\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    trust_remote_code=True,\n",
    "    seed=SEED,\n",
    "    enforce_eager=True,  # Disable cudagraph for determinism\n",
    "    enable_prefix_caching=False  # Disable prefix caching for clean experiment\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    "    logprobs=TOP_LOGPROBS,\n",
    "    skip_special_tokens=False\n",
    ")\n",
    "\n",
    "print(\"Sampling parameters:\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Top logprobs: {TOP_LOGPROBS}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# WARMUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Running warmup...\")\n",
    "# vLLM's LLM.generate() expects string prompts, not message dicts\n",
    "# The chat template was already applied above during validation\n",
    "warmup_output = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "print(f\"Warmup complete - generated {len(warmup_output[0].outputs[0].token_ids)} tokens\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "results_tokens = []\n",
    "results_logprobs = []\n",
    "results_texts = []\n",
    "results_distributions = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Repetition {rep + 1}/{NUM_REPETITIONS}...\")\n",
    "    \n",
    "    # Use the string prompt (chat template already applied)\n",
    "    outputs = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract token IDs\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    # Extract generated text\n",
    "    text = output.outputs[0].text\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    # Extract logprobs for selected tokens\n",
    "    logprobs_data = output.outputs[0].logprobs\n",
    "    selected_logprobs = [lp[token_ids[i]].logprob for i, lp in enumerate(logprobs_data)]\n",
    "    results_logprobs.append(np.array(selected_logprobs))\n",
    "    \n",
    "    # Extract full top-k distributions\n",
    "    rep_distributions = []\n",
    "    for position_logprobs in logprobs_data:\n",
    "        # Get top-k sorted by logprob (descending)\n",
    "        sorted_items = sorted(position_logprobs.items(), \n",
    "                            key=lambda x: x[1].logprob, \n",
    "                            reverse=True)[:TOP_LOGPROBS]\n",
    "        rep_distributions.append([(tok, lp.logprob) for tok, lp in sorted_items])\n",
    "    results_distributions.append(rep_distributions)\n",
    "    \n",
    "    print(f\"  Generated {len(token_ids)} tokens\")\n",
    "\n",
    "print()\n",
    "print(\"All repetitions complete!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check token sequence identity\n",
    "print(\"Checking token sequences...\")\n",
    "tokens_identical = all(\n",
    "    results_tokens[0] == results_tokens[i] \n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "if not tokens_identical:\n",
    "    print(\"\\n⚠️  Token sequences differ!\")\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[0] != results_tokens[i]:\n",
    "            diff_positions = [\n",
    "                j for j in range(min(len(results_tokens[0]), len(results_tokens[i])))\n",
    "                if results_tokens[0][j] != results_tokens[i][j]\n",
    "            ]\n",
    "            print(f\"  Rep 0 vs Rep {i}: {len(diff_positions)} positions differ\")\n",
    "            if diff_positions:\n",
    "                print(f\"    First difference at position {diff_positions[0]}\")\n",
    "\n",
    "# Check logprobs for selected tokens\n",
    "print(\"\\nChecking selected token logprobs...\")\n",
    "first_logprobs = results_logprobs[0]\n",
    "logprobs_exact = all(\n",
    "    np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Selected token logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "# Check top-k distributions\n",
    "print(\"\\nChecking full top-k distributions...\")\n",
    "distributions_exact = True\n",
    "distribution_mismatches = []\n",
    "\n",
    "first_dist = results_distributions[0]\n",
    "for rep_idx in range(1, NUM_REPETITIONS):\n",
    "    for pos_idx in range(len(first_dist)):\n",
    "        dist_a = first_dist[pos_idx]\n",
    "        dist_b = results_distributions[rep_idx][pos_idx]\n",
    "        \n",
    "        # Check if token IDs match in same order\n",
    "        tokens_match = [t[0] for t in dist_a] == [t[0] for t in dist_b]\n",
    "        \n",
    "        # Check if logprobs are bit-exact\n",
    "        if tokens_match:\n",
    "            logprobs_match = all(\n",
    "                abs(dist_a[i][1] - dist_b[i][1]) < 1e-10 \n",
    "                for i in range(len(dist_a))\n",
    "            )\n",
    "            if not logprobs_match:\n",
    "                distributions_exact = False\n",
    "                distribution_mismatches.append((rep_idx, pos_idx))\n",
    "        else:\n",
    "            distributions_exact = False\n",
    "            distribution_mismatches.append((rep_idx, pos_idx))\n",
    "\n",
    "print(f\"Top-k distributions bit-exact: {distributions_exact}\")\n",
    "\n",
    "if not distributions_exact:\n",
    "    print(f\"\\n⚠️  Found {len(distribution_mismatches)} position mismatches in distributions\")\n",
    "    if len(distribution_mismatches) <= 5:\n",
    "        for rep_idx, pos_idx in distribution_mismatches:\n",
    "            print(f\"  Rep 0 vs Rep {rep_idx}, position {pos_idx}\")\n",
    "    else:\n",
    "        print(f\"  First 5: {distribution_mismatches[:5]}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "    \n",
    "    # Element-wise statistics\n",
    "    all_logprobs = np.array(results_logprobs)\n",
    "    std_per_token = all_logprobs.std(axis=0)\n",
    "    print(f\"\\nPer-token std statistics:\")\n",
    "    print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "    print(f\"  Max: {std_per_token.max():.6e}\")\n",
    "    print(f\"  Median: {np.median(std_per_token):.6e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact and distributions_exact:\n",
    "    print(\"✓ PERFECT REPRODUCIBILITY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Full top-k distributions: bit-exact\")\n",
    "    print(\"  - DeepSeek-Coder-V2-Lite-Instruct with vLLM is deterministic for this config\")\n",
    "elif tokens_identical and logprobs_exact and not distributions_exact:\n",
    "    print(\"⚠️  SELECTED TOKENS EXACT, DISTRIBUTIONS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Top-k distributions: numerical variation detected\")\n",
    "    print(\"  → May indicate computational instability in non-selected paths\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"⚠️  TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: small numerical variation\")\n",
    "    max_l2 = max(l2_distances) if not logprobs_exact else 0.0\n",
    "    if max_l2 < 1e-6:\n",
    "        print(f\"  - Variation very small (L2={max_l2:.2e})\")\n",
    "        print(\"  → Likely acceptable for forensics\")\n",
    "    else:\n",
    "        print(f\"  - Variation notable (L2={max_l2:.2e})\")\n",
    "        print(\"  → Investigate noise source\")\n",
    "else:\n",
    "    print(\"❌ TOKEN SEQUENCES DIFFER\")\n",
    "    print(\"  - This should NOT happen with temperature=0\")\n",
    "    print(\"  → Something is wrong, investigate\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"deepseek_coder_v2_lite_instruct_logprobs_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"prompt_source\": \"file\" if prompt_file else \"hardcoded\",\n",
    "    \"prompt_file\": os.path.basename(prompt_file) if prompt_file else None,\n",
    "    \"prompt_text\": prompt_text,  # Save actual prompt for reproducibility\n",
    "    \"prompt_length_chars\": len(prompt_text),\n",
    "    \"prompt_length_tokens\": prompt_length,\n",
    "    \"message_format\": \"openai_compatible\",\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_model_len\": MAX_MODEL_LEN,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"warmup_enabled\": True,\n",
    "        \"prefix_caching_disabled\": True,\n",
    "        \"enforce_eager\": True,\n",
    "        \"top_logprobs\": TOP_LOGPROBS\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"distributions_exact\": distributions_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact and distributions_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts,\n",
    "    \"top_k_distributions\": [\n",
    "        [[(int(tok), float(prob)) for tok, prob in dist] for dist in rep_dists]\n",
    "        for rep_dists in results_distributions\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_file = f\"vllm_deepseek_coder_v2_lite_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85220abf-0251-42da-88d4-051f481ebe5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
