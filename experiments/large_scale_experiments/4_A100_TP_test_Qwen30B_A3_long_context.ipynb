{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37252a6-7e0c-4264-a8b5-84832b379d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "vLLM LOGPROBS EXTRACTION TEST\n",
      "================================================================================\n",
      "\n",
      "Found file: Verification-for-International-AI-Governance.pdf\n",
      "Loading 172 pages from PDF...\n",
      "  Processed 10/172 pages\n",
      "  Processed 20/172 pages\n",
      "  Processed 30/172 pages\n",
      "  Processed 40/172 pages\n",
      "  Processed 50/172 pages\n",
      "  Processed 60/172 pages\n",
      "  Processed 70/172 pages\n",
      "  Processed 80/172 pages\n",
      "  Processed 90/172 pages\n",
      "  Processed 100/172 pages\n",
      "  Processed 110/172 pages\n",
      "  Processed 120/172 pages\n",
      "  Processed 130/172 pages\n",
      "  Processed 140/172 pages\n",
      "  Processed 150/172 pages\n",
      "  Processed 160/172 pages\n",
      "  Processed 170/172 pages\n",
      "Loaded 535619 characters from PDF (172 pages)\n",
      "\n",
      "Applying manual chat template...\n",
      "Formatted prompt length: 535724 characters\n",
      "\n",
      "Loading tokenizer to validate prompt length...\n",
      "Prompt statistics:\n",
      "  Characters: 535,724\n",
      "  Tokens: 120,413\n",
      "  Max model length: 150,000\n",
      "  Generation tokens: 20\n",
      "  Total required: 120,433\n",
      "\n",
      "✓ Prompt length validation passed\n",
      "  Remaining capacity: 29,567 tokens\n",
      "\n",
      "Note on prompt formatting:\n",
      "  Using MANUAL chat template for reproducibility\n",
      "  Why not llm.chat():\n",
      "    - llm.chat() depends on tokenizer's chat template\n",
      "    - Templates can change between model/vLLM versions\n",
      "    - Creates confounding variable for forensics testing\n",
      "  Manual template gives:\n",
      "    - Full control over exact formatting\n",
      "    - Reproducible across environments\n",
      "    - No hidden dependencies\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen3-30B-A3B-Thinking-2507\n",
      "  Tensor parallel: 4\n",
      "  Max model len: 150,000\n",
      "  GPU memory utilization: 0.9 (90%)\n",
      "  Max tokens: 20\n",
      "  Repetitions: 5\n",
      "  Temperature: 0.0 (greedy)\n",
      "  Top logprobs: 10\n",
      "\n",
      "GPU Info:\n",
      "  Device: NVIDIA A100-SXM4-80GB\n",
      "  Available: 4 GPUs\n",
      "\n",
      "Loading model with vLLM...\n",
      "INFO 11-09 22:36:47 [utils.py:233] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'max_model_len': 150000, 'tensor_parallel_size': 4, 'enable_prefix_caching': False, 'disable_log_stats': True, 'model': 'Qwen/Qwen3-30B-A3B-Thinking-2507'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 22:36:57 [model.py:547] Resolved architecture: Qwen3MoeForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 22:36:57 [model.py:1510] Using max model len 150000\n",
      "INFO 11-09 22:37:01 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 11-09 22:37:02 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 22:37:07 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:37:08 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:37:08 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-30B-A3B-Thinking-2507', speculative_config=None, tokenizer='Qwen/Qwen3-30B-A3B-Thinking-2507', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=150000, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-30B-A3B-Thinking-2507, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m WARNING 11-09 22:37:08 [multiproc_executor.py:720] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:37:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_bfbb339f'), local_subscribe_addr='ipc:///tmp/9a2a4ec0-7239-4393-b983-53ce0a9de865', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 22:37:12 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-09 22:37:12 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-09 22:37:12 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-09 22:37:12 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-09 22:37:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_b6f86f1e'), local_subscribe_addr='ipc:///tmp/233fb4a6-6e93-4099-bb24-913f0610bad3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-09 22:37:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_02aa2d44'), local_subscribe_addr='ipc:///tmp/50fc94bc-55e6-4d71-8ee6-a6b0df6e6925', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-09 22:37:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_df81935f'), local_subscribe_addr='ipc:///tmp/ec70c28f-6ffb-4855-9f20-aadf028544cb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-09 22:37:17 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f14b39ad'), local_subscribe_addr='ipc:///tmp/fece701f-c60d-42f5-a047-3ac7668807bb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 11-09 22:37:18 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 22:37:18 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 22:37:18 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 22:37:18 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 22:37:18 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 22:37:18 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 22:37:18 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 22:37:18 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "WARNING 11-09 22:37:18 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-09 22:37:18 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-09 22:37:18 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-09 22:37:18 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "INFO 11-09 22:37:18 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-09 22:37:18 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-09 22:37:18 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-09 22:37:18 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-09 22:37:18 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_3cb0ed66'), local_subscribe_addr='ipc:///tmp/c3e04104-4d32-46a0-abe6-74fb96964cf5', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 11-09 22:37:19 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 22:37:19 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 22:37:19 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 22:37:19 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 22:37:19 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 22:37:19 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 22:37:19 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 22:37:19 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 22:37:19 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 11-09 22:37:19 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 11-09 22:37:19 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-09 22:37:19 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "WARNING 11-09 22:37:19 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-09 22:37:20 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-09 22:37:20 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-09 22:37:20 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:37:20 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-30B-A3B-Thinking-2507...\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:37:20 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-30B-A3B-Thinking-2507...\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:37:20 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-30B-A3B-Thinking-2507...\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:37:20 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen3-30B-A3B-Thinking-2507...\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:37:20 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:37:20 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:37:20 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:37:20 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:37:20 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:37:20 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:37:20 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:37:20 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:37:20 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:37:20 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:37:21 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:37:21 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/16 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 1/16 [00:05<01:23,  5.54s/it]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 2/16 [00:34<04:28, 19.14s/it]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 3/16 [01:02<05:01, 23.21s/it]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 4/16 [01:33<05:17, 26.50s/it]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 5/16 [02:06<05:17, 28.82s/it]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 6/16 [02:37<04:53, 29.39s/it]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 7/16 [03:05<04:20, 28.99s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 8/16 [03:33<03:49, 28.70s/it]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 9/16 [03:59<03:15, 27.91s/it]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 10/16 [04:28<02:48, 28.09s/it]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 11/16 [04:56<02:21, 28.26s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 12/16 [05:23<01:51, 27.86s/it]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 13/16 [05:51<01:23, 27.77s/it]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 14/16 [06:20<00:56, 28.09s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 15/16 [06:49<00:28, 28.53s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 16/16 [07:17<00:00, 28.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 16/16 [07:17<00:00, 27.35s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:44:40 [default_loader.py:267] Loading weights took 437.73 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:44:40 [default_loader.py:267] Loading weights took 438.84 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:44:40 [default_loader.py:267] Loading weights took 438.62 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:44:40 [default_loader.py:267] Loading weights took 438.34 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:44:40 [gpu_model_runner.py:2653] Model loading took 14.3001 GiB and 439.467982 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:44:40 [gpu_model_runner.py:2653] Model loading took 14.3001 GiB and 439.653293 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:44:40 [gpu_model_runner.py:2653] Model loading took 14.3001 GiB and 439.665687 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:44:41 [gpu_model_runner.py:2653] Model loading took 14.3001 GiB and 439.532100 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:44:51 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/b6ad9aa9ed/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:44:51 [backends.py:559] Dynamo bytecode transform time: 9.83 s\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:44:51 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/b6ad9aa9ed/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:44:51 [backends.py:559] Dynamo bytecode transform time: 10.13 s\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:44:51 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/b6ad9aa9ed/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:44:51 [backends.py:559] Dynamo bytecode transform time: 10.23 s\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:44:51 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/b6ad9aa9ed/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:44:51 [backends.py:559] Dynamo bytecode transform time: 10.37 s\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:44:58 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:44:58 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:44:58 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:44:59 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:45:41 [shm_broadcast.py:466] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation).\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:45:44 [backends.py:218] Compiling a graph for dynamic shape takes 52.52 s\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:45:44 [backends.py:218] Compiling a graph for dynamic shape takes 52.40 s\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:45:45 [backends.py:218] Compiling a graph for dynamic shape takes 53.04 s\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:45:46 [backends.py:218] Compiling a graph for dynamic shape takes 54.41 s\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:45:49 [fused_moe.py:788] Using configuration from /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:45:49 [fused_moe.py:788] Using configuration from /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:45:49 [fused_moe.py:788] Using configuration from /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:45:49 [fused_moe.py:788] Using configuration from /usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=192,device_name=NVIDIA_A100-SXM4-80GB.json for MoE layer.\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:45:51 [monitor.py:34] torch.compile takes 62.35 s in total\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:45:51 [monitor.py:34] torch.compile takes 64.78 s in total\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:45:51 [monitor.py:34] torch.compile takes 62.54 s in total\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:45:51 [monitor.py:34] torch.compile takes 63.27 s in total\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:45:53 [gpu_worker.py:298] Available KV cache memory: 55.10 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:45:53 [gpu_worker.py:298] Available KV cache memory: 55.10 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:45:53 [gpu_worker.py:298] Available KV cache memory: 55.10 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:45:53 [gpu_worker.py:298] Available KV cache memory: 55.10 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:45:54 [kv_cache_utils.py:1087] GPU KV cache size: 2,407,264 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:45:54 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 16.05x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:45:54 [kv_cache_utils.py:1087] GPU KV cache size: 2,407,264 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:45:54 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 16.05x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:45:54 [kv_cache_utils.py:1087] GPU KV cache size: 2,407,264 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:45:54 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 16.05x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:45:54 [kv_cache_utils.py:1087] GPU KV cache size: 2,407,264 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:45:54 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 16.05x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:17<00:00,  3.92it/s]\n",
      "Capturing CUDA graphs (decode, FULL):  71%|███████▏  | 25/35 [00:03<00:01,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:46:14 [custom_all_reduce.py:203] Registering 9894 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:46:15 [custom_all_reduce.py:203] Registering 9894 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL):  77%|███████▋  | 27/35 [00:03<00:01,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:46:15 [custom_all_reduce.py:203] Registering 9894 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:04<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:46:16 [custom_all_reduce.py:203] Registering 9894 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP3 pid=1231)\u001b[0;0m INFO 11-09 22:46:16 [gpu_model_runner.py:3480] Graph capturing finished in 23 secs, took 1.47 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=1229)\u001b[0;0m INFO 11-09 22:46:16 [gpu_model_runner.py:3480] Graph capturing finished in 23 secs, took 1.47 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=1230)\u001b[0;0m INFO 11-09 22:46:17 [gpu_model_runner.py:3480] Graph capturing finished in 23 secs, took 1.47 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=1228)\u001b[0;0m INFO 11-09 22:46:17 [gpu_model_runner.py:3480] Graph capturing finished in 23 secs, took 1.47 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1095)\u001b[0;0m INFO 11-09 22:46:17 [core.py:210] init engine (profile, create kv cache, warmup model) took 96.09 seconds\n",
      "INFO 11-09 22:46:19 [llm.py:306] Supported_tasks: ['generate']\n",
      "Model loaded successfully!\n",
      "\n",
      "Running warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ada3eb665d54e078c700699a6f057ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53b5c43b27f408e932bcfcdd939de56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup complete\n",
      "\n",
      "================================================================================\n",
      "Running 5 test repetitions\n",
      "================================================================================\n",
      "\n",
      "Repetition 1/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38b18314a6024678aff214388d5960b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3856d5379cc64028a609b2db6c4e0755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 2/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b176222f434243e4a7525cea07777d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d45207ca3b284aa3ab62d6340359cc8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 3/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f04cc4ba3074b978fc069ecf866346e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b5df5df37b48c08ad25c2bba150060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 4/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5734ff57364a4c15b83b47888fc3fc55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e72d87488704d359113c0287a208120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 5/5...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f558593d406f48d888fe0685049ba165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d1deacd361543a7abeb09e0ee5fb609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "\n",
      "All repetitions complete!\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Checking token sequences...\n",
      "Token sequences identical: True\n",
      "\n",
      "Checking selected token logprobs...\n",
      "Selected token logprobs bit-exact: True\n",
      "\n",
      "Checking full top-k distributions...\n",
      "Top-k distributions bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "\n",
      "✓ PERFECT REPRODUCIBILITY\n",
      "  - Token sequences: bit-exact\n",
      "  - Selected token logprobs: bit-exact\n",
      "  - Full top-k distributions: bit-exact\n",
      "  - vLLM is deterministic for this config\n",
      "\n",
      "Results saved to: vllm_logprobs_test_20251109_224725.json\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM Tensor Parallelism Test with Auto File Detection\n",
    "Tests bit-exact reproducibility across multiple runs with TP\n",
    "Automatically finds txt/pdf files in current directory\n",
    "Validates prompt length before loading model weights\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen3-30B-A3B-Thinking-2507\"  # Change to \"moonshotai/Kimi-K2-Thinking\" for K2\n",
    "TENSOR_PARALLEL_SIZE = 4\n",
    "MAX_MODEL_LEN = 150000\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 20\n",
    "NUM_REPETITIONS = 5\n",
    "TEMPERATURE = 0.0  # Greedy decoding\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Prompt source - finds first txt or pdf in current directory\n",
    "AUTO_FIND_FILE = True  # Set to False to use hardcoded content\n",
    "\n",
    "# Chat template configuration\n",
    "SYSTEM_PROMPT = \"You are a helpful AI assistant.\"\n",
    "USER_TASK = \"Please provide a detailed summary of the following text.\"\n",
    "\n",
    "# Hardcoded content (used if AUTO_FIND_FILE=False or no files found)\n",
    "HARDCODED_CONTENT = \"\"\"The development of large language models has fundamentally transformed natural language processing and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FILE LOADING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def find_prompt_file():\n",
    "    \"\"\"Find first txt or pdf file in current directory\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # Look for txt files first, then pdf\n",
    "    txt_files = glob.glob(os.path.join(cwd, \"*.txt\"))\n",
    "    pdf_files = glob.glob(os.path.join(cwd, \"*.pdf\"))\n",
    "    \n",
    "    if txt_files:\n",
    "        return txt_files[0]\n",
    "    elif pdf_files:\n",
    "        return pdf_files[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_text_from_file(filepath):\n",
    "    \"\"\"Load text from txt or pdf file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    if filepath.endswith('.txt'):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print(f\"Loaded {len(text)} characters from txt file\")\n",
    "        return text\n",
    "    \n",
    "    elif filepath.endswith('.pdf'):\n",
    "        try:\n",
    "            import PyPDF2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"PyPDF2 required for PDF loading. Install with: pip install PyPDF2\")\n",
    "        \n",
    "        text = []\n",
    "        with open(filepath, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Loading {num_pages} pages from PDF...\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text.append(page_text)\n",
    "                if page_num % 10 == 0:\n",
    "                    print(f\"  Processed {page_num}/{num_pages} pages\")\n",
    "        \n",
    "        full_text = '\\n'.join(text)\n",
    "        print(f\"Loaded {len(full_text)} characters from PDF ({num_pages} pages)\")\n",
    "        return full_text\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath}. Use .txt or .pdf\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"vLLM LOGPROBS EXTRACTION TEST\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load document content\n",
    "prompt_file = None\n",
    "if AUTO_FIND_FILE:\n",
    "    prompt_file = find_prompt_file()\n",
    "    if prompt_file:\n",
    "        print(f\"Found file: {os.path.basename(prompt_file)}\")\n",
    "        DOCUMENT_CONTENT = load_text_from_file(prompt_file)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No txt/pdf files found in current directory\")\n",
    "        print(\"Using hardcoded content\")\n",
    "        DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "        print()\n",
    "else:\n",
    "    DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "    print(\"Using hardcoded content\")\n",
    "    print()\n",
    "\n",
    "# Apply manual chat template\n",
    "# Format controlled manually for reproducibility across vLLM versions\n",
    "# Avoiding llm.chat() which depends on tokenizer's chat template\n",
    "print(\"Applying manual chat template...\")\n",
    "PROMPT = f\"\"\"System: {SYSTEM_PROMPT}\n",
    "\n",
    "User: {USER_TASK}\n",
    "\n",
    "{DOCUMENT_CONTENT}\"\"\"\n",
    "print(f\"Formatted prompt length: {len(PROMPT)} characters\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER PRECHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading tokenizer to validate prompt length...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir='/workspace/huggingface_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Tokenize prompt\n",
    "prompt_tokens = tokenizer.encode(PROMPT)\n",
    "prompt_length = len(prompt_tokens)\n",
    "\n",
    "print(f\"Prompt statistics:\")\n",
    "print(f\"  Characters: {len(PROMPT):,}\")\n",
    "print(f\"  Tokens: {prompt_length:,}\")\n",
    "print(f\"  Max model length: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Generation tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Total required: {prompt_length + MAX_TOKENS:,}\")\n",
    "print()\n",
    "\n",
    "# Validate length\n",
    "if prompt_length > MAX_MODEL_LEN:\n",
    "    print(f\"❌ ERROR: Prompt is too long!\")\n",
    "    print(f\"  Prompt has {prompt_length:,} tokens\")\n",
    "    print(f\"  Model max is {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Exceeds by {prompt_length - MAX_MODEL_LEN:,} tokens\")\n",
    "    print()\n",
    "    print(\"Solutions:\")\n",
    "    print(f\"  1. Increase MAX_MODEL_LEN to at least {prompt_length + MAX_TOKENS}\")\n",
    "    print(f\"  2. Truncate/reduce the prompt\")\n",
    "    exit(1)\n",
    "\n",
    "if prompt_length + MAX_TOKENS > MAX_MODEL_LEN:\n",
    "    print(f\"⚠ WARNING: Prompt + generation may exceed context\")\n",
    "    print(f\"  Prompt: {prompt_length:,} tokens\")\n",
    "    print(f\"  Generation: {MAX_TOKENS} tokens\")\n",
    "    print(f\"  Total: {prompt_length + MAX_TOKENS:,} tokens\")\n",
    "    print(f\"  Model max: {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Consider increasing MAX_MODEL_LEN to {prompt_length + MAX_TOKENS + 100}\")\n",
    "    print()\n",
    "else:\n",
    "    print(f\"✓ Prompt length validation passed\")\n",
    "    print(f\"  Remaining capacity: {MAX_MODEL_LEN - prompt_length - MAX_TOKENS:,} tokens\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# VLLM PROMPT TEMPLATE HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Note on prompt formatting:\")\n",
    "print(\"  Using MANUAL chat template for reproducibility\")\n",
    "print(\"  Why not llm.chat():\")\n",
    "print(\"    - llm.chat() depends on tokenizer's chat template\")\n",
    "print(\"    - Templates can change between model/vLLM versions\")\n",
    "print(\"    - Creates confounding variable for forensics testing\")\n",
    "print(\"  Manual template gives:\")\n",
    "print(\"    - Full control over exact formatting\")\n",
    "print(\"    - Reproducible across environments\")\n",
    "print(\"    - No hidden dependencies\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  GPU memory utilization: {GPU_MEMORY_UTILIZATION} ({int(GPU_MEMORY_UTILIZATION*100)}%)\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE} (greedy)\")\n",
    "print(f\"  Top logprobs: {TOP_LOGPROBS}\")\n",
    "print()\n",
    "\n",
    "print(\"GPU Info:\")\n",
    "print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  Available: {torch.cuda.device_count()} GPUs\")\n",
    "print()\n",
    "\n",
    "print(\"Loading model with vLLM...\")\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    download_dir='/workspace/huggingface_cache',\n",
    "    dtype='bfloat16',\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    enable_prefix_caching=False,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    disable_log_stats=True\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "print()\n",
    "\n",
    "# Sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    "    logprobs=TOP_LOGPROBS,\n",
    "    prompt_logprobs=None\n",
    ")\n",
    "\n",
    "# Warmup run\n",
    "print(\"Running warmup...\")\n",
    "warmup_output = llm.generate([PROMPT], sampling_params=sampling_params)\n",
    "print(\"Warmup complete\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TEST RUNS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Running {NUM_REPETITIONS} test repetitions\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "results_tokens = []\n",
    "results_logprobs = []\n",
    "results_texts = []\n",
    "results_distributions = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Repetition {rep + 1}/{NUM_REPETITIONS}...\")\n",
    "    \n",
    "    outputs = llm.generate([PROMPT], sampling_params=sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract token IDs\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    # Extract generated text\n",
    "    text = output.outputs[0].text\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    # Extract logprobs for selected tokens\n",
    "    logprobs_data = output.outputs[0].logprobs\n",
    "    selected_logprobs = [lp[token_ids[i]].logprob for i, lp in enumerate(logprobs_data)]\n",
    "    results_logprobs.append(np.array(selected_logprobs))\n",
    "    \n",
    "    # Extract full top-k distributions\n",
    "    rep_distributions = []\n",
    "    for position_logprobs in logprobs_data:\n",
    "        # Get top-k sorted by logprob (descending)\n",
    "        sorted_items = sorted(position_logprobs.items(), \n",
    "                            key=lambda x: x[1].logprob, \n",
    "                            reverse=True)[:TOP_LOGPROBS]\n",
    "        rep_distributions.append([(tok, lp.logprob) for tok, lp in sorted_items])\n",
    "    results_distributions.append(rep_distributions)\n",
    "    \n",
    "    print(f\"  Generated {len(token_ids)} tokens\")\n",
    "\n",
    "print()\n",
    "print(\"All repetitions complete!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check token sequence identity\n",
    "print(\"Checking token sequences...\")\n",
    "tokens_identical = all(\n",
    "    results_tokens[0] == results_tokens[i] \n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "if not tokens_identical:\n",
    "    print(\"\\n⚠ Token sequences differ!\")\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[0] != results_tokens[i]:\n",
    "            diff_positions = [\n",
    "                j for j in range(min(len(results_tokens[0]), len(results_tokens[i])))\n",
    "                if results_tokens[0][j] != results_tokens[i][j]\n",
    "            ]\n",
    "            print(f\"  Rep 0 vs Rep {i}: {len(diff_positions)} positions differ\")\n",
    "            if diff_positions:\n",
    "                print(f\"    First difference at position {diff_positions[0]}\")\n",
    "\n",
    "# Check logprobs for selected tokens\n",
    "print(\"\\nChecking selected token logprobs...\")\n",
    "first_logprobs = results_logprobs[0]\n",
    "logprobs_exact = all(\n",
    "    np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Selected token logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "# Check top-k distributions\n",
    "print(\"\\nChecking full top-k distributions...\")\n",
    "distributions_exact = True\n",
    "distribution_mismatches = []\n",
    "\n",
    "first_dist = results_distributions[0]\n",
    "for rep_idx in range(1, NUM_REPETITIONS):\n",
    "    for pos_idx in range(len(first_dist)):\n",
    "        dist_a = first_dist[pos_idx]\n",
    "        dist_b = results_distributions[rep_idx][pos_idx]\n",
    "        \n",
    "        # Check if token IDs match in same order\n",
    "        tokens_match = [t[0] for t in dist_a] == [t[0] for t in dist_b]\n",
    "        \n",
    "        # Check if logprobs are bit-exact\n",
    "        if tokens_match:\n",
    "            logprobs_match = all(\n",
    "                abs(dist_a[i][1] - dist_b[i][1]) < 1e-10 \n",
    "                for i in range(len(dist_a))\n",
    "            )\n",
    "            if not logprobs_match:\n",
    "                distributions_exact = False\n",
    "                distribution_mismatches.append((rep_idx, pos_idx))\n",
    "        else:\n",
    "            distributions_exact = False\n",
    "            distribution_mismatches.append((rep_idx, pos_idx))\n",
    "\n",
    "print(f\"Top-k distributions bit-exact: {distributions_exact}\")\n",
    "\n",
    "if not distributions_exact:\n",
    "    print(f\"\\n⚠ Found {len(distribution_mismatches)} position mismatches in distributions\")\n",
    "    if len(distribution_mismatches) <= 5:\n",
    "        for rep_idx, pos_idx in distribution_mismatches:\n",
    "            print(f\"  Rep 0 vs Rep {rep_idx}, position {pos_idx}\")\n",
    "    else:\n",
    "        print(f\"  First 5: {distribution_mismatches[:5]}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "    \n",
    "    # Element-wise statistics\n",
    "    all_logprobs = np.array(results_logprobs)\n",
    "    std_per_token = all_logprobs.std(axis=0)\n",
    "    print(f\"\\nPer-token std statistics:\")\n",
    "    print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "    print(f\"  Max: {std_per_token.max():.6e}\")\n",
    "    print(f\"  Median: {np.median(std_per_token):.6e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact and distributions_exact:\n",
    "    print(\"✓ PERFECT REPRODUCIBILITY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Full top-k distributions: bit-exact\")\n",
    "    print(\"  - vLLM is deterministic for this config\")\n",
    "elif tokens_identical and logprobs_exact and not distributions_exact:\n",
    "    print(\"⚠ SELECTED TOKENS EXACT, DISTRIBUTIONS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Top-k distributions: numerical variation detected\")\n",
    "    print(\"  → May indicate computational instability in non-selected paths\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"⚠ TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: small numerical variation\")\n",
    "    max_l2 = max(l2_distances) if not logprobs_exact else 0.0\n",
    "    if max_l2 < 1e-6:\n",
    "        print(f\"  - Variation very small (L2={max_l2:.2e})\")\n",
    "        print(\"  → Likely acceptable for forensics\")\n",
    "    else:\n",
    "        print(f\"  - Variation notable (L2={max_l2:.2e})\")\n",
    "        print(\"  → Investigate noise source\")\n",
    "else:\n",
    "    print(\"✗ TOKEN SEQUENCES DIFFER\")\n",
    "    print(\"  - This should NOT happen with temperature=0\")\n",
    "    print(\"  → Something is wrong, investigate\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"vllm_logprobs_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"prompt_source\": \"file\" if prompt_file else \"hardcoded\",\n",
    "    \"prompt_file\": os.path.basename(prompt_file) if prompt_file else None,\n",
    "    \"prompt_length_chars\": len(PROMPT),\n",
    "    \"prompt_length_tokens\": prompt_length,\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_model_len\": MAX_MODEL_LEN,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"warmup_enabled\": True,\n",
    "        \"prefix_caching_disabled\": True,\n",
    "        \"top_logprobs\": TOP_LOGPROBS\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"distributions_exact\": distributions_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact and distributions_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts,\n",
    "    \"top_k_distributions\": [\n",
    "        [[(int(tok), float(prob)) for tok, prob in dist] for dist in rep_dists]\n",
    "        for rep_dists in results_distributions\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_file = f\"vllm_logprobs_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9cf6b1-f5b7-4bde-8947-eaef18eef83a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
