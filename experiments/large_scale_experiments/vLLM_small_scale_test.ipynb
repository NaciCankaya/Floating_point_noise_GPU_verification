{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f41690a5-dd37-4b2e-8f33-0a73c43442bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9637e98c-8cb6-4531-8cac-f5e8df952abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "vLLM LOGPROBS EXTRACTION TEST\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Tensor parallel: 1\n",
      "  Max model len: 2048\n",
      "  GPU memory utilization: 0.4 (40%)\n",
      "  Max tokens: 20\n",
      "  Repetitions: 5\n",
      "  Temperature: 0.0 (greedy)\n",
      "\n",
      "GPU Info:\n",
      "  Device: NVIDIA A40\n",
      "  Available: 1 GPUs\n",
      "\n",
      "Loading model with vLLM...\n",
      "INFO 11-09 19:27:04 [utils.py:233] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'max_model_len': 2048, 'gpu_memory_utilization': 0.4, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-7B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 19:27:04 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 11-09 19:27:04 [model.py:1510] Using max model len 2048\n",
      "INFO 11-09 19:27:04 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 19:27:09 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:27:11 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:27:11 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:27:13 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m WARNING 11-09 19:27:13 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:27:13 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:27:13 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:27:14 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:27:14 [weight_utils.py:392] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:15<00:47, 15.71s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:34<00:34, 17.44s/it]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:50<00:17, 17.03s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:09<00:00, 17.48s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [01:09<00:00, 17.27s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:23 [default_loader.py:267] Loading weights took 69.25 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:24 [gpu_model_runner.py:2653] Model loading took 14.2488 GiB and 69.703528 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:29 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/860723eb7f/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:29 [backends.py:559] Dynamo bytecode transform time: 4.70 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:30 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.664 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:31 [monitor.py:34] torch.compile takes 4.70 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:33 [gpu_worker.py:298] Available KV cache memory: 2.04 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:33 [kv_cache_utils.py:1087] GPU KV cache size: 38,256 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:33 [kv_cache_utils.py:1091] Maximum concurrency for 2,048 tokens per request: 18.68x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 16.13it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 20.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:39 [gpu_model_runner.py:3480] Graph capturing finished in 6 secs, took 0.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1851)\u001b[0;0m INFO 11-09 19:28:39 [core.py:210] init engine (profile, create kv cache, warmup model) took 15.81 seconds\n",
      "INFO 11-09 19:28:40 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1109 19:28:40.046337555 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "\n",
      "Sampling params: temperature=0.0, max_tokens=20, seed=42\n",
      "\n",
      "================================================================================\n",
      "WARM-UP PASS\n",
      "================================================================================\n",
      "\n",
      "Running warm-up pass to initialize CUDA kernels...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17ee1c15f764ea4a8452fba5e9296c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f731a0de181b48edaad66fd9cf64910b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Warm-up complete - CUDA kernels compiled and cached\n",
      "\n",
      "================================================================================\n",
      "RUNNING 5 FORWARD PASSES\n",
      "================================================================================\n",
      "\n",
      "Rep 1/5: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c48aa344ebf452c8c0f6e531434e437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "797cb1c0acca488693c54fded8c847d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens, mean logprob=-0.5974\n",
      "Rep 2/5: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be57bf19b1c4c798141e68e16c760bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0252d9a0d7a4c038f6574ad1377adcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens, mean logprob=-0.5974\n",
      "Rep 3/5: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd9e6da24b248b096ad73334e3a008f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfdb6f4e6f1461c9679c611c0c26eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens, mean logprob=-0.5974\n",
      "Rep 4/5: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4497476c464d6488ecff8592d9f476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb8daf982204f7da75df31fb7a7481c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens, mean logprob=-0.5974\n",
      "Rep 5/5: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07e8ed5e53f6411e9e6a4f42c008b98d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31eb2aa728a4947905b0418b87ade6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 tokens, mean logprob=-0.5974\n",
      "\n",
      "First generation:\n",
      "  Text: ' However, the development of these models has also raised ethical \n",
      "concerns, particularly around iss...'\n",
      "  Tokens: [4354, 11, 279, 4401, 315, 1493, 4119, 702, 1083, 9226]...\n",
      "  Logprobs (first 5): [-1.10141468e+00 -3.20667859e-05 -1.61608422e+00 -2.52417779e+00\n",
      " -4.36653823e-01]\n",
      "\n",
      "================================================================================\n",
      "REPRODUCIBILITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Token sequences identical: True\n",
      "\n",
      "Logprobs bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "\n",
      "✓ PERFECT REPRODUCIBILITY\n",
      "  - Token sequences: bit-exact\n",
      "  - Logprobs: bit-exact\n",
      "  - vLLM is deterministic for this config\n",
      "  → Ready to scale up to K2 Thinking on Blackwell\n",
      "\n",
      "Results saved to: /workspace/vllm_logprobs_test_20251109_192845.json\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n",
      "\n",
      "If successful, next steps:\n",
      "  1. Scale up to K2 Thinking\n",
      "  2. Test on 4× B200 with tensor parallelism\n",
      "  3. Run with 100K context\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM Logprobs Extraction Test - Standalone Script\n",
    "Test if we can extract logprobs from vLLM for reproducibility forensics\n",
    "\n",
    "Paste this entire script into a Jupyter cell and run.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG SECTION - CHANGE THESE PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # Small model for testing\n",
    "TENSOR_PARALLEL_SIZE = 1  # Single GPU for now\n",
    "MAX_MODEL_LEN = 2048  # Context length (reduced for memory)\n",
    "GPU_MEMORY_UTILIZATION = 0.4  # Use 40% of GPU memory (default is 0.9)\n",
    "\n",
    "# Generation parameters\n",
    "NUM_REPETITIONS = 5\n",
    "MAX_TOKENS = 20  # Generate 20 tokens per pass\n",
    "TEMPERATURE = 0.0  # Greedy sampling for determinism\n",
    "SEED = 42\n",
    "\n",
    "# Input text\n",
    "INPUT_TEXT = \"\"\"The field of artificial intelligence has witnessed remarkable \n",
    "transformation over the past decade, driven primarily by advances in deep learning \n",
    "and the emergence of increasingly sophisticated language models. These models, trained \n",
    "on vast corpora of text data, have demonstrated remarkable capabilities across a wide \n",
    "range of tasks.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"vLLM LOGPROBS EXTRACTION TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {MAX_MODEL_LEN}\")\n",
    "print(f\"  GPU memory utilization: {GPU_MEMORY_UTILIZATION} ({GPU_MEMORY_UTILIZATION*100:.0f}%)\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE} (greedy)\")\n",
    "print()\n",
    "print(f\"GPU Info:\")\n",
    "print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  Available: {torch.cuda.device_count()} GPUs\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading model with vLLM...\")\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    download_dir=\"/workspace/huggingface_cache\",\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "print(\"✓ Model loaded\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLING PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    logprobs=1,  # Return logprob of chosen token\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Sampling params: temperature={TEMPERATURE}, max_tokens={MAX_TOKENS}, seed={SEED}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# LOGPROBS EXTRACTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_logprobs_vector(output):\n",
    "    \"\"\"\n",
    "    Extract logprobs from vLLM output for forensic comparison\n",
    "    \n",
    "    For each generated token, extract the logprob of that token.\n",
    "    Returns a numpy array of logprobs.\n",
    "    \"\"\"\n",
    "    logprobs_list = []\n",
    "    \n",
    "    # Get the token IDs that were actually generated\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    \n",
    "    # output.outputs[0].logprobs is a list (one entry per generated token)\n",
    "    # Each entry is a dict mapping token_id -> Logprob object\n",
    "    for i, token_logprobs_dict in enumerate(output.outputs[0].logprobs):\n",
    "        if token_logprobs_dict is None:\n",
    "            continue\n",
    "        \n",
    "        # Get the token ID that was actually generated at this position\n",
    "        generated_token_id = token_ids[i]\n",
    "        \n",
    "        # Get the logprob for this token from the dict\n",
    "        if generated_token_id in token_logprobs_dict:\n",
    "            logprob_obj = token_logprobs_dict[generated_token_id]\n",
    "            logprobs_list.append(logprob_obj.logprob)\n",
    "        else:\n",
    "            # This shouldn't happen, but handle it gracefully\n",
    "            print(f\"Warning: Token {generated_token_id} not in logprobs dict at position {i}\")\n",
    "            logprobs_list.append(None)\n",
    "    \n",
    "    return np.array(logprobs_list)\n",
    "\n",
    "# ============================================================================\n",
    "# WARM-UP PASS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WARM-UP PASS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Running warm-up pass to initialize CUDA kernels...\")\n",
    "_ = llm.generate([INPUT_TEXT], sampling_params)\n",
    "print(\"✓ Warm-up complete - CUDA kernels compiled and cached\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN REPEATED FORWARD PASSES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"RUNNING {NUM_REPETITIONS} FORWARD PASSES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "results_logprobs = []\n",
    "results_tokens = []\n",
    "results_texts = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Rep {rep + 1}/{NUM_REPETITIONS}:\", end=\" \")\n",
    "    \n",
    "    # Generate (vLLM automatically manages cache per request)\n",
    "    output = llm.generate([INPUT_TEXT], sampling_params)[0]\n",
    "    \n",
    "    # Extract logprobs\n",
    "    logprobs_vec = extract_logprobs_vector(output)\n",
    "    results_logprobs.append(logprobs_vec)\n",
    "    \n",
    "    # Extract tokens\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    # Extract text\n",
    "    text = output.outputs[0].text\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    print(f\"{len(token_ids)} tokens, mean logprob={np.mean(logprobs_vec):.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show first generation\n",
    "print(\"First generation:\")\n",
    "print(f\"  Text: '{results_texts[0][:100]}...'\")\n",
    "print(f\"  Tokens: {results_tokens[0][:10]}...\")\n",
    "print(f\"  Logprobs (first 5): {results_logprobs[0][:5]}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REPRODUCIBILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Check token sequences\n",
    "first_tokens = results_tokens[0]\n",
    "tokens_identical = all(results_tokens[i] == first_tokens for i in range(NUM_REPETITIONS))\n",
    "\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "if not tokens_identical:\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[i] != first_tokens:\n",
    "            print(f\"  Rep 0 vs Rep {i}: DIFFER\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check logprobs\n",
    "first_logprobs = results_logprobs[0]\n",
    "\n",
    "# Bit-exact comparison\n",
    "logprobs_exact = all(np.array_equal(first_logprobs, results_logprobs[i]) \n",
    "                     for i in range(1, NUM_REPETITIONS))\n",
    "\n",
    "print(f\"Logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "    \n",
    "    # Element-wise statistics\n",
    "    all_logprobs = np.array(results_logprobs)\n",
    "    std_per_token = all_logprobs.std(axis=0)\n",
    "    print(f\"\\nPer-token std statistics:\")\n",
    "    print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "    print(f\"  Max: {std_per_token.max():.6e}\")\n",
    "    print(f\"  Median: {np.median(std_per_token):.6e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact:\n",
    "    print(\"✓ PERFECT REPRODUCIBILITY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: bit-exact\")\n",
    "    print(\"  - vLLM is deterministic for this config\")\n",
    "    print(\"  → Ready to scale up to K2 Thinking on Blackwell\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"⚠ TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: small numerical variation\")\n",
    "    max_l2 = max(l2_distances) if not logprobs_exact else 0.0\n",
    "    if max_l2 < 1e-6:\n",
    "        print(f\"  - Variation very small (L2={max_l2:.2e})\")\n",
    "        print(\"  → Likely acceptable for forensics\")\n",
    "    else:\n",
    "        print(f\"  - Variation notable (L2={max_l2:.2e})\")\n",
    "        print(\"  → Investigate noise source\")\n",
    "else:\n",
    "    print(\"✗ TOKEN SEQUENCES DIFFER\")\n",
    "    print(\"  - This should NOT happen with temperature=0\")\n",
    "    print(\"  → Something is wrong, investigate\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"vllm_logprobs_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"warmup_enabled\": True\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts\n",
    "}\n",
    "\n",
    "output_file = f\"/workspace/vllm_logprobs_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"If successful, next steps:\")\n",
    "print(\"  1. Scale up to K2 Thinking\")\n",
    "print(\"  2. Test on 4× B200 with tensor parallelism\")\n",
    "print(\"  3. Run with 100K context\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64debb9f-277b-4ad1-a2de-aa3f58c12f77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
