{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2e1a9-f1a6-4ad0-aa79-166bd31cb2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "vLLM LOGPROBS EXTRACTION TEST - KIMI K2\n",
      "================================================================================\n",
      "\n",
      "Found file: Verification-for-International-AI-Governance.pdf\n",
      "Loading 172 pages from PDF...\n",
      "  Processed 10/172 pages\n",
      "  Processed 20/172 pages\n",
      "  Processed 30/172 pages\n",
      "  Processed 40/172 pages\n",
      "  Processed 50/172 pages\n",
      "  Processed 60/172 pages\n",
      "  Processed 70/172 pages\n",
      "  Processed 80/172 pages\n",
      "  Processed 90/172 pages\n",
      "  Processed 100/172 pages\n",
      "  Processed 110/172 pages\n",
      "  Processed 120/172 pages\n",
      "  Processed 130/172 pages\n",
      "  Processed 140/172 pages\n",
      "  Processed 150/172 pages\n",
      "  Processed 160/172 pages\n",
      "  Processed 170/172 pages\n",
      "Loaded 535619 characters from PDF (172 pages)\n",
      "\n",
      "Applying Kimi K2 chat template...\n",
      "Formatted prompt length: 535848 characters\n",
      "\n",
      "Loading tokenizer to validate prompt length...\n",
      "Prompt statistics:\n",
      "  Characters: 535,848\n",
      "  Tokens: 115,730\n",
      "  Max model length: 150,000\n",
      "  Generation tokens: 20\n",
      "  Total required: 115,750\n",
      "\n",
      "âœ“ Prompt length validation passed\n",
      "  Remaining capacity: 34,250 tokens\n",
      "\n",
      "Note on prompt formatting:\n",
      "  Using OFFICIAL Kimi K2 chat template\n",
      "  Template structure:\n",
      "    - <|im_system|>system<|im_middle|>...<|im_end|>\n",
      "    - <|im_user|>user<|im_middle|>...<|im_end|>\n",
      "    - <|im_assistant|>assistant<|im_middle|> (generation start)\n",
      "  Why manual template:\n",
      "    - Full control over exact formatting\n",
      "    - Reproducible across environments\n",
      "    - Matches model's training format\n",
      "\n",
      "Configuration:\n",
      "  Model: moonshotai/Kimi-K2-Thinking\n",
      "  Tensor parallel: 8\n",
      "  Max model len: 150,000\n",
      "  GPU memory utilization: 0.9 (90%)\n",
      "  Max tokens: 20\n",
      "  Repetitions: 20\n",
      "  Temperature: 0.0 (greedy)\n",
      "  Top logprobs: 10\n",
      "\n",
      "GPU Info:\n",
      "  Device: NVIDIA H200\n",
      "  Available: 8 GPUs\n",
      "\n",
      "Loading model with vLLM...\n",
      "INFO 11-10 02:31:58 [utils.py:233] non-default args: {'trust_remote_code': True, 'download_dir': '/tmp/hf_cache', 'dtype': 'bfloat16', 'max_model_len': 150000, 'tensor_parallel_size': 8, 'enable_prefix_caching': False, 'disable_log_stats': True, 'model': 'moonshotai/Kimi-K2-Thinking'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "A new version of the following files was downloaded from https://huggingface.co/moonshotai/Kimi-K2-Thinking:\n",
      "- configuration_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "You are using a model of type kimi_k2 to instantiate a model of type deepseek_v3. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-10 02:31:59 [config.py:388] Replacing legacy 'type' key with 'rope_type'\n",
      "INFO 11-10 02:31:59 [model.py:547] Resolved architecture: DeepseekV3ForCausalLM\n",
      "INFO 11-10 02:31:59 [model.py:1510] Using max model len 150000\n",
      "INFO 11-10 02:31:59 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 11-10 02:31:59 [cuda.py:166] Forcing kv cache block size to 64 for FlashMLA backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/moonshotai/Kimi-K2-Thinking:\n",
      "- tokenization_kimi.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-10 02:32:00 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-10 02:32:06 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:32:12 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:32:12 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='moonshotai/Kimi-K2-Thinking', speculative_config=None, tokenizer='moonshotai/Kimi-K2-Thinking', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=150000, download_dir='/tmp/hf_cache', load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=moonshotai/Kimi-K2-Thinking, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m WARNING 11-10 02:32:12 [multiproc_executor.py:720] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:32:12 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3, 4, 5, 6, 7], buffer_handle=(8, 16777216, 10, 'psm_dffb3cf9'), local_subscribe_addr='ipc:///tmp/4e75c079-c59d-44e7-9ed8-dbb02f8d44f1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-10 02:32:17 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-10 02:32:18 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-10 02:32:18 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-10 02:32:18 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-10 02:32:18 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-10 02:32:18 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-10 02:32:18 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-10 02:32:18 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-10 02:32:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_06671cbe'), local_subscribe_addr='ipc:///tmp/f33bbbb3-8eab-45e4-b189-d7ee37b1aaaa', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-10 02:32:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3086f3dd'), local_subscribe_addr='ipc:///tmp/e5399ced-9fd1-4516-9420-49ba546af08a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-10 02:32:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_215f4d4b'), local_subscribe_addr='ipc:///tmp/eadef0c4-9073-486c-84b3-2afd8814c211', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-10 02:32:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_2835a876'), local_subscribe_addr='ipc:///tmp/c7e70c40-626a-4d46-90b8-52ee426236af', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-10 02:32:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_4d66357b'), local_subscribe_addr='ipc:///tmp/bf75746b-14de-45b2-856c-9bdb78d30bff', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-10 02:32:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_e5ea5db2'), local_subscribe_addr='ipc:///tmp/254c049e-e4e2-4d88-8d9e-718bdd68ecaf', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-10 02:32:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f05016b8'), local_subscribe_addr='ipc:///tmp/758b5403-a3af-4140-aa34-42630b218259', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-10 02:32:42 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_fc124593'), local_subscribe_addr='ipc:///tmp/205bcaf6-53b3-46df-8af8-3a2de034d264', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 7[Gloo] Rank  is connected to 7 peer ranks. 6Expected number of connected peer ranks is : 7 is connected to 7\n",
      " peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 0 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 3 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 6 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "INFO 11-10 02:32:45 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:45 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:45 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:45 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:45 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:45 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:45 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:45 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:45 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:45 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:45 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:45 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:45 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:45 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:45 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:45 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:48 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 02:32:48 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 02:32:48 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 02:32:48 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 02:32:48 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 02:32:48 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 02:32:48 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 02:32:48 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 02:32:48 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3, 4, 5, 6, 7], buffer_handle=(7, 4194304, 6, 'psm_6ac6ff55'), local_subscribe_addr='ipc:///tmp/4e853a4e-9a94-4495-84c2-a1cc4979e72b', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : [Gloo] Rank 0\n",
      "0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank \n",
      "0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank \n",
      "0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0[Gloo] Rank \n",
      "0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 5 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "INFO 11-10 02:32:49 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:49 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "[Gloo] Rank 2 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 4 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank 1 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "[Gloo] Rank [Gloo] Rank 6 is connected to 7 peer ranks. 0[Gloo] Rank  is connected to Expected number of connected peer ranks is : 773 peer ranks.  is connected to Expected number of connected peer ranks is : \n",
      "77 peer ranks. Expected number of connected peer ranks is : \n",
      "7\n",
      "[Gloo] Rank 7 is connected to 7 peer ranks. Expected number of connected peer ranks is : 7\n",
      "INFO 11-10 02:32:49 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:49 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:49 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:49 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:49 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:49 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:49 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:49 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:49 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:49 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:49 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:49 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 02:32:49 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:49 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 02:32:50 [parallel_state.py:1208] rank 1 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 11-10 02:32:50 [parallel_state.py:1208] rank 6 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 6, EP rank 6\n",
      "INFO 11-10 02:32:50 [parallel_state.py:1208] rank 2 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 11-10 02:32:50 [parallel_state.py:1208] rank 4 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 4, EP rank 4\n",
      "INFO 11-10 02:32:50 [parallel_state.py:1208] rank 7 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 7, EP rank 7\n",
      "INFO 11-10 02:32:50 [parallel_state.py:1208] rank 5 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 5, EP rank 5\n",
      "INFO 11-10 02:32:50 [parallel_state.py:1208] rank 0 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-10 02:32:50 [parallel_state.py:1208] rank 3 in world size 8 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 11-10 02:32:50 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-10 02:32:50 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-10 02:32:50 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-10 02:32:50 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-10 02:32:50 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-10 02:32:50 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-10 02:32:50 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-10 02:32:50 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2602] Starting to load model moonshotai/Kimi-K2-Thinking...\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2602] Starting to load model moonshotai/Kimi-K2-Thinking...\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2602] Starting to load model moonshotai/Kimi-K2-Thinking...\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2602] Starting to load model moonshotai/Kimi-K2-Thinking...\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2602] Starting to load model moonshotai/Kimi-K2-Thinking...\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2602] Starting to load model moonshotai/Kimi-K2-Thinking...\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2602] Starting to load model moonshotai/Kimi-K2-Thinking...\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2602] Starting to load model moonshotai/Kimi-K2-Thinking...\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:32:51 [cuda.py:288] Using FlashMLA backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:32:51 [compressed_tensors_moe.py:130] Using CompressedTensorsWNA16MarlinMoEMethod\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:32:51 [cuda.py:288] Using FlashMLA backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:32:51 [compressed_tensors_moe.py:130] Using CompressedTensorsWNA16MarlinMoEMethod\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:32:51 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:32:51 [cuda.py:288] Using FlashMLA backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:32:51 [cuda.py:288] Using FlashMLA backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:32:51 [compressed_tensors_moe.py:130] Using CompressedTensorsWNA16MarlinMoEMethod\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:32:51 [compressed_tensors_moe.py:130] Using CompressedTensorsWNA16MarlinMoEMethod\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:32:51 [cuda.py:288] Using FlashMLA backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:32:51 [compressed_tensors_moe.py:130] Using CompressedTensorsWNA16MarlinMoEMethod\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:32:51 [cuda.py:288] Using FlashMLA backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:32:51 [compressed_tensors_moe.py:130] Using CompressedTensorsWNA16MarlinMoEMethod\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:32:51 [cuda.py:288] Using FlashMLA backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:32:51 [compressed_tensors_moe.py:130] Using CompressedTensorsWNA16MarlinMoEMethod\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:32:51 [cuda.py:288] Using FlashMLA backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:32:51 [compressed_tensors_moe.py:130] Using CompressedTensorsWNA16MarlinMoEMethod\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:32:52 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:32:52 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:32:53 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:32:53 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:32:53 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:32:53 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:32:53 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:32:53 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:45:09 [weight_utils.py:413] Time spent downloading weights for moonshotai/Kimi-K2-Thinking: 737.258145 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/62 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:   2% Completed | 1/62 [00:00<00:21,  2.84it/s]\n",
      "Loading safetensors checkpoint shards:   3% Completed | 2/62 [00:11<06:38,  6.65s/it]\n",
      "Loading safetensors checkpoint shards:   5% Completed | 3/62 [00:24<09:25,  9.58s/it]\n",
      "Loading safetensors checkpoint shards:   6% Completed | 4/62 [00:36<10:04, 10.42s/it]\n",
      "Loading safetensors checkpoint shards:   8% Completed | 5/62 [00:45<09:34, 10.07s/it]\n",
      "Loading safetensors checkpoint shards:  10% Completed | 6/62 [00:53<08:44,  9.37s/it]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 7/62 [01:01<08:04,  8.81s/it]\n",
      "Loading safetensors checkpoint shards:  13% Completed | 8/62 [01:08<07:23,  8.22s/it]\n",
      "Loading safetensors checkpoint shards:  15% Completed | 9/62 [01:15<07:05,  8.04s/it]\n",
      "Loading safetensors checkpoint shards:  16% Completed | 10/62 [01:23<06:43,  7.76s/it]\n",
      "Loading safetensors checkpoint shards:  18% Completed | 11/62 [01:29<06:14,  7.34s/it]\n",
      "Loading safetensors checkpoint shards:  19% Completed | 12/62 [01:36<05:58,  7.17s/it]\n",
      "Loading safetensors checkpoint shards:  21% Completed | 13/62 [01:42<05:41,  6.97s/it]\n",
      "Loading safetensors checkpoint shards:  23% Completed | 14/62 [01:49<05:27,  6.83s/it]\n",
      "Loading safetensors checkpoint shards:  24% Completed | 15/62 [01:55<05:13,  6.66s/it]\n",
      "Loading safetensors checkpoint shards:  26% Completed | 16/62 [02:02<05:04,  6.62s/it]\n",
      "Loading safetensors checkpoint shards:  27% Completed | 17/62 [02:09<05:04,  6.76s/it]\n",
      "Loading safetensors checkpoint shards:  29% Completed | 18/62 [02:16<05:08,  7.00s/it]\n",
      "Loading safetensors checkpoint shards:  31% Completed | 19/62 [02:25<05:28,  7.63s/it]\n",
      "Loading safetensors checkpoint shards:  32% Completed | 20/62 [02:35<05:43,  8.18s/it]\n",
      "Loading safetensors checkpoint shards:  34% Completed | 21/62 [02:43<05:41,  8.33s/it]\n",
      "Loading safetensors checkpoint shards:  35% Completed | 22/62 [02:51<05:19,  7.99s/it]\n",
      "Loading safetensors checkpoint shards:  37% Completed | 23/62 [02:58<04:59,  7.68s/it]\n",
      "Loading safetensors checkpoint shards:  39% Completed | 24/62 [03:04<04:41,  7.41s/it]\n",
      "Loading safetensors checkpoint shards:  40% Completed | 25/62 [03:14<05:02,  8.17s/it]\n",
      "Loading safetensors checkpoint shards:  42% Completed | 26/62 [03:24<05:13,  8.72s/it]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 27/62 [03:34<05:13,  8.95s/it]\n",
      "Loading safetensors checkpoint shards:  45% Completed | 28/62 [03:43<05:08,  9.09s/it]\n",
      "Loading safetensors checkpoint shards:  47% Completed | 29/62 [03:52<05:00,  9.12s/it]\n",
      "Loading safetensors checkpoint shards:  48% Completed | 30/62 [04:02<04:56,  9.28s/it]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 31/62 [04:12<04:53,  9.47s/it]\n",
      "Loading safetensors checkpoint shards:  52% Completed | 32/62 [04:21<04:41,  9.39s/it]\n",
      "Loading safetensors checkpoint shards:  53% Completed | 33/62 [04:26<03:56,  8.17s/it]\n",
      "Loading safetensors checkpoint shards:  55% Completed | 34/62 [04:32<03:24,  7.30s/it]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 35/62 [04:37<02:58,  6.60s/it]\n",
      "Loading safetensors checkpoint shards:  58% Completed | 36/62 [04:42<02:39,  6.12s/it]\n",
      "Loading safetensors checkpoint shards:  60% Completed | 37/62 [04:47<02:24,  5.79s/it]\n",
      "Loading safetensors checkpoint shards:  61% Completed | 38/62 [04:52<02:13,  5.56s/it]\n",
      "Loading safetensors checkpoint shards:  63% Completed | 39/62 [04:57<02:04,  5.41s/it]\n",
      "Loading safetensors checkpoint shards:  65% Completed | 40/62 [05:00<01:47,  4.88s/it]\n",
      "Loading safetensors checkpoint shards:  66% Completed | 41/62 [05:04<01:33,  4.46s/it]\n",
      "Loading safetensors checkpoint shards:  68% Completed | 42/62 [05:07<01:23,  4.16s/it]\n",
      "Loading safetensors checkpoint shards:  69% Completed | 43/62 [05:11<01:14,  3.95s/it]\n",
      "Loading safetensors checkpoint shards:  71% Completed | 44/62 [05:14<01:08,  3.80s/it]\n",
      "Loading safetensors checkpoint shards:  73% Completed | 45/62 [05:18<01:02,  3.70s/it]\n",
      "Loading safetensors checkpoint shards:  74% Completed | 46/62 [05:21<00:58,  3.64s/it]\n",
      "Loading safetensors checkpoint shards:  76% Completed | 47/62 [05:25<00:53,  3.58s/it]\n",
      "Loading safetensors checkpoint shards:  77% Completed | 48/62 [05:28<00:49,  3.55s/it]\n",
      "Loading safetensors checkpoint shards:  79% Completed | 49/62 [05:32<00:45,  3.52s/it]\n",
      "Loading safetensors checkpoint shards:  81% Completed | 50/62 [05:35<00:41,  3.50s/it]\n",
      "Loading safetensors checkpoint shards:  82% Completed | 51/62 [05:39<00:38,  3.49s/it]\n",
      "Loading safetensors checkpoint shards:  84% Completed | 52/62 [05:42<00:34,  3.48s/it]\n",
      "Loading safetensors checkpoint shards:  85% Completed | 53/62 [05:45<00:31,  3.47s/it]\n",
      "Loading safetensors checkpoint shards:  87% Completed | 54/62 [05:49<00:27,  3.44s/it]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 55/62 [05:52<00:24,  3.45s/it]\n",
      "Loading safetensors checkpoint shards:  90% Completed | 56/62 [05:56<00:20,  3.46s/it]\n",
      "Loading safetensors checkpoint shards:  92% Completed | 57/62 [05:59<00:17,  3.46s/it]\n",
      "Loading safetensors checkpoint shards:  94% Completed | 58/62 [06:03<00:13,  3.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:51:16 [default_loader.py:267] Loading weights took 365.34 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  95% Completed | 59/62 [06:06<00:10,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:51:18 [default_loader.py:267] Loading weights took 367.04 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:51:19 [gpu_model_runner.py:2653] Model loading took 71.0037 GiB and 1107.744352 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  97% Completed | 60/62 [06:10<00:06,  3.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:51:21 [gpu_model_runner.py:2653] Model loading took 71.0037 GiB and 1109.288250 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:51:22 [default_loader.py:267] Loading weights took 371.61 seconds\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:51:23 [default_loader.py:267] Loading weights took 371.15 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:  98% Completed | 61/62 [06:13<00:03,  3.47s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 62/62 [06:13<00:00,  2.48s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 62/62 [06:13<00:00,  6.03s/it]\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:51:24 [default_loader.py:267] Loading weights took 373.83 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:51:25 [gpu_model_runner.py:2653] Model loading took 71.0037 GiB and 1113.478509 seconds\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:51:26 [gpu_model_runner.py:2653] Model loading took 71.0037 GiB and 1114.202914 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:51:27 [gpu_model_runner.py:2653] Model loading took 71.0037 GiB and 1115.684575 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:51:30 [default_loader.py:267] Loading weights took 378.15 seconds\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:51:31 [default_loader.py:267] Loading weights took 380.16 seconds\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:51:32 [default_loader.py:267] Loading weights took 380.54 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:51:32 [gpu_model_runner.py:2653] Model loading took 71.0037 GiB and 1120.957151 seconds\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:51:34 [gpu_model_runner.py:2653] Model loading took 71.0037 GiB and 1122.782119 seconds\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:51:34 [gpu_model_runner.py:2653] Model loading took 71.0037 GiB and 1123.061147 seconds\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:51:42 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/5bafa3f841/rank_5_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:51:42 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/5bafa3f841/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:51:42 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/5bafa3f841/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:51:42 [backends.py:559] Dynamo bytecode transform time: 6.97 s\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:51:42 [backends.py:559] Dynamo bytecode transform time: 6.93 s\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:51:42 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/5bafa3f841/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:51:42 [backends.py:559] Dynamo bytecode transform time: 6.95 s\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:51:42 [backends.py:559] Dynamo bytecode transform time: 6.89 s\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:51:43 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/5bafa3f841/rank_6_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:51:43 [backends.py:559] Dynamo bytecode transform time: 8.44 s\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:51:45 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/5bafa3f841/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:51:45 [backends.py:559] Dynamo bytecode transform time: 9.84 s\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:51:46 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/5bafa3f841/rank_7_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:51:46 [backends.py:559] Dynamo bytecode transform time: 11.23 s\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:51:47 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/5bafa3f841/rank_4_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:51:47 [backends.py:559] Dynamo bytecode transform time: 12.06 s\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:51:50 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:51:50 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:51:50 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:51:51 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:51:53 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:51:54 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:51:55 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:51:56 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:52:34 [shm_broadcast.py:466] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation).\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:53:01 [backends.py:218] Compiling a graph for dynamic shape takes 78.71 s\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:53:03 [backends.py:218] Compiling a graph for dynamic shape takes 81.41 s\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:53:08 [backends.py:218] Compiling a graph for dynamic shape takes 86.13 s\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:53:10 [backends.py:218] Compiling a graph for dynamic shape takes 84.54 s\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:53:12 [backends.py:218] Compiling a graph for dynamic shape takes 85.67 s\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:53:13 [backends.py:218] Compiling a graph for dynamic shape takes 89.20 s\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:53:13 [backends.py:218] Compiling a graph for dynamic shape takes 90.98 s\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:53:15 [backends.py:218] Compiling a graph for dynamic shape takes 87.80 s\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:53:21 [monitor.py:34] torch.compile takes 99.86 s in total\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:53:21 [monitor.py:34] torch.compile takes 96.91 s in total\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:53:21 [monitor.py:34] torch.compile takes 85.60 s in total\n",
      "INFO 11-10 02:53:21 [monitor.py:34] torch.compile takes 93.08 s in total\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:53:21 [monitor.py:34] torch.compile takes 97.65 s in total\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:53:21 [monitor.py:34] torch.compile takes 97.91 s in total\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:53:21 [monitor.py:34] torch.compile takes 88.38 s in total\n",
      "INFO 11-10 02:53:21 [monitor.py:34] torch.compile takes 94.38 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:53:34 [shm_broadcast.py:466] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation).\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:54:09 [gpu_worker.py:298] Available KV cache memory: 50.93 GiB\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:54:09 [gpu_worker.py:298] Available KV cache memory: 50.93 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:54:09 [gpu_worker.py:298] Available KV cache memory: 50.93 GiB\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:54:09 [gpu_worker.py:298] Available KV cache memory: 50.93 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:54:09 [gpu_worker.py:298] Available KV cache memory: 50.93 GiB\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:54:09 [gpu_worker.py:298] Available KV cache memory: 50.93 GiB\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:54:09 [gpu_worker.py:298] Available KV cache memory: 50.93 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:54:09 [gpu_worker.py:298] Available KV cache memory: 50.93 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1087] GPU KV cache size: 778,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 5.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1087] GPU KV cache size: 778,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 5.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1087] GPU KV cache size: 778,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 5.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1087] GPU KV cache size: 778,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 5.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1087] GPU KV cache size: 778,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 5.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1087] GPU KV cache size: 778,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 5.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1087] GPU KV cache size: 778,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 5.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1087] GPU KV cache size: 778,176 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:10 [kv_cache_utils.py:1091] Maximum concurrency for 150,000 tokens per request: 5.19x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m 2025-11-10 02:54:10,471 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m 2025-11-10 02:54:10,472 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m 2025-11-10 02:54:10,475 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m 2025-11-10 02:54:10,475 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m 2025-11-10 02:54:10,476 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m 2025-11-10 02:54:10,476 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m 2025-11-10 02:54:10,476 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m 2025-11-10 02:54:10,477 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m 2025-11-10 02:54:11,216 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m \u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m 2025-11-10 02:54:11,216 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "2025-11-10 02:54:11,216 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m 2025-11-10 02:54:11,216 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m 2025-11-10 02:54:11,216 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m 2025-11-10 02:54:11,216 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m 2025-11-10 02:54:11,216 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m 2025-11-10 02:54:11,216 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:18<00:00,  3.56it/s]\n",
      "Capturing CUDA graphs (decode, FULL):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:15<00:00,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:54:46 [custom_all_reduce.py:203] Registering 1230 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:54:46 [custom_all_reduce.py:203] Registering 1230 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:54:46 [custom_all_reduce.py:203] Registering 1230 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:54:46 [custom_all_reduce.py:203] Registering 1230 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:54:46 [custom_all_reduce.py:203] Registering 1230 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:54:46 [custom_all_reduce.py:203] Registering 1230 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:54:46 [custom_all_reduce.py:203] Registering 1230 cuda graph addresses\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:54:46 [custom_all_reduce.py:203] Registering 1230 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:16<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP7 pid=1210)\u001b[0;0m INFO 11-10 02:54:47 [gpu_model_runner.py:3480] Graph capturing finished in 36 secs, took 0.50 GiB\n",
      "\u001b[1;36m(Worker_TP3 pid=1206)\u001b[0;0m INFO 11-10 02:54:47 [gpu_model_runner.py:3480] Graph capturing finished in 36 secs, took 0.50 GiB\n",
      "\u001b[1;36m(Worker_TP5 pid=1208)\u001b[0;0m INFO 11-10 02:54:47 [gpu_model_runner.py:3480] Graph capturing finished in 36 secs, took 0.50 GiB\n",
      "\u001b[1;36m(Worker_TP6 pid=1209)\u001b[0;0m INFO 11-10 02:54:47 [gpu_model_runner.py:3480] Graph capturing finished in 36 secs, took 0.50 GiB\n",
      "\u001b[1;36m(Worker_TP1 pid=1204)\u001b[0;0m INFO 11-10 02:54:47 [gpu_model_runner.py:3480] Graph capturing finished in 36 secs, took 0.50 GiB\n",
      "\u001b[1;36m(Worker_TP0 pid=1203)\u001b[0;0m INFO 11-10 02:54:47 [gpu_model_runner.py:3480] Graph capturing finished in 36 secs, took 0.51 GiB\n",
      "\u001b[1;36m(Worker_TP2 pid=1205)\u001b[0;0m INFO 11-10 02:54:47 [gpu_model_runner.py:3480] Graph capturing finished in 36 secs, took 0.50 GiB\n",
      "\u001b[1;36m(Worker_TP4 pid=1207)\u001b[0;0m INFO 11-10 02:54:47 [gpu_model_runner.py:3480] Graph capturing finished in 36 secs, took 0.50 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m INFO 11-10 02:54:47 [core.py:210] init engine (profile, create kv cache, warmup model) took 192.56 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m A new version of the following files was downloaded from https://huggingface.co/moonshotai/Kimi-K2-Thinking:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m - tokenization_kimi.py\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m . Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1061)\u001b[0;0m WARNING 11-10 02:54:48 [tokenizer.py:253] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\n",
      "INFO 11-10 02:54:49 [llm.py:306] Supported_tasks: ['generate']\n",
      "Model loaded successfully!\n",
      "\n",
      "Running warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94352a183756410084d25ed29093b58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5f6cb7d93c4664876214782f1af09a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup complete\n",
      "\n",
      "================================================================================\n",
      "Running 20 test repetitions\n",
      "================================================================================\n",
      "\n",
      "Repetition 1/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508071a0dfa04eb3ab2c8e4e27b87834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7bc469788ed49ddbe40cf746f2536f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 2/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73cb774d8588433abb92ee9ec5a5a861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305d88e905b648ccb47a98d87682cf87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 3/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "577f3eaa0b934fcebf8591ed3837eb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1573aa81a5234227ba35ce4111736c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 4/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8f9545368c7459983742b784983fa31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d7408094f98479b991db1f74016abc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 5/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312b060c19cf4837a3894bf16ec4374e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2266155d5e4440f08b4bb2cf2f750692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 6/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "495fbd3880eb4d3b902defa4b2f5d9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fed7f46ed94ebcaf8d260b89803db3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 7/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5b5c7e5f3324c29af431f149cd0b39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d881426a9bff4396bd86b29fc0f9ab0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 8/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28e47c4ce104a73b86d7465269bccad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11ce4cb06e4427dbf7739b41a432b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 9/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b00a8ee5c0401ea66b1101ca6bb62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549d1a561deb40408da496fe3ecf6fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 10/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b35f53e7d184ad7bd7bf2fba92552bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ff9896f1444d3db3e1fa38b7c434a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 11/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2b6692b22794740bc84fbce35b699c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf3f3c9ce5142e389c628c935fcac02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 12/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7837613238445558a79d34b18ef514f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164beb3938364c8bb54d74e82c4ec994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 13/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21d1127f8550498685ba3c6e8703f64d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de0fb526f0af4909adcc1f0c029f9730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 14/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220ae40b4b1d4aa78497015a328b46ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de36991f94a4192b1d22ad60ced2503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 15/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90acacdb8fea4c5480cd0287c6ccd881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff8ac20c8ab242ccb584f36810f02aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 16/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02db2ca1d935491c98642ad67aa497e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "517478330e094633b4e9c7c3b6484e26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 17/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3197c3a524ae43e7b2f96efd27344334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696d5fe640c2495a884a3732bf0a4abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 18/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85279c2834a4b14a35db4eb0b67471a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85481520ef64714b41714c1d1501a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 19/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07738935501c4cd9a1ba0221e46aabbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calling super().encode with {'truncation': False}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6007255d9ce4f31a7be9b874067b34b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM Tensor Parallelism Test with Auto File Detection\n",
    "Tests bit-exact reproducibility across multiple runs with TP\n",
    "Automatically finds txt/pdf files in current directory\n",
    "Validates prompt length before loading model weights\n",
    "Uses proper Kimi K2 chat template format\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"moonshotai/Kimi-K2-Thinking\"\n",
    "TENSOR_PARALLEL_SIZE = 8\n",
    "MAX_MODEL_LEN = 150000\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 20\n",
    "NUM_REPETITIONS = 20\n",
    "TEMPERATURE = 0.0  # Greedy decoding\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Prompt source - finds first txt or pdf in current directory\n",
    "AUTO_FIND_FILE = True  # Set to False to use hardcoded content\n",
    "\n",
    "# Chat template configuration - using Kimi K2 defaults\n",
    "SYSTEM_PROMPT = \"You are Kimi, an AI assistant created by Moonshot AI.\"\n",
    "USER_TASK = \"Please provide a detailed summary of the following text.\"\n",
    "\n",
    "# Hardcoded content (used if AUTO_FIND_FILE=False or no files found)\n",
    "HARDCODED_CONTENT = \"\"\"The development of large language models has fundamentally transformed natural language processing and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FILE LOADING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def find_prompt_file():\n",
    "    \"\"\"Find first txt or pdf file in current directory\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # Look for txt files first, then pdf\n",
    "    txt_files = glob.glob(os.path.join(cwd, \"*.txt\"))\n",
    "    pdf_files = glob.glob(os.path.join(cwd, \"*.pdf\"))\n",
    "    \n",
    "    if txt_files:\n",
    "        return txt_files[0]\n",
    "    elif pdf_files:\n",
    "        return pdf_files[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_text_from_file(filepath):\n",
    "    \"\"\"Load text from txt or pdf file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    if filepath.endswith('.txt'):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print(f\"Loaded {len(text)} characters from txt file\")\n",
    "        return text\n",
    "    \n",
    "    elif filepath.endswith('.pdf'):\n",
    "        try:\n",
    "            import PyPDF2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"PyPDF2 required for PDF loading. Install with: pip install PyPDF2\")\n",
    "        \n",
    "        text = []\n",
    "        with open(filepath, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Loading {num_pages} pages from PDF...\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text.append(page_text)\n",
    "                if page_num % 10 == 0:\n",
    "                    print(f\"  Processed {page_num}/{num_pages} pages\")\n",
    "        \n",
    "        full_text = '\\n'.join(text)\n",
    "        print(f\"Loaded {len(full_text)} characters from PDF ({num_pages} pages)\")\n",
    "        return full_text\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath}. Use .txt or .pdf\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"vLLM LOGPROBS EXTRACTION TEST - KIMI K2\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load document content\n",
    "prompt_file = None\n",
    "if AUTO_FIND_FILE:\n",
    "    prompt_file = find_prompt_file()\n",
    "    if prompt_file:\n",
    "        print(f\"Found file: {os.path.basename(prompt_file)}\")\n",
    "        DOCUMENT_CONTENT = load_text_from_file(prompt_file)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No txt/pdf files found in current directory\")\n",
    "        print(\"Using hardcoded content\")\n",
    "        DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "        print()\n",
    "else:\n",
    "    DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "    print(\"Using hardcoded content\")\n",
    "    print()\n",
    "\n",
    "# Apply Kimi K2 chat template\n",
    "# Based on official template: https://huggingface.co/moonshotai/Kimi-K2-Thinking/blob/main/chat_template.jinja\n",
    "print(\"Applying Kimi K2 chat template...\")\n",
    "\n",
    "# Combine user task with document\n",
    "user_content = f\"{USER_TASK}\\n\\n{DOCUMENT_CONTENT}\"\n",
    "\n",
    "# Format following Kimi K2 template structure:\n",
    "# 1. System message with default Kimi prompt\n",
    "# 2. User message\n",
    "# 3. Assistant start token (model will generate after this)\n",
    "PROMPT = (\n",
    "    f\"<|im_system|>system<|im_middle|>{SYSTEM_PROMPT}<|im_end|>\"\n",
    "    f\"<|im_user|>user<|im_middle|>{user_content}<|im_end|>\"\n",
    "    f\"<|im_assistant|>assistant<|im_middle|>\"\n",
    ")\n",
    "\n",
    "print(f\"Formatted prompt length: {len(PROMPT)} characters\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER PRECHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading tokenizer to validate prompt length...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir='/tmp/hf_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Tokenize prompt\n",
    "prompt_tokens = tokenizer.encode(PROMPT)\n",
    "prompt_length = len(prompt_tokens)\n",
    "\n",
    "print(f\"Prompt statistics:\")\n",
    "print(f\"  Characters: {len(PROMPT):,}\")\n",
    "print(f\"  Tokens: {prompt_length:,}\")\n",
    "print(f\"  Max model length: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Generation tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Total required: {prompt_length + MAX_TOKENS:,}\")\n",
    "print()\n",
    "\n",
    "# Validate length\n",
    "if prompt_length > MAX_MODEL_LEN:\n",
    "    print(f\"âŒ ERROR: Prompt is too long!\")\n",
    "    print(f\"  Prompt has {prompt_length:,} tokens\")\n",
    "    print(f\"  Model max is {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Exceeds by {prompt_length - MAX_MODEL_LEN:,} tokens\")\n",
    "    print()\n",
    "    print(\"Solutions:\")\n",
    "    print(f\"  1. Increase MAX_MODEL_LEN to at least {prompt_length + MAX_TOKENS}\")\n",
    "    print(f\"  2. Truncate/reduce the prompt\")\n",
    "    exit(1)\n",
    "\n",
    "if prompt_length + MAX_TOKENS > MAX_MODEL_LEN:\n",
    "    print(f\"âš  WARNING: Prompt + generation may exceed context\")\n",
    "    print(f\"  Prompt: {prompt_length:,} tokens\")\n",
    "    print(f\"  Generation: {MAX_TOKENS} tokens\")\n",
    "    print(f\"  Total: {prompt_length + MAX_TOKENS:,} tokens\")\n",
    "    print(f\"  Model max: {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Consider increasing MAX_MODEL_LEN to {prompt_length + MAX_TOKENS + 100}\")\n",
    "    print()\n",
    "else:\n",
    "    print(f\"âœ“ Prompt length validation passed\")\n",
    "    print(f\"  Remaining capacity: {MAX_MODEL_LEN - prompt_length - MAX_TOKENS:,} tokens\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# VLLM PROMPT TEMPLATE HANDLING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Note on prompt formatting:\")\n",
    "print(\"  Using OFFICIAL Kimi K2 chat template\")\n",
    "print(\"  Template structure:\")\n",
    "print(\"    - <|im_system|>system<|im_middle|>...<|im_end|>\")\n",
    "print(\"    - <|im_user|>user<|im_middle|>...<|im_end|>\")\n",
    "print(\"    - <|im_assistant|>assistant<|im_middle|> (generation start)\")\n",
    "print(\"  Why manual template:\")\n",
    "print(\"    - Full control over exact formatting\")\n",
    "print(\"    - Reproducible across environments\")\n",
    "print(\"    - Matches model's training format\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  GPU memory utilization: {GPU_MEMORY_UTILIZATION} ({int(GPU_MEMORY_UTILIZATION*100)}%)\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE} (greedy)\")\n",
    "print(f\"  Top logprobs: {TOP_LOGPROBS}\")\n",
    "print()\n",
    "\n",
    "print(\"GPU Info:\")\n",
    "print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  Available: {torch.cuda.device_count()} GPUs\")\n",
    "print()\n",
    "\n",
    "print(\"Loading model with vLLM...\")\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    trust_remote_code=True,\n",
    "    download_dir='/tmp/hf_cache',\n",
    "    dtype='bfloat16',\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    enable_prefix_caching=False,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    disable_log_stats=True\n",
    ")\n",
    "print(\"Model loaded successfully!\")\n",
    "print()\n",
    "\n",
    "# Sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    "    logprobs=TOP_LOGPROBS,\n",
    "    prompt_logprobs=None\n",
    ")\n",
    "\n",
    "# Warmup run\n",
    "print(\"Running warmup...\")\n",
    "warmup_output = llm.generate([PROMPT], sampling_params=sampling_params)\n",
    "print(\"Warmup complete\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TEST RUNS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Running {NUM_REPETITIONS} test repetitions\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "results_tokens = []\n",
    "results_logprobs = []\n",
    "results_texts = []\n",
    "results_distributions = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Repetition {rep + 1}/{NUM_REPETITIONS}...\")\n",
    "    \n",
    "    outputs = llm.generate([PROMPT], sampling_params=sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract token IDs\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    # Extract generated text\n",
    "    text = output.outputs[0].text\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    # Extract logprobs for selected tokens\n",
    "    logprobs_data = output.outputs[0].logprobs\n",
    "    selected_logprobs = [lp[token_ids[i]].logprob for i, lp in enumerate(logprobs_data)]\n",
    "    results_logprobs.append(np.array(selected_logprobs))\n",
    "    \n",
    "    # Extract full top-k distributions\n",
    "    rep_distributions = []\n",
    "    for position_logprobs in logprobs_data:\n",
    "        # Get top-k sorted by logprob (descending)\n",
    "        sorted_items = sorted(position_logprobs.items(), \n",
    "                            key=lambda x: x[1].logprob, \n",
    "                            reverse=True)[:TOP_LOGPROBS]\n",
    "        rep_distributions.append([(tok, lp.logprob) for tok, lp in sorted_items])\n",
    "    results_distributions.append(rep_distributions)\n",
    "    \n",
    "    print(f\"  Generated {len(token_ids)} tokens\")\n",
    "\n",
    "print()\n",
    "print(\"All repetitions complete!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check token sequence identity\n",
    "print(\"Checking token sequences...\")\n",
    "tokens_identical = all(\n",
    "    results_tokens[0] == results_tokens[i] \n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "if not tokens_identical:\n",
    "    print(\"\\nâš  Token sequences differ!\")\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[0] != results_tokens[i]:\n",
    "            diff_positions = [\n",
    "                j for j in range(min(len(results_tokens[0]), len(results_tokens[i])))\n",
    "                if results_tokens[0][j] != results_tokens[i][j]\n",
    "            ]\n",
    "            print(f\"  Rep 0 vs Rep {i}: {len(diff_positions)} positions differ\")\n",
    "            if diff_positions:\n",
    "                print(f\"    First difference at position {diff_positions[0]}\")\n",
    "\n",
    "# Check logprobs for selected tokens\n",
    "print(\"\\nChecking selected token logprobs...\")\n",
    "first_logprobs = results_logprobs[0]\n",
    "logprobs_exact = all(\n",
    "    np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Selected token logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "# Check top-k distributions\n",
    "print(\"\\nChecking full top-k distributions...\")\n",
    "distributions_exact = True\n",
    "distribution_mismatches = []\n",
    "\n",
    "first_dist = results_distributions[0]\n",
    "for rep_idx in range(1, NUM_REPETITIONS):\n",
    "    for pos_idx in range(len(first_dist)):\n",
    "        dist_a = first_dist[pos_idx]\n",
    "        dist_b = results_distributions[rep_idx][pos_idx]\n",
    "        \n",
    "        # Check if token IDs match in same order\n",
    "        tokens_match = [t[0] for t in dist_a] == [t[0] for t in dist_b]\n",
    "        \n",
    "        # Check if logprobs are bit-exact\n",
    "        if tokens_match:\n",
    "            logprobs_match = all(\n",
    "                abs(dist_a[i][1] - dist_b[i][1]) < 1e-10 \n",
    "                for i in range(len(dist_a))\n",
    "            )\n",
    "            if not logprobs_match:\n",
    "                distributions_exact = False\n",
    "                distribution_mismatches.append((rep_idx, pos_idx))\n",
    "        else:\n",
    "            distributions_exact = False\n",
    "            distribution_mismatches.append((rep_idx, pos_idx))\n",
    "\n",
    "print(f\"Top-k distributions bit-exact: {distributions_exact}\")\n",
    "\n",
    "if not distributions_exact:\n",
    "    print(f\"\\nâš  Found {len(distribution_mismatches)} position mismatches in distributions\")\n",
    "    if len(distribution_mismatches) <= 5:\n",
    "        for rep_idx, pos_idx in distribution_mismatches:\n",
    "            print(f\"  Rep 0 vs Rep {rep_idx}, position {pos_idx}\")\n",
    "    else:\n",
    "        print(f\"  First 5: {distribution_mismatches[:5]}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "    \n",
    "    # Element-wise statistics\n",
    "    all_logprobs = np.array(results_logprobs)\n",
    "    std_per_token = all_logprobs.std(axis=0)\n",
    "    print(f\"\\nPer-token std statistics:\")\n",
    "    print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "    print(f\"  Max: {std_per_token.max():.6e}\")\n",
    "    print(f\"  Median: {np.median(std_per_token):.6e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact and distributions_exact:\n",
    "    print(\"âœ“ PERFECT REPRODUCIBILITY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Full top-k distributions: bit-exact\")\n",
    "    print(\"  - vLLM is deterministic for this config\")\n",
    "elif tokens_identical and logprobs_exact and not distributions_exact:\n",
    "    print(\"âš  SELECTED TOKENS EXACT, DISTRIBUTIONS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Top-k distributions: numerical variation detected\")\n",
    "    print(\"  â†’ May indicate computational instability in non-selected paths\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"âš  TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: small numerical variation\")\n",
    "    max_l2 = max(l2_distances) if not logprobs_exact else 0.0\n",
    "    if max_l2 < 1e-6:\n",
    "        print(f\"  - Variation very small (L2={max_l2:.2e})\")\n",
    "        print(\"  â†’ Likely acceptable for forensics\")\n",
    "    else:\n",
    "        print(f\"  - Variation notable (L2={max_l2:.2e})\")\n",
    "        print(\"  â†’ Investigate noise source\")\n",
    "else:\n",
    "    print(\"âœ— TOKEN SEQUENCES DIFFER\")\n",
    "    print(\"  - This should NOT happen with temperature=0\")\n",
    "    print(\"  â†’ Something is wrong, investigate\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"vllm_kimi_k2_logprobs_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"prompt_source\": \"file\" if prompt_file else \"hardcoded\",\n",
    "    \"prompt_file\": os.path.basename(prompt_file) if prompt_file else None,\n",
    "    \"prompt_length_chars\": len(PROMPT),\n",
    "    \"prompt_length_tokens\": prompt_length,\n",
    "    \"chat_template\": \"kimi_k2_official\",\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_model_len\": MAX_MODEL_LEN,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"warmup_enabled\": True,\n",
    "        \"prefix_caching_disabled\": True,\n",
    "        \"top_logprobs\": TOP_LOGPROBS\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"distributions_exact\": distributions_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact and distributions_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts,\n",
    "    \"top_k_distributions\": [\n",
    "        [[(int(tok), float(prob)) for tok, prob in dist] for dist in rep_dists]\n",
    "        for rep_dists in results_distributions\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_file = f\"vllm_kimi_k2_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560d1db-dac3-4403-9170-b8d0d8d5ab35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
