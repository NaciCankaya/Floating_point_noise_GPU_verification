{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5ba75-c5d0-4434-9ca8-49b4ba515fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "vLLM LOGPROBS EXTRACTION TEST - GLM-4.6-FP8\n",
      "================================================================================\n",
      "\n",
      "Found file: Verification-for-International-AI-Governance.pdf\n",
      "Loading 172 pages from PDF...\n",
      "  Processed 10/172 pages\n",
      "  Processed 20/172 pages\n",
      "  Processed 30/172 pages\n",
      "  Processed 40/172 pages\n",
      "  Processed 50/172 pages\n",
      "  Processed 60/172 pages\n",
      "  Processed 70/172 pages\n",
      "  Processed 80/172 pages\n",
      "  Processed 90/172 pages\n",
      "  Processed 100/172 pages\n",
      "  Processed 110/172 pages\n",
      "  Processed 120/172 pages\n",
      "  Processed 130/172 pages\n",
      "  Processed 140/172 pages\n",
      "  Processed 150/172 pages\n",
      "  Processed 160/172 pages\n",
      "  Processed 170/172 pages\n",
      "Loaded 535619 characters from PDF (172 pages)\n",
      "\n",
      "Using standard OpenAI-compatible message format\n",
      "Message content length: 535677 characters\n",
      "Note: Chat template will be applied to convert messages to string prompt\n",
      "\n",
      "Loading tokenizer to validate prompt length...\n",
      "Chat template applied successfully\n",
      "\n",
      "Prompt statistics:\n",
      "  Characters: 535,711\n",
      "  Tokens: 116,210\n",
      "  Max model length: 120,000\n",
      "  Generation tokens: 20\n",
      "  Total required: 116,230\n",
      "\n",
      "âœ“ Prompt length validation passed\n",
      "  Remaining capacity: 3,770 tokens\n",
      "\n",
      "Configuration:\n",
      "  Model: zai-org/GLM-4.6-FP8\n",
      "  Tensor parallel: 4\n",
      "  Max model len: 120,000\n",
      "  Max tokens: 20\n",
      "  Temperature: 0.0\n",
      "  Seed: 42\n",
      "  Repetitions: 20\n",
      "\n",
      "Loading model...\n",
      "Note: vLLM will automatically detect FP8 quantization config\n",
      "\n",
      "INFO 11-10 13:49:05 [utils.py:233] non-default args: {'trust_remote_code': True, 'seed': 42, 'max_model_len': 120000, 'tensor_parallel_size': 4, 'enable_prefix_caching': False, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'zai-org/GLM-4.6-FP8'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-10 13:49:05 [model.py:547] Resolved architecture: Glm4MoeForCausalLM\n",
      "INFO 11-10 13:49:05 [model.py:1510] Using max model len 120000\n",
      "INFO 11-10 13:49:05 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 11-10 13:49:05 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:06 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:06 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='zai-org/GLM-4.6-FP8', speculative_config=None, tokenizer='zai-org/GLM-4.6-FP8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=120000, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=zai-org/GLM-4.6-FP8, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m WARNING 11-10 13:49:06 [multiproc_executor.py:720] Reducing Torch parallelism from 96 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_909ad6bb'), local_subscribe_addr='ipc:///tmp/de071937-bf21-4c7c-9d0b-f1f6382066e8', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_19830b2a'), local_subscribe_addr='ipc:///tmp/18c6e4cc-9a7f-4dd1-965b-6cd88ccd42c4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_20e70113'), local_subscribe_addr='ipc:///tmp/4d3e4191-5285-4bb2-b17a-5d8bf46a213d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_019047d5'), local_subscribe_addr='ipc:///tmp/81e8ffd0-0b3e-4780-97a7-bdc0f1a65727', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:07 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d861af6f'), local_subscribe_addr='ipc:///tmp/85faf581-5ad9-4114-bbdc-c227c36983ab', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 13:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 13:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 13:49:09 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 13:49:09 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:10 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:10 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 13:49:10 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-10 13:49:10 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_e87ad5f9'), local_subscribe_addr='ipc:///tmp/d1fff76c-a68c-4e39-a5cd-912d867d8128', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:10 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 13:49:10 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 13:49:10 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:10 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-10 13:49:10 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 13:49:10 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-10 13:49:10 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:10 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank [Gloo] Rank 0 is connected to 00 is connected to  peer ranks. 0Expected number of connected peer ranks is :  peer ranks. 0Expected number of connected peer ranks is : 0\n",
      "\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:11 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 11-10 13:49:11 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-10 13:49:11 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 11-10 13:49:11 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:12 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:12 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "INFO 11-10 13:49:12 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:12 [topk_topp_sampler.py:55] Using FlashInfer for top-p & top-k sampling.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m INFO 11-10 13:49:12 [gpu_model_runner.py:2602] Starting to load model zai-org/GLM-4.6-FP8...\n",
      "INFO 11-10 13:49:12 [gpu_model_runner.py:2602] Starting to load model zai-org/GLM-4.6-FP8...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m INFO 11-10 13:49:12 [gpu_model_runner.py:2602] Starting to load model zai-org/GLM-4.6-FP8...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m INFO 11-10 13:49:12 [gpu_model_runner.py:2602] Starting to load model zai-org/GLM-4.6-FP8...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m INFO 11-10 13:49:13 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m INFO 11-10 13:49:13 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m INFO 11-10 13:49:13 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m INFO 11-10 13:49:13 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "INFO 11-10 13:49:13 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m INFO 11-10 13:49:13 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m INFO 11-10 13:49:13 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m INFO 11-10 13:49:13 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m INFO 11-10 13:49:13 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "INFO 11-10 13:49:13 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m INFO 11-10 13:49:14 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m INFO 11-10 13:49:14 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m INFO 11-10 13:49:14 [weight_utils.py:413] Time spent downloading weights for zai-org/GLM-4.6-FP8: 0.854898 seconds\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb995e6541e480f84f14fa47909b120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/93 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m INFO 11-10 13:49:50 [default_loader.py:267] Loading weights took 34.51 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m INFO 11-10 13:49:50 [gpu_model_runner.py:2653] Model loading took 83.1971 GiB and 36.967564 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:49:54 [default_loader.py:267] Loading weights took 39.88 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m INFO 11-10 13:49:54 [default_loader.py:267] Loading weights took 39.99 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m INFO 11-10 13:49:54 [gpu_model_runner.py:2653] Model loading took 83.1971 GiB and 41.146638 seconds\n",
      "INFO 11-10 13:49:54 [gpu_model_runner.py:2653] Model loading took 83.1971 GiB and 41.134867 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m INFO 11-10 13:49:56 [default_loader.py:267] Loading weights took 41.61 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m INFO 11-10 13:49:57 [gpu_model_runner.py:2653] Model loading took 83.1971 GiB and 43.685984 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m INFO 11-10 13:50:02 [gpu_worker.py:298] Available KV cache memory: 37.24 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m INFO 11-10 13:50:02 [gpu_worker.py:298] Available KV cache memory: 37.24 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m INFO 11-10 13:50:02 [gpu_worker.py:298] Available KV cache memory: 37.24 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m INFO 11-10 13:50:02 [gpu_worker.py:298] Available KV cache memory: 37.24 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:02 [kv_cache_utils.py:1087] GPU KV cache size: 424,384 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:02 [kv_cache_utils.py:1091] Maximum concurrency for 120,000 tokens per request: 3.54x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:02 [kv_cache_utils.py:1087] GPU KV cache size: 424,384 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:02 [kv_cache_utils.py:1091] Maximum concurrency for 120,000 tokens per request: 3.54x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:02 [kv_cache_utils.py:1087] GPU KV cache size: 424,384 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:02 [kv_cache_utils.py:1091] Maximum concurrency for 120,000 tokens per request: 3.54x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:02 [kv_cache_utils.py:1087] GPU KV cache size: 424,384 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:02 [kv_cache_utils.py:1091] Maximum concurrency for 120,000 tokens per request: 3.54x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m 2025-11-10 13:50:02,907 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2025-11-10 13:50:02,907 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2025-11-10 13:50:02,907 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2025-11-10 13:50:02,907 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m 2025-11-10 13:50:03,723 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "2025-11-10 13:50:03,723 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "2025-11-10 13:50:03,723 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "2025-11-10 13:50:03,723 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=25422)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m WARNING 11-10 13:50:03 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "\u001b[1;36m(Worker_TP3 pid=25426)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=25420)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=25424)\u001b[0;0m WARNING 11-10 13:50:03 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "WARNING 11-10 13:50:03 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "WARNING 11-10 13:50:03 [cudagraph_dispatcher.py:106] cudagraph dispatching keys are not initialized. No cudagraph will be used.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:04 [core.py:210] init engine (profile, create kv cache, warmup model) took 6.58 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=25398)\u001b[0;0m INFO 11-10 13:50:05 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "INFO 11-10 13:50:05 [llm.py:306] Supported_tasks: ['generate']\n",
      "Model loaded successfully!\n",
      "\n",
      "Sampling parameters:\n",
      "  Temperature: 0.0\n",
      "  Max tokens: 20\n",
      "  Seed: 42\n",
      "  Top logprobs: 10\n",
      "\n",
      "Running warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e518a21728704ef0980fac13b2357471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9838bfc2b84642a8a198b2dc75e0c3af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup complete - generated 20 tokens\n",
      "\n",
      "================================================================================\n",
      "RUNNING EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Repetition 1/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68d5ef2b6c1a48c9b56a7779ae83714f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722317e6d3e34d09a91cbf20c2c8de49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 2/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554ee176892c45fca6808202a6ddd70a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c85b8c9a3919413d9fd1e1f56ae54430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 3/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7972fb11f94dc7a80d8b8ccc369943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f181949cdada4f0e8c7a940661fbcce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 4/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40efbf85c2a4707844d1cc03a5e3904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d379b38b7023482aa8caec3aa7288727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 5/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22c54de42d14af5822242ba439af1c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51d3099b50c642d69502035367ecf1e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 6/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cb4b21ebf154a2faf7392ee93d50df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7513d55d54754f7aa48011eebde189fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 7/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "342113d84f6c4e6499b96240c5dcb642",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d73ca2b392445aa954017c75132386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 8/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96fa881ab7b7432fad83b5101a3f7a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da7803c90a8642c0ac775e081f15c55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 9/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d32a98a48334e74b2c683f2dc36dc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa96665217134cf393fb98cffa510ad7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 10/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2edf4ef209d41e78d3041f6b4002ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8531f2a229d74e4c8b04401d91df29b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 11/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c460bf9b1ed4584bc750a693448192d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a7732877bd4d97a5bb45d8a21585bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 12/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9c7e061c4a4950b023df95f80cc964",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1442d37fe2b04f30a95333d322ff8d36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 13/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24af22f0126740fcbbed30d9a3842d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ffaf9a6ba146ee94a6c13e2c497b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 14/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea69a3deb7f4d8786a3021979309457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93f2f50a0704e0b8af920c6bd1e9945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 15/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7ae473b93d4f8a8fd45baa603fd78f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e44d35f27de45cc95098f0a5ff91be4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 16/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f133d52189e14b4fbfecdc405f3678ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522e91632c2c467daa66cf6d85999c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 17/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6f454576e24413b6b821cb934a40eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fb73cafec841b0831c2a5d5443bcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 20 tokens\n",
      "Repetition 18/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4f0aef1e2d84cb5bd94e3510e83d403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d15f2ad704114e478d3206afee17d2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM Tensor Parallelism Test - GLM-4.6-FP8\n",
    "Tests bit-exact reproducibility across multiple runs with TP\n",
    "Automatically finds txt/pdf files in current directory\n",
    "Uses standard OpenAI-compatible message format\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"zai-org/GLM-4.6-FP8\"\n",
    "TENSOR_PARALLEL_SIZE = 4\n",
    "MAX_MODEL_LEN = 120000\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 20\n",
    "NUM_REPETITIONS = 20\n",
    "TEMPERATURE = 0.0  # Greedy decoding\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Prompt source - finds first txt or pdf in current directory\n",
    "AUTO_FIND_FILE = True  # Set to False to use hardcoded content\n",
    "\n",
    "# User task\n",
    "USER_TASK = \"Please provide a detailed summary of the following text.\"\n",
    "\n",
    "# Hardcoded content (used if AUTO_FIND_FILE=False or no files found)\n",
    "HARDCODED_CONTENT = \"\"\"The development of large language models has fundamentally transformed natural language processing and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FILE LOADING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def find_prompt_file():\n",
    "    \"\"\"Find first txt or pdf file in current directory\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # Look for txt files first, then pdf\n",
    "    txt_files = glob.glob(os.path.join(cwd, \"*.txt\"))\n",
    "    pdf_files = glob.glob(os.path.join(cwd, \"*.pdf\"))\n",
    "    \n",
    "    if txt_files:\n",
    "        return txt_files[0]\n",
    "    elif pdf_files:\n",
    "        return pdf_files[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_text_from_file(filepath):\n",
    "    \"\"\"Load text from txt or pdf file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    if filepath.endswith('.txt'):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print(f\"Loaded {len(text)} characters from txt file\")\n",
    "        return text\n",
    "    \n",
    "    elif filepath.endswith('.pdf'):\n",
    "        try:\n",
    "            import PyPDF2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"PyPDF2 required for PDF loading. Install with: pip install PyPDF2\")\n",
    "        \n",
    "        text = []\n",
    "        with open(filepath, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Loading {num_pages} pages from PDF...\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text.append(page_text)\n",
    "                if page_num % 10 == 0:\n",
    "                    print(f\"  Processed {page_num}/{num_pages} pages\")\n",
    "        \n",
    "        full_text = '\\n'.join(text)\n",
    "        print(f\"Loaded {len(full_text)} characters from PDF ({num_pages} pages)\")\n",
    "        return full_text\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath}. Use .txt or .pdf\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"vLLM LOGPROBS EXTRACTION TEST - GLM-4.6-FP8\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load document content\n",
    "prompt_file = None\n",
    "if AUTO_FIND_FILE:\n",
    "    prompt_file = find_prompt_file()\n",
    "    if prompt_file:\n",
    "        print(f\"Found file: {os.path.basename(prompt_file)}\")\n",
    "        DOCUMENT_CONTENT = load_text_from_file(prompt_file)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No txt/pdf files found in current directory\")\n",
    "        print(\"Using hardcoded content\")\n",
    "        DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "        print()\n",
    "else:\n",
    "    DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "    print(\"Using hardcoded content\")\n",
    "    print()\n",
    "\n",
    "# Prepare messages in standard OpenAI format\n",
    "# vLLM's tokenizer will apply the model's chat template to convert this to a string\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{USER_TASK}\\n\\n{DOCUMENT_CONTENT}\"}\n",
    "]\n",
    "\n",
    "print(\"Using standard OpenAI-compatible message format\")\n",
    "print(f\"Message content length: {len(messages[0]['content'])} characters\")\n",
    "print(\"Note: Chat template will be applied to convert messages to string prompt\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER PRECHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading tokenizer to validate prompt length...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir='/tmp/hf_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply chat template to convert messages to string prompt\n",
    "# This is the actual prompt that will be used for generation\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Chat template applied successfully\")\n",
    "print()\n",
    "\n",
    "# Tokenize prompt\n",
    "prompt_tokens = tokenizer.encode(prompt_text)\n",
    "prompt_length = len(prompt_tokens)\n",
    "\n",
    "print(f\"Prompt statistics:\")\n",
    "print(f\"  Characters: {len(prompt_text):,}\")\n",
    "print(f\"  Tokens: {prompt_length:,}\")\n",
    "print(f\"  Max model length: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Generation tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Total required: {prompt_length + MAX_TOKENS:,}\")\n",
    "print()\n",
    "\n",
    "# Validate length\n",
    "if prompt_length > MAX_MODEL_LEN:\n",
    "    print(f\"âŒ ERROR: Prompt is too long!\")\n",
    "    print(f\"  Prompt has {prompt_length:,} tokens\")\n",
    "    print(f\"  Model max is {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Exceeds by {prompt_length - MAX_MODEL_LEN:,} tokens\")\n",
    "    print()\n",
    "    print(\"Solutions:\")\n",
    "    print(f\"  1. Increase MAX_MODEL_LEN to at least {prompt_length + MAX_TOKENS}\")\n",
    "    print(f\"  2. Truncate/reduce the prompt\")\n",
    "    exit(1)\n",
    "\n",
    "if prompt_length + MAX_TOKENS > MAX_MODEL_LEN:\n",
    "    print(f\"âš ï¸  WARNING: Prompt + generation may exceed context\")\n",
    "    print(f\"  Prompt: {prompt_length:,} tokens\")\n",
    "    print(f\"  Generation: {MAX_TOKENS} tokens\")\n",
    "    print(f\"  Total: {prompt_length + MAX_TOKENS:,} tokens\")\n",
    "    print(f\"  Model max: {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Consider increasing MAX_MODEL_LEN to {prompt_length + MAX_TOKENS + 100}\")\n",
    "    print()\n",
    "else:\n",
    "    print(f\"âœ“ Prompt length validation passed\")\n",
    "    print(f\"  Remaining capacity: {MAX_MODEL_LEN - prompt_length - MAX_TOKENS:,} tokens\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print()\n",
    "\n",
    "print(\"Loading model...\")\n",
    "print(\"Note: vLLM will automatically detect FP8 quantization config\")\n",
    "print()\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    trust_remote_code=True,\n",
    "    seed=SEED,\n",
    "    enforce_eager=True,  # Disable cudagraph for determinism\n",
    "    enable_prefix_caching=False  # Disable prefix caching for clean experiment\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    "    logprobs=TOP_LOGPROBS,\n",
    "    skip_special_tokens=False\n",
    ")\n",
    "\n",
    "print(\"Sampling parameters:\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Top logprobs: {TOP_LOGPROBS}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# WARMUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Running warmup...\")\n",
    "# vLLM's LLM.generate() expects string prompts, not message dicts\n",
    "# The chat template was already applied above during validation\n",
    "warmup_output = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "print(f\"Warmup complete - generated {len(warmup_output[0].outputs[0].token_ids)} tokens\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "results_tokens = []\n",
    "results_logprobs = []\n",
    "results_texts = []\n",
    "results_distributions = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Repetition {rep + 1}/{NUM_REPETITIONS}...\")\n",
    "    \n",
    "    # Use the string prompt (chat template already applied)\n",
    "    outputs = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract token IDs\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    # Extract generated text\n",
    "    text = output.outputs[0].text\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    # Extract logprobs for selected tokens\n",
    "    logprobs_data = output.outputs[0].logprobs\n",
    "    selected_logprobs = [lp[token_ids[i]].logprob for i, lp in enumerate(logprobs_data)]\n",
    "    results_logprobs.append(np.array(selected_logprobs))\n",
    "    \n",
    "    # Extract full top-k distributions\n",
    "    rep_distributions = []\n",
    "    for position_logprobs in logprobs_data:\n",
    "        # Get top-k sorted by logprob (descending)\n",
    "        sorted_items = sorted(position_logprobs.items(), \n",
    "                            key=lambda x: x[1].logprob, \n",
    "                            reverse=True)[:TOP_LOGPROBS]\n",
    "        rep_distributions.append([(tok, lp.logprob) for tok, lp in sorted_items])\n",
    "    results_distributions.append(rep_distributions)\n",
    "    \n",
    "    print(f\"  Generated {len(token_ids)} tokens\")\n",
    "\n",
    "print()\n",
    "print(\"All repetitions complete!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check token sequence identity\n",
    "print(\"Checking token sequences...\")\n",
    "tokens_identical = all(\n",
    "    results_tokens[0] == results_tokens[i] \n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "if not tokens_identical:\n",
    "    print(\"\\nâš ï¸  Token sequences differ!\")\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[0] != results_tokens[i]:\n",
    "            diff_positions = [\n",
    "                j for j in range(min(len(results_tokens[0]), len(results_tokens[i])))\n",
    "                if results_tokens[0][j] != results_tokens[i][j]\n",
    "            ]\n",
    "            print(f\"  Rep 0 vs Rep {i}: {len(diff_positions)} positions differ\")\n",
    "            if diff_positions:\n",
    "                print(f\"    First difference at position {diff_positions[0]}\")\n",
    "\n",
    "# Check logprobs for selected tokens\n",
    "print(\"\\nChecking selected token logprobs...\")\n",
    "first_logprobs = results_logprobs[0]\n",
    "logprobs_exact = all(\n",
    "    np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Selected token logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "# Check top-k distributions\n",
    "print(\"\\nChecking full top-k distributions...\")\n",
    "distributions_exact = True\n",
    "distribution_mismatches = []\n",
    "\n",
    "first_dist = results_distributions[0]\n",
    "for rep_idx in range(1, NUM_REPETITIONS):\n",
    "    for pos_idx in range(len(first_dist)):\n",
    "        dist_a = first_dist[pos_idx]\n",
    "        dist_b = results_distributions[rep_idx][pos_idx]\n",
    "        \n",
    "        # Check if token IDs match in same order\n",
    "        tokens_match = [t[0] for t in dist_a] == [t[0] for t in dist_b]\n",
    "        \n",
    "        # Check if logprobs are bit-exact\n",
    "        if tokens_match:\n",
    "            logprobs_match = all(\n",
    "                abs(dist_a[i][1] - dist_b[i][1]) < 1e-10 \n",
    "                for i in range(len(dist_a))\n",
    "            )\n",
    "            if not logprobs_match:\n",
    "                distributions_exact = False\n",
    "                distribution_mismatches.append((rep_idx, pos_idx))\n",
    "        else:\n",
    "            distributions_exact = False\n",
    "            distribution_mismatches.append((rep_idx, pos_idx))\n",
    "\n",
    "print(f\"Top-k distributions bit-exact: {distributions_exact}\")\n",
    "\n",
    "if not distributions_exact:\n",
    "    print(f\"\\nâš ï¸  Found {len(distribution_mismatches)} position mismatches in distributions\")\n",
    "    if len(distribution_mismatches) <= 5:\n",
    "        for rep_idx, pos_idx in distribution_mismatches:\n",
    "            print(f\"  Rep 0 vs Rep {rep_idx}, position {pos_idx}\")\n",
    "    else:\n",
    "        print(f\"  First 5: {distribution_mismatches[:5]}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "    \n",
    "    # Element-wise statistics\n",
    "    all_logprobs = np.array(results_logprobs)\n",
    "    std_per_token = all_logprobs.std(axis=0)\n",
    "    print(f\"\\nPer-token std statistics:\")\n",
    "    print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "    print(f\"  Max: {std_per_token.max():.6e}\")\n",
    "    print(f\"  Median: {np.median(std_per_token):.6e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact and distributions_exact:\n",
    "    print(\"âœ“ PERFECT REPRODUCIBILITY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Full top-k distributions: bit-exact\")\n",
    "    print(\"  - vLLM + GLM-4.6-FP8 is deterministic for this config\")\n",
    "elif tokens_identical and logprobs_exact and not distributions_exact:\n",
    "    print(\"âš ï¸  SELECTED TOKENS EXACT, DISTRIBUTIONS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Top-k distributions: numerical variation detected\")\n",
    "    print(\"  â†’ May indicate computational instability in non-selected paths\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"âš ï¸  TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: small numerical variation\")\n",
    "    max_l2 = max(l2_distances) if not logprobs_exact else 0.0\n",
    "    if max_l2 < 1e-6:\n",
    "        print(f\"  - Variation very small (L2={max_l2:.2e})\")\n",
    "        print(\"  â†’ Likely acceptable for forensics\")\n",
    "    else:\n",
    "        print(f\"  - Variation notable (L2={max_l2:.2e})\")\n",
    "        print(\"  â†’ Investigate noise source\")\n",
    "else:\n",
    "    print(\"âŒ TOKEN SEQUENCES DIFFER\")\n",
    "    print(\"  - This should NOT happen with temperature=0\")\n",
    "    print(\"  â†’ Something is wrong, investigate\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"vllm_glm46_fp8_logprobs_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"prompt_source\": \"file\" if prompt_file else \"hardcoded\",\n",
    "    \"prompt_file\": os.path.basename(prompt_file) if prompt_file else None,\n",
    "    \"prompt_text\": prompt_text,  # Save actual prompt for reproducibility\n",
    "    \"prompt_length_chars\": len(prompt_text),\n",
    "    \"prompt_length_tokens\": prompt_length,\n",
    "    \"message_format\": \"openai_compatible\",\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_model_len\": MAX_MODEL_LEN,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"warmup_enabled\": True,\n",
    "        \"prefix_caching_disabled\": True,\n",
    "        \"enforce_eager\": True,\n",
    "        \"top_logprobs\": TOP_LOGPROBS\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"distributions_exact\": distributions_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact and distributions_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts,\n",
    "    \"top_k_distributions\": [\n",
    "        [[(int(tok), float(prob)) for tok, prob in dist] for dist in rep_dists]\n",
    "        for rep_dists in results_distributions\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_file = f\"vllm_glm46_fp8_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c6da3-98d5-40d2-93d7-6ffdc32f3a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
