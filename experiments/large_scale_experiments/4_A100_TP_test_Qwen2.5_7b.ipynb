{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1442768f-adcc-47c3-8cb0-276ad4e0c548",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 20:41:40 [__init__.py:216] Automatically detected platform cuda.\n",
      "================================================================================\n",
      "vLLM LOGPROBS EXTRACTION TEST\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Tensor parallel: 4\n",
      "  Max model len: 8048\n",
      "  GPU memory utilization: 0.8 (80%)\n",
      "  Max tokens: 20\n",
      "  Repetitions: 5\n",
      "  Temperature: 0.0 (greedy)\n",
      "\n",
      "GPU Info:\n",
      "  Device: NVIDIA A100-SXM4-80GB\n",
      "  Available: 4 GPUs\n",
      "\n",
      "Loading model with vLLM...\n",
      "INFO 11-09 20:41:44 [utils.py:233] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'max_model_len': 8048, 'tensor_parallel_size': 4, 'enable_prefix_caching': False, 'gpu_memory_utilization': 0.8, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-7B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5f3084b042c4b5d98f4e67e70b8e0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 20:41:53 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 20:41:53 [model.py:1510] Using max model len 8048\n",
      "INFO 11-09 20:41:56 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6140cf0c5aff4ab7ba8a026efb603271",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fe318d9fe2490b9dd20d66117d8c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8f2e08db8b49e5ac6aed340bd1f878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc676166a054a9c80018d4b3974d0ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7731f2e825e94f678a889d1e7265f320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-09 20:41:58 [__init__.py:3036] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 20:42:02 [__init__.py:216] Automatically detected platform cuda.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1359)\u001b[0;0m INFO 11-09 20:42:03 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1359)\u001b[0;0m INFO 11-09 20:42:03 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8048, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1359)\u001b[0;0m WARNING 11-09 20:42:03 [multiproc_executor.py:720] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1359)\u001b[0;0m INFO 11-09 20:42:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_a7ca02c1'), local_subscribe_addr='ipc:///tmp/3156bf7f-6721-49f1-aa4f-ca08f8e7703e', remote_subscribe_addr=None, remote_addr_ipv6=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-09 20:42:07 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-09 20:42:07 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-09 20:42:07 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-09 20:42:07 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 11-09 20:42:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_8735dea1'), local_subscribe_addr='ipc:///tmp/25bff50d-102f-4a83-b5d9-6c057738ca20', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-09 20:42:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_02698cc8'), local_subscribe_addr='ipc:///tmp/2a291431-5a0e-4da4-8ce8-0dc51e3d89d1', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-09 20:42:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_0884d325'), local_subscribe_addr='ipc:///tmp/c71c781f-fe19-4094-ad9b-fc37b6c861a3', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "INFO 11-09 20:42:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9ebd9d69'), local_subscribe_addr='ipc:///tmp/737b01e2-cfb0-4180-b310-3c54e9fd8ba0', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 11-09 20:42:12 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 20:42:12 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 20:42:12 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 20:42:12 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 20:42:12 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 20:42:12 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 20:42:12 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 20:42:12 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "WARNING 11-09 20:42:13 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-09 20:42:13 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-09 20:42:13 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-09 20:42:13 [symm_mem.py:58] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "INFO 11-09 20:42:13 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-09 20:42:13 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-09 20:42:13 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-09 20:42:13 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.\n",
      "INFO 11-09 20:42:13 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_56546d6a'), local_subscribe_addr='ipc:///tmp/3759d008-6423-4eb1-9378-956a17a4c6f7', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 11-09 20:42:13 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 20:42:13 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 20:42:13 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 20:42:13 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 20:42:13 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 20:42:13 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 20:42:13 [__init__.py:1384] Found nccl from library libnccl.so.2\n",
      "INFO 11-09 20:42:13 [pynccl.py:103] vLLM is using nccl==2.27.3\n",
      "INFO 11-09 20:42:13 [parallel_state.py:1208] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 11-09 20:42:13 [parallel_state.py:1208] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 11-09 20:42:13 [parallel_state.py:1208] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 11-09 20:42:13 [parallel_state.py:1208] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 11-09 20:42:14 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-09 20:42:14 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-09 20:42:14 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:42:14 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "\u001b[1;36m(Worker_TP1 pid=1494)\u001b[0;0m INFO 11-09 20:42:14 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "WARNING 11-09 20:42:14 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(Worker_TP2 pid=1495)\u001b[0;0m INFO 11-09 20:42:14 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m INFO 11-09 20:42:14 [gpu_model_runner.py:2602] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:42:14 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP1 pid=1494)\u001b[0;0m INFO 11-09 20:42:14 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP2 pid=1495)\u001b[0;0m INFO 11-09 20:42:14 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:42:14 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m INFO 11-09 20:42:14 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(Worker_TP1 pid=1494)\u001b[0;0m INFO 11-09 20:42:14 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP2 pid=1495)\u001b[0;0m INFO 11-09 20:42:14 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m INFO 11-09 20:42:14 [cuda.py:366] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:42:14 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP1 pid=1494)\u001b[0;0m INFO 11-09 20:42:14 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m INFO 11-09 20:42:14 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP2 pid=1495)\u001b[0;0m INFO 11-09 20:42:14 [weight_utils.py:392] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:44:27 [weight_utils.py:413] Time spent downloading weights for Qwen/Qwen2.5-7B-Instruct: 133.061430 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.10it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.49it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  2.32it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  2.40it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m INFO 11-09 20:44:29 [default_loader.py:267] Loading weights took 1.72 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:44:29 [default_loader.py:267] Loading weights took 2.03 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=1494)\u001b[0;0m INFO 11-09 20:44:30 [default_loader.py:267] Loading weights took 1.73 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m INFO 11-09 20:44:30 [gpu_model_runner.py:2653] Model loading took 3.5547 GiB and 135.488367 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=1495)\u001b[0;0m INFO 11-09 20:44:30 [default_loader.py:267] Loading weights took 1.89 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:44:30 [gpu_model_runner.py:2653] Model loading took 3.5547 GiB and 135.660971 seconds\n",
      "\u001b[1;36m(Worker_TP1 pid=1494)\u001b[0;0m INFO 11-09 20:44:30 [gpu_model_runner.py:2653] Model loading took 3.5547 GiB and 136.016446 seconds\n",
      "\u001b[1;36m(Worker_TP2 pid=1495)\u001b[0;0m INFO 11-09 20:44:30 [gpu_model_runner.py:2653] Model loading took 3.5547 GiB and 136.018848 seconds\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:44:35 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/cbc9cfe1cb/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP1 pid=1494)\u001b[0;0m INFO 11-09 20:44:35 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/cbc9cfe1cb/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP2 pid=1495)\u001b[0;0m INFO 11-09 20:44:35 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/cbc9cfe1cb/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:44:35 [backends.py:559] Dynamo bytecode transform time: 4.95 s\n",
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m INFO 11-09 20:44:35 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/cbc9cfe1cb/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(Worker_TP1 pid=1494)\u001b[0;0m INFO 11-09 20:44:35 [backends.py:559] Dynamo bytecode transform time: 4.95 s\n",
      "\u001b[1;36m(Worker_TP2 pid=1495)\u001b[0;0m INFO 11-09 20:44:35 [backends.py:559] Dynamo bytecode transform time: 4.96 s\n",
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m INFO 11-09 20:44:35 [backends.py:559] Dynamo bytecode transform time: 4.95 s\n",
      "\u001b[1;36m(Worker_TP3 pid=1496)\u001b[0;0m INFO 11-09 20:44:39 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP2 pid=1495)\u001b[0;0m INFO 11-09 20:44:39 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP1 pid=1494)\u001b[0;0m INFO 11-09 20:44:39 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(Worker_TP0 pid=1493)\u001b[0;0m INFO 11-09 20:44:39 [backends.py:197] Cache the graph for dynamic shape for later use\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM Logprobs Extraction Test - Standalone Script\n",
    "Test if we can extract logprobs from vLLM for reproducibility forensics\n",
    "\n",
    "Paste this entire script into a Jupyter cell and run.\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIG SECTION - CHANGE THESE PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"  # Small model for testing\n",
    "TENSOR_PARALLEL_SIZE = 4  \n",
    "MAX_MODEL_LEN = 8048  # Context length (reduced for memory)\n",
    "GPU_MEMORY_UTILIZATION = 0.8  # Use 40% of GPU memory (default is 0.9)\n",
    "\n",
    "# Generation parameters\n",
    "NUM_REPETITIONS = 5\n",
    "MAX_TOKENS = 20  # Generate 20 tokens per pass\n",
    "TEMPERATURE = 0.0  # Greedy sampling for determinism\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10  # Store top-10 logprobs for forensic analysis\n",
    "\n",
    "# Input text\n",
    "INPUT_TEXT = \"\"\"The field of artificial intelligence has witnessed remarkable \n",
    "transformation over the past decade, driven primarily by advances in deep learning \n",
    "and the emergence of increasingly sophisticated language models. These models, trained \n",
    "on vast corpora of text data, have demonstrated remarkable capabilities across a wide \n",
    "range of tasks.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"vLLM LOGPROBS EXTRACTION TEST\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {MAX_MODEL_LEN}\")\n",
    "print(f\"  GPU memory utilization: {GPU_MEMORY_UTILIZATION} ({GPU_MEMORY_UTILIZATION*100:.0f}%)\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE} (greedy)\")\n",
    "print()\n",
    "print(f\"GPU Info:\")\n",
    "print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  Available: {torch.cuda.device_count()} GPUs\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading model with vLLM...\")\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    download_dir=\"/workspace/huggingface_cache\",\n",
    "    dtype=\"bfloat16\",\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    trust_remote_code=True,\n",
    "    enable_prefix_caching=False,  # Disable caching for full reproducibility test\n",
    ")\n",
    "print(\"✓ Model loaded (prefix caching disabled)\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLING PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    logprobs=TOP_LOGPROBS,  # Return top-k logprobs for forensic analysis\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "print(f\"Sampling params: temperature={TEMPERATURE}, max_tokens={MAX_TOKENS}, seed={SEED}, top_logprobs={TOP_LOGPROBS}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# LOGPROBS EXTRACTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_logprobs_vector(output):\n",
    "    \"\"\"\n",
    "    Extract logprobs from vLLM output for forensic comparison\n",
    "    \n",
    "    Returns two arrays:\n",
    "    - selected_logprobs: logprobs of tokens that were actually selected (for basic check)\n",
    "    - top_k_distributions: full top-k distribution at each position (for deep forensics)\n",
    "    \"\"\"\n",
    "    selected_logprobs = []\n",
    "    top_k_distributions = []\n",
    "    \n",
    "    # Get the token IDs that were actually generated\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    \n",
    "    # output.outputs[0].logprobs is a list (one entry per generated token)\n",
    "    # Each entry is a dict mapping token_id -> Logprob object\n",
    "    for i, token_logprobs_dict in enumerate(output.outputs[0].logprobs):\n",
    "        if token_logprobs_dict is None:\n",
    "            continue\n",
    "        \n",
    "        # Get the token ID that was actually generated at this position\n",
    "        generated_token_id = token_ids[i]\n",
    "        \n",
    "        # Extract selected token logprob\n",
    "        if generated_token_id in token_logprobs_dict:\n",
    "            logprob_obj = token_logprobs_dict[generated_token_id]\n",
    "            selected_logprobs.append(logprob_obj.logprob)\n",
    "        else:\n",
    "            print(f\"Warning: Token {generated_token_id} not in logprobs dict at position {i}\")\n",
    "            selected_logprobs.append(None)\n",
    "        \n",
    "        # Extract full top-k distribution for this position\n",
    "        # Store as list of (token_id, logprob) tuples, sorted by logprob descending\n",
    "        position_dist = [\n",
    "            (tid, lp_obj.logprob) \n",
    "            for tid, lp_obj in token_logprobs_dict.items()\n",
    "        ]\n",
    "        position_dist.sort(key=lambda x: x[1], reverse=True)  # Sort by logprob\n",
    "        top_k_distributions.append(position_dist)\n",
    "    \n",
    "    return np.array(selected_logprobs), top_k_distributions\n",
    "\n",
    "# ============================================================================\n",
    "# WARM-UP PASS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WARM-UP PASS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Running warm-up pass to initialize CUDA kernels...\")\n",
    "_ = llm.generate([INPUT_TEXT], sampling_params)\n",
    "print(\"✓ Warm-up complete - CUDA kernels compiled and cached\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# RUN REPEATED FORWARD PASSES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"RUNNING {NUM_REPETITIONS} FORWARD PASSES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "results_logprobs = []\n",
    "results_distributions = []\n",
    "results_tokens = []\n",
    "results_texts = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Rep {rep + 1}/{NUM_REPETITIONS}:\", end=\" \")\n",
    "    \n",
    "    # Generate (vLLM automatically manages cache per request)\n",
    "    output = llm.generate([INPUT_TEXT], sampling_params)[0]\n",
    "    \n",
    "    # Extract logprobs and distributions\n",
    "    logprobs_vec, top_k_dist = extract_logprobs_vector(output)\n",
    "    results_logprobs.append(logprobs_vec)\n",
    "    results_distributions.append(top_k_dist)\n",
    "    \n",
    "    # Extract tokens\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    # Extract text\n",
    "    text = output.outputs[0].text\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    print(f\"{len(token_ids)} tokens, mean logprob={np.mean(logprobs_vec):.4f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Show first generation\n",
    "print(\"First generation:\")\n",
    "print(f\"  Text: '{results_texts[0][:100]}...'\")\n",
    "print(f\"  Tokens: {results_tokens[0][:10]}...\")\n",
    "print(f\"  Logprobs (first 5): {results_logprobs[0][:5]}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REPRODUCIBILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "# Check token sequences\n",
    "first_tokens = results_tokens[0]\n",
    "tokens_identical = all(results_tokens[i] == first_tokens for i in range(NUM_REPETITIONS))\n",
    "\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "if not tokens_identical:\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[i] != first_tokens:\n",
    "            print(f\"  Rep 0 vs Rep {i}: DIFFER\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Check logprobs\n",
    "first_logprobs = results_logprobs[0]\n",
    "\n",
    "# Bit-exact comparison\n",
    "logprobs_exact = all(np.array_equal(first_logprobs, results_logprobs[i]) \n",
    "                     for i in range(1, NUM_REPETITIONS))\n",
    "\n",
    "print(f\"Logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "# Check top-k distributions\n",
    "print(\"\\nChecking full top-k distributions...\")\n",
    "distributions_exact = True\n",
    "distribution_mismatches = []\n",
    "\n",
    "first_dist = results_distributions[0]\n",
    "for rep_idx in range(1, NUM_REPETITIONS):\n",
    "    for pos_idx in range(len(first_dist)):\n",
    "        dist_a = first_dist[pos_idx]\n",
    "        dist_b = results_distributions[rep_idx][pos_idx]\n",
    "        \n",
    "        # Check if token IDs match in same order\n",
    "        tokens_match = [t[0] for t in dist_a] == [t[0] for t in dist_b]\n",
    "        \n",
    "        # Check if logprobs are bit-exact\n",
    "        if tokens_match:\n",
    "            logprobs_match = all(\n",
    "                abs(dist_a[i][1] - dist_b[i][1]) < 1e-10 \n",
    "                for i in range(len(dist_a))\n",
    "            )\n",
    "            if not logprobs_match:\n",
    "                distributions_exact = False\n",
    "                distribution_mismatches.append((rep_idx, pos_idx))\n",
    "        else:\n",
    "            distributions_exact = False\n",
    "            distribution_mismatches.append((rep_idx, pos_idx))\n",
    "\n",
    "print(f\"Top-k distributions bit-exact: {distributions_exact}\")\n",
    "\n",
    "if not distributions_exact:\n",
    "    print(f\"\\n⚠ Found {len(distribution_mismatches)} position mismatches in distributions\")\n",
    "    if len(distribution_mismatches) <= 5:\n",
    "        for rep_idx, pos_idx in distribution_mismatches:\n",
    "            print(f\"  Rep 0 vs Rep {rep_idx}, position {pos_idx}\")\n",
    "    else:\n",
    "        print(f\"  First 5: {distribution_mismatches[:5]}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "    \n",
    "    # Element-wise statistics\n",
    "    all_logprobs = np.array(results_logprobs)\n",
    "    std_per_token = all_logprobs.std(axis=0)\n",
    "    print(f\"\\nPer-token std statistics:\")\n",
    "    print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "    print(f\"  Max: {std_per_token.max():.6e}\")\n",
    "    print(f\"  Median: {np.median(std_per_token):.6e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact and distributions_exact:\n",
    "    print(\"✓ PERFECT REPRODUCIBILITY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Full top-k distributions: bit-exact\")\n",
    "    print(\"  - vLLM is deterministic for this config\")\n",
    "    print(\"  → Ready to scale up to K2 Thinking on Blackwell\")\n",
    "elif tokens_identical and logprobs_exact and not distributions_exact:\n",
    "    print(\"⚠ SELECTED TOKENS EXACT, DISTRIBUTIONS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Top-k distributions: numerical variation detected\")\n",
    "    print(\"  → May indicate computational instability in non-selected paths\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"✓ PERFECT REPRODUCIBILITY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: bit-exact\")\n",
    "    print(\"  - vLLM is deterministic for this config\")\n",
    "    print(\"  → Ready to scale up to K2 Thinking on Blackwell\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"⚠ TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: small numerical variation\")\n",
    "    max_l2 = max(l2_distances) if not logprobs_exact else 0.0\n",
    "    if max_l2 < 1e-6:\n",
    "        print(f\"  - Variation very small (L2={max_l2:.2e})\")\n",
    "        print(\"  → Likely acceptable for forensics\")\n",
    "    else:\n",
    "        print(f\"  - Variation notable (L2={max_l2:.2e})\")\n",
    "        print(\"  → Investigate noise source\")\n",
    "else:\n",
    "    print(\"✗ TOKEN SEQUENCES DIFFER\")\n",
    "    print(\"  - This should NOT happen with temperature=0\")\n",
    "    print(\"  → Something is wrong, investigate\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"vllm_logprobs_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"warmup_enabled\": True,\n",
    "        \"prefix_caching_disabled\": True,\n",
    "        \"top_logprobs\": TOP_LOGPROBS\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"distributions_exact\": distributions_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact and distributions_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts,\n",
    "    \"note\": \"Top-k distributions not saved to JSON (too large), check bit-exact flag above\"\n",
    "}\n",
    "\n",
    "output_file = f\"/workspace/vllm_logprobs_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"If successful, next steps:\")\n",
    "print(\"  1. Scale up to K2 Thinking\")\n",
    "print(\"  2. Test on 4× B200 with tensor parallelism\")\n",
    "print(\"  3. Run with 100K context\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85a050-0070-46e7-bb9f-b926114c2050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
