{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db079732-5614-47ea-8832-902898a87488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREFILL vs DECODE EXPERIMENT\n",
      "================================================================================\n",
      "Log file: /workspace/experiments/experiment_log_20251124_164613.txt\n",
      "\n",
      "Environment:\n",
      "  hostname: c895fe420c2d\n",
      "  platform: Linux-5.15.0-140-generic-x86_64-with-glibc2.35\n",
      "  python_version: 3.10.18\n",
      "  torch_version: 2.6.0+cu118\n",
      "  cuda_version: 11.8\n",
      "  transformers_version: 4.57.1\n",
      "  gpu_name: NVIDIA A100-SXM4-80GB\n",
      "  gpu_count: 1\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Layers: [28]\n",
      "  Batch sizes: [4, 5, 8, 9, 16, 17]\n",
      "  Max tokens: 20\n",
      "\n",
      "Loading model...\n",
      "Found 3 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n",
      "    → 120214 tokens\n",
      "  Loading: /workspace/Epoch_data.pdf\n",
      "    → 29497 tokens\n",
      "  Loading: /workspace/Llama3.1.pdf\n",
      "    → 99282 tokens\n",
      "Total tokens: 248993\n",
      "Creating 51 slices of 512 tokens each\n",
      "Created 3 reference sequences with 16 dummies each\n",
      "\n",
      "Experiment design:\n",
      "  - Decode runs at 6 batch sizes\n",
      "  - Prefill reproduces at 6 batch sizes\n",
      "  - Matrix: 6 decode × 6 prefill = 36 comparisons per reference\n",
      "  - Works purely at token ID level (no tokenization artifacts)\n",
      "  - NEW: Within-mode batch size comparisons (sanity check)\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81c5c5cc19674a49ba62f2c2de418130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_0\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 512 tokens\n",
      "(All batch sizes will use this length)\n",
      "\n",
      "Decode runs:\n",
      "  bs=4...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=5...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=8...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=9...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=16...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=17...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=4:\n",
      "    IDs:  [1096, 1895, 40324, 279, 4650, 369, 6489, 22901, 198, 351, 57775, 311, 1824, 279, 8480, 4401, 323, 990, 315, 15235]\n",
      "    Text: ' This report explores the potential for international verification\\nagreements to support the responsible development and use of AI'\n",
      "    ✓\n",
      "  bs=5:\n",
      "    IDs:  [1096, 1895, 40324, 279, 4650, 369, 6489, 22901, 198, 351, 57775, 311, 1824, 279, 8480, 4401, 323, 990, 315, 15235]\n",
      "    Text: ' This report explores the potential for international verification\\nagreements to support the responsible development and use of AI'\n",
      "    ✓\n",
      "  bs=8:\n",
      "    IDs:  [1096, 1895, 40324, 279, 4650, 369, 6489, 22901, 198, 351, 57775, 311, 1824, 279, 8480, 4401, 323, 990, 315, 15235]\n",
      "    Text: ' This report explores the potential for international verification\\nagreements to support the responsible development and use of AI'\n",
      "    ✓\n",
      "  bs=9:\n",
      "    IDs:  [1096, 1895, 40324, 279, 4650, 369, 6489, 22901, 198, 351, 57775, 311, 1824, 279, 8480, 4401, 323, 990, 315, 15235]\n",
      "    Text: ' This report explores the potential for international verification\\nagreements to support the responsible development and use of AI'\n",
      "    ✓\n",
      "  bs=16:\n",
      "    IDs:  [1096, 1895, 40324, 279, 4650, 369, 6489, 22901, 198, 351, 57775, 311, 1824, 279, 8480, 4401, 323, 990, 315, 15235]\n",
      "    Text: ' This report explores the potential for international verification\\nagreements to support the responsible development and use of AI'\n",
      "    ✓\n",
      "  bs=17:\n",
      "    IDs:  [1096, 1895, 40324, 279, 4650, 369, 6489, 22901, 198, 351, 57775, 311, 1824, 279, 8480, 4401, 323, 990, 315, 15235]\n",
      "    Text: ' This report explores the potential for international verification\\nagreements to support the responsible development and use of AI'\n",
      "    ✓\n",
      "\n",
      "✓ Element 0 generates IDENTICAL tokens across all batch sizes\n",
      "  → Can meaningfully compare activations for same token sequence\n",
      "\n",
      "Prefill reproductions:\n",
      "\n",
      "  Reproducing decode bs=4:\n",
      "    with bs=4... [Ext: 532 tokens, exact neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=5... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=8... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=9... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=16... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=17... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "\n",
      "  Reproducing decode bs=5:\n",
      "    with bs=4... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=5... [Ext: 532 tokens, exact neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=8... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=9... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=16... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=17... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "\n",
      "  Reproducing decode bs=8:\n",
      "    with bs=4... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=5... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=8... [Ext: 532 tokens, exact neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=9... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=16... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=17... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "\n",
      "  Reproducing decode bs=9:\n",
      "    with bs=4... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=5... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=8... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=9... [Ext: 532 tokens, exact neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=16... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=17... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "\n",
      "  Reproducing decode bs=16:\n",
      "    with bs=4... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=5... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=8... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=9... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=16... [Ext: 532 tokens, exact neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=17... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "\n",
      "  Reproducing decode bs=17:\n",
      "    with bs=4... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=5... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=8... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=9... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=16... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=17... [Ext: 532 tokens, exact neighbors] → Extract at: [528, 529, 530]\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_1\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 512 tokens\n",
      "(All batch sizes will use this length)\n",
      "\n",
      "Decode runs:\n",
      "  bs=4...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=5...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=8...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=9...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=16...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "  bs=17...       Prompt length: 512 tokens → Final: 532 tokens (20 generated)\n",
      "    Extract positions: [528, 529, 530]\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=4:\n",
      "    IDs:  [9677, 9831, 198, 983, 4193, 311, 11075, 429, 2188, 315, 43643, 5267, 19, 75141, 2188, 315, 27231, 374, 4362, 16804]\n",
      "    Text: ' parties willing\\nto accept to achieve that level of certainty?\\n4.What level of transparency is needed—and'\n",
      "    ✓\n",
      "  bs=5:\n",
      "    IDs:  [9677, 9831, 198, 983, 4193, 311, 11075, 429, 2188, 315, 43643, 5267, 19, 75141, 2188, 315, 27231, 374, 4362, 16804]\n",
      "    Text: ' parties willing\\nto accept to achieve that level of certainty?\\n4.What level of transparency is needed—and'\n",
      "    ✓\n",
      "  bs=8:\n",
      "    IDs:  [9677, 9831, 198, 983, 4193, 311, 11075, 429, 2188, 315, 43643, 5267, 19, 75141, 2188, 315, 27231, 374, 4362, 16804]\n",
      "    Text: ' parties willing\\nto accept to achieve that level of certainty?\\n4.What level of transparency is needed—and'\n",
      "    ✓\n",
      "  bs=9:\n",
      "    IDs:  [9677, 9831, 198, 983, 4193, 311, 11075, 429, 2188, 315, 43643, 5267, 19, 75141, 2188, 315, 27231, 374, 4362, 16804]\n",
      "    Text: ' parties willing\\nto accept to achieve that level of certainty?\\n4.What level of transparency is needed—and'\n",
      "    ✓\n",
      "  bs=16:\n",
      "    IDs:  [9677, 9831, 198, 983, 4193, 311, 11075, 429, 2188, 315, 43643, 5267, 19, 75141, 2188, 315, 27231, 374, 4362, 16804]\n",
      "    Text: ' parties willing\\nto accept to achieve that level of certainty?\\n4.What level of transparency is needed—and'\n",
      "    ✓\n",
      "  bs=17:\n",
      "    IDs:  [9677, 9831, 198, 983, 4193, 311, 11075, 429, 2188, 315, 43643, 5267, 19, 75141, 2188, 315, 27231, 374, 4362, 16804]\n",
      "    Text: ' parties willing\\nto accept to achieve that level of certainty?\\n4.What level of transparency is needed—and'\n",
      "    ✓\n",
      "\n",
      "✓ Element 0 generates IDENTICAL tokens across all batch sizes\n",
      "  → Can meaningfully compare activations for same token sequence\n",
      "\n",
      "Prefill reproductions:\n",
      "\n",
      "  Reproducing decode bs=4:\n",
      "    with bs=4... [Ext: 532 tokens, exact neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=5... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=8... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=9... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=16... [Ext: 532 tokens, arb neighbors] → Extract at: [528, 529, 530]\n",
      "    with bs=17... [Ext: 532 tokens, arb neighbors]"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Prefill vs Decode Experiment - Batch Size Detectability\n",
    "\n",
    "Fixed version that:\n",
    "1. Works purely at token ID level (no decode/re-encode cycles)\n",
    "2. Stores exact token IDs from decode\n",
    "3. Compares logprobs for SAME token IDs between decode and prefill\n",
    "4. Properly serializes numpy arrays to JSON\n",
    "5. ADDED: Within-mode batch size comparisons (decode vs decode, prefill vs prefill)\n",
    "\n",
    "Usage:\n",
    "    python prefill_decode_experiment.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "from datetime import datetime as dt_for_log\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    \"\"\"Setup logging to file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = dt_for_log.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_path = os.path.join(output_dir, f\"experiment_log_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    \"\"\"Print to both console and log file.\"\"\"\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        # Remove 'file' from kwargs if present, then write to log\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    \"\"\"Close log file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "# Will be initialized from PDF in main()\n",
    "REFERENCE_SEQUENCES = None\n",
    "DUMMY_SETS = None\n",
    "\n",
    "BATCH_SIZES = [4, 5, 8, 9, 16, 17]\n",
    "LAYER_INDICES = [28]\n",
    "MAX_NEW_TOKENS = 20\n",
    "TOKENS_PER_SLICE = 512\n",
    "\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text content from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def create_sequences_from_pdf(tokenizer, num_references=3):\n",
    "    \"\"\"\n",
    "    Load all PDFs from current directory, tokenize, and split into equal-length slices.\n",
    "    Returns REFERENCE_SEQUENCES and DUMMY_SETS dictionaries.\n",
    "    \"\"\"\n",
    "    # Find all PDFs in current directory\n",
    "    try:\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        script_dir = os.getcwd()  # Jupyter notebook fallback\n",
    "    \n",
    "    pdf_files = glob.glob(os.path.join(script_dir, \"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        pdf_files = glob.glob(\"*.pdf\")\n",
    "    if not pdf_files:\n",
    "        pdf_files = glob.glob(\"/workspace/*.pdf\")\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF file found in current directory or /workspace\")\n",
    "    \n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    \n",
    "    # Load and tokenize all PDFs\n",
    "    all_tokens = []\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        all_tokens.extend(tokens)\n",
    "        log_print(f\"    → {len(tokens)} tokens\")\n",
    "    \n",
    "    log_print(f\"Total tokens: {len(all_tokens)}\")\n",
    "    \n",
    "    # Calculate number of slices needed\n",
    "    max_batch_size = max(BATCH_SIZES)\n",
    "    slices_needed = num_references * max_batch_size\n",
    "    tokens_needed = slices_needed * TOKENS_PER_SLICE\n",
    "    \n",
    "    if len(all_tokens) < tokens_needed:\n",
    "        raise ValueError(f\"PDFs too short. Need {tokens_needed} tokens ({slices_needed} slices × {TOKENS_PER_SLICE} tokens) but only have {len(all_tokens)} tokens\")\n",
    "    \n",
    "    log_print(f\"Creating {slices_needed} slices of {TOKENS_PER_SLICE} tokens each\")\n",
    "    \n",
    "    slices = []\n",
    "    for i in range(slices_needed):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        slice_tokens = all_tokens[start:end]\n",
    "        slice_text = tokenizer.decode(slice_tokens)\n",
    "        slices.append(slice_text)\n",
    "    \n",
    "    # Build reference sequences and dummy sets\n",
    "    reference_sequences = {}\n",
    "    dummy_sets = {}\n",
    "    \n",
    "    for ref_idx in range(num_references):\n",
    "        ref_name = f\"ref_{ref_idx}\"\n",
    "        base_idx = ref_idx * max_batch_size\n",
    "        \n",
    "        reference_sequences[ref_name] = slices[base_idx]\n",
    "        dummy_sets[ref_name] = slices[base_idx + 1 : base_idx + max_batch_size]\n",
    "    \n",
    "    return reference_sequences, dummy_sets\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "    \n",
    "    return info\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_signals_from_output(outputs, layer_indices, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs from element 0 at specified position.\n",
    "    Returns top-10 token IDs so prefill can compare the same tokens.\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "    \n",
    "    # Key vectors from element 0\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "    \n",
    "    # Logprobs from element 0 - get top 10\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    top10 = torch.topk(log_probs, k=10)\n",
    "    \n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': top10.indices.cpu().tolist(),\n",
    "        'log_probs': top10.values.cpu().tolist()\n",
    "    }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "\n",
    "def extract_signals_for_token_ids(outputs, layer_indices, token_ids, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs for SPECIFIC token IDs.\n",
    "    Used in prefill to compare the same tokens as decode.\n",
    "    \n",
    "    Args:\n",
    "        token_ids: List of token IDs to extract logprobs for (from decode's top-10)\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "    \n",
    "    # Key vectors from element 0\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "    \n",
    "    # Logprobs for SPECIFIC token IDs (from decode's top 10)\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Extract logprobs for the specified token IDs\n",
    "    token_ids_tensor = torch.tensor(token_ids, device=logits.device)\n",
    "    selected_logprobs = log_probs[token_ids_tensor]\n",
    "    \n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': token_ids,  # Same as decode\n",
    "        'log_probs': selected_logprobs.cpu().tolist()\n",
    "    }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# ============================================================================\n",
    "# DECODE GENERATION & EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def compute_min_length_across_batches(ref_text, ref_name, tokenizer, batch_sizes):\n",
    "    \"\"\"\n",
    "    Pre-compute minimum sequence length across all batch configurations.\n",
    "    This ensures all batch sizes use the same sequence length.\n",
    "    \"\"\"\n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    min_length = float('inf')\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        if batch_size == 1:\n",
    "            batch_texts = [ref_text]\n",
    "        else:\n",
    "            batch_texts = [ref_text] + ref_dummies[:batch_size-1]\n",
    "        \n",
    "        token_lengths = [len(tokenizer.encode(t, add_special_tokens=True)) for t in batch_texts]\n",
    "        min_length = min(min_length, min(token_lengths))\n",
    "    \n",
    "    return min_length\n",
    "\n",
    "\n",
    "def run_decode_with_extraction(model, tokenizer, ref_text, ref_name, batch_size, \n",
    "                                layer_indices, forced_length=None):\n",
    "    \"\"\"\n",
    "    Run decode generation and extract signals from last 3 generation steps.\n",
    "    Works purely at token ID level - no decode/re-encode cycles.\n",
    "    \n",
    "    Returns exact token IDs used for prefill to reproduce.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Build batch texts\n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    if batch_size == 1:\n",
    "        batch_texts = [ref_text]\n",
    "    else:\n",
    "        batch_texts = [ref_text] + ref_dummies[:batch_size-1]\n",
    "    \n",
    "    # Tokenize ONCE - store the exact token IDs\n",
    "    all_token_ids = [tokenizer.encode(t, add_special_tokens=True) for t in batch_texts]\n",
    "    \n",
    "    # Truncate at token ID level\n",
    "    if forced_length is not None:\n",
    "        min_length = forced_length\n",
    "    else:\n",
    "        min_length = min(len(ids) for ids in all_token_ids)\n",
    "    \n",
    "    truncated_token_ids = [ids[:min_length] for ids in all_token_ids]\n",
    "    \n",
    "    # Build input tensors DIRECTLY from token IDs - no decode/re-encode\n",
    "    input_ids = torch.tensor(truncated_token_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    log_print(f\"      Prompt length: {prompt_length} tokens\", end=\"\")\n",
    "    \n",
    "    # Track generation for ALL batch positions\n",
    "    all_batch_generated_ids = [[] for _ in range(batch_size)]\n",
    "    generation_signals = []\n",
    "    \n",
    "    # FIRST STEP: Prefill with full prompt\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "    \n",
    "    past_kv = outputs.past_key_values\n",
    "    \n",
    "    # Get first token\n",
    "    next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "    for batch_idx in range(batch_size):\n",
    "        all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "    \n",
    "    # Extract signals from element 0\n",
    "    signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "    \n",
    "    # Absolute position: input length - 1\n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    \n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    # Update attention mask\n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((inputs['attention_mask'].shape[0], 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS: True autoregressive with cache\n",
    "    for step in range(1, MAX_NEW_TOKENS):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        # Get next tokens\n",
    "        next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "        for batch_idx in range(batch_size):\n",
    "            all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "        \n",
    "        # Extract signals from element 0\n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        \n",
    "        # Absolute position in full sequence\n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        # Update attention mask\n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((attention_mask.shape[0], 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Check for EOS in position 0\n",
    "        if all_batch_generated_ids[0][-1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Extract last 3\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "    \n",
    "    del outputs\n",
    "    del inputs\n",
    "    del past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    final_length = prompt_length + num_generated\n",
    "    log_print(f\" → Final: {final_length} tokens ({num_generated} generated)\")\n",
    "    \n",
    "    return {\n",
    "        'generated_ids': all_batch_generated_ids[0],  # Position 0 tokens\n",
    "        'all_batch_generated_ids': all_batch_generated_ids,  # ALL positions\n",
    "        'prompt_token_ids': truncated_token_ids,  # EXACT token IDs used in prompt\n",
    "        'prompt_length': prompt_length,\n",
    "        'signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# PREFILL REPRODUCTION\n",
    "# ============================================================================\n",
    "\n",
    "def run_prefill_verification(model, tokenizer, ref_name, decode_metadata, \n",
    "                             batch_size, layer_indices, is_diagonal=False):\n",
    "    \"\"\"\n",
    "    Run prefill to reproduce decoder's activations.\n",
    "    Uses EXACT token IDs from decode - no tokenization artifacts.\n",
    "    \n",
    "    Args:\n",
    "        is_diagonal: True if same batch size (uses actual neighbors from decode)\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Get exact prompt IDs used in decode for element 0\n",
    "    ref_prompt_ids = decode_metadata['prompt_token_ids'][0]\n",
    "    generated_ids = decode_metadata['generated_ids']\n",
    "    \n",
    "    # Build extended sequence at ID level\n",
    "    extended_ref_ids = ref_prompt_ids + generated_ids\n",
    "    \n",
    "    log_print(f\" [Ext: {len(extended_ref_ids)} tokens\", end=\"\")\n",
    "    \n",
    "    # Build batch\n",
    "    if batch_size == 1:\n",
    "        batch_ids = [extended_ref_ids]\n",
    "    else:\n",
    "        batch_ids = [extended_ref_ids]\n",
    "        \n",
    "        if is_diagonal:\n",
    "            # Use EXACT neighbor IDs from decode\n",
    "            log_print(f\", exact neighbors\", end=\"\")\n",
    "            for i in range(1, batch_size):\n",
    "                neighbor_prompt_ids = decode_metadata['prompt_token_ids'][i]\n",
    "                neighbor_gen_ids = decode_metadata['all_batch_generated_ids'][i]\n",
    "                extended_neighbor_ids = neighbor_prompt_ids + neighbor_gen_ids\n",
    "                batch_ids.append(extended_neighbor_ids)\n",
    "        else:\n",
    "            # Off-diagonal: construct length-matched neighbors at ID level\n",
    "            log_print(f\", arb neighbors\", end=\"\")\n",
    "            ref_dummies = DUMMY_SETS[ref_name]\n",
    "            target_length = len(extended_ref_ids)\n",
    "            \n",
    "            for i in range(batch_size - 1):\n",
    "                dummy_ids = tokenizer.encode(ref_dummies[i], add_special_tokens=True)\n",
    "                \n",
    "                # Truncate or repeat to match length\n",
    "                if len(dummy_ids) >= target_length:\n",
    "                    dummy_ids = dummy_ids[:target_length]\n",
    "                else:\n",
    "                    # Repeat tokens to reach target length\n",
    "                    repeats = (target_length // len(dummy_ids)) + 1\n",
    "                    dummy_ids = (dummy_ids * repeats)[:target_length]\n",
    "                \n",
    "                batch_ids.append(dummy_ids)\n",
    "    \n",
    "    # Build input tensors DIRECTLY from IDs - no decode/encode cycle\n",
    "    input_ids = torch.tensor(batch_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "    log_print(f\"]\", end=\"\")\n",
    "    \n",
    "    # Single forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "    \n",
    "    # Extract at same positions as decode, using decode's token IDs for logprobs\n",
    "    prefill_signals = {}\n",
    "    extraction_positions = []\n",
    "    \n",
    "    for pos_label, decode_step_data in decode_metadata['signals'].items():\n",
    "        decode_abs_pos = decode_step_data['absolute_position']\n",
    "        decode_token_ids = decode_step_data['signals']['logprobs']['token_ids']\n",
    "        \n",
    "        extraction_positions.append(decode_abs_pos)\n",
    "        \n",
    "        # Extract using SAME token IDs as decode for logprobs\n",
    "        signals = extract_signals_for_token_ids(\n",
    "            outputs, layer_indices, decode_token_ids, position=decode_abs_pos\n",
    "        )\n",
    "        \n",
    "        prefill_signals[pos_label] = {\n",
    "            'absolute_position': decode_abs_pos,\n",
    "            'signals': signals\n",
    "        }\n",
    "    \n",
    "    log_print(f\" → Extract at: {extraction_positions}\")\n",
    "    \n",
    "    del outputs\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return prefill_signals\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_l2_distance(vec1, vec2):\n",
    "    \"\"\"Compute L2 distance between two vectors.\"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return float(np.linalg.norm(v1 - v2))\n",
    "\n",
    "\n",
    "def compute_logprob_distance(logprobs1, logprobs2):\n",
    "    \"\"\"\n",
    "    Compute L2 distance between logprob distributions.\n",
    "    Both should have same token_ids in same order.\n",
    "    \"\"\"\n",
    "    probs1 = np.array(logprobs1['log_probs'])\n",
    "    probs2 = np.array(logprobs2['log_probs'])\n",
    "    return float(np.linalg.norm(probs1 - probs2))\n",
    "\n",
    "\n",
    "def compare_signals_generic(signals1, signals2, layer_indices):\n",
    "    \"\"\"Generic signal comparison - works for both decode-decode and prefill-prefill.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "    \n",
    "    all_key_dists = []\n",
    "    all_logprob_dists = []\n",
    "    \n",
    "    for pos_label in common_positions:\n",
    "        # Extract the actual signals (handle nested structure)\n",
    "        if 'signals' in signals1[pos_label]:\n",
    "            sig1 = signals1[pos_label]['signals']\n",
    "            sig2 = signals2[pos_label]['signals']\n",
    "        else:\n",
    "            sig1 = signals1[pos_label]\n",
    "            sig2 = signals2[pos_label]\n",
    "        \n",
    "        # Key vectors\n",
    "        for layer_name in sig1['key_vectors'].keys():\n",
    "            dist = compute_l2_distance(\n",
    "                sig1['key_vectors'][layer_name],\n",
    "                sig2['key_vectors'][layer_name]\n",
    "            )\n",
    "            all_key_dists.append(dist)\n",
    "        \n",
    "        # Logprobs\n",
    "        dist = compute_logprob_distance(sig1['logprobs'], sig2['logprobs'])\n",
    "        all_logprob_dists.append(dist)\n",
    "    \n",
    "    return {\n",
    "        'key_vectors_max': max(all_key_dists) if all_key_dists else 0.0,\n",
    "        'key_vectors_mean': np.mean(all_key_dists) if all_key_dists else 0.0,\n",
    "        'logprobs_max': max(all_logprob_dists) if all_logprob_dists else 0.0,\n",
    "        'logprobs_mean': np.mean(all_logprob_dists) if all_logprob_dists else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_decode_vs_prefill(decode_data, prefill_data, layer_indices):\n",
    "    \"\"\"Compare decode signals against prefill reproduction.\"\"\"\n",
    "    return compare_signals_generic(decode_data['signals'], prefill_data, layer_indices)\n",
    "\n",
    "\n",
    "def check_token_consistency(decode_measurements, tokenizer):\n",
    "    \"\"\"Check if element 0 generates same tokens across batch sizes.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    tokens_by_bs = {}\n",
    "    for bs, data in decode_measurements.items():\n",
    "        tokens_by_bs[bs] = data['generated_ids']\n",
    "    \n",
    "    bs_list = sorted(tokens_by_bs.keys())\n",
    "    reference_tokens = tokens_by_bs[bs_list[0]]\n",
    "    \n",
    "    all_same = True\n",
    "    log_print(\"\\nGenerated tokens by batch size:\")\n",
    "    for bs in bs_list:\n",
    "        tokens = tokens_by_bs[bs]\n",
    "        match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "        decoded_text = tokenizer.decode(tokens)\n",
    "        log_print(f\"  bs={bs}:\")\n",
    "        log_print(f\"    IDs:  {tokens}\")\n",
    "        log_print(f\"    Text: {repr(decoded_text)}\")\n",
    "        log_print(f\"    {match_str}\")\n",
    "        if tokens != reference_tokens:\n",
    "            all_same = False\n",
    "    \n",
    "    if all_same:\n",
    "        log_print(\"\\n✓ Element 0 generates IDENTICAL tokens across all batch sizes\")\n",
    "        log_print(\"  → Can meaningfully compare activations for same token sequence\")\n",
    "    else:\n",
    "        log_print(\"\\n⚠ Element 0 generates DIFFERENT tokens across batch sizes\")\n",
    "        log_print(\"  → Batch composition affects generation\")\n",
    "    \n",
    "    return all_same\n",
    "\n",
    "\n",
    "def analyze_within_mode_batch_effects(measurements, layer_indices):\n",
    "    \"\"\"\n",
    "    NEW ANALYSIS: Compare activations across batch sizes WITHIN each mode.\n",
    "    \n",
    "    This tests whether batch size affects computation even when tokens are identical.\n",
    "    Expected: YES - batch size should create numerical differences due to kernels/parallelism.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"WITHIN-MODE BATCH SIZE EFFECTS\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(\"\\nSanity check: Does batch size affect activations even with identical tokens?\")\n",
    "    log_print(\"Expected: YES - batch size changes computation even for same sequence\\n\")\n",
    "    \n",
    "    # DEBUG: Show total measurements\n",
    "    log_print(f\"DEBUG: Total measurements received: {len(measurements)}\")\n",
    "    if measurements:\n",
    "        log_print(f\"DEBUG: First measurement keys: {list(measurements[0].keys())}\")\n",
    "        log_print(f\"DEBUG: First measurement decode_batch_size: {measurements[0].get('decode_batch_size')}\")\n",
    "        log_print(f\"DEBUG: First measurement ref_name: {measurements[0].get('ref_name')}\")\n",
    "        log_print(f\"DEBUG: decode_data signals keys: {list(measurements[0].get('decode_data', {}).get('signals', {}).keys())}\")\n",
    "    \n",
    "    # Group measurements by reference and mode\n",
    "    by_ref = {}\n",
    "    for m in measurements:\n",
    "        ref = m['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {'decode': {}, 'prefill_diagonal': {}}\n",
    "        \n",
    "        # Decode data\n",
    "        by_ref[ref]['decode'][m['decode_batch_size']] = m['decode_data']['signals']\n",
    "        \n",
    "        # Prefill data (only diagonal - where we have real neighbors)\n",
    "        if m['is_diagonal']:\n",
    "            by_ref[ref]['prefill_diagonal'][m['prefill_batch_size']] = m['prefill_data']\n",
    "    \n",
    "    results = {'decode': [], 'prefill': []}\n",
    "    \n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{ref_name.upper()}\")\n",
    "        log_print(\"-\" * 80)\n",
    "        \n",
    "        # DEBUG: Show what batch sizes we have\n",
    "        log_print(f\"  DEBUG: decode has {len(by_ref[ref_name]['decode'])} batch sizes: {sorted(by_ref[ref_name]['decode'].keys())}\")\n",
    "        log_print(f\"  DEBUG: prefill_diagonal has {len(by_ref[ref_name]['prefill_diagonal'])} batch sizes: {sorted(by_ref[ref_name]['prefill_diagonal'].keys())}\")\n",
    "        \n",
    "        # DECODE: Compare across batch sizes\n",
    "        log_print(\"\\nDECODE mode (autoregressive):\")\n",
    "        decode_data = by_ref[ref_name]['decode']\n",
    "        \n",
    "        if len(decode_data) >= 2:\n",
    "            available_bs = sorted(decode_data.keys())\n",
    "            bs_pairs = [(available_bs[i], available_bs[j]) \n",
    "                        for i in range(len(available_bs)) \n",
    "                        for j in range(i+1, len(available_bs))]\n",
    "            \n",
    "            for bs1, bs2 in bs_pairs:\n",
    "                distances = compare_signals_generic(\n",
    "                    decode_data[bs1], \n",
    "                    decode_data[bs2], \n",
    "                    layer_indices\n",
    "                )\n",
    "                \n",
    "                log_print(f\"  bs={bs1} vs bs={bs2}:\")\n",
    "                log_print(f\"    Key vectors: Δ_max = {distances['key_vectors_max']:.2e}\")\n",
    "                log_print(f\"    Logprobs:    Δ_max = {distances['logprobs_max']:.2e}\")\n",
    "                \n",
    "                results['decode'].append({\n",
    "                    'ref': ref_name,\n",
    "                    'bs1': bs1,\n",
    "                    'bs2': bs2,\n",
    "                    'key_distance': distances['key_vectors_max'],\n",
    "                    'logprob_distance': distances['logprobs_max']\n",
    "                })\n",
    "        \n",
    "        # PREFILL: Compare across batch sizes (diagonal only)\n",
    "        log_print(\"\\nPREFILL mode (parallel forward):\")\n",
    "        prefill_data = by_ref[ref_name]['prefill_diagonal']\n",
    "        \n",
    "        if len(prefill_data) >= 2:\n",
    "            available_bs = sorted(prefill_data.keys())\n",
    "            bs_pairs = [(available_bs[i], available_bs[j]) \n",
    "                        for i in range(len(available_bs)) \n",
    "                        for j in range(i+1, len(available_bs))]\n",
    "            \n",
    "            for bs1, bs2 in bs_pairs:\n",
    "                distances = compare_signals_generic(\n",
    "                    prefill_data[bs1], \n",
    "                    prefill_data[bs2], \n",
    "                    layer_indices\n",
    "                )\n",
    "                \n",
    "                log_print(f\"  bs={bs1} vs bs={bs2}:\")\n",
    "                log_print(f\"    Key vectors: Δ_max = {distances['key_vectors_max']:.2e}\")\n",
    "                log_print(f\"    Logprobs:    Δ_max = {distances['logprobs_max']:.2e}\")\n",
    "                \n",
    "                results['prefill'].append({\n",
    "                    'ref': ref_name,\n",
    "                    'bs1': bs1,\n",
    "                    'bs2': bs2,\n",
    "                    'key_distance': distances['key_vectors_max'],\n",
    "                    'logprob_distance': distances['logprobs_max']\n",
    "                })\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"WITHIN-MODE AGGREGATE STATISTICS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    summary = {}\n",
    "    \n",
    "    for mode in ['decode', 'prefill']:\n",
    "        mode_results = results[mode]\n",
    "        \n",
    "        if not mode_results:\n",
    "            continue\n",
    "        \n",
    "        key_dists = [r['key_distance'] for r in mode_results]\n",
    "        logprob_dists = [r['logprob_distance'] for r in mode_results]\n",
    "        \n",
    "        log_print(f\"\\n{mode.upper()} mode:\")\n",
    "        log_print(f\"  Key vectors: μ = {np.mean(key_dists):.2e}, max = {max(key_dists):.2e}\")\n",
    "        log_print(f\"  Logprobs:    μ = {np.mean(logprob_dists):.2e}, max = {max(logprob_dists):.2e}\")\n",
    "        \n",
    "        # Check if any are exactly zero\n",
    "        key_zeros = sum(1 for d in key_dists if d == 0.0)\n",
    "        logprob_zeros = sum(1 for d in logprob_dists if d == 0.0)\n",
    "        \n",
    "        if key_zeros > 0:\n",
    "            log_print(f\"  ⚠ {key_zeros}/{len(key_dists)} key comparisons are EXACTLY ZERO\")\n",
    "        if logprob_zeros > 0:\n",
    "            log_print(f\"  ⚠ {logprob_zeros}/{len(logprob_dists)} logprob comparisons are EXACTLY ZERO\")\n",
    "        \n",
    "        # Track which batch pairs produce zeros, grouped by reference\n",
    "        if key_zeros > 0 or logprob_zeros > 0:\n",
    "            zeros_by_ref = {}\n",
    "            for r in mode_results:\n",
    "                ref = r['ref']\n",
    "                pair = (r['bs1'], r['bs2'])\n",
    "                if ref not in zeros_by_ref:\n",
    "                    zeros_by_ref[ref] = {'key': [], 'logprob': []}\n",
    "                if r['key_distance'] == 0.0:\n",
    "                    zeros_by_ref[ref]['key'].append(pair)\n",
    "                if r['logprob_distance'] == 0.0:\n",
    "                    zeros_by_ref[ref]['logprob'].append(pair)\n",
    "            \n",
    "            log_print(f\"\\n  Zero locations by reference:\")\n",
    "            for ref in sorted(zeros_by_ref.keys()):\n",
    "                key_pairs = zeros_by_ref[ref]['key']\n",
    "                log_pairs = zeros_by_ref[ref]['logprob']\n",
    "                if key_pairs or log_pairs:\n",
    "                    log_print(f\"    {ref}:\")\n",
    "                    if key_pairs:\n",
    "                        log_print(f\"      key zeros:     {sorted(key_pairs)}\")\n",
    "                    if log_pairs:\n",
    "                        log_print(f\"      logprob zeros: {sorted(log_pairs)}\")\n",
    "            \n",
    "            # Check consistency across references\n",
    "            all_refs = sorted(zeros_by_ref.keys())\n",
    "            if len(all_refs) >= 2:\n",
    "                key_sets = [set(zeros_by_ref[ref]['key']) for ref in all_refs]\n",
    "                log_sets = [set(zeros_by_ref[ref]['logprob']) for ref in all_refs]\n",
    "                \n",
    "                key_intersection = set.intersection(*key_sets) if all(key_sets) else set()\n",
    "                log_intersection = set.intersection(*log_sets) if all(log_sets) else set()\n",
    "                \n",
    "                log_print(f\"\\n  Cross-reference consistency:\")\n",
    "                if key_intersection:\n",
    "                    log_print(f\"    Key zeros consistent across ALL refs: {sorted(key_intersection)}\")\n",
    "                else:\n",
    "                    log_print(f\"    Key zeros: NO pairs are zero across all refs (coincidental)\")\n",
    "                if log_intersection:\n",
    "                    log_print(f\"    Logprob zeros consistent across ALL refs: {sorted(log_intersection)}\")\n",
    "                else:\n",
    "                    log_print(f\"    Logprob zeros: NO pairs are zero across all refs (coincidental)\")\n",
    "        \n",
    "        summary[mode] = {\n",
    "            'key_vectors_mean': float(np.mean(key_dists)),\n",
    "            'key_vectors_max': float(max(key_dists)),\n",
    "            'logprobs_mean': float(np.mean(logprob_dists)),\n",
    "            'logprobs_max': float(max(logprob_dists)),\n",
    "            'comparisons': mode_results\n",
    "        }\n",
    "    \n",
    "    # Conclusion\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"CONCLUSION: WITHIN-MODE BATCH EFFECTS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    threshold = 1e-10  # Essentially non-zero\n",
    "    \n",
    "    for mode in ['decode', 'prefill']:\n",
    "        if mode not in summary:\n",
    "            continue\n",
    "        \n",
    "        key_mean = summary[mode]['key_vectors_mean']\n",
    "        log_mean = summary[mode]['logprobs_mean']\n",
    "        \n",
    "        log_print(f\"\\n{mode.upper()}:\")\n",
    "        if key_mean > threshold and log_mean > threshold:\n",
    "            log_print(f\"  ✓ Batch size DOES affect computation (both keys and logprobs differ)\")\n",
    "            log_print(f\"    Keys: {key_mean:.2e}, Logprobs: {log_mean:.2e}\")\n",
    "        elif key_mean > threshold:\n",
    "            log_print(f\"  ~ Batch size affects keys but NOT logprobs\")\n",
    "            log_print(f\"    Keys: {key_mean:.2e}, Logprobs: {log_mean:.2e}\")\n",
    "        elif log_mean > threshold:\n",
    "            log_print(f\"  ~ Batch size affects logprobs but NOT keys\")\n",
    "            log_print(f\"    Keys: {key_mean:.2e}, Logprobs: {log_mean:.2e}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ Batch size does NOT affect computation (both zero)\")\n",
    "            log_print(f\"    This would invalidate the experiment premise!\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "\n",
    "def analyze_experiment(measurements, layer_indices):\n",
    "    \"\"\"Compute and display decode vs prefill comparison matrices.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ANALYSIS: DECODE vs PREFILL MATRIX\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    # Group by reference\n",
    "    by_ref = {}\n",
    "    for m in measurements:\n",
    "        ref = m['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (m['decode_batch_size'], m['prefill_batch_size'])\n",
    "        by_ref[ref][key] = m\n",
    "    \n",
    "    # Compute matrices for each reference\n",
    "    all_matrices = {'key_vectors': [], 'logprobs': []}\n",
    "    \n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        \n",
    "        # Compute distance matrices\n",
    "        n_batch_sizes = len(BATCH_SIZES)\n",
    "        matrix_key = np.zeros((n_batch_sizes, n_batch_sizes))\n",
    "        matrix_logprob = np.zeros((n_batch_sizes, n_batch_sizes))\n",
    "        \n",
    "        for i, decode_bs in enumerate(BATCH_SIZES):\n",
    "            for j, prefill_bs in enumerate(BATCH_SIZES):\n",
    "                key = (decode_bs, prefill_bs)\n",
    "                if key in ref_data:\n",
    "                    m = ref_data[key]\n",
    "                    distances = compare_decode_vs_prefill(\n",
    "                        m['decode_data'],\n",
    "                        m['prefill_data'],\n",
    "                        layer_indices\n",
    "                    )\n",
    "                    matrix_key[i, j] = distances['key_vectors_max']\n",
    "                    matrix_logprob[i, j] = distances['logprobs_max']\n",
    "        \n",
    "        # Display matrices\n",
    "        n_bs = len(BATCH_SIZES)\n",
    "        header = \"                \" + \"\".join([f\"Prefill bs={bs:>3}  \" for bs in BATCH_SIZES])\n",
    "        \n",
    "        log_print(\"\\nKey Vectors (max L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, decode_bs in enumerate(BATCH_SIZES):\n",
    "            row_str = f\"Decode bs={decode_bs:<4}  \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:11.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        log_print(\"\\nLogprobs (max L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, decode_bs in enumerate(BATCH_SIZES):\n",
    "            row_str = f\"Decode bs={decode_bs:<4}  \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:11.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        all_matrices['key_vectors'].append(matrix_key)\n",
    "        all_matrices['logprobs'].append(matrix_logprob)\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE STATISTICS (AVERAGE ACROSS REFERENCES)\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    n_bs = len(BATCH_SIZES)\n",
    "    \n",
    "    for signal_type in ['key_vectors', 'logprobs']:\n",
    "        matrices = all_matrices[signal_type]\n",
    "        avg_matrix = np.mean(matrices, axis=0)\n",
    "        \n",
    "        header = \"                \" + \"\".join([f\"Prefill bs={bs:>3}  \" for bs in BATCH_SIZES])\n",
    "        \n",
    "        log_print(f\"\\n{signal_type.upper()}:\")\n",
    "        log_print(header)\n",
    "        for i, decode_bs in enumerate(BATCH_SIZES):\n",
    "            row_str = f\"Decode bs={decode_bs:<4}  \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {avg_matrix[i,j]:11.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        # Extract diagonal (noise) and off-diagonal (signal)\n",
    "        diagonal = np.array([avg_matrix[i, i] for i in range(n_bs)])\n",
    "        off_diagonal = np.array([avg_matrix[i, j] for i in range(n_bs) for j in range(n_bs) if i != j])\n",
    "        \n",
    "        noise_mean = np.mean(diagonal)\n",
    "        noise_std = np.std(diagonal)\n",
    "        signal_mean = np.mean(off_diagonal)\n",
    "        signal_std = np.std(off_diagonal)\n",
    "        snr = signal_mean / noise_mean if noise_mean > 0 else float('inf')\n",
    "        \n",
    "        log_print(f\"\\n  Diagonal (noise - decode bs = prefill bs):\")\n",
    "        log_print(f\"    μ = {noise_mean:.2e}, σ = {noise_std:.2e}\")\n",
    "        log_print(f\"    Values: {[f'{d:.2e}' for d in diagonal]}\")\n",
    "        \n",
    "        log_print(f\"\\n  Off-diagonal (signal - decode bs ≠ prefill bs):\")\n",
    "        log_print(f\"    μ = {signal_mean:.2e}, σ = {signal_std:.2e}\")\n",
    "        \n",
    "        log_print(f\"\\n  SNR (signal/noise): {snr:.2f}×\")\n",
    "        \n",
    "        results[signal_type] = {\n",
    "            'matrix': avg_matrix.tolist(),\n",
    "            'noise_mean': float(noise_mean),\n",
    "            'noise_std': float(noise_std),\n",
    "            'signal_mean': float(signal_mean),\n",
    "            'signal_std': float(signal_std),\n",
    "            'snr': float(snr)\n",
    "        }\n",
    "    \n",
    "    # Conclusion\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"CONCLUSION\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    threshold = 1.5\n",
    "    \n",
    "    key_snr = results['key_vectors']['snr']\n",
    "    log_snr = results['logprobs']['snr']\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors: SNR = {key_snr:.2f}× {'✓ DETECTABLE' if key_snr >= threshold else '✗ NOT DETECTABLE'}\")\n",
    "    log_print(f\"Logprobs:    SNR = {log_snr:.2f}× {'✓ DETECTABLE' if log_snr >= threshold else '✗ NOT DETECTABLE'}\")\n",
    "    \n",
    "    if key_snr >= threshold and log_snr >= threshold:\n",
    "        log_print(\"\\n✓ BATCH SIZE MISMATCHES ARE DETECTABLE\")\n",
    "        log_print(\"  → Prefill can detect decode batch size\")\n",
    "        log_print(\"  → Off-diagonal distances >> diagonal distances\")\n",
    "    elif key_snr >= threshold or log_snr >= threshold:\n",
    "        log_print(\"\\n~ PARTIAL DETECTABILITY\")\n",
    "        det = 'Key vectors' if key_snr >= threshold else 'Logprobs'\n",
    "        log_print(f\"  → {det} can detect batch size mismatches\")\n",
    "    else:\n",
    "        log_print(\"\\n✗ BATCH SIZE MISMATCHES NOT RELIABLY DETECTABLE\")\n",
    "        log_print(\"  → Off-diagonal distances similar to diagonal distances\")\n",
    "        log_print(\"  → Cannot reliably detect decode batch size from prefill\")\n",
    "    \n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    matrices_serializable = {\n",
    "        'key_vectors': [m.tolist() for m in all_matrices['key_vectors']],\n",
    "        'logprobs': [m.tolist() for m in all_matrices['logprobs']]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'matrices': matrices_serializable,\n",
    "        'statistics': results\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    global REFERENCE_SEQUENCES, DUMMY_SETS\n",
    "    \n",
    "    # Setup logging to file\n",
    "    log_path = setup_logging()\n",
    "    \n",
    "    system_info = collect_system_info()\n",
    "    \n",
    "    log_print(\"=\"*80)\n",
    "    log_print(\"PREFILL vs DECODE EXPERIMENT\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"Log file: {log_path}\")\n",
    "    log_print(f\"\\nEnvironment:\")\n",
    "    for k, v in system_info.items():\n",
    "        log_print(f\"  {k}: {v}\")\n",
    "    \n",
    "    log_print(f\"\\nConfiguration:\")\n",
    "    log_print(f\"  Model: {MODEL_NAME}\")\n",
    "    log_print(f\"  Layers: {LAYER_INDICES}\")\n",
    "    log_print(f\"  Batch sizes: {BATCH_SIZES}\")\n",
    "    log_print(f\"  Max tokens: {MAX_NEW_TOKENS}\")\n",
    "    log_print()\n",
    "    \n",
    "    # Load model\n",
    "    log_print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    \n",
    "    # Initialize sequences from PDF\n",
    "    REFERENCE_SEQUENCES, DUMMY_SETS = create_sequences_from_pdf(tokenizer)\n",
    "    log_print(f\"Created {len(REFERENCE_SEQUENCES)} reference sequences with {len(DUMMY_SETS[list(DUMMY_SETS.keys())[0]])} dummies each\\n\")\n",
    "    \n",
    "    log_print(\"Experiment design:\")\n",
    "    log_print(f\"  - Decode runs at {len(BATCH_SIZES)} batch sizes\")\n",
    "    log_print(f\"  - Prefill reproduces at {len(BATCH_SIZES)} batch sizes\")\n",
    "    log_print(f\"  - Matrix: {len(BATCH_SIZES)} decode × {len(BATCH_SIZES)} prefill = {len(BATCH_SIZES)**2} comparisons per reference\")\n",
    "    log_print(\"  - Works purely at token ID level (no tokenization artifacts)\")\n",
    "    log_print(\"  - NEW: Within-mode batch size comparisons (sanity check)\")\n",
    "    log_print()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    log_print(\"✓ Model loaded\\n\")\n",
    "    \n",
    "    # Results structure\n",
    "    results = {\n",
    "        'metadata': {\n",
    "            'environment': system_info,\n",
    "            'model': MODEL_NAME,\n",
    "            'layer_indices': LAYER_INDICES,\n",
    "            'batch_sizes': BATCH_SIZES,\n",
    "            'max_new_tokens': MAX_NEW_TOKENS,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        },\n",
    "        'measurements': []\n",
    "    }\n",
    "    \n",
    "    # Run experiments\n",
    "    for ref_name, ref_text in REFERENCE_SEQUENCES.items():\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"REFERENCE: {ref_name}\")\n",
    "        log_print(f\"{'='*80}\")\n",
    "        \n",
    "        # Pre-compute minimum length across all batch configurations\n",
    "        min_prompt_length = compute_min_length_across_batches(\n",
    "            ref_text, ref_name, tokenizer, BATCH_SIZES\n",
    "        )\n",
    "        log_print(f\"\\nGlobal minimum prompt length: {min_prompt_length} tokens\")\n",
    "        log_print(\"(All batch sizes will use this length)\\n\")\n",
    "        \n",
    "        # Step 1: Run decode at each batch size\n",
    "        log_print(f\"Decode runs:\")\n",
    "        decode_measurements = {}\n",
    "        \n",
    "        for decode_bs in BATCH_SIZES:\n",
    "            log_print(f\"  bs={decode_bs}...\", end=\" \")\n",
    "            decode_data = run_decode_with_extraction(\n",
    "                model, tokenizer, ref_text, ref_name, decode_bs, LAYER_INDICES,\n",
    "                forced_length=min_prompt_length\n",
    "            )\n",
    "            \n",
    "            # Show extraction positions\n",
    "            extract_positions = [step['absolute_position'] for step in decode_data['signals'].values()]\n",
    "            log_print(f\"    Extract positions: {extract_positions}\")\n",
    "            \n",
    "            decode_measurements[decode_bs] = decode_data\n",
    "        \n",
    "        # Check token consistency\n",
    "        check_token_consistency(decode_measurements, tokenizer)\n",
    "        \n",
    "        # Step 2: Run prefill reproduction at all batch sizes\n",
    "        log_print(f\"\\nPrefill reproductions:\")\n",
    "        \n",
    "        for decode_bs in BATCH_SIZES:\n",
    "            decode_data = decode_measurements[decode_bs]\n",
    "            log_print(f\"\\n  Reproducing decode bs={decode_bs}:\")\n",
    "            \n",
    "            for prefill_bs in BATCH_SIZES:\n",
    "                log_print(f\"    with bs={prefill_bs}...\", end=\"\")\n",
    "                \n",
    "                # Determine if this is diagonal (same batch size for decode and prefill)\n",
    "                is_diagonal = (decode_bs == prefill_bs)\n",
    "                \n",
    "                prefill_data = run_prefill_verification(\n",
    "                    model, tokenizer, ref_name, decode_data, prefill_bs,\n",
    "                    LAYER_INDICES, is_diagonal=is_diagonal\n",
    "                )\n",
    "                \n",
    "                results['measurements'].append({\n",
    "                    'ref_name': ref_name,\n",
    "                    'decode_batch_size': decode_bs,\n",
    "                    'prefill_batch_size': prefill_bs,\n",
    "                    'is_diagonal': is_diagonal,\n",
    "                    'decode_data': {\n",
    "                        'generated_ids': decode_data['generated_ids'],\n",
    "                        'all_batch_generated_ids': decode_data['all_batch_generated_ids'],\n",
    "                        'prompt_length': decode_data['prompt_length'],\n",
    "                        'signals': decode_data['signals'],\n",
    "                        'num_generated': decode_data['num_generated']\n",
    "                    },\n",
    "                    'prefill_data': prefill_data\n",
    "                })\n",
    "    \n",
    "    # Save data\n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(output_dir, f\"prefill_decode_{timestamp}.json\")\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    log_print(f\"\\n✓ Data saved to: {filepath}\")\n",
    "    \n",
    "    # NEW: Analyze within-mode batch effects\n",
    "    within_mode_results = analyze_within_mode_batch_effects(results['measurements'], LAYER_INDICES)\n",
    "    \n",
    "    # Run decode vs prefill analysis\n",
    "    analysis_results = analyze_experiment(results['measurements'], LAYER_INDICES)\n",
    "    \n",
    "    # Save with both analyses\n",
    "    results['analysis'] = {\n",
    "        'within_mode_batch_effects': within_mode_results,\n",
    "        'decode_vs_prefill': analysis_results\n",
    "    }\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "    log_print(f\"✓ Analysis saved (file size: {file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"EXPERIMENT COMPLETE\")\n",
    "    log_print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    close_logging()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_ipython()\n",
    "    except NameError:\n",
    "        pass\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98aefd-ec1b-48e4-8ec8-ea87a36a82ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
