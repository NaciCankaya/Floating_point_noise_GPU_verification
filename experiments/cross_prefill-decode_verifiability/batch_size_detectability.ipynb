{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2728246b-f02a-41a3-87de-8a44459d0071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREFILL vs DECODE EXPERIMENT - CLAIM VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "Environment:\n",
      "  hostname: 812aecb237c2\n",
      "  platform: Linux-6.8.0-49-generic-x86_64-with-glibc2.39\n",
      "  python_version: 3.12.11\n",
      "  torch_version: 2.8.0+cu126\n",
      "  cuda_version: 12.6\n",
      "  transformers_version: 4.57.1\n",
      "  gpu_name: NVIDIA A100-SXM4-80GB\n",
      "  gpu_count: 1\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Layers: [1, 4, 10, 18, 28]\n",
      "  Batch sizes: [1, 2, 4]\n",
      "  References: 3\n",
      "  Max tokens: 20\n",
      "\n",
      "Experiment design:\n",
      "  - Decoder makes claims at bs=1,2,4\n",
      "  - Prefiller verifies each claim at bs=1,2,4\n",
      "  - Matrix: 3 claims × 3 verifications = 9 comparisons per reference\n",
      "  - Works purely at token ID level (no tokenization artifacts)\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc0b0ee3c764cac8c58067a208d3ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_technical\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 56 tokens\n",
      "(All batch sizes will use this length)\n",
      "\n",
      "Decode claims:\n",
      "  bs=1...       Prompt length: 56 tokens → Final: 76 tokens (20 generated)\n",
      "    Extract positions: [72, 73, 74]\n",
      "  bs=2...       Prompt length: 56 tokens → Final: 76 tokens (20 generated)\n",
      "    Extract positions: [72, 73, 74]\n",
      "  bs=4...       Prompt length: 56 tokens → Final: 76 tokens (20 generated)\n",
      "    Extract positions: [72, 73, 74]\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1: [279, 1614, 374, 14900, 311, 12767, 14713, 315, 1467, 821]... ✓\n",
      "  bs=2: [279, 1614, 374, 14900, 311, 12767, 14713, 315, 1467, 821]... ✓\n",
      "  bs=4: [279, 1614, 374, 14900, 311, 12767, 14713, 315, 1467, 821]... ✓\n",
      "\n",
      "✓ Element 0 generates IDENTICAL tokens across all batch sizes\n",
      "\n",
      "Prefill verifications:\n",
      "\n",
      "  Verifying claim from bs=1:\n",
      "    with bs=1... [Ext: 76 tokens] → Extract at: [72, 73, 74]\n",
      "    with bs=2... [Ext: 76 tokens, arb neighbors] → Extract at: [72, 73, 74]\n",
      "    with bs=4... [Ext: 76 tokens, arb neighbors] → Extract at: [72, 73, 74]\n",
      "\n",
      "  Verifying claim from bs=2:\n",
      "    with bs=1... [Ext: 76 tokens] → Extract at: [72, 73, 74]\n",
      "    with bs=2... [Ext: 76 tokens, exact neighbors] → Extract at: [72, 73, 74]\n",
      "    with bs=4... [Ext: 76 tokens, arb neighbors] → Extract at: [72, 73, 74]\n",
      "\n",
      "  Verifying claim from bs=4:\n",
      "    with bs=1... [Ext: 76 tokens] → Extract at: [72, 73, 74]\n",
      "    with bs=2... [Ext: 76 tokens, arb neighbors] → Extract at: [72, 73, 74]\n",
      "    with bs=4... [Ext: 76 tokens, exact neighbors] → Extract at: [72, 73, 74]\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_narrative\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 66 tokens\n",
      "(All batch sizes will use this length)\n",
      "\n",
      "Decode claims:\n",
      "  bs=1...       Prompt length: 66 tokens → Final: 86 tokens (20 generated)\n",
      "    Extract positions: [82, 83, 84]\n",
      "  bs=2...       Prompt length: 66 tokens → Final: 86 tokens (20 generated)\n",
      "    Extract positions: [82, 83, 84]\n",
      "  bs=4...       Prompt length: 66 tokens → Final: 86 tokens (20 generated)\n",
      "    Extract positions: [82, 83, 84]\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1: [7702, 11, 264, 5530, 315, 64775, 323, 78322, 22266, 518]... ✓\n",
      "  bs=2: [7702, 11, 264, 5530, 315, 64775, 323, 78322, 22266, 518]... ✓\n",
      "  bs=4: [7702, 11, 264, 5530, 315, 64775, 323, 78322, 22266, 518]... ✓\n",
      "\n",
      "✓ Element 0 generates IDENTICAL tokens across all batch sizes\n",
      "\n",
      "Prefill verifications:\n",
      "\n",
      "  Verifying claim from bs=1:\n",
      "    with bs=1... [Ext: 86 tokens] → Extract at: [82, 83, 84]\n",
      "    with bs=2... [Ext: 86 tokens, arb neighbors] → Extract at: [82, 83, 84]\n",
      "    with bs=4... [Ext: 86 tokens, arb neighbors] → Extract at: [82, 83, 84]\n",
      "\n",
      "  Verifying claim from bs=2:\n",
      "    with bs=1... [Ext: 86 tokens] → Extract at: [82, 83, 84]\n",
      "    with bs=2... [Ext: 86 tokens, exact neighbors] → Extract at: [82, 83, 84]\n",
      "    with bs=4... [Ext: 86 tokens, arb neighbors] → Extract at: [82, 83, 84]\n",
      "\n",
      "  Verifying claim from bs=4:\n",
      "    with bs=1... [Ext: 86 tokens] → Extract at: [82, 83, 84]\n",
      "    with bs=2... [Ext: 86 tokens, arb neighbors] → Extract at: [82, 83, 84]\n",
      "    with bs=4... [Ext: 86 tokens, exact neighbors] → Extract at: [82, 83, 84]\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_code\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 54 tokens\n",
      "(All batch sizes will use this length)\n",
      "\n",
      "Decode claims:\n",
      "  bs=1...       Prompt length: 54 tokens → Final: 74 tokens (20 generated)\n",
      "    Extract positions: [70, 71, 72]\n",
      "  bs=2...       Prompt length: 54 tokens → Final: 74 tokens (20 generated)\n",
      "    Extract positions: [70, 71, 72]\n",
      "  bs=4...       Prompt length: 54 tokens → Final: 74 tokens (20 generated)\n",
      "    Extract positions: [70, 71, 72]\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1: [2750, 311, 41229, 10542, 1817, 11906, 382, 10048, 2900, 279]... ✓\n",
      "  bs=2: [2750, 311, 41229, 10542, 1817, 11906, 382, 10048, 2900, 279]... ✓\n",
      "  bs=4: [2750, 311, 41229, 10542, 1817, 11906, 382, 10048, 2900, 279]... ✓\n",
      "\n",
      "✓ Element 0 generates IDENTICAL tokens across all batch sizes\n",
      "\n",
      "Prefill verifications:\n",
      "\n",
      "  Verifying claim from bs=1:\n",
      "    with bs=1... [Ext: 74 tokens] → Extract at: [70, 71, 72]\n",
      "    with bs=2... [Ext: 74 tokens, arb neighbors] → Extract at: [70, 71, 72]\n",
      "    with bs=4... [Ext: 74 tokens, arb neighbors] → Extract at: [70, 71, 72]\n",
      "\n",
      "  Verifying claim from bs=2:\n",
      "    with bs=1... [Ext: 74 tokens] → Extract at: [70, 71, 72]\n",
      "    with bs=2... [Ext: 74 tokens, exact neighbors] → Extract at: [70, 71, 72]\n",
      "    with bs=4... [Ext: 74 tokens, arb neighbors] → Extract at: [70, 71, 72]\n",
      "\n",
      "  Verifying claim from bs=4:\n",
      "    with bs=1... [Ext: 74 tokens] → Extract at: [70, 71, 72]\n",
      "    with bs=2... [Ext: 74 tokens, arb neighbors] → Extract at: [70, 71, 72]\n",
      "    with bs=4... [Ext: 74 tokens, exact neighbors] → Extract at: [70, 71, 72]\n",
      "\n",
      "✓ Data saved to: /workspace/experiments/prefill_decode_20251114_014631.json\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS: CLAIM vs VERIFICATION MATRIX\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_CODE\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (max L2 distance):\n",
      "                Verify bs=1   Verify bs=2   Verify bs=4\n",
      "Claim bs=1        2.67e-01     2.99e-01     6.44e-01\n",
      "Claim bs=2        3.82e-01     4.01e-01     5.70e-01\n",
      "Claim bs=4        5.90e-01     5.93e-01     4.01e-01\n",
      "\n",
      "Logprobs (max L2 distance):\n",
      "                Verify bs=1   Verify bs=2   Verify bs=4\n",
      "Claim bs=1        3.16e-01     4.22e-01     2.42e-01\n",
      "Claim bs=2        2.90e-01     3.62e-01     3.20e-01\n",
      "Claim bs=4        4.64e-01     4.47e-01     4.80e-01\n",
      "\n",
      "================================================================================\n",
      "REF_NARRATIVE\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (max L2 distance):\n",
      "                Verify bs=1   Verify bs=2   Verify bs=4\n",
      "Claim bs=1        3.90e-01     3.28e-01     1.04e+00\n",
      "Claim bs=2        3.43e-01     3.54e-01     1.04e+00\n",
      "Claim bs=4        3.74e-01     3.30e-01     1.03e+00\n",
      "\n",
      "Logprobs (max L2 distance):\n",
      "                Verify bs=1   Verify bs=2   Verify bs=4\n",
      "Claim bs=1        2.92e-01     2.50e-01     2.71e-01\n",
      "Claim bs=2        2.86e-01     3.70e-01     3.31e-01\n",
      "Claim bs=4        3.25e-01     1.26e-01     2.41e-01\n",
      "\n",
      "================================================================================\n",
      "REF_TECHNICAL\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (max L2 distance):\n",
      "                Verify bs=1   Verify bs=2   Verify bs=4\n",
      "Claim bs=1        3.15e-01     3.19e-01     5.21e-01\n",
      "Claim bs=2        3.13e-01     3.23e-01     5.22e-01\n",
      "Claim bs=4        3.08e-01     3.13e-01     5.22e-01\n",
      "\n",
      "Logprobs (max L2 distance):\n",
      "                Verify bs=1   Verify bs=2   Verify bs=4\n",
      "Claim bs=1        4.68e-01     2.17e-01     2.31e-01\n",
      "Claim bs=2        5.59e-01     2.17e-01     2.31e-01\n",
      "Claim bs=4        4.68e-01     2.30e-01     3.04e-01\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE STATISTICS (AVERAGE ACROSS REFERENCES)\n",
      "================================================================================\n",
      "\n",
      "KEY_VECTORS:\n",
      "                Verify bs=1   Verify bs=2   Verify bs=4\n",
      "Claim bs=1        3.24e-01     3.15e-01     7.36e-01\n",
      "Claim bs=2        3.46e-01     3.59e-01     7.12e-01\n",
      "Claim bs=4        4.24e-01     4.12e-01     6.52e-01\n",
      "\n",
      "  Diagonal (noise - claim matches verification):\n",
      "    μ = 4.45e-01, σ = 1.47e-01\n",
      "    Values: ['3.24e-01', '3.59e-01', '6.52e-01']\n",
      "\n",
      "  Off-diagonal (signal - claim doesn't match verification):\n",
      "    μ = 4.91e-01, σ = 1.69e-01\n",
      "\n",
      "  SNR (signal/noise): 1.10×\n",
      "\n",
      "LOGPROBS:\n",
      "                Verify bs=1   Verify bs=2   Verify bs=4\n",
      "Claim bs=1        3.59e-01     2.96e-01     2.48e-01\n",
      "Claim bs=2        3.78e-01     3.16e-01     2.94e-01\n",
      "Claim bs=4        4.19e-01     2.68e-01     3.42e-01\n",
      "\n",
      "  Diagonal (noise - claim matches verification):\n",
      "    μ = 3.39e-01, σ = 1.74e-02\n",
      "    Values: ['3.59e-01', '3.16e-01', '3.42e-01']\n",
      "\n",
      "  Off-diagonal (signal - claim doesn't match verification):\n",
      "    μ = 3.17e-01, σ = 6.09e-02\n",
      "\n",
      "  SNR (signal/noise): 0.94×\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "\n",
      "Key Vectors: SNR = 1.10× ✗ NOT DETECTABLE\n",
      "Logprobs:    SNR = 0.94× ✗ NOT DETECTABLE\n",
      "\n",
      "✗ BATCH SIZE MISMATCHES NOT RELIABLY DETECTABLE\n",
      "  → Verification distance similar whether claimed bs matches or not\n",
      "  → Cannot reliably catch decoder lying about batch size\n",
      "✓ Analysis saved (file size: 11.4 MB)\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Prefill vs Decode Experiment - Claim Verification Matrix\n",
    "\n",
    "Fixed version that:\n",
    "1. Works purely at token ID level (no decode/re-encode cycles)\n",
    "2. Stores exact token IDs from decode\n",
    "3. Compares logprobs for SAME token IDs between decode and prefill\n",
    "4. Properly serializes numpy arrays to JSON\n",
    "\n",
    "Usage:\n",
    "    python prefill_decode_experiment.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "REFERENCE_SEQUENCES = {\n",
    "    \"ref_technical\": \"\"\"Large language models have revolutionized natural language processing through their ability to capture complex patterns in text data. The transformer architecture, introduced in 2017, employs self-attention mechanisms that allow the model to weigh the importance of different tokens in the input sequence. During training, these models learn to predict the next token in a sequence by optimizing a cross-entropy loss function across billions of text examples.\"\"\",\n",
    "    \n",
    "    \"ref_narrative\": \"\"\"The morning sun filtered through the ancient oak trees as Sarah walked along the forest path, her boots crunching softly on the fallen leaves. She had been coming to these woods since childhood, when her grandmother first taught her to identify the different bird calls echoing through the canopy. Now, decades later, she found herself returning to this same trail whenever life felt overwhelming.\"\"\",\n",
    "    \n",
    "    \"ref_code\": \"\"\"The database migration system implements a sophisticated version control mechanism for schema changes. Each migration file contains both an upgrade and downgrade function, allowing the system to roll forward or backward through schema versions. The migration engine maintains a table tracking which migrations have been applied, using timestamps and hash values to ensure consistency across different environments.\"\"\"\n",
    "}\n",
    "\n",
    "DUMMY_SETS = {\n",
    "    \"ref_technical\": [\n",
    "        \"\"\"Quantum computing leverages the principles of quantum mechanics to perform computations that would be intractable for classical computers. At the heart of quantum computation lies the qubit, a quantum bit that can exist in a superposition of both 0 and 1 states simultaneously. When multiple qubits are entangled, they form a quantum register capable of representing an exponentially large state space.\"\"\",\n",
    "        \n",
    "        \"\"\"The neural architecture search algorithm systematically explores different model configurations to identify optimal designs for specific tasks. Modern approaches use reinforcement learning or evolutionary algorithms to navigate the vast search space of possible architectures. The process evaluates candidate models on validation data, gradually converging toward efficient and effective network topologies.\"\"\",\n",
    "        \n",
    "        \"\"\"Distributed consensus protocols enable multiple nodes in a network to agree on a single value despite potential failures or malicious actors. The Byzantine Generals Problem formalizes the challenge of achieving consensus when some participants may behave arbitrarily. Practical solutions like Paxos and Raft provide mechanisms for fault-tolerant agreement in real-world systems.\"\"\"\n",
    "    ],\n",
    "    \n",
    "    \"ref_narrative\": [\n",
    "        \"\"\"The old lighthouse stood sentinel on the rocky promontory, its weathered walls bearing testament to countless storms. Local legends spoke of the keeper who vanished one winter night, leaving only his log book with a final cryptic entry. Now automated, the beacon still swept across the dark waters, a guardian whose original purpose had long been superseded by modern navigation systems.\"\"\",\n",
    "        \n",
    "        \"\"\"Marcus found the letter tucked between the pages of his grandfather's journal, the paper yellowed and fragile with age. The handwriting was unfamiliar, yet the words spoke of events his family had never discussed. As he read, pieces of his heritage began to fall into place, revealing a story that had been deliberately hidden for three generations.\"\"\",\n",
    "        \n",
    "        \"\"\"The jazz club occupied a basement space that seemed to exist outside of time, where smoke still hung in the air despite the ban and the music felt like it emerged from another era. Every Thursday, the same musicians gathered to play standards that few in the younger generation recognized. Yet something about the atmosphere drew people in, seeking connection to an authenticity they sensed was disappearing from the world.\"\"\"\n",
    "    ],\n",
    "    \n",
    "    \"ref_code\": [\n",
    "        \"\"\"The distributed caching layer implements consistent hashing to minimize cache invalidation when nodes are added or removed from the cluster. Virtual nodes provide better load distribution across physical servers, while replication ensures availability even during node failures. The system monitors hit rates and eviction patterns to automatically adjust cache allocation strategies.\"\"\",\n",
    "        \n",
    "        \"\"\"The API gateway performs request routing, authentication, rate limiting, and response transformation for microservices. Each service registers its endpoints with the gateway, which maintains a dynamic routing table. The gateway implements circuit breakers to prevent cascade failures and provides detailed metrics for monitoring service health.\"\"\",\n",
    "        \n",
    "        \"\"\"The message queue system guarantees exactly-once delivery through a combination of acknowledgments, persistent storage, and idempotency tokens. Publishers receive confirmation only after messages are durably written to replicated storage. Consumers process messages within transactions, ensuring atomic updates across message consumption and business logic execution.\"\"\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "BATCH_SIZES = [1, 2, 4]\n",
    "LAYER_INDICES = [1, 4, 10, 18, 28]\n",
    "MAX_NEW_TOKENS = 20\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "    \n",
    "    return info\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_signals_from_output(outputs, layer_indices, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs from element 0 at specified position.\n",
    "    Returns top-10 token IDs so prefill can compare the same tokens.\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "    \n",
    "    # Key vectors from element 0\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "    \n",
    "    # Logprobs from element 0 - get top 10\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    top10 = torch.topk(log_probs, k=10)\n",
    "    \n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': top10.indices.cpu().tolist(),\n",
    "        'log_probs': top10.values.cpu().tolist()\n",
    "    }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "\n",
    "def extract_signals_for_token_ids(outputs, layer_indices, token_ids, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs for SPECIFIC token IDs.\n",
    "    Used in prefill to compare the same tokens as decode.\n",
    "    \n",
    "    Args:\n",
    "        token_ids: List of token IDs to extract logprobs for (from decode's top-10)\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "    \n",
    "    # Key vectors from element 0\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "    \n",
    "    # Logprobs for SPECIFIC token IDs (from decode's top 10)\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # Extract logprobs for the specified token IDs\n",
    "    token_ids_tensor = torch.tensor(token_ids, device=logits.device)\n",
    "    selected_logprobs = log_probs[token_ids_tensor]\n",
    "    \n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': token_ids,  # Same as decode\n",
    "        'log_probs': selected_logprobs.cpu().tolist()\n",
    "    }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# ============================================================================\n",
    "# DECODE GENERATION & EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def compute_min_length_across_batches(ref_text, ref_name, tokenizer, batch_sizes):\n",
    "    \"\"\"\n",
    "    Pre-compute minimum sequence length across all batch configurations.\n",
    "    This ensures all batch sizes use the same sequence length.\n",
    "    \"\"\"\n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    min_length = float('inf')\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        if batch_size == 1:\n",
    "            batch_texts = [ref_text]\n",
    "        elif batch_size == 2:\n",
    "            batch_texts = [ref_text, ref_dummies[0]]\n",
    "        elif batch_size == 4:\n",
    "            batch_texts = [ref_text] + ref_dummies[:3]\n",
    "        \n",
    "        token_lengths = [len(tokenizer.encode(t, add_special_tokens=True)) for t in batch_texts]\n",
    "        min_length = min(min_length, min(token_lengths))\n",
    "    \n",
    "    return min_length\n",
    "\n",
    "\n",
    "def run_decode_with_extraction(model, tokenizer, ref_text, ref_name, batch_size, \n",
    "                                layer_indices, forced_length=None):\n",
    "    \"\"\"\n",
    "    Run decode generation and extract signals from last 3 generation steps.\n",
    "    Works purely at token ID level - no decode/re-encode cycles.\n",
    "    \n",
    "    Returns exact token IDs used for prefill to reproduce.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Build batch texts\n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    if batch_size == 1:\n",
    "        batch_texts = [ref_text]\n",
    "    elif batch_size == 2:\n",
    "        batch_texts = [ref_text, ref_dummies[0]]\n",
    "    elif batch_size == 4:\n",
    "        batch_texts = [ref_text] + ref_dummies[:3]\n",
    "    \n",
    "    # Tokenize ONCE - store the exact token IDs\n",
    "    all_token_ids = [tokenizer.encode(t, add_special_tokens=True) for t in batch_texts]\n",
    "    \n",
    "    # Truncate at token ID level\n",
    "    if forced_length is not None:\n",
    "        min_length = forced_length\n",
    "    else:\n",
    "        min_length = min(len(ids) for ids in all_token_ids)\n",
    "    \n",
    "    truncated_token_ids = [ids[:min_length] for ids in all_token_ids]\n",
    "    \n",
    "    # Build input tensors DIRECTLY from token IDs - no decode/re-encode\n",
    "    input_ids = torch.tensor(truncated_token_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    print(f\"      Prompt length: {prompt_length} tokens\", end=\"\")\n",
    "    \n",
    "    # Track generation for ALL batch positions\n",
    "    all_batch_generated_ids = [[] for _ in range(batch_size)]\n",
    "    generation_signals = []\n",
    "    \n",
    "    # FIRST STEP: Prefill with full prompt\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "    \n",
    "    past_kv = outputs.past_key_values\n",
    "    \n",
    "    # Get first token\n",
    "    next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "    for batch_idx in range(batch_size):\n",
    "        all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "    \n",
    "    # Extract signals from element 0\n",
    "    signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "    \n",
    "    # Absolute position: input length - 1\n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    \n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    # Update attention mask\n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((inputs['attention_mask'].shape[0], 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS: True autoregressive with cache\n",
    "    for step in range(1, MAX_NEW_TOKENS):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        # Get next tokens\n",
    "        next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "        for batch_idx in range(batch_size):\n",
    "            all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "        \n",
    "        # Extract signals from element 0\n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        \n",
    "        # Absolute position in full sequence\n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        # Update attention mask\n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((attention_mask.shape[0], 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Check for EOS in position 0\n",
    "        if all_batch_generated_ids[0][-1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Extract last 3\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "    \n",
    "    del outputs\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    final_length = prompt_length + num_generated\n",
    "    print(f\" → Final: {final_length} tokens ({num_generated} generated)\")\n",
    "    \n",
    "    return {\n",
    "        'generated_ids': all_batch_generated_ids[0],  # Position 0 tokens\n",
    "        'all_batch_generated_ids': all_batch_generated_ids,  # ALL positions\n",
    "        'prompt_token_ids': truncated_token_ids,  # EXACT token IDs used in prompt\n",
    "        'prompt_length': prompt_length,\n",
    "        'signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# PREFILL REPRODUCTION\n",
    "# ============================================================================\n",
    "\n",
    "def run_prefill_verification(model, tokenizer, ref_name, decode_metadata, \n",
    "                             batch_size, layer_indices, is_diagonal=False):\n",
    "    \"\"\"\n",
    "    Run prefill to verify decoder's claim.\n",
    "    Uses EXACT token IDs from decode - no tokenization artifacts.\n",
    "    \n",
    "    Args:\n",
    "        is_diagonal: True if verifying honest claim (same bs), uses actual neighbors\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Get exact prompt IDs used in decode for element 0\n",
    "    ref_prompt_ids = decode_metadata['prompt_token_ids'][0]\n",
    "    generated_ids = decode_metadata['generated_ids']\n",
    "    \n",
    "    # Build extended sequence at ID level\n",
    "    extended_ref_ids = ref_prompt_ids + generated_ids\n",
    "    \n",
    "    print(f\" [Ext: {len(extended_ref_ids)} tokens\", end=\"\")\n",
    "    \n",
    "    # Build batch\n",
    "    if batch_size == 1:\n",
    "        batch_ids = [extended_ref_ids]\n",
    "    else:\n",
    "        batch_ids = [extended_ref_ids]\n",
    "        \n",
    "        if is_diagonal:\n",
    "            # Use EXACT neighbor IDs from decode\n",
    "            print(f\", exact neighbors\", end=\"\")\n",
    "            for i in range(1, batch_size):\n",
    "                neighbor_prompt_ids = decode_metadata['prompt_token_ids'][i]\n",
    "                neighbor_gen_ids = decode_metadata['all_batch_generated_ids'][i]\n",
    "                extended_neighbor_ids = neighbor_prompt_ids + neighbor_gen_ids\n",
    "                batch_ids.append(extended_neighbor_ids)\n",
    "        else:\n",
    "            # Off-diagonal: construct length-matched neighbors at ID level\n",
    "            print(f\", arb neighbors\", end=\"\")\n",
    "            ref_dummies = DUMMY_SETS[ref_name]\n",
    "            target_length = len(extended_ref_ids)\n",
    "            \n",
    "            for i in range(batch_size - 1):\n",
    "                dummy_ids = tokenizer.encode(ref_dummies[i], add_special_tokens=True)\n",
    "                \n",
    "                # Truncate or repeat to match length\n",
    "                if len(dummy_ids) >= target_length:\n",
    "                    dummy_ids = dummy_ids[:target_length]\n",
    "                else:\n",
    "                    # Repeat tokens to reach target length\n",
    "                    repeats = (target_length // len(dummy_ids)) + 1\n",
    "                    dummy_ids = (dummy_ids * repeats)[:target_length]\n",
    "                \n",
    "                batch_ids.append(dummy_ids)\n",
    "    \n",
    "    # Build input tensors DIRECTLY from IDs - no decode/encode cycle\n",
    "    input_ids = torch.tensor(batch_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "    print(f\"]\", end=\"\")\n",
    "    \n",
    "    # Single forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "    \n",
    "    # Extract at same positions as decode, using decode's token IDs for logprobs\n",
    "    prefill_signals = {}\n",
    "    extraction_positions = []\n",
    "    \n",
    "    for pos_label, decode_step_data in decode_metadata['signals'].items():\n",
    "        decode_abs_pos = decode_step_data['absolute_position']\n",
    "        decode_token_ids = decode_step_data['signals']['logprobs']['token_ids']\n",
    "        \n",
    "        extraction_positions.append(decode_abs_pos)\n",
    "        \n",
    "        # Extract using SAME token IDs as decode for logprobs\n",
    "        signals = extract_signals_for_token_ids(\n",
    "            outputs, layer_indices, decode_token_ids, position=decode_abs_pos\n",
    "        )\n",
    "        \n",
    "        prefill_signals[pos_label] = {\n",
    "            'absolute_position': decode_abs_pos,\n",
    "            'signals': signals\n",
    "        }\n",
    "    \n",
    "    print(f\" → Extract at: {extraction_positions}\")\n",
    "    \n",
    "    del outputs\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return prefill_signals\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_l2_distance(vec1, vec2):\n",
    "    \"\"\"Compute L2 distance between two vectors.\"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return float(np.linalg.norm(v1 - v2))\n",
    "\n",
    "\n",
    "def compute_logprob_distance(logprobs1, logprobs2):\n",
    "    \"\"\"\n",
    "    Compute L2 distance between logprob distributions.\n",
    "    Both should have same token_ids in same order.\n",
    "    \"\"\"\n",
    "    probs1 = np.array(logprobs1['log_probs'])\n",
    "    probs2 = np.array(logprobs2['log_probs'])\n",
    "    return float(np.linalg.norm(probs1 - probs2))\n",
    "\n",
    "\n",
    "def compare_claim_vs_verification(decode_data, prefill_data, layer_indices):\n",
    "    \"\"\"Compare decoder's claim against prefiller's verification.\"\"\"\n",
    "    common_positions = set(decode_data['signals'].keys()) & set(prefill_data.keys())\n",
    "    \n",
    "    all_key_dists = []\n",
    "    all_logprob_dists = []\n",
    "    \n",
    "    for pos_label in common_positions:\n",
    "        decode_signals = decode_data['signals'][pos_label]['signals']\n",
    "        prefill_signals = prefill_data[pos_label]['signals']\n",
    "        \n",
    "        # Key vectors\n",
    "        for layer_name in decode_signals['key_vectors'].keys():\n",
    "            dist = compute_l2_distance(\n",
    "                decode_signals['key_vectors'][layer_name],\n",
    "                prefill_signals['key_vectors'][layer_name]\n",
    "            )\n",
    "            all_key_dists.append(dist)\n",
    "        \n",
    "        # Logprobs\n",
    "        dist = compute_logprob_distance(decode_signals['logprobs'], prefill_signals['logprobs'])\n",
    "        all_logprob_dists.append(dist)\n",
    "    \n",
    "    return {\n",
    "        'key_vectors_max': max(all_key_dists) if all_key_dists else 0.0,\n",
    "        'key_vectors_mean': np.mean(all_key_dists) if all_key_dists else 0.0,\n",
    "        'logprobs_max': max(all_logprob_dists) if all_logprob_dists else 0.0,\n",
    "        'logprobs_mean': np.mean(all_logprob_dists) if all_logprob_dists else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "def check_token_consistency(decode_measurements):\n",
    "    \"\"\"Check if element 0 generates same tokens across batch sizes.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    tokens_by_bs = {}\n",
    "    for bs, data in decode_measurements.items():\n",
    "        tokens_by_bs[bs] = data['generated_ids']\n",
    "    \n",
    "    bs_list = sorted(tokens_by_bs.keys())\n",
    "    reference_tokens = tokens_by_bs[bs_list[0]]\n",
    "    \n",
    "    all_same = True\n",
    "    print(\"\\nGenerated tokens by batch size:\")\n",
    "    for bs in bs_list:\n",
    "        tokens = tokens_by_bs[bs]\n",
    "        match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "        print(f\"  bs={bs}: {tokens[:10]}{'...' if len(tokens) > 10 else ''} {match_str}\")\n",
    "        if tokens != reference_tokens:\n",
    "            all_same = False\n",
    "    \n",
    "    if all_same:\n",
    "        print(\"\\n✓ Element 0 generates IDENTICAL tokens across all batch sizes\")\n",
    "    else:\n",
    "        print(\"\\n⚠ Element 0 generates DIFFERENT tokens across batch sizes\")\n",
    "        print(\"  This is expected if batch composition affects generation\")\n",
    "    \n",
    "    return all_same\n",
    "\n",
    "\n",
    "def analyze_experiment(measurements, layer_indices):\n",
    "    \"\"\"Compute and display claim-verification matrices.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS: CLAIM vs VERIFICATION MATRIX\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Group by reference\n",
    "    by_ref = {}\n",
    "    for m in measurements:\n",
    "        ref = m['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (m['decode_batch_size'], m['prefill_batch_size'])\n",
    "        by_ref[ref][key] = m\n",
    "    \n",
    "    # Compute matrices for each reference\n",
    "    all_matrices = {'key_vectors': [], 'logprobs': []}\n",
    "    \n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"{ref_name.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        \n",
    "        # Compute distance matrices\n",
    "        matrix_key = np.zeros((3, 3))\n",
    "        matrix_logprob = np.zeros((3, 3))\n",
    "        \n",
    "        for i, decode_bs in enumerate(BATCH_SIZES):\n",
    "            for j, prefill_bs in enumerate(BATCH_SIZES):\n",
    "                key = (decode_bs, prefill_bs)\n",
    "                if key in ref_data:\n",
    "                    m = ref_data[key]\n",
    "                    distances = compare_claim_vs_verification(\n",
    "                        m['decode_data'],\n",
    "                        m['prefill_data'],\n",
    "                        layer_indices\n",
    "                    )\n",
    "                    matrix_key[i, j] = distances['key_vectors_max']\n",
    "                    matrix_logprob[i, j] = distances['logprobs_max']\n",
    "        \n",
    "        # Display matrices\n",
    "        print(\"\\nKey Vectors (max L2 distance):\")\n",
    "        print(\"                Verify bs=1   Verify bs=2   Verify bs=4\")\n",
    "        for i, decode_bs in enumerate(BATCH_SIZES):\n",
    "            row_str = f\"Claim bs={decode_bs}   \"\n",
    "            for j in range(3):\n",
    "                row_str += f\"  {matrix_key[i,j]:11.2e}\"\n",
    "            print(row_str)\n",
    "        \n",
    "        print(\"\\nLogprobs (max L2 distance):\")\n",
    "        print(\"                Verify bs=1   Verify bs=2   Verify bs=4\")\n",
    "        for i, decode_bs in enumerate(BATCH_SIZES):\n",
    "            row_str = f\"Claim bs={decode_bs}   \"\n",
    "            for j in range(3):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:11.2e}\"\n",
    "            print(row_str)\n",
    "        \n",
    "        all_matrices['key_vectors'].append(matrix_key)\n",
    "        all_matrices['logprobs'].append(matrix_logprob)\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGGREGATE STATISTICS (AVERAGE ACROSS REFERENCES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for signal_type in ['key_vectors', 'logprobs']:\n",
    "        matrices = all_matrices[signal_type]\n",
    "        avg_matrix = np.mean(matrices, axis=0)\n",
    "        \n",
    "        print(f\"\\n{signal_type.upper()}:\")\n",
    "        print(\"                Verify bs=1   Verify bs=2   Verify bs=4\")\n",
    "        for i, decode_bs in enumerate(BATCH_SIZES):\n",
    "            row_str = f\"Claim bs={decode_bs}   \"\n",
    "            for j in range(3):\n",
    "                row_str += f\"  {avg_matrix[i,j]:11.2e}\"\n",
    "            print(row_str)\n",
    "        \n",
    "        # Extract diagonal (noise) and off-diagonal (signal)\n",
    "        diagonal = np.array([avg_matrix[i, i] for i in range(3)])\n",
    "        off_diagonal = np.array([avg_matrix[i, j] for i in range(3) for j in range(3) if i != j])\n",
    "        \n",
    "        noise_mean = np.mean(diagonal)\n",
    "        noise_std = np.std(diagonal)\n",
    "        signal_mean = np.mean(off_diagonal)\n",
    "        signal_std = np.std(off_diagonal)\n",
    "        snr = signal_mean / noise_mean if noise_mean > 0 else float('inf')\n",
    "        \n",
    "        print(f\"\\n  Diagonal (noise - claim matches verification):\")\n",
    "        print(f\"    μ = {noise_mean:.2e}, σ = {noise_std:.2e}\")\n",
    "        print(f\"    Values: {[f'{d:.2e}' for d in diagonal]}\")\n",
    "        \n",
    "        print(f\"\\n  Off-diagonal (signal - claim doesn't match verification):\")\n",
    "        print(f\"    μ = {signal_mean:.2e}, σ = {signal_std:.2e}\")\n",
    "        \n",
    "        print(f\"\\n  SNR (signal/noise): {snr:.2f}×\")\n",
    "        \n",
    "        results[signal_type] = {\n",
    "            'matrix': avg_matrix.tolist(),\n",
    "            'noise_mean': float(noise_mean),\n",
    "            'noise_std': float(noise_std),\n",
    "            'signal_mean': float(signal_mean),\n",
    "            'signal_std': float(signal_std),\n",
    "            'snr': float(snr)\n",
    "        }\n",
    "    \n",
    "    # Conclusion\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CONCLUSION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    threshold = 1.5\n",
    "    \n",
    "    key_snr = results['key_vectors']['snr']\n",
    "    log_snr = results['logprobs']['snr']\n",
    "    \n",
    "    print(f\"\\nKey Vectors: SNR = {key_snr:.2f}× {'✓ DETECTABLE' if key_snr >= threshold else '✗ NOT DETECTABLE'}\")\n",
    "    print(f\"Logprobs:    SNR = {log_snr:.2f}× {'✓ DETECTABLE' if log_snr >= threshold else '✗ NOT DETECTABLE'}\")\n",
    "    \n",
    "    if key_snr >= threshold and log_snr >= threshold:\n",
    "        print(\"\\n✓ BATCH SIZE MISMATCHES ARE DETECTABLE\")\n",
    "        print(\"  → Prefiller can detect when decoder lies about batch size\")\n",
    "        print(\"  → Verification fails when claimed bs ≠ verification bs\")\n",
    "    elif key_snr >= threshold or log_snr >= threshold:\n",
    "        print(\"\\n~ PARTIAL DETECTABILITY\")\n",
    "        det = 'Key vectors' if key_snr >= threshold else 'Logprobs'\n",
    "        print(f\"  → {det} can detect batch size mismatches\")\n",
    "    else:\n",
    "        print(\"\\n✗ BATCH SIZE MISMATCHES NOT RELIABLY DETECTABLE\")\n",
    "        print(\"  → Verification distance similar whether claimed bs matches or not\")\n",
    "        print(\"  → Cannot reliably catch decoder lying about batch size\")\n",
    "    \n",
    "    # Convert numpy arrays to lists for JSON serialization\n",
    "    matrices_serializable = {\n",
    "        'key_vectors': [m.tolist() for m in all_matrices['key_vectors']],\n",
    "        'logprobs': [m.tolist() for m in all_matrices['logprobs']]\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'matrices': matrices_serializable,\n",
    "        'statistics': results\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    system_info = collect_system_info()\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PREFILL vs DECODE EXPERIMENT - CLAIM VERIFICATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nEnvironment:\")\n",
    "    for k, v in system_info.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    print(f\"\\nConfiguration:\")\n",
    "    print(f\"  Model: {MODEL_NAME}\")\n",
    "    print(f\"  Layers: {LAYER_INDICES}\")\n",
    "    print(f\"  Batch sizes: {BATCH_SIZES}\")\n",
    "    print(f\"  References: {len(REFERENCE_SEQUENCES)}\")\n",
    "    print(f\"  Max tokens: {MAX_NEW_TOKENS}\")\n",
    "    print()\n",
    "    print(\"Experiment design:\")\n",
    "    print(\"  - Decoder makes claims at bs=1,2,4\")\n",
    "    print(\"  - Prefiller verifies each claim at bs=1,2,4\")\n",
    "    print(\"  - Matrix: 3 claims × 3 verifications = 9 comparisons per reference\")\n",
    "    print(\"  - Works purely at token ID level (no tokenization artifacts)\")\n",
    "    print()\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"✓ Model loaded\\n\")\n",
    "    \n",
    "    # Results structure\n",
    "    results = {\n",
    "        'metadata': {\n",
    "            'environment': system_info,\n",
    "            'model': MODEL_NAME,\n",
    "            'layer_indices': LAYER_INDICES,\n",
    "            'batch_sizes': BATCH_SIZES,\n",
    "            'max_new_tokens': MAX_NEW_TOKENS,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        },\n",
    "        'measurements': []\n",
    "    }\n",
    "    \n",
    "    # Run experiments\n",
    "    for ref_name, ref_text in REFERENCE_SEQUENCES.items():\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"REFERENCE: {ref_name}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Pre-compute minimum length across all batch configurations\n",
    "        min_prompt_length = compute_min_length_across_batches(\n",
    "            ref_text, ref_name, tokenizer, BATCH_SIZES\n",
    "        )\n",
    "        print(f\"\\nGlobal minimum prompt length: {min_prompt_length} tokens\")\n",
    "        print(\"(All batch sizes will use this length)\\n\")\n",
    "        \n",
    "        # Step 1: Generate claims at each batch size\n",
    "        print(f\"Decode claims:\")\n",
    "        decode_measurements = {}\n",
    "        \n",
    "        for decode_bs in BATCH_SIZES:\n",
    "            print(f\"  bs={decode_bs}...\", end=\" \")\n",
    "            decode_data = run_decode_with_extraction(\n",
    "                model, tokenizer, ref_text, ref_name, decode_bs, LAYER_INDICES,\n",
    "                forced_length=min_prompt_length\n",
    "            )\n",
    "            \n",
    "            # Show extraction positions\n",
    "            extract_positions = [step['absolute_position'] for step in decode_data['signals'].values()]\n",
    "            print(f\"    Extract positions: {extract_positions}\")\n",
    "            \n",
    "            decode_measurements[decode_bs] = decode_data\n",
    "        \n",
    "        # Check token consistency\n",
    "        check_token_consistency(decode_measurements)\n",
    "        \n",
    "        # Step 2: For each claim, verify at all batch sizes\n",
    "        print(f\"\\nPrefill verifications:\")\n",
    "        \n",
    "        for decode_bs in BATCH_SIZES:\n",
    "            decode_data = decode_measurements[decode_bs]\n",
    "            print(f\"\\n  Verifying claim from bs={decode_bs}:\")\n",
    "            \n",
    "            for prefill_bs in BATCH_SIZES:\n",
    "                print(f\"    with bs={prefill_bs}...\", end=\"\")\n",
    "                \n",
    "                # Determine if this is diagonal (honest claim: same batch size)\n",
    "                is_diagonal = (decode_bs == prefill_bs)\n",
    "                \n",
    "                prefill_data = run_prefill_verification(\n",
    "                    model, tokenizer, ref_name, decode_data, prefill_bs,\n",
    "                    LAYER_INDICES, is_diagonal=is_diagonal\n",
    "                )\n",
    "                \n",
    "                results['measurements'].append({\n",
    "                    'ref_name': ref_name,\n",
    "                    'decode_batch_size': decode_bs,\n",
    "                    'prefill_batch_size': prefill_bs,\n",
    "                    'is_diagonal': is_diagonal,\n",
    "                    'decode_data': {\n",
    "                        'generated_ids': decode_data['generated_ids'],\n",
    "                        'all_batch_generated_ids': decode_data['all_batch_generated_ids'],\n",
    "                        'prompt_token_ids': decode_data['prompt_token_ids'],\n",
    "                        'prompt_length': decode_data['prompt_length'],\n",
    "                        'signals': decode_data['signals'],\n",
    "                        'num_generated': decode_data['num_generated']\n",
    "                    },\n",
    "                    'prefill_data': prefill_data\n",
    "                })\n",
    "    \n",
    "    # Save data\n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filepath = os.path.join(output_dir, f\"prefill_decode_{timestamp}.json\")\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Data saved to: {filepath}\")\n",
    "    \n",
    "    # Run analysis\n",
    "    analysis_results = analyze_experiment(results['measurements'], LAYER_INDICES)\n",
    "    \n",
    "    # Save with analysis\n",
    "    results['analysis'] = analysis_results\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "    print(f\"✓ Analysis saved (file size: {file_size_mb:.1f} MB)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"EXPERIMENT COMPLETE\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_ipython()\n",
    "    except NameError:\n",
    "        pass\n",
    "    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b97862-5424-4c0d-a88c-55606cedc930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
