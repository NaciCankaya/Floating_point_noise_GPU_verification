{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "545e394e-7bd7-48f7-a92e-b1fce73691f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TORCH.COMPILE + FLASHATTENTION 2 SPEED BENCHMARK\n",
      "============================================================\n",
      "\n",
      "System Info:\n",
      "  Hostname: 0a394501b784\n",
      "  GPU: NVIDIA A100 80GB PCIe\n",
      "  PyTorch: 2.8.0+cu128\n",
      "  CUDA: 12.8\n",
      "\n",
      "Loading Qwen/Qwen2.5-7B-Instruct with FlashAttention 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3059bbfab43042389c6dc04a4c7898cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PHASE 1: FA2 EAGER MODE (FlashAttention 2, no compilation)\n",
      "============================================================\n",
      "\n",
      "  Conversation 1:\n",
      "    Prompt tokens: 46\n",
      "    Warming up (2 runs)...\n",
      "    Timing (5 runs)...\n",
      "    Generated (200 tokens):\n",
      "      'Supervised and unsupervised machine learning are two primary approaches in the field of artificial intelligence, each with distinct characteristics an...'\n",
      "    Time: 5.595s ± 0.093s\n",
      "    Speed: 35.8 tokens/sec\n",
      "\n",
      "  Conversation 2:\n",
      "    Prompt tokens: 40\n",
      "    Warming up (2 runs)...\n",
      "    Timing (5 runs)...\n",
      "    Generated (200 tokens):\n",
      "      'Photosynthesis is a fascinating process that plants, algae, and some bacteria use to convert light energy from the sun into chemical energy stored in ...'\n",
      "    Time: 5.253s ± 0.044s\n",
      "    Speed: 38.1 tokens/sec\n",
      "\n",
      "  Conversation 3:\n",
      "    Prompt tokens: 44\n",
      "    Warming up (2 runs)...\n",
      "    Timing (5 runs)...\n",
      "    Generated (200 tokens):\n",
      "      'Climate change is a complex phenomenon with both natural and human factors contributing to its observed trends. The scientific consensus is that while...'\n",
      "    Time: 5.229s ± 0.007s\n",
      "    Speed: 38.2 tokens/sec\n",
      "\n",
      "============================================================\n",
      "EAGER MODE SUMMARY\n",
      "============================================================\n",
      "  Mean time: 5.359s ± 0.177s\n",
      "  Mean speed: 37.4 tokens/sec\n",
      "\n",
      "============================================================\n",
      "PHASE 2: FA2 + COMPILED MODE (FlashAttention 2 + torch.compile)\n",
      "============================================================\n",
      "\n",
      "Reloading model with FlashAttention 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6539a3c1804ef994b0b76021c825d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compiling model with mode='default'...\n",
      "✓ Model compiled\n",
      "\n",
      "Triggering compilation (this is slow, ~1-2 minutes)...\n",
      "✓ Compilation complete (1.4s)\n",
      "  (This overhead is excluded from speed measurements)\n",
      "\n",
      "Benchmarking compiled model...\n",
      "\n",
      "  Conversation 1:\n",
      "    Prompt tokens: 46\n",
      "    Warming up (2 runs)...\n",
      "    Timing (5 runs)...\n",
      "    Generated (200 tokens):\n",
      "      'Supervised and unsupervised machine learning are two fundamental approaches in the field of artificial intelligence and data science, each with distin...'\n",
      "    Time: 5.392s ± 0.033s\n",
      "    Speed: 37.1 tokens/sec\n",
      "\n",
      "  Conversation 2:\n",
      "    Prompt tokens: 40\n",
      "    Warming up (2 runs)...\n",
      "    Timing (5 runs)...\n",
      "    Generated (200 tokens):\n",
      "      'Photosynthesis is a fascinating process that plants, algae, and some bacteria use to convert light energy from the sun into chemical energy stored in ...'\n",
      "    Time: 5.378s ± 0.105s\n",
      "    Speed: 37.2 tokens/sec\n",
      "\n",
      "  Conversation 3:\n",
      "    Prompt tokens: 44\n",
      "    Warming up (2 runs)...\n",
      "    Timing (5 runs)...\n",
      "    Generated (200 tokens):\n",
      "      'Climate change is a complex phenomenon with both natural and human factors contributing to its observed trends. The scientific consensus is that while...'\n",
      "    Time: 5.553s ± 0.117s\n",
      "    Speed: 36.0 tokens/sec\n",
      "\n",
      "============================================================\n",
      "COMPILED MODE SUMMARY\n",
      "============================================================\n",
      "  Mean time: 5.441s ± 0.122s\n",
      "  Mean speed: 36.8 tokens/sec\n",
      "  Compilation overhead: 1.4s (excluded)\n",
      "\n",
      "============================================================\n",
      "PERFORMANCE COMPARISON: FA2 vs FA2+COMPILE\n",
      "============================================================\n",
      "\n",
      "Time per generation:\n",
      "  FA2 Eager:    5.359s ± 0.177s\n",
      "  FA2+Compile:  5.441s ± 0.122s\n",
      "  Speedup:      0.98x\n",
      "\n",
      "Throughput:\n",
      "  FA2 Eager:    37.4 tokens/sec\n",
      "  FA2+Compile:  36.8 tokens/sec\n",
      "  Improvement:  0.98x (-1.6% faster)\n",
      "\n",
      "Compilation cost:\n",
      "  One-time overhead: 1.4s\n",
      "  Break-even after: -17 generations\n",
      "\n",
      "✓ Results saved to /workspace/torch_compile_speed_20251103_164554.json\n",
      "\n",
      "============================================================\n",
      "BENCHMARK COMPLETE\n",
      "============================================================\n",
      "\n",
      "Key findings:\n",
      "  • Speedup: 0.98x\n",
      "  • Throughput improvement: -1.6%\n",
      "  • Compilation overhead: 1.4s\n",
      "  • Worth it for production: ✗ No\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "torch.compile Decode Speed Benchmark\n",
    "Proper measurement with compilation overhead excluded\n",
    "Uses Qwen chat template and realistic prompts\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "print(\"=\"*60)\n",
    "print(\"TORCH.COMPILE + FLASHATTENTION 2 SPEED BENCHMARK\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSystem Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA: {torch.version.cuda}\")\n",
    "print()\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name} with FlashAttention 2...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "# Qwen chat template - system + user messages\n",
    "# Qwen uses: system message is optional, user/assistant alternation\n",
    "test_conversations = [\n",
    "    {\n",
    "        \"system\": \"You are a helpful and harmless AI assistant.\",\n",
    "        \"user\": \"Explain the key differences between supervised and unsupervised machine learning. Include examples of when each approach is most useful.\"\n",
    "    },\n",
    "    {\n",
    "        \"system\": \"You are a helpful and harmless AI assistant.\",\n",
    "        \"user\": \"Write a detailed explanation of how photosynthesis works, suitable for a high school biology student.\"\n",
    "    },\n",
    "    {\n",
    "        \"system\": \"You are a helpful and harmless AI assistant.\",\n",
    "        \"user\": \"What are the main causes of climate change? Discuss both natural and human factors, and explain the scientific consensus.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def format_conversation(conversation):\n",
    "    \"\"\"Format conversation using Qwen chat template\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": conversation[\"system\"]},\n",
    "        {\"role\": \"user\", \"content\": conversation[\"user\"]}\n",
    "    ]\n",
    "    \n",
    "    # Use tokenizer's chat template\n",
    "    formatted = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    return formatted\n",
    "\n",
    "def benchmark_generation(model, tokenizer, conversations, max_new_tokens=200, num_runs=5, warmup_runs=2):\n",
    "    \"\"\"\n",
    "    Benchmark generation speed\n",
    "    Returns: list of times per conversation, tokens/sec stats\n",
    "    \"\"\"\n",
    "    all_times = []\n",
    "    all_tokens_per_sec = []\n",
    "    \n",
    "    for conv_idx, conversation in enumerate(conversations):\n",
    "        prompt = format_conversation(conversation)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        prompt_tokens = inputs['input_ids'].shape[1]\n",
    "        \n",
    "        print(f\"\\n  Conversation {conv_idx + 1}:\")\n",
    "        print(f\"    Prompt tokens: {prompt_tokens}\")\n",
    "        \n",
    "        # Warmup\n",
    "        if warmup_runs > 0:\n",
    "            print(f\"    Warming up ({warmup_runs} runs)...\")\n",
    "            for _ in range(warmup_runs):\n",
    "                with torch.no_grad():\n",
    "                    _ = model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=tokenizer.eos_token_id\n",
    "                    )\n",
    "                torch.cuda.synchronize()\n",
    "        \n",
    "        # Timed runs\n",
    "        print(f\"    Timing ({num_runs} runs)...\")\n",
    "        run_times = []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time.perf_counter()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_new_tokens,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = time.perf_counter() - start\n",
    "            \n",
    "            # Count actual tokens generated\n",
    "            generated_tokens = outputs.shape[1] - prompt_tokens\n",
    "            tokens_per_sec = generated_tokens / elapsed\n",
    "            \n",
    "            run_times.append(elapsed)\n",
    "            all_tokens_per_sec.append(tokens_per_sec)\n",
    "            \n",
    "            if run == 0:\n",
    "                generated_text = tokenizer.decode(\n",
    "                    outputs[0][prompt_tokens:],\n",
    "                    skip_special_tokens=True\n",
    "                )\n",
    "                print(f\"    Generated ({generated_tokens} tokens):\")\n",
    "                print(f\"      '{generated_text[:150]}...'\")\n",
    "        \n",
    "        mean_time = np.mean(run_times)\n",
    "        std_time = np.std(run_times)\n",
    "        mean_tps = np.mean(all_tokens_per_sec[-num_runs:])\n",
    "        \n",
    "        print(f\"    Time: {mean_time:.3f}s ± {std_time:.3f}s\")\n",
    "        print(f\"    Speed: {mean_tps:.1f} tokens/sec\")\n",
    "        \n",
    "        all_times.extend(run_times)\n",
    "        \n",
    "        del inputs, outputs\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_times, all_tokens_per_sec\n",
    "\n",
    "max_new_tokens = 200\n",
    "num_runs = 5\n",
    "warmup_runs = 2\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 1: EAGER MODE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 1: FA2 EAGER MODE (FlashAttention 2, no compilation)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "eager_times, eager_tps = benchmark_generation(\n",
    "    model, tokenizer, test_conversations,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    num_runs=num_runs,\n",
    "    warmup_runs=warmup_runs\n",
    ")\n",
    "\n",
    "eager_mean = np.mean(eager_times)\n",
    "eager_std = np.std(eager_times)\n",
    "eager_mean_tps = np.mean(eager_tps)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EAGER MODE SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Mean time: {eager_mean:.3f}s ± {eager_std:.3f}s\")\n",
    "print(f\"  Mean speed: {eager_mean_tps:.1f} tokens/sec\")\n",
    "\n",
    "# Clean up eager model\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "# ============================================================================\n",
    "# PHASE 2: COMPILED MODE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PHASE 2: FA2 + COMPILED MODE (FlashAttention 2 + torch.compile)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Reload model\n",
    "print(\"\\nReloading model with FlashAttention 2...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "\n",
    "# Compile\n",
    "print(\"\\nCompiling model with mode='default'...\")\n",
    "model.forward = torch.compile(model.forward, mode=\"default\")\n",
    "print(\"✓ Model compiled\")\n",
    "\n",
    "# Trigger compilation with a dummy run\n",
    "print(\"\\nTriggering compilation (this is slow, ~1-2 minutes)...\")\n",
    "dummy_prompt = format_conversation(test_conversations[0])\n",
    "dummy_inputs = tokenizer(dummy_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "compile_start = time.perf_counter()\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model.generate(\n",
    "        **dummy_inputs,\n",
    "        max_new_tokens=50,  # Shorter for compilation\n",
    "        do_sample=False,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "compile_time = time.perf_counter() - compile_start\n",
    "\n",
    "print(f\"✓ Compilation complete ({compile_time:.1f}s)\")\n",
    "print(f\"  (This overhead is excluded from speed measurements)\")\n",
    "\n",
    "del dummy_inputs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Now benchmark with compiled model\n",
    "print(\"\\nBenchmarking compiled model...\")\n",
    "\n",
    "compiled_times, compiled_tps = benchmark_generation(\n",
    "    model, tokenizer, test_conversations,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    num_runs=num_runs,\n",
    "    warmup_runs=warmup_runs\n",
    ")\n",
    "\n",
    "compiled_mean = np.mean(compiled_times)\n",
    "compiled_std = np.std(compiled_times)\n",
    "compiled_mean_tps = np.mean(compiled_tps)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"COMPILED MODE SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Mean time: {compiled_mean:.3f}s ± {compiled_std:.3f}s\")\n",
    "print(f\"  Mean speed: {compiled_mean_tps:.1f} tokens/sec\")\n",
    "print(f\"  Compilation overhead: {compile_time:.1f}s (excluded)\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON\n",
    "# ============================================================================\n",
    "\n",
    "speedup = eager_mean / compiled_mean\n",
    "tps_improvement = compiled_mean_tps / eager_mean_tps\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE COMPARISON: FA2 vs FA2+COMPILE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTime per generation:\")\n",
    "print(f\"  FA2 Eager:    {eager_mean:.3f}s ± {eager_std:.3f}s\")\n",
    "print(f\"  FA2+Compile:  {compiled_mean:.3f}s ± {compiled_std:.3f}s\")\n",
    "print(f\"  Speedup:      {speedup:.2f}x\")\n",
    "\n",
    "print(f\"\\nThroughput:\")\n",
    "print(f\"  FA2 Eager:    {eager_mean_tps:.1f} tokens/sec\")\n",
    "print(f\"  FA2+Compile:  {compiled_mean_tps:.1f} tokens/sec\")\n",
    "print(f\"  Improvement:  {tps_improvement:.2f}x ({(tps_improvement-1)*100:.1f}% faster)\")\n",
    "\n",
    "print(f\"\\nCompilation cost:\")\n",
    "print(f\"  One-time overhead: {compile_time:.1f}s\")\n",
    "print(f\"  Break-even after: {compile_time / (eager_mean - compiled_mean):.0f} generations\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output = {\n",
    "    \"experiment\": \"torch_compile_decode_speed_benchmark\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"pytorch\": torch.__version__,\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"hostname\": HOSTNAME\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"num_conversations\": len(test_conversations),\n",
    "        \"runs_per_conversation\": num_runs,\n",
    "        \"warmup_runs\": warmup_runs,\n",
    "        \"compile_mode\": \"default\",\n",
    "        \"attn_implementation\": \"flash_attention_2\"\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"eager\": {\n",
    "            \"mean_time_sec\": float(eager_mean),\n",
    "            \"std_time_sec\": float(eager_std),\n",
    "            \"mean_tokens_per_sec\": float(eager_mean_tps),\n",
    "            \"all_times\": [float(t) for t in eager_times]\n",
    "        },\n",
    "        \"compiled\": {\n",
    "            \"mean_time_sec\": float(compiled_mean),\n",
    "            \"std_time_sec\": float(compiled_std),\n",
    "            \"mean_tokens_per_sec\": float(compiled_mean_tps),\n",
    "            \"compilation_time_sec\": float(compile_time),\n",
    "            \"all_times\": [float(t) for t in compiled_times]\n",
    "        },\n",
    "        \"comparison\": {\n",
    "            \"speedup\": float(speedup),\n",
    "            \"throughput_improvement\": float(tps_improvement),\n",
    "            \"break_even_generations\": float(compile_time / (eager_mean - compiled_mean)) if eager_mean > compiled_mean else None\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "output_file = f\"torch_compile_speed_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "output_path = f\"/workspace/{output_file}\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKey findings:\")\n",
    "print(f\"  • Speedup: {speedup:.2f}x\")\n",
    "print(f\"  • Throughput improvement: {(tps_improvement-1)*100:.1f}%\")\n",
    "print(f\"  • Compilation overhead: {compile_time:.1f}s\")\n",
    "print(f\"  • Worth it for production: {'✓ Yes' if speedup > 1.1 else '⚠ Marginal' if speedup > 1.0 else '✗ No'}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c360173-06d8-4097-983b-300f94149dbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
