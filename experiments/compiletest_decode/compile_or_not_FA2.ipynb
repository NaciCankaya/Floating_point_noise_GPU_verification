{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29dea5cb-9319-4d47-993e-1a7b3b716469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.4/803.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2025.10.23 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.11.0\n",
      "Collecting ninja\n",
      "  Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (25.0)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, ninja\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [ninja]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ninja-1.13.0 wheel-0.45.1\n",
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.8.3.tar.gz (8.4 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu128)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
      "Downloading einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Building wheels for collected packages: flash-attn\n",
      "\u001b[33m  DEPRECATION: Building 'flash-attn' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'flash-attn'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl size=256040057 sha256=f25da18657a87fc83dc1bfb8b7751b82246e9db355510226b674fd437c34b5fb\n",
      "  Stored in directory: /workspace/.cache/pip/wheels/3d/59/46/f282c12c73dd4bb3c2e3fe199f1a0d0f8cec06df0cccfeee27\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2/2\u001b[0m [flash-attn]2\u001b[0m [flash-attn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed einops-0.8.1 flash-attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install hf_transfer\n",
    "!pip install accelerate\n",
    "!pip install ninja packaging wheel\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c1635f8-0c3e-432b-8c05-364d4b5d5914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KEY VECTOR FORENSICS: torch.compile decode\n",
      "============================================================\n",
      "\n",
      "System Info:\n",
      "  Hostname: 0a394501b784\n",
      "  GPU: NVIDIA A100 80GB PCIe\n",
      "  PyTorch: 2.8.0+cu128\n",
      "  CUDA: 12.8\n",
      "\n",
      "Loading Qwen/Qwen2.5-7B-Instruct in BF16 with FlashAttention 2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f835cd23b0c04cd090a3e5aab9b8b91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n",
      "  Layers: 28\n",
      "  Attention heads: 28\n",
      "  Head dimension: 128\n",
      "  Key vector dimension: 3584\n",
      "\n",
      "Prompt: 'The capital of France is' (5 tokens)\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: EAGER vs COMPILED (Decode Verification)\n",
      "============================================================\n",
      "Attention: FlashAttention 2 (enabled for both modes)\n",
      "Decode steps: 20\n",
      "Repetitions: 10\n",
      "Extraction: Key vectors from decode KV cache (post-generation)\n",
      "No fallback: If cache not accessible, experiment fails\n",
      "Goal: Does torch.compile+FA2 amplify deviation vs FA2 alone?\n",
      "\n",
      "============================================================\n",
      "Phase 1: EAGER MODE\n",
      "============================================================\n",
      "Collecting key vectors...\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  Generated: ' Paris. Which of the following statements about Paris is true?\n",
      "A. It is located in the northern'\n",
      "  Steps: 20\n",
      "  Key vector shape per step: torch.Size([512])\n",
      "  First step key norm: 924.00\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "‚úì Eager: Perfect reproducibility (L2=0.0)\n",
      "\n",
      "============================================================\n",
      "Phase 2: COMPILED MODE\n",
      "============================================================\n",
      "Compiling model with mode='default'...\n",
      "  (Using 'default' instead of 'reduce-overhead' to avoid CUDA graph issues)\n",
      "‚úì Model compiled\n",
      "\n",
      "Collecting key vectors...\n",
      "  First run: Triggering compilation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1103 16:33:21.778000 293 torch/_dynamo/convert_frame.py:1016] [11/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1103 16:33:21.778000 293 torch/_dynamo/convert_frame.py:1016] [11/8]    function: 'forward' (/usr/local/lib/python3.12/dist-packages/transformers/models/qwen2/modeling_qwen2.py:140)\n",
      "W1103 16:33:21.778000 293 torch/_dynamo/convert_frame.py:1016] [11/8]    last reason: 11/7: past_key_values.layers[7].is_initialized == False      \n",
      "W1103 16:33:21.778000 293 torch/_dynamo/convert_frame.py:1016] [11/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1103 16:33:21.778000 293 torch/_dynamo/convert_frame.py:1016] [11/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  Generated: ' Paris. Which of the following statements about Paris is true?\n",
      "A. It is located in the northern'\n",
      "  Steps: 20\n",
      "  Time: 12.28s\n",
      "  First step key norm: 924.00\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "  ‚úì Successfully retrieved KV cache from decode\n",
      "  KV cache shape: torch.Size([1, 4, 24, 128])\n",
      "  Generated tokens: 20\n",
      "‚úì Compiled: Perfect reproducibility (L2=0.0)\n",
      "\n",
      "============================================================\n",
      "SYSTEMATIC DEVIATION: EAGER vs COMPILED\n",
      "============================================================\n",
      "\n",
      "L2 distance per decode step (key vectors):\n",
      "  Step  0: L2=0.511719\n",
      "  Step  1: L2=1.601562\n",
      "  Step  2: L2=0.355469\n",
      "  Step  3: L2=0.328125\n",
      "  Step  4: L2=0.345703\n",
      "  Step  5: L2=0.589844\n",
      "  Step  6: L2=0.388672\n",
      "  Step  7: L2=0.333984\n",
      "  Step  8: L2=0.386719\n",
      "  Step  9: L2=0.289062\n",
      "  Step 10: L2=0.341797\n",
      "  Step 11: L2=2.046875\n",
      "  Step 12: L2=0.269531\n",
      "  Step 13: L2=0.296875\n",
      "  Step 14: L2=1.070312\n",
      "  Step 15: L2=0.292969\n",
      "  Step 16: L2=1.070312\n",
      "  Step 17: L2=1.117188\n",
      "  Step 18: L2=0.304688\n",
      "  Step 19: L2=0.298828\n",
      "\n",
      "Summary:\n",
      "  Mean L2: 0.613281\n",
      "  Max L2:  2.046875\n",
      "  Min L2:  0.269531\n",
      "\n",
      "Deviation pattern:\n",
      "  ? Variable (no clear pattern)\n",
      "\n",
      "Comparison to hidden states:\n",
      "  Key vector dim: 3584\n",
      "  Hidden state dim: 3584\n",
      "  Dimensions match: ‚úì\n",
      "  Max key vector L2: 2.05\n",
      "  Prefill hidden state L2: 5.47 (from previous experiment)\n",
      "\n",
      "Token sequences:\n",
      "  ‚úì Identical\n",
      "\n",
      "============================================================\n",
      "FORENSIC ANALYSIS\n",
      "============================================================\n",
      "‚úì GOOD: L2 max=2.05\n",
      "  Key vectors are forensically useful\n",
      "\n",
      "Reproducibility:\n",
      "  ‚úì Perfect within both conditions\n",
      "  Systematic deviation dominates\n",
      "\n",
      "Practical advantages:\n",
      "  ‚úì Zero overhead (KV cache exists anyway)\n",
      "  ‚úì Smaller vectors (3584 vs full hidden state)\n",
      "  ‚úì No need for output_hidden_states=True\n",
      "  ‚úì Works with CUDA graphs\n",
      "\n",
      "============================================================\n",
      "KEY VECTORS vs HIDDEN STATES FOR FORENSICS\n",
      "============================================================\n",
      "\n",
      "Key vectors (this experiment):\n",
      "  Dimension: 3584\n",
      "  Max L2 deviation: 2.05\n",
      "  Overhead: Zero (already in memory)\n",
      "  Extraction: From KV cache\n",
      "\n",
      "Hidden states (prefill experiment):\n",
      "  Dimension: 3584\n",
      "  Max L2 deviation: 5.47\n",
      "  Overhead: output_hidden_states=True\n",
      "  Extraction: From model outputs\n",
      "\n",
      "Relative signal strength: 0.37x\n",
      "  ‚Üí Key vectors provide partial forensic signal\n",
      "\n",
      "‚úì Results saved to /workspace/key_vector_forensics_20251103_163330.json\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT COMPLETE\n",
      "============================================================\n",
      "\n",
      "Key findings:\n",
      "  ‚Ä¢ Key vector extraction: ‚úì Works\n",
      "  ‚Ä¢ Max L2 deviation: 2.0469\n",
      "  ‚Ä¢ Forensic viability: ‚úì Yes\n",
      "  ‚Ä¢ Overhead: Zero\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Key Vector Forensics Experiment\n",
    "Tests if key vectors from KV cache are forensically effective for detecting torch.compile\n",
    "Minimal overhead approach - KV cache exists anyway in production decode\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KEY VECTOR FORENSICS: torch.compile decode\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nSystem Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA: {torch.version.cuda}\")\n",
    "print()\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "def collect_key_vectors_post_generation(model, tokenizer, prompt, max_new_tokens=20):\n",
    "    \"\"\"\n",
    "    Generate tokens, then extract key vectors from the KV cache built during decode\n",
    "    No fallback - if generate() doesn't return the cache, we fail\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Generate and request the past_key_values to be returned\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,  # Greedy/deterministic\n",
    "            return_dict_in_generate=True,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_ids = outputs.sequences[0]\n",
    "    generated_tokens = generated_ids[inputs['input_ids'].shape[1]:].cpu().tolist()\n",
    "    \n",
    "    # Check if we got past_key_values from generate\n",
    "    if not hasattr(outputs, 'past_key_values') or outputs.past_key_values is None:\n",
    "        raise RuntimeError(\n",
    "            \"model.generate() did not return past_key_values. \"\n",
    "            \"The KV cache from decode is not accessible. \"\n",
    "            \"This means we cannot extract activations from decode without \"\n",
    "            \"interfering with the generation loop.\"\n",
    "        )\n",
    "    \n",
    "    print(\"  ‚úì Successfully retrieved KV cache from decode\")\n",
    "    past_kv = outputs.past_key_values\n",
    "    \n",
    "    # Extract key vectors from the KV cache built during decode\n",
    "    last_layer_kv = past_kv[-1]  # Last layer\n",
    "    key_cache = last_layer_kv[0]  # Keys (not values)\n",
    "    \n",
    "    # IMPORTANT: The KV cache returned by generate() only contains the GENERATED tokens\n",
    "    # Not the prompt tokens! So indices are 0 to num_generated-1\n",
    "    num_generated = len(generated_tokens)\n",
    "    \n",
    "    print(f\"  KV cache shape: {key_cache.shape}\")\n",
    "    print(f\"  Generated tokens: {num_generated}\")\n",
    "    \n",
    "    key_vectors = []\n",
    "    for i in range(num_generated):\n",
    "        # Index directly into the generated positions (0 to num_generated-1)\n",
    "        key_vec = key_cache[0, :, i, :].reshape(-1).cpu().clone()\n",
    "        key_vectors.append(key_vec)\n",
    "    \n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return torch.stack(key_vectors), generated_tokens\n",
    "\n",
    "print(f\"Loading {model_name} in BF16 with FlashAttention 2...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"flash_attention_2\"  # Enable FlashAttention 2\n",
    ")\n",
    "\n",
    "# Get model architecture info\n",
    "num_layers = len(model.model.layers)\n",
    "num_heads = model.config.num_attention_heads\n",
    "head_dim = model.config.hidden_size // num_heads\n",
    "key_vector_dim = num_heads * head_dim\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"  Layers: {num_layers}\")\n",
    "print(f\"  Attention heads: {num_heads}\")\n",
    "print(f\"  Head dimension: {head_dim}\")\n",
    "print(f\"  Key vector dimension: {key_vector_dim}\")\n",
    "print()\n",
    "\n",
    "prompt = \"The capital of France is\"\n",
    "prompt_tokens = len(tokenizer.encode(prompt))\n",
    "print(f\"Prompt: '{prompt}' ({prompt_tokens} tokens)\\n\")\n",
    "\n",
    "num_reps = 10\n",
    "max_new_tokens = 20\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT: EAGER vs COMPILED (Decode Verification)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Attention: FlashAttention 2 (enabled for both modes)\")\n",
    "print(f\"Decode steps: {max_new_tokens}\")\n",
    "print(f\"Repetitions: {num_reps}\")\n",
    "print(f\"Extraction: Key vectors from decode KV cache (post-generation)\")\n",
    "print(f\"No fallback: If cache not accessible, experiment fails\")\n",
    "print(f\"Goal: Does torch.compile+FA2 amplify deviation vs FA2 alone?\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 1: EAGER MODE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Phase 1: EAGER MODE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results_eager = []\n",
    "tokens_eager = []\n",
    "\n",
    "print(\"Collecting key vectors...\")\n",
    "for rep in range(num_reps):\n",
    "    key_vecs, tokens = collect_key_vectors_post_generation(\n",
    "        model, tokenizer, prompt,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    results_eager.append(key_vecs)\n",
    "    tokens_eager.append(tokens)\n",
    "    \n",
    "    if rep == 0:\n",
    "        generated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        print(f\"  Generated: '{generated_text}'\")\n",
    "        print(f\"  Steps: {len(tokens)}\")\n",
    "        print(f\"  Key vector shape per step: {key_vecs[0].shape}\")\n",
    "        print(f\"  First step key norm: {torch.norm(key_vecs[0]).item():.2f}\")\n",
    "\n",
    "# Check reproducibility\n",
    "first_eager = results_eager[0]\n",
    "eager_reproducible = all(torch.equal(first_eager, results_eager[i]) for i in range(1, num_reps))\n",
    "\n",
    "if eager_reproducible:\n",
    "    print(f\"‚úì Eager: Perfect reproducibility (L2=0.0)\")\n",
    "else:\n",
    "    print(f\"‚úó Eager: VARIATION DETECTED\")\n",
    "    for i in range(1, num_reps):\n",
    "        if not torch.equal(first_eager, results_eager[i]):\n",
    "            l2_per_step = torch.norm(first_eager - results_eager[i], dim=-1)\n",
    "            print(f\"  Rep 0 vs {i}: Max L2={l2_per_step.max().item():.6f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TEST 2: COMPILED MODE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"Phase 2: COMPILED MODE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compile - use \"default\" mode which doesn't aggressively use CUDA graphs\n",
    "print(\"Compiling model with mode='default'...\")\n",
    "print(\"  (Using 'default' instead of 'reduce-overhead' to avoid CUDA graph issues)\")\n",
    "model.forward = torch.compile(model.forward, mode=\"default\")\n",
    "print(\"‚úì Model compiled\")\n",
    "print()\n",
    "\n",
    "print(\"Collecting key vectors...\")\n",
    "results_compiled = []\n",
    "tokens_compiled = []\n",
    "\n",
    "for rep in range(num_reps):\n",
    "    if rep == 0:\n",
    "        print(\"  First run: Triggering compilation...\")\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    key_vecs, tokens = collect_key_vectors_post_generation(\n",
    "        model, tokenizer, prompt,\n",
    "        max_new_tokens=max_new_tokens\n",
    "    )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    results_compiled.append(key_vecs)\n",
    "    tokens_compiled.append(tokens)\n",
    "    \n",
    "    if rep == 0:\n",
    "        generated_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "        print(f\"  Generated: '{generated_text}'\")\n",
    "        print(f\"  Steps: {len(tokens)}\")\n",
    "        print(f\"  Time: {elapsed:.2f}s\")\n",
    "        print(f\"  First step key norm: {torch.norm(key_vecs[0]).item():.2f}\")\n",
    "\n",
    "# Check reproducibility\n",
    "first_compiled = results_compiled[0]\n",
    "compiled_reproducible = all(torch.equal(first_compiled, results_compiled[i]) for i in range(1, num_reps))\n",
    "\n",
    "if compiled_reproducible:\n",
    "    print(f\"‚úì Compiled: Perfect reproducibility (L2=0.0)\")\n",
    "else:\n",
    "    print(f\"‚úó Compiled: VARIATION DETECTED\")\n",
    "    for i in range(1, num_reps):\n",
    "        if not torch.equal(first_compiled, results_compiled[i]):\n",
    "            l2_per_step = torch.norm(first_compiled - results_compiled[i], dim=-1)\n",
    "            print(f\"  Rep 0 vs {i}: Max L2={l2_per_step.max().item():.6f}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS: EAGER vs COMPILED\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SYSTEMATIC DEVIATION: EAGER vs COMPILED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "mean_eager = torch.stack(results_eager).mean(dim=0)      # [steps, key_dim]\n",
    "mean_compiled = torch.stack(results_compiled).mean(dim=0)\n",
    "\n",
    "l2_per_step = torch.norm(mean_eager - mean_compiled, dim=-1)\n",
    "\n",
    "print(f\"\\nL2 distance per decode step (key vectors):\")\n",
    "for step in range(len(l2_per_step)):\n",
    "    print(f\"  Step {step:2d}: L2={l2_per_step[step].item():.6f}\")\n",
    "\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  Mean L2: {l2_per_step.mean().item():.6f}\")\n",
    "print(f\"  Max L2:  {l2_per_step.max().item():.6f}\")\n",
    "print(f\"  Min L2:  {l2_per_step.min().item():.6f}\")\n",
    "\n",
    "# Analyze deviation pattern\n",
    "print(f\"\\nDeviation pattern:\")\n",
    "if l2_per_step[-1] > l2_per_step[0]:\n",
    "    growth = l2_per_step[-1] / l2_per_step[0] if l2_per_step[0] > 0 else float('inf')\n",
    "    print(f\"  ‚úì Accumulating (grows {growth:.2f}x from first to last step)\")\n",
    "elif l2_per_step.std() < l2_per_step.mean() * 0.1:\n",
    "    print(f\"  ‚Üí Stable (consistent deviation across steps)\")\n",
    "else:\n",
    "    print(f\"  ? Variable (no clear pattern)\")\n",
    "\n",
    "# Compare to typical hidden state deviations\n",
    "# From prefill experiment: L2=5.47 for hidden states (dim=3584)\n",
    "# Key vectors have dim = num_heads * head_dim = 28 * 128 = 3584 (same!)\n",
    "print(f\"\\nComparison to hidden states:\")\n",
    "print(f\"  Key vector dim: {key_vector_dim}\")\n",
    "print(f\"  Hidden state dim: 3584\")\n",
    "print(f\"  Dimensions match: {'‚úì' if key_vector_dim == 3584 else '‚úó'}\")\n",
    "print(f\"  Max key vector L2: {l2_per_step.max().item():.2f}\")\n",
    "print(f\"  Prefill hidden state L2: 5.47 (from previous experiment)\")\n",
    "\n",
    "# Token divergence check\n",
    "tokens_match = all(\n",
    "    tokens_eager[0][i] == tokens_compiled[0][i]\n",
    "    for i in range(min(len(tokens_eager[0]), len(tokens_compiled[0])))\n",
    ")\n",
    "\n",
    "print(f\"\\nToken sequences:\")\n",
    "if tokens_match:\n",
    "    print(f\"  ‚úì Identical\")\n",
    "else:\n",
    "    print(f\"  ‚úó DIVERGED\")\n",
    "    for i in range(min(len(tokens_eager[0]), len(tokens_compiled[0]))):\n",
    "        if tokens_eager[0][i] != tokens_compiled[0][i]:\n",
    "            print(f\"  First divergence at step {i}\")\n",
    "            break\n",
    "\n",
    "# ============================================================================\n",
    "# INTERPRETATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FORENSIC ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "max_l2 = l2_per_step.max().item()\n",
    "\n",
    "if max_l2 > 10:\n",
    "    print(f\"üìä EXCELLENT: L2 max={max_l2:.2f}\")\n",
    "    print(f\"  Key vectors show strong forensic signal\")\n",
    "elif max_l2 > 1:\n",
    "    print(f\"‚úì GOOD: L2 max={max_l2:.2f}\")\n",
    "    print(f\"  Key vectors are forensically useful\")\n",
    "elif max_l2 > 0.1:\n",
    "    print(f\"‚ö† WEAK: L2 max={max_l2:.3f}\")\n",
    "    print(f\"  Small but potentially detectable\")\n",
    "else:\n",
    "    print(f\"‚úó INSUFFICIENT: L2 max={max_l2:.6f}\")\n",
    "    print(f\"  Key vectors do not show useful signal\")\n",
    "\n",
    "print(f\"\\nReproducibility:\")\n",
    "if eager_reproducible and compiled_reproducible:\n",
    "    print(f\"  ‚úì Perfect within both conditions\")\n",
    "    print(f\"  Systematic deviation dominates\")\n",
    "else:\n",
    "    print(f\"  ‚ö† Some variation within conditions\")\n",
    "\n",
    "print(f\"\\nPractical advantages:\")\n",
    "print(f\"  ‚úì Zero overhead (KV cache exists anyway)\")\n",
    "print(f\"  ‚úì Smaller vectors ({key_vector_dim} vs full hidden state)\")\n",
    "print(f\"  ‚úì No need for output_hidden_states=True\")\n",
    "print(f\"  {'‚úì' if not 'Error' in str(results_compiled) else '‚úó'} Works with CUDA graphs\")\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARISON TO HIDDEN STATES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY VECTORS vs HIDDEN STATES FOR FORENSICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nKey vectors (this experiment):\")\n",
    "print(f\"  Dimension: {key_vector_dim}\")\n",
    "print(f\"  Max L2 deviation: {max_l2:.2f}\")\n",
    "print(f\"  Overhead: Zero (already in memory)\")\n",
    "print(f\"  Extraction: From KV cache\")\n",
    "\n",
    "print(f\"\\nHidden states (prefill experiment):\")\n",
    "print(f\"  Dimension: 3584\")\n",
    "print(f\"  Max L2 deviation: 5.47\")\n",
    "print(f\"  Overhead: output_hidden_states=True\")\n",
    "print(f\"  Extraction: From model outputs\")\n",
    "\n",
    "if key_vector_dim == 3584:\n",
    "    ratio = max_l2 / 5.47\n",
    "    print(f\"\\nRelative signal strength: {ratio:.2f}x\")\n",
    "    if ratio > 0.5:\n",
    "        print(f\"  ‚úì Key vectors provide comparable forensic signal\")\n",
    "    elif ratio > 0.1:\n",
    "        print(f\"  ‚Üí Key vectors provide partial forensic signal\")\n",
    "    else:\n",
    "        print(f\"  ‚úó Key vectors provide insufficient signal\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output = {\n",
    "    \"experiment\": \"key_vector_forensics_torch_compile_decode\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"pytorch\": torch.__version__,\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"architecture\": {\n",
    "        \"num_layers\": num_layers,\n",
    "        \"num_heads\": num_heads,\n",
    "        \"head_dim\": head_dim,\n",
    "        \"key_vector_dim\": key_vector_dim\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"prompt\": prompt,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"repetitions\": num_reps,\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"compile_mode\": \"reduce-overhead\",\n",
    "        \"extraction_method\": \"kv_cache_last_layer\"\n",
    "    },\n",
    "    \"reproducibility\": {\n",
    "        \"eager\": eager_reproducible,\n",
    "        \"compiled\": compiled_reproducible\n",
    "    },\n",
    "    \"systematic_deviation\": {\n",
    "        \"l2_per_step\": l2_per_step.tolist(),\n",
    "        \"mean_l2\": float(l2_per_step.mean()),\n",
    "        \"max_l2\": float(l2_per_step.max()),\n",
    "        \"min_l2\": float(l2_per_step.min())\n",
    "    },\n",
    "    \"token_divergence\": {\n",
    "        \"tokens_match\": tokens_match,\n",
    "        \"eager_tokens\": tokens_eager[0],\n",
    "        \"compiled_tokens\": tokens_compiled[0]\n",
    "    }\n",
    "}\n",
    "\n",
    "output_file = f\"key_vector_forensics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "output_path = f\"/workspace/{output_file}\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Results saved to {output_path}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nKey findings:\")\n",
    "print(f\"  ‚Ä¢ Key vector extraction: {'‚úì Works' if compiled_reproducible else '‚úó Failed'}\")\n",
    "print(f\"  ‚Ä¢ Max L2 deviation: {max_l2:.4f}\")\n",
    "print(f\"  ‚Ä¢ Forensic viability: {'‚úì Yes' if max_l2 > 1 else '‚ö† Marginal' if max_l2 > 0.1 else '‚úó No'}\")\n",
    "print(f\"  ‚Ä¢ Overhead: Zero\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51020bba-a011-497c-b39a-d28221a7f524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
