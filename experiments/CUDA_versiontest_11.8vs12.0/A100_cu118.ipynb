{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59ae658c-b336-4a71-946b-e82baa5c2cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2025.10.23 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1\n",
      "Collecting hf_transfer\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.11.0\n",
      "Collecting ninja\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (25.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, ninja\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ninja]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ninja-1.13.0 wheel-0.45.1\n",
      "Collecting flash-attn\n",
      "  Using cached flash_attn-2.8.3-cp312-cp312-linux_x86_64.whl\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from flash-attn) (2.8.0+cu128)\n",
      "Collecting einops (from flash-attn)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->flash-attn) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->flash-attn) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->flash-attn) (3.0.3)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: einops, flash-attn\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [flash-attn]2\u001b[0m [flash-attn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed einops-0.8.1 flash-attn-2.8.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install hf_transfer\n",
    "!pip install accelerate\n",
    "!pip install ninja packaging wheel\n",
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36facbf-3e40-46a7-b89b-3647fce26249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.4.0+cu118\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.4.0%2Bcu118-cp312-cp312-linux_x86_64.whl (857.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.7/857.7 MB\u001b[0m \u001b[31m152.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (4.15.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (80.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.20.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu118) (11.8.86)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0+cu118)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m163.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0+cu118) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0+cu118) (1.3.0)\n",
      "Installing collected packages: triton, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.5.0\n",
      "\u001b[2K    Uninstalling triton-3.5.0:\n",
      "\u001b[2K      Successfully uninstalled triton-3.5.0\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: torch 2.9.0\u001b[0m \u001b[32m0/2\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling torch-2.9.0:[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.9.0\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch]32m1/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.8.0+cu128 requires torch==2.8.0, but you have torch 2.4.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.4.0+cu118 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.4.0+cu118 --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28d944bf-9ec5-4242-9a52-fc5ed049442d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping flash-attn as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall torchvision -y\n",
    "!pip uninstall flash-attn -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d901a7c-f1c2-430f-b49f-43ad3be035f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUDA VERSION FORENSICS EXPERIMENT\n",
      "============================================================\n",
      "\n",
      "System Info:\n",
      "  Hostname: 75a750b4edaf\n",
      "  Container: 75a750b4edaf\n",
      "  GPU: NVIDIA A100 80GB PCIe\n",
      "  PyTorch: 2.4.0+cu118\n",
      "  CUDA Runtime: 11.8\n",
      "  CUDA Build: cu118\n",
      "\n",
      "GPU Memory:\n",
      "  Total: 79.3 GB\n",
      "  Currently allocated: 0.00 GB\n",
      "\n",
      "Loading Qwen/Qwen2.5-7B-Instruct in BF16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cdde50d25fe4df4b7d71ffa2c120d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model attention implementation: eager\n",
      "✓ Loaded prompt from dummytext.txt\n",
      "Prompt tokens: 3404\n",
      "\n",
      "============================================================\n",
      "CUDA VERSION COMPARISON EXPERIMENT\n",
      "CUDA Build: cu118\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Precision: BF16 (bfloat16)\n",
      "Attention: flash_attention_2\n",
      "Prompt tokens: 3404\n",
      "Repetitions: 5\n",
      "============================================================\n",
      "\n",
      "\n",
      "Collecting multi-layer activations (5 repetitions)...\n",
      "  Extracting from layers: [1, 2, 3, 4, 7, 10, 14, 18, 22, 28]\n",
      "  Rep 0 norms: layer_1=8.62, layer_2=14.75, layer_3=17.12, layer_4=18.50, layer_7=35.75, layer_10=55.75, layer_14=65.50, layer_18=79.00, layer_22=159.00, layer_28=308.00\n",
      "  Rep 0 memory: before=14.19GB, after=14.20GB\n",
      "  ✓ Completed all repetitions\n",
      "\n",
      "Repeatability (last layer): ✓ All identical\n",
      "\n",
      "Layer activation norms:\n",
      "  layer_1: 8.62\n",
      "  layer_2: 14.75\n",
      "  layer_3: 17.12\n",
      "  layer_4: 18.50\n",
      "  layer_7: 35.75\n",
      "  layer_10: 55.75\n",
      "  layer_14: 65.50\n",
      "  layer_18: 79.00\n",
      "  layer_22: 159.00\n",
      "  layer_28: 308.00\n",
      "\n",
      "✓ Results saved to /workspace/experiments/NVIDIA_A100_80GB_PCIe_cu118_7b_bf16_eager_20251103_135936.json\n",
      "✓ File size: ~2218.0 KB\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT COMPLETE\n",
      "CUDA Build: cu118\n",
      "============================================================\n",
      "\n",
      "Next steps:\n",
      "1. If this was cu118: Restart kernel, install cu120, run again\n",
      "2. If this was cu120: Compare the two JSON files\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "CUDA Version Forensics Experiment\n",
    "Tests if different CUDA toolkit versions create detectable activation deviations\n",
    "Same PyTorch version (2.4.0), different CUDA toolkits (11.8 vs 12.0)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUDA VERSION FORENSICS EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# System info\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(f\"\\nSystem Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  Container: {CONTAINER_ID}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA Runtime: {torch.version.cuda}\")\n",
    "\n",
    "# Extract CUDA version from PyTorch build\n",
    "pytorch_version = torch.__version__\n",
    "if '+cu' in pytorch_version:\n",
    "    cuda_build = pytorch_version.split('+cu')[1].split('.')[0]  # e.g., \"118\" or \"120\"\n",
    "    cuda_version_str = f\"cu{cuda_build}\"\n",
    "    print(f\"  CUDA Build: {cuda_version_str}\")\n",
    "else:\n",
    "    print(f\"  WARNING: Could not detect CUDA version from PyTorch build\")\n",
    "    cuda_version_str = \"unknown\"\n",
    "\n",
    "print()\n",
    "\n",
    "# Check GPU memory\n",
    "mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "mem_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "print(f\"GPU Memory:\")\n",
    "print(f\"  Total: {mem_total:.1f} GB\")\n",
    "print(f\"  Currently allocated: {mem_allocated:.2f} GB\")\n",
    "print()\n",
    "\n",
    "def collect_activations_multilayer(model, tokenizer, prompt, device=\"cuda\"):\n",
    "    \"\"\"Extract hidden states from multiple layers\"\"\"\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, use_cache=False)\n",
    "    \n",
    "    last_valid_pos = inputs['attention_mask'][0].sum() - 1\n",
    "    num_layers = len(outputs.hidden_states) - 1\n",
    "    \n",
    "    # Sample layers: dense early, then sparse\n",
    "    layer_indices = [1, 2, 3, 4, 7, 10, 14, 18, 22, num_layers]\n",
    "    \n",
    "    activations = {}\n",
    "    for idx in layer_indices:\n",
    "        layer_activation = outputs.hidden_states[idx][0, last_valid_pos, :].cpu().clone()\n",
    "        activations[f\"layer_{idx}\"] = layer_activation\n",
    "    \n",
    "    # Aggressive cleanup\n",
    "    del outputs.hidden_states\n",
    "    del outputs\n",
    "    del inputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return activations, seq_len, layer_indices\n",
    "\n",
    "# Setup\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name} in BF16...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Check what attention implementation is actually being used\n",
    "config_impl = getattr(model.config, '_attn_implementation', 'not set')\n",
    "print(f\"Model attention implementation: {config_impl}\")\n",
    "\n",
    "# Load prompt from file, or use default\n",
    "prompt_file = \"dummytext.txt\"\n",
    "try:\n",
    "    with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "        prompt = f.read().strip()\n",
    "    print(f\"✓ Loaded prompt from {prompt_file}\")\n",
    "except FileNotFoundError:\n",
    "    prompt = \"\"\"The development of large language models has fundamentally transformed natural language processing \n",
    "and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated \n",
    "remarkable capabilities across a wide range of tasks, from translation and summarization to question answering \n",
    "and creative writing. However, their deployment raises significant challenges related to computational efficiency, \n",
    "interpretability, and safety.\"\"\"\n",
    "    print(f\"⚠ Using default prompt\")\n",
    "\n",
    "prompt_tokens = len(tokenizer.encode(prompt))\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print()\n",
    "\n",
    "# Run experiment\n",
    "num_repetitions = 5\n",
    "results = {}\n",
    "all_activations = {}\n",
    "layer_indices = None\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CUDA VERSION COMPARISON EXPERIMENT\")\n",
    "print(f\"CUDA Build: {cuda_version_str}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Precision: BF16 (bfloat16)\")\n",
    "print(f\"Attention: flash_attention_2\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Repetitions: {num_repetitions}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"\\nCollecting multi-layer activations ({num_repetitions} repetitions)...\")\n",
    "\n",
    "for rep in range(num_repetitions):\n",
    "    mem_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    \n",
    "    activations, seq_len, layer_idx_list = collect_activations_multilayer(\n",
    "        model, tokenizer, prompt, device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    mem_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    \n",
    "    if layer_indices is None:\n",
    "        layer_indices = layer_idx_list\n",
    "        print(f\"  Extracting from layers: {layer_indices}\")\n",
    "        for layer_name in activations.keys():\n",
    "            results[layer_name] = []\n",
    "    \n",
    "    # Store activations for each layer\n",
    "    for layer_name, activation in activations.items():\n",
    "        results[layer_name].append(activation)\n",
    "    \n",
    "    if rep == 0:\n",
    "        print(f\"  Rep 0 norms: {', '.join([f'{k}={torch.norm(v).item():.2f}' for k, v in activations.items()])}\")\n",
    "        print(f\"  Rep 0 memory: before={mem_before:.2f}GB, after={mem_after:.2f}GB\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del activations\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"  ✓ Completed all repetitions\\n\")\n",
    "\n",
    "# Stack repetitions into tensors\n",
    "for layer_name in results.keys():\n",
    "    results[layer_name] = torch.stack(results[layer_name])\n",
    "\n",
    "# Check repeatability\n",
    "last_layer_name = f\"layer_{layer_indices[-1]}\"\n",
    "first_rep = results[last_layer_name][0]\n",
    "all_identical = all(\n",
    "    torch.equal(first_rep, results[last_layer_name][i]) \n",
    "    for i in range(1, num_repetitions)\n",
    ")\n",
    "print(f\"Repeatability (last layer): {'✓ All identical' if all_identical else '⚠ Varies'}\")\n",
    "\n",
    "# Convert to numpy for JSON storage\n",
    "for layer_name, tensor in results.items():\n",
    "    all_activations[layer_name] = tensor.float().numpy().tolist()\n",
    "\n",
    "# Compute statistics\n",
    "layer_stats = {}\n",
    "for layer_name in results.keys():\n",
    "    mean_activation = results[layer_name].mean(dim=0)\n",
    "    layer_stats[layer_name] = {\n",
    "        \"mean_norm\": float(torch.norm(mean_activation)),\n",
    "        \"std_within_reps\": float(torch.stack([\n",
    "            torch.norm(results[layer_name][i] - mean_activation) \n",
    "            for i in range(num_repetitions)\n",
    "        ]).std())\n",
    "    }\n",
    "\n",
    "print(\"\\nLayer activation norms:\")\n",
    "for layer_name in sorted(layer_stats.keys(), key=lambda x: int(x.split('_')[1])):\n",
    "    norm = layer_stats[layer_name][\"mean_norm\"]\n",
    "    print(f\"  {layer_name}: {norm:.2f}\")\n",
    "\n",
    "# Save results\n",
    "output = {\n",
    "    \"experiment\": \"cuda_version_forensics\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"software\": {\n",
    "        \"pytorch_version\": torch.__version__,\n",
    "        \"cuda_runtime\": torch.version.cuda,\n",
    "        \"cuda_build\": cuda_version_str,\n",
    "        \"attention_implementation\": config_impl\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"repetitions\": num_repetitions,\n",
    "        \"layers_sampled\": layer_indices,\n",
    "        \"hidden_dim\": int(results[last_layer_name][0].shape[0])\n",
    "    },\n",
    "    \"layer_statistics\": layer_stats,\n",
    "    \"reproducibility\": {\n",
    "        \"all_repetitions_identical\": all_identical\n",
    "    },\n",
    "    \"raw_activations\": all_activations\n",
    "}\n",
    "\n",
    "# Filename includes CUDA version\n",
    "gpu_name = torch.cuda.get_device_name(0).replace(' ', '_')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_file = f\"{gpu_name}_{cuda_version_str}_7b_bf16_eager_{timestamp}.json\"\n",
    "output_path = f\"/workspace/experiments/{output_file}\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(f\"✓ File size: ~{len(json.dumps(output)) / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(f\"CUDA Build: {cuda_version_str}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. If this was cu118: Restart kernel, install cu120, run again\")\n",
    "print(\"2. If this was cu120: Compare the two JSON files\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d9d38fd-56fb-41de-8672-6757e58ce821",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.4.0+cu118\n",
      "Uninstalling torch-2.4.0+cu118:\n",
      "  Successfully uninstalled torch-2.4.0+cu118\n",
      "\u001b[33mWARNING: Skipping flash-attn as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu120\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.4.0+cu120 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.4.0+cu120\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.11.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Collecting torch>=2.0.0 (from accelerate)\n",
      "  Using cached torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Collecting triton==3.5.0 (from torch>=2.0.0->accelerate)\n",
      "  Using cached triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Using cached torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "Using cached triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "Installing collected packages: triton, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.0.0\n",
      "\u001b[2K    Uninstalling triton-3.0.0:\n",
      "\u001b[2K      Successfully uninstalled triton-3.0.0\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torch]32m1/2\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.8.0+cu128 requires torch==2.8.0, but you have torch 2.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.9.0 triton-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Restart kernel/Python completely\n",
    "!pip uninstall torch flash-attn -y\n",
    "\n",
    "# Install PyTorch first\n",
    "!pip install torch==2.4.0+cu120 --index-url https://download.pytorch.org/whl/cu120\n",
    "\n",
    "# Then install everything else\n",
    "!pip install transformers hf_transfer accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f3df87-a02c-4e47-86c7-730a8171e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUDA VERSION FORENSICS EXPERIMENT\n",
      "============================================================\n",
      "\n",
      "System Info:\n",
      "  Hostname: 75a750b4edaf\n",
      "  Container: 75a750b4edaf\n",
      "  GPU: NVIDIA A100 80GB PCIe\n",
      "  PyTorch: 2.4.0+cu118\n",
      "  CUDA Runtime: 11.8\n",
      "  CUDA Build: cu118\n",
      "\n",
      "GPU Memory:\n",
      "  Total: 79.3 GB\n",
      "  Currently allocated: 14.20 GB\n",
      "\n",
      "Loading Qwen/Qwen2.5-7B-Instruct in BF16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e2f2e5d91c49a499d3ed7ddad5ebe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model attention implementation: eager\n",
      "✓ Loaded prompt from dummytext.txt\n",
      "Prompt tokens: 3404\n",
      "\n",
      "============================================================\n",
      "CUDA VERSION COMPARISON EXPERIMENT\n",
      "CUDA Build: cu118\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Precision: BF16 (bfloat16)\n",
      "Attention: flash_attention_2\n",
      "Prompt tokens: 3404\n",
      "Repetitions: 5\n",
      "============================================================\n",
      "\n",
      "\n",
      "Collecting multi-layer activations (5 repetitions)...\n",
      "  Extracting from layers: [1, 2, 3, 4, 7, 10, 14, 18, 22, 28]\n",
      "  Rep 0 norms: layer_1=8.62, layer_2=14.75, layer_3=17.12, layer_4=18.50, layer_7=35.75, layer_10=55.75, layer_14=65.50, layer_18=79.00, layer_22=159.00, layer_28=308.00\n",
      "  Rep 0 memory: before=26.36GB, after=14.20GB\n",
      "  ✓ Completed all repetitions\n",
      "\n",
      "Repeatability (last layer): ✓ All identical\n",
      "\n",
      "Layer activation norms:\n",
      "  layer_1: 8.62\n",
      "  layer_2: 14.75\n",
      "  layer_3: 17.12\n",
      "  layer_4: 18.50\n",
      "  layer_7: 35.75\n",
      "  layer_10: 55.75\n",
      "  layer_14: 65.50\n",
      "  layer_18: 79.00\n",
      "  layer_22: 159.00\n",
      "  layer_28: 308.00\n",
      "\n",
      "✓ Results saved to /workspace/experiments/NVIDIA_A100_80GB_PCIe_cu118_7b_bf16_eager_20251103_140148.json\n",
      "✓ File size: ~2218.0 KB\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT COMPLETE\n",
      "CUDA Build: cu118\n",
      "============================================================\n",
      "\n",
      "Next steps:\n",
      "1. If this was cu118: Restart kernel, install cu120, run again\n",
      "2. If this was cu120: Compare the two JSON files\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "CUDA Version Forensics Experiment\n",
    "Tests if different CUDA toolkit versions create detectable activation deviations\n",
    "Same PyTorch version (2.4.0), different CUDA toolkits (11.8 vs 12.0)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUDA VERSION FORENSICS EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# System info\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(f\"\\nSystem Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  Container: {CONTAINER_ID}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA Runtime: {torch.version.cuda}\")\n",
    "\n",
    "# Extract CUDA version from PyTorch build\n",
    "pytorch_version = torch.__version__\n",
    "if '+cu' in pytorch_version:\n",
    "    cuda_build = pytorch_version.split('+cu')[1].split('.')[0]  # e.g., \"118\" or \"120\"\n",
    "    cuda_version_str = f\"cu{cuda_build}\"\n",
    "    print(f\"  CUDA Build: {cuda_version_str}\")\n",
    "else:\n",
    "    print(f\"  WARNING: Could not detect CUDA version from PyTorch build\")\n",
    "    cuda_version_str = \"unknown\"\n",
    "\n",
    "print()\n",
    "\n",
    "# Check GPU memory\n",
    "mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "mem_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "print(f\"GPU Memory:\")\n",
    "print(f\"  Total: {mem_total:.1f} GB\")\n",
    "print(f\"  Currently allocated: {mem_allocated:.2f} GB\")\n",
    "print()\n",
    "\n",
    "def collect_activations_multilayer(model, tokenizer, prompt, device=\"cuda\"):\n",
    "    \"\"\"Extract hidden states from multiple layers\"\"\"\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, use_cache=False)\n",
    "    \n",
    "    last_valid_pos = inputs['attention_mask'][0].sum() - 1\n",
    "    num_layers = len(outputs.hidden_states) - 1\n",
    "    \n",
    "    # Sample layers: dense early, then sparse\n",
    "    layer_indices = [1, 2, 3, 4, 7, 10, 14, 18, 22, num_layers]\n",
    "    \n",
    "    activations = {}\n",
    "    for idx in layer_indices:\n",
    "        layer_activation = outputs.hidden_states[idx][0, last_valid_pos, :].cpu().clone()\n",
    "        activations[f\"layer_{idx}\"] = layer_activation\n",
    "    \n",
    "    # Aggressive cleanup\n",
    "    del outputs.hidden_states\n",
    "    del outputs\n",
    "    del inputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return activations, seq_len, layer_indices\n",
    "\n",
    "# Setup\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name} in BF16...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Check what attention implementation is actually being used\n",
    "config_impl = getattr(model.config, '_attn_implementation', 'not set')\n",
    "print(f\"Model attention implementation: {config_impl}\")\n",
    "\n",
    "# Load prompt from file, or use default\n",
    "prompt_file = \"dummytext.txt\"\n",
    "try:\n",
    "    with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "        prompt = f.read().strip()\n",
    "    print(f\"✓ Loaded prompt from {prompt_file}\")\n",
    "except FileNotFoundError:\n",
    "    prompt = \"\"\"The development of large language models has fundamentally transformed natural language processing \n",
    "and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated \n",
    "remarkable capabilities across a wide range of tasks, from translation and summarization to question answering \n",
    "and creative writing. However, their deployment raises significant challenges related to computational efficiency, \n",
    "interpretability, and safety.\"\"\"\n",
    "    print(f\"⚠ Using default prompt\")\n",
    "\n",
    "prompt_tokens = len(tokenizer.encode(prompt))\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print()\n",
    "\n",
    "# Run experiment\n",
    "num_repetitions = 5\n",
    "results = {}\n",
    "all_activations = {}\n",
    "layer_indices = None\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CUDA VERSION COMPARISON EXPERIMENT\")\n",
    "print(f\"CUDA Build: {cuda_version_str}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Precision: BF16 (bfloat16)\")\n",
    "print(f\"Attention: flash_attention_2\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Repetitions: {num_repetitions}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"\\nCollecting multi-layer activations ({num_repetitions} repetitions)...\")\n",
    "\n",
    "for rep in range(num_repetitions):\n",
    "    mem_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    \n",
    "    activations, seq_len, layer_idx_list = collect_activations_multilayer(\n",
    "        model, tokenizer, prompt, device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    mem_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    \n",
    "    if layer_indices is None:\n",
    "        layer_indices = layer_idx_list\n",
    "        print(f\"  Extracting from layers: {layer_indices}\")\n",
    "        for layer_name in activations.keys():\n",
    "            results[layer_name] = []\n",
    "    \n",
    "    # Store activations for each layer\n",
    "    for layer_name, activation in activations.items():\n",
    "        results[layer_name].append(activation)\n",
    "    \n",
    "    if rep == 0:\n",
    "        print(f\"  Rep 0 norms: {', '.join([f'{k}={torch.norm(v).item():.2f}' for k, v in activations.items()])}\")\n",
    "        print(f\"  Rep 0 memory: before={mem_before:.2f}GB, after={mem_after:.2f}GB\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del activations\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"  ✓ Completed all repetitions\\n\")\n",
    "\n",
    "# Stack repetitions into tensors\n",
    "for layer_name in results.keys():\n",
    "    results[layer_name] = torch.stack(results[layer_name])\n",
    "\n",
    "# Check repeatability\n",
    "last_layer_name = f\"layer_{layer_indices[-1]}\"\n",
    "first_rep = results[last_layer_name][0]\n",
    "all_identical = all(\n",
    "    torch.equal(first_rep, results[last_layer_name][i]) \n",
    "    for i in range(1, num_repetitions)\n",
    ")\n",
    "print(f\"Repeatability (last layer): {'✓ All identical' if all_identical else '⚠ Varies'}\")\n",
    "\n",
    "# Convert to numpy for JSON storage\n",
    "for layer_name, tensor in results.items():\n",
    "    all_activations[layer_name] = tensor.float().numpy().tolist()\n",
    "\n",
    "# Compute statistics\n",
    "layer_stats = {}\n",
    "for layer_name in results.keys():\n",
    "    mean_activation = results[layer_name].mean(dim=0)\n",
    "    layer_stats[layer_name] = {\n",
    "        \"mean_norm\": float(torch.norm(mean_activation)),\n",
    "        \"std_within_reps\": float(torch.stack([\n",
    "            torch.norm(results[layer_name][i] - mean_activation) \n",
    "            for i in range(num_repetitions)\n",
    "        ]).std())\n",
    "    }\n",
    "\n",
    "print(\"\\nLayer activation norms:\")\n",
    "for layer_name in sorted(layer_stats.keys(), key=lambda x: int(x.split('_')[1])):\n",
    "    norm = layer_stats[layer_name][\"mean_norm\"]\n",
    "    print(f\"  {layer_name}: {norm:.2f}\")\n",
    "\n",
    "# Save results\n",
    "output = {\n",
    "    \"experiment\": \"cuda_version_forensics\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"software\": {\n",
    "        \"pytorch_version\": torch.__version__,\n",
    "        \"cuda_runtime\": torch.version.cuda,\n",
    "        \"cuda_build\": cuda_version_str,\n",
    "        \"attention_implementation\": config_impl\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"repetitions\": num_repetitions,\n",
    "        \"layers_sampled\": layer_indices,\n",
    "        \"hidden_dim\": int(results[last_layer_name][0].shape[0])\n",
    "    },\n",
    "    \"layer_statistics\": layer_stats,\n",
    "    \"reproducibility\": {\n",
    "        \"all_repetitions_identical\": all_identical\n",
    "    },\n",
    "    \"raw_activations\": all_activations\n",
    "}\n",
    "\n",
    "# Filename includes CUDA version\n",
    "gpu_name = torch.cuda.get_device_name(0).replace(' ', '_')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_file = f\"{gpu_name}_{cuda_version_str}_7b_bf16_eager_{timestamp}.json\"\n",
    "output_path = f\"/workspace/experiments/{output_file}\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(f\"✓ File size: ~{len(json.dumps(output)) / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(f\"CUDA Build: {cuda_version_str}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. If this was cu118: Restart kernel, install cu120, run again\")\n",
    "print(\"2. If this was cu120: Compare the two JSON files\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32acd34-81cd-4048-8a71-31e77d70d982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
