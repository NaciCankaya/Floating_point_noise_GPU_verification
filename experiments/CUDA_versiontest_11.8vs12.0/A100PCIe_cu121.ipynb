{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e3b37f1-ffa0-4cb6-aed5-ca7048c9ff2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch==2.4.0+cu121\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.4.0%2Bcu121-cp312-cp312-linux_x86_64.whl (799.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m799.0/799.0 MB\u001b[0m \u001b[31m132.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu121) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu121) (4.15.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu121) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu121) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu121) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu121) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.4.0+cu121) (80.9.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m139.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m135.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m139.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m140.6 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m168.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m144.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m160.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m172.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m172.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.4.0+cu121)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.0.0 (from torch==2.4.0+cu121)\n",
      "  Using cached https://download.pytorch.org/whl/triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.4.0+cu121) (12.8.93)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.4.0+cu121) (3.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.4.0+cu121) (1.3.0)\n",
      "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
      "\u001b[2K  Attempting uninstall: triton\n",
      "\u001b[2K    Found existing installation: triton 3.5.0\n",
      "\u001b[2K    Uninstalling triton-3.5.0:\n",
      "\u001b[2K      Successfully uninstalled triton-3.5.0\n",
      "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12━━━━━━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.8.9013\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.8.90:━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.8.900/13\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12━━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.5/13\u001b[0m [triton]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.5:━━━\u001b[0m \u001b[32m 0/13\u001b[0m [triton]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.5 0/13\u001b[0m [triton]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.8.93[0m \u001b[32m 2/13\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.8.93:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/13\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93━\u001b[0m \u001b[32m 2/13\u001b[0m [nvidia-nccl-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-curand-cu12━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.9.90━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.9.90:━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.9.90━━━\u001b[0m \u001b[32m 3/13\u001b[0m [nvidia-cusparse-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.3.83━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.3.83:━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83━━━━\u001b[0m \u001b[32m 4/13\u001b[0m [nvidia-curand-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 5/13\u001b[0m [nvidia-cufft-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90━\u001b[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:━━━━━━━━━━━━\u001b[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93━\u001b[0m \u001b[32m 6/13\u001b[0m [nvidia-cuda-runtime-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu120m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90━\u001b[0m \u001b[32m 7/13\u001b[0m [nvidia-cuda-nvrtc-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.8.4.1━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.8.4.1:0m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/13\u001b[0m [nvidia-cuda-cupti-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.3.90[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.3.90:0m━━━━━━━━━━━━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90━\u001b[0m \u001b[32m 9/13\u001b[0m [nvidia-cublas-cu12]\n",
      "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.10.2.21━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.10.2.21:[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21━━━━\u001b[0m \u001b[32m10/13\u001b[0m [nvidia-cusolver-cu12]\n",
      "\u001b[2K  Attempting uninstall: torch━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m11/13\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Found existing installation: torch 2.9.01m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m11/13\u001b[0m [nvidia-cudnn-cu12]\n",
      "\u001b[2K    Uninstalling torch-2.9.0:━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m12/13\u001b[0m [torch]nn-cu12]\n",
      "\u001b[2K      Successfully uninstalled torch-2.9.0━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m12/13\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13/13\u001b[0m [torch]m12/13\u001b[0m [torch]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.8.0+cu128 requires torch==2.8.0, but you have torch 2.4.0+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0+cu121 triton-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.4.0+cu121 --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f019333a-5322-41df-a322-99053abcb605",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CUDA VERSION FORENSICS EXPERIMENT\n",
      "============================================================\n",
      "\n",
      "System Info:\n",
      "  Hostname: 75a750b4edaf\n",
      "  Container: 75a750b4edaf\n",
      "  GPU: NVIDIA A100 80GB PCIe\n",
      "  PyTorch: 2.4.0+cu121\n",
      "  CUDA Runtime: 12.1\n",
      "  CUDA Build: cu121\n",
      "\n",
      "GPU Memory:\n",
      "  Total: 79.3 GB\n",
      "  Currently allocated: 0.00 GB\n",
      "\n",
      "Loading Qwen/Qwen2.5-7B-Instruct in BF16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c96e59a5f24531ac906384baf9e38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model attention implementation: eager\n",
      "✓ Loaded prompt from dummytext.txt\n",
      "Prompt tokens: 3404\n",
      "\n",
      "============================================================\n",
      "CUDA VERSION COMPARISON EXPERIMENT\n",
      "CUDA Build: cu121\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Precision: BF16 (bfloat16)\n",
      "Attention: flash_attention_2\n",
      "Prompt tokens: 3404\n",
      "Repetitions: 5\n",
      "============================================================\n",
      "\n",
      "\n",
      "Collecting multi-layer activations (5 repetitions)...\n",
      "  Extracting from layers: [1, 2, 3, 4, 7, 10, 14, 18, 22, 28]\n",
      "  Rep 0 norms: layer_1=8.62, layer_2=14.81, layer_3=17.12, layer_4=18.50, layer_7=35.50, layer_10=55.50, layer_14=66.00, layer_18=79.00, layer_22=160.00, layer_28=312.00\n",
      "  Rep 0 memory: before=14.19GB, after=14.20GB\n",
      "  ✓ Completed all repetitions\n",
      "\n",
      "Repeatability (last layer): ✓ All identical\n",
      "\n",
      "Layer activation norms:\n",
      "  layer_1: 8.62\n",
      "  layer_2: 14.81\n",
      "  layer_3: 17.12\n",
      "  layer_4: 18.50\n",
      "  layer_7: 35.50\n",
      "  layer_10: 55.50\n",
      "  layer_14: 66.00\n",
      "  layer_18: 79.00\n",
      "  layer_22: 160.00\n",
      "  layer_28: 312.00\n",
      "\n",
      "✓ Results saved to /workspace/experiments/NVIDIA_A100_80GB_PCIe_cu121_7b_bf16_eager_20251103_140554.json\n",
      "✓ File size: ~2216.7 KB\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT COMPLETE\n",
      "CUDA Build: cu121\n",
      "============================================================\n",
      "\n",
      "Next steps:\n",
      "1. If this was cu118: Restart kernel, install cu120, run again\n",
      "2. If this was cu120: Compare the two JSON files\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "CUDA Version Forensics Experiment\n",
    "Tests if different CUDA toolkit versions create detectable activation deviations\n",
    "Same PyTorch version (2.4.0), different CUDA toolkits (11.8 vs 12.0)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CUDA VERSION FORENSICS EXPERIMENT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# System info\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(f\"\\nSystem Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  Container: {CONTAINER_ID}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA Runtime: {torch.version.cuda}\")\n",
    "\n",
    "# Extract CUDA version from PyTorch build\n",
    "pytorch_version = torch.__version__\n",
    "if '+cu' in pytorch_version:\n",
    "    cuda_build = pytorch_version.split('+cu')[1].split('.')[0]  # e.g., \"118\" or \"120\"\n",
    "    cuda_version_str = f\"cu{cuda_build}\"\n",
    "    print(f\"  CUDA Build: {cuda_version_str}\")\n",
    "else:\n",
    "    print(f\"  WARNING: Could not detect CUDA version from PyTorch build\")\n",
    "    cuda_version_str = \"unknown\"\n",
    "\n",
    "print()\n",
    "\n",
    "# Check GPU memory\n",
    "mem_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "mem_allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "print(f\"GPU Memory:\")\n",
    "print(f\"  Total: {mem_total:.1f} GB\")\n",
    "print(f\"  Currently allocated: {mem_allocated:.2f} GB\")\n",
    "print()\n",
    "\n",
    "def collect_activations_multilayer(model, tokenizer, prompt, device=\"cuda\"):\n",
    "    \"\"\"Extract hidden states from multiple layers\"\"\"\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, use_cache=False)\n",
    "    \n",
    "    last_valid_pos = inputs['attention_mask'][0].sum() - 1\n",
    "    num_layers = len(outputs.hidden_states) - 1\n",
    "    \n",
    "    # Sample layers: dense early, then sparse\n",
    "    layer_indices = [1, 2, 3, 4, 7, 10, 14, 18, 22, num_layers]\n",
    "    \n",
    "    activations = {}\n",
    "    for idx in layer_indices:\n",
    "        layer_activation = outputs.hidden_states[idx][0, last_valid_pos, :].cpu().clone()\n",
    "        activations[f\"layer_{idx}\"] = layer_activation\n",
    "    \n",
    "    # Aggressive cleanup\n",
    "    del outputs.hidden_states\n",
    "    del outputs\n",
    "    del inputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return activations, seq_len, layer_indices\n",
    "\n",
    "# Setup\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name} in BF16...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")\n",
    "\n",
    "# Check what attention implementation is actually being used\n",
    "config_impl = getattr(model.config, '_attn_implementation', 'not set')\n",
    "print(f\"Model attention implementation: {config_impl}\")\n",
    "\n",
    "# Load prompt from file, or use default\n",
    "prompt_file = \"dummytext.txt\"\n",
    "try:\n",
    "    with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "        prompt = f.read().strip()\n",
    "    print(f\"✓ Loaded prompt from {prompt_file}\")\n",
    "except FileNotFoundError:\n",
    "    prompt = \"\"\"The development of large language models has fundamentally transformed natural language processing \n",
    "and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated \n",
    "remarkable capabilities across a wide range of tasks, from translation and summarization to question answering \n",
    "and creative writing. However, their deployment raises significant challenges related to computational efficiency, \n",
    "interpretability, and safety.\"\"\"\n",
    "    print(f\"⚠ Using default prompt\")\n",
    "\n",
    "prompt_tokens = len(tokenizer.encode(prompt))\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print()\n",
    "\n",
    "# Run experiment\n",
    "num_repetitions = 5\n",
    "results = {}\n",
    "all_activations = {}\n",
    "layer_indices = None\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"CUDA VERSION COMPARISON EXPERIMENT\")\n",
    "print(f\"CUDA Build: {cuda_version_str}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Precision: BF16 (bfloat16)\")\n",
    "print(f\"Attention: flash_attention_2\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Repetitions: {num_repetitions}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "print(f\"\\nCollecting multi-layer activations ({num_repetitions} repetitions)...\")\n",
    "\n",
    "for rep in range(num_repetitions):\n",
    "    mem_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    \n",
    "    activations, seq_len, layer_idx_list = collect_activations_multilayer(\n",
    "        model, tokenizer, prompt, device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    mem_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    \n",
    "    if layer_indices is None:\n",
    "        layer_indices = layer_idx_list\n",
    "        print(f\"  Extracting from layers: {layer_indices}\")\n",
    "        for layer_name in activations.keys():\n",
    "            results[layer_name] = []\n",
    "    \n",
    "    # Store activations for each layer\n",
    "    for layer_name, activation in activations.items():\n",
    "        results[layer_name].append(activation)\n",
    "    \n",
    "    if rep == 0:\n",
    "        print(f\"  Rep 0 norms: {', '.join([f'{k}={torch.norm(v).item():.2f}' for k, v in activations.items()])}\")\n",
    "        print(f\"  Rep 0 memory: before={mem_before:.2f}GB, after={mem_after:.2f}GB\")\n",
    "    \n",
    "    # Cleanup\n",
    "    del activations\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"  ✓ Completed all repetitions\\n\")\n",
    "\n",
    "# Stack repetitions into tensors\n",
    "for layer_name in results.keys():\n",
    "    results[layer_name] = torch.stack(results[layer_name])\n",
    "\n",
    "# Check repeatability\n",
    "last_layer_name = f\"layer_{layer_indices[-1]}\"\n",
    "first_rep = results[last_layer_name][0]\n",
    "all_identical = all(\n",
    "    torch.equal(first_rep, results[last_layer_name][i]) \n",
    "    for i in range(1, num_repetitions)\n",
    ")\n",
    "print(f\"Repeatability (last layer): {'✓ All identical' if all_identical else '⚠ Varies'}\")\n",
    "\n",
    "# Convert to numpy for JSON storage\n",
    "for layer_name, tensor in results.items():\n",
    "    all_activations[layer_name] = tensor.float().numpy().tolist()\n",
    "\n",
    "# Compute statistics\n",
    "layer_stats = {}\n",
    "for layer_name in results.keys():\n",
    "    mean_activation = results[layer_name].mean(dim=0)\n",
    "    layer_stats[layer_name] = {\n",
    "        \"mean_norm\": float(torch.norm(mean_activation)),\n",
    "        \"std_within_reps\": float(torch.stack([\n",
    "            torch.norm(results[layer_name][i] - mean_activation) \n",
    "            for i in range(num_repetitions)\n",
    "        ]).std())\n",
    "    }\n",
    "\n",
    "print(\"\\nLayer activation norms:\")\n",
    "for layer_name in sorted(layer_stats.keys(), key=lambda x: int(x.split('_')[1])):\n",
    "    norm = layer_stats[layer_name][\"mean_norm\"]\n",
    "    print(f\"  {layer_name}: {norm:.2f}\")\n",
    "\n",
    "# Save results\n",
    "output = {\n",
    "    \"experiment\": \"cuda_version_forensics\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"software\": {\n",
    "        \"pytorch_version\": torch.__version__,\n",
    "        \"cuda_runtime\": torch.version.cuda,\n",
    "        \"cuda_build\": cuda_version_str,\n",
    "        \"attention_implementation\": config_impl\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"repetitions\": num_repetitions,\n",
    "        \"layers_sampled\": layer_indices,\n",
    "        \"hidden_dim\": int(results[last_layer_name][0].shape[0])\n",
    "    },\n",
    "    \"layer_statistics\": layer_stats,\n",
    "    \"reproducibility\": {\n",
    "        \"all_repetitions_identical\": all_identical\n",
    "    },\n",
    "    \"raw_activations\": all_activations\n",
    "}\n",
    "\n",
    "# Filename includes CUDA version\n",
    "gpu_name = torch.cuda.get_device_name(0).replace(' ', '_')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "output_file = f\"{gpu_name}_{cuda_version_str}_7b_bf16_eager_{timestamp}.json\"\n",
    "output_path = f\"/workspace/experiments/{output_file}\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(f\"✓ File size: ~{len(json.dumps(output)) / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(f\"CUDA Build: {cuda_version_str}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. If this was cu118: Restart kernel, install cu120, run again\")\n",
    "print(\"2. If this was cu120: Compare the two JSON files\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f44cf-906d-49d2-9bb1-19473ed6831c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
