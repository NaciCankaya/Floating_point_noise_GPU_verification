{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ab59ab-1b9f-4d4b-bb70-39666e3de7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in notebook mode...\n",
      "Will collect all batch size configurations automatically.\n",
      "\n",
      "======================================================================\n",
      "BATCH SIZE MATRIX EXPERIMENT\n",
      "======================================================================\n",
      "\n",
      "Hardware: A100 (NVIDIA A100 80GB PCIe)\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Layers: [1, 4, 10, 18, 28]\n",
      "Reference sequences: 3\n",
      "Batch sizes: [1, 2, 4]\n",
      "Total measurements: 9\n",
      "\n",
      "System Info:\n",
      "  Hostname: 34d100e052e4\n",
      "  GPU: NVIDIA A100 80GB PCIe\n",
      "  PyTorch: 2.8.0+cu129\n",
      "  CUDA: 12.9\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17049aa698564520a44f7c07146b87a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32159261848436fbd6cda754ddaf18b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746ca8a6f6ae4645bce78ae8953db21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "890c82b774e64adcbb66114d94d59b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6126f53f4f48f5bb4f6cec4744c452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387a3267f2c84789983e02b002ba0442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f59fc28607a24050a44382d85ce0335e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa9090577ff4a50925804de021b3a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68547b4d48bf4a1c8f268b48b8628341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1733bcf9cb464f469561e8f01eb8afbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c9769f24864baa9824bae1e759e784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Batch Size Matrix Experiment\n",
    "\n",
    "Runs all batch size configurations (bs=1, bs=2, bs=4) for multiple reference sequences.\n",
    "Each reference sequence gets unique dummy sequences for batching.\n",
    "\n",
    "Design:\n",
    "- 3 reference sequences\n",
    "- Each tested at bs=1, bs=2, bs=4\n",
    "- 9 total measurements per GPU\n",
    "- Enables full cross-comparison matrix\n",
    "\n",
    "CRITICAL: Truncates all sequences in batch to shortest length to avoid padding artifacts\n",
    "\n",
    "Usage:\n",
    "    python batch_matrix_experiment.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "# ============================================================================\n",
    "# REFERENCE SEQUENCES\n",
    "# ============================================================================\n",
    "\n",
    "REFERENCE_SEQUENCES = {\n",
    "    \"ref_technical\": \"\"\"Large language models have revolutionized natural language processing through their ability to capture complex patterns in text data. The transformer architecture, introduced in 2017, employs self-attention mechanisms that allow the model to weigh the importance of different tokens in the input sequence. During training, these models learn to predict the next token in a sequence by optimizing a cross-entropy loss function across billions of text examples.\"\"\",\n",
    "    \n",
    "    \"ref_narrative\": \"\"\"The morning sun filtered through the ancient oak trees as Sarah walked along the forest path, her boots crunching softly on the fallen leaves. She had been coming to these woods since childhood, when her grandmother first taught her to identify the different bird calls echoing through the canopy. Now, decades later, she found herself returning to this same trail whenever life felt overwhelming.\"\"\",\n",
    "    \n",
    "    \"ref_code\": \"\"\"The database migration system implements a sophisticated version control mechanism for schema changes. Each migration file contains both an upgrade and downgrade function, allowing the system to roll forward or backward through schema versions. The migration engine maintains a table tracking which migrations have been applied, using timestamps and hash values to ensure consistency across different environments.\"\"\"\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# DUMMY SEQUENCES (3 sets of 3 dummies, one set per reference)\n",
    "# ============================================================================\n",
    "\n",
    "DUMMY_SETS = {\n",
    "    \"ref_technical\": [\n",
    "        \"\"\"Quantum computing leverages the principles of quantum mechanics to perform computations that would be intractable for classical computers. At the heart of quantum computation lies the qubit, a quantum bit that can exist in a superposition of both 0 and 1 states simultaneously. When multiple qubits are entangled, they form a quantum register capable of representing an exponentially large state space.\"\"\",\n",
    "        \n",
    "        \"\"\"The neural architecture search algorithm systematically explores different model configurations to identify optimal designs for specific tasks. Modern approaches use reinforcement learning or evolutionary algorithms to navigate the vast search space of possible architectures. The process evaluates candidate models on validation data, gradually converging toward efficient and effective network topologies.\"\"\",\n",
    "        \n",
    "        \"\"\"Distributed consensus protocols enable multiple nodes in a network to agree on a single value despite potential failures or malicious actors. The Byzantine Generals Problem formalizes the challenge of achieving consensus when some participants may behave arbitrarily. Practical solutions like Paxos and Raft provide mechanisms for fault-tolerant agreement in real-world systems.\"\"\"\n",
    "    ],\n",
    "    \n",
    "    \"ref_narrative\": [\n",
    "        \"\"\"The old lighthouse stood sentinel on the rocky promontory, its weathered walls bearing testament to countless storms. Local legends spoke of the keeper who vanished one winter night, leaving only his log book with a final cryptic entry. Now automated, the beacon still swept across the dark waters, a guardian whose original purpose had long been superseded by modern navigation systems.\"\"\",\n",
    "        \n",
    "        \"\"\"Marcus found the letter tucked between the pages of his grandfather's journal, the paper yellowed and fragile with age. The handwriting was unfamiliar, yet the words spoke of events his family had never discussed. As he read, pieces of his heritage began to fall into place, revealing a story that had been deliberately hidden for three generations.\"\"\",\n",
    "        \n",
    "        \"\"\"The jazz club occupied a basement space that seemed to exist outside of time, where smoke still hung in the air despite the ban and the music felt like it emerged from another era. Every Thursday, the same musicians gathered to play standards that few in the younger generation recognized. Yet something about the atmosphere drew people in, seeking connection to an authenticity they sensed was disappearing from the world.\"\"\"\n",
    "    ],\n",
    "    \n",
    "    \"ref_code\": [\n",
    "        \"\"\"The distributed caching layer implements consistent hashing to minimize cache invalidation when nodes are added or removed from the cluster. Virtual nodes provide better load distribution across physical servers, while replication ensures availability even during node failures. The system monitors hit rates and eviction patterns to automatically adjust cache allocation strategies.\"\"\",\n",
    "        \n",
    "        \"\"\"The API gateway performs request routing, authentication, rate limiting, and response transformation for microservices. Each service registers its endpoints with the gateway, which maintains a dynamic routing table. The gateway implements circuit breakers to prevent cascade failures and provides detailed metrics for monitoring service health.\"\"\",\n",
    "        \n",
    "        \"\"\"The message queue system guarantees exactly-once delivery through a combination of acknowledgments, persistent storage, and idempotency tokens. Publishers receive confirmation only after messages are durably written to replicated storage. Consumers process messages within transactions, ensuring atomic updates across message consumption and business logic execution.\"\"\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "LAYER_INDICES = [1, 4, 10, 18, 28]\n",
    "BATCH_SIZES = [1, 2, 4]\n",
    "\n",
    "# ============================================================================\n",
    "# EXTRACTION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_last_token_keys(model, tokenizer, texts, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Extract key vectors from LAST TOKEN position for element 0 in batch.\n",
    "    \n",
    "    CRITICAL: Truncates all sequences to shortest length to avoid padding artifacts.\n",
    "    \n",
    "    Args:\n",
    "        texts: list of strings (batch)\n",
    "        \n",
    "    Returns:\n",
    "        dict: {\n",
    "            'layer_1': tensor of shape (key_dim,) for last token\n",
    "            'layer_4': tensor of shape (key_dim,),\n",
    "            ...\n",
    "        }\n",
    "        extraction_info: dict with extraction_position, original_lengths, truncated_length\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Step 1: Get token counts for all texts\n",
    "    original_lengths = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        original_lengths.append(len(tokens))\n",
    "    \n",
    "    # Step 2: Find minimum length\n",
    "    min_length = min(original_lengths)\n",
    "    \n",
    "    # Step 3: Truncate all texts to min_length tokens\n",
    "    truncated_texts = []\n",
    "    for text in texts:\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        truncated_tokens = tokens[:min_length]\n",
    "        truncated_text = tokenizer.decode(truncated_tokens, skip_special_tokens=False)\n",
    "        truncated_texts.append(truncated_text)\n",
    "    \n",
    "    # Step 4: Tokenize truncated batch\n",
    "    inputs = tokenizer(truncated_texts, return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Step 5: Verify element 0 has correct length\n",
    "    element0_length = inputs['attention_mask'][0].sum().item()\n",
    "    if element0_length != min_length:\n",
    "        raise ValueError(f\"Padding protection failed! Expected {min_length}, got {element0_length}\")\n",
    "    \n",
    "    # Step 6: Extraction position is last token (min_length - 1, 0-indexed)\n",
    "    extraction_pos = min_length - 1\n",
    "    \n",
    "    # Step 7: Run model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, use_cache=True)\n",
    "    \n",
    "    # Step 8: Extract key vectors from last token of element 0\n",
    "    key_vectors = {}\n",
    "    for layer_idx in LAYER_INDICES:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]  # (batch, num_heads, seq_len, head_dim)\n",
    "        last_token_keys = layer_keys[0, :, extraction_pos, :]  # (num_heads, head_dim)\n",
    "        key_dim = last_token_keys.shape[0] * last_token_keys.shape[1]\n",
    "        key_vectors[f'layer_{layer_idx}'] = last_token_keys.reshape(key_dim).cpu().clone()\n",
    "    \n",
    "    del outputs\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    extraction_info = {\n",
    "        'extraction_position': extraction_pos,\n",
    "        'original_lengths': original_lengths,\n",
    "        'truncated_length': min_length\n",
    "    }\n",
    "    \n",
    "    return key_vectors, extraction_info\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    # Auto-detect hardware\n",
    "    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "    if \"H100\" in gpu_name:\n",
    "        hardware_label = \"h100\"\n",
    "    elif \"A100\" in gpu_name:\n",
    "        hardware_label = \"a100\"\n",
    "    elif \"L40S\" in gpu_name or \"L40s\" in gpu_name:\n",
    "        hardware_label = \"l40s\"\n",
    "    else:\n",
    "        hardware_label = \"gpu\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"BATCH SIZE MATRIX EXPERIMENT\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"\\nHardware: {hardware_label.upper()} ({gpu_name})\")\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(f\"Layers: {LAYER_INDICES}\")\n",
    "    print(f\"Reference sequences: {len(REFERENCE_SEQUENCES)}\")\n",
    "    print(f\"Batch sizes: {BATCH_SIZES}\")\n",
    "    print(f\"Total measurements: {len(REFERENCE_SEQUENCES) * len(BATCH_SIZES)}\")\n",
    "    print()\n",
    "    \n",
    "    # System info\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    print(f\"System Info:\")\n",
    "    print(f\"  Hostname: {hostname}\")\n",
    "    print(f\"  GPU: {gpu_name}\")\n",
    "    print(f\"  PyTorch: {torch.__version__}\")\n",
    "    print(f\"  CUDA: {torch.version.cuda}\")\n",
    "    print()\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(f\"✓ Model loaded\\n\")\n",
    "    \n",
    "    # Prepare results structure\n",
    "    results = {\n",
    "        'metadata': {\n",
    "            'hardware': hardware_label,\n",
    "            'hostname': hostname,\n",
    "            'gpu': gpu_name,\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'cuda_version': torch.version.cuda,\n",
    "            'model': MODEL_NAME,\n",
    "            'layer_indices': LAYER_INDICES,\n",
    "            'batch_sizes_tested': BATCH_SIZES,\n",
    "            'num_reference_sequences': len(REFERENCE_SEQUENCES),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'experiment_type': 'batch_matrix'\n",
    "        },\n",
    "        'measurements': {}\n",
    "    }\n",
    "    \n",
    "    # Run all combinations\n",
    "    measurement_count = 0\n",
    "    total_measurements = len(REFERENCE_SEQUENCES) * len(BATCH_SIZES)\n",
    "    \n",
    "    for ref_name, ref_text in REFERENCE_SEQUENCES.items():\n",
    "        ref_dummies = DUMMY_SETS[ref_name]\n",
    "        \n",
    "        print(f\"Reference: {ref_name}\")\n",
    "        \n",
    "        for batch_size in BATCH_SIZES:\n",
    "            measurement_count += 1\n",
    "            measurement_name = f\"{ref_name}_bs{batch_size}\"\n",
    "            \n",
    "            print(f\"  [{measurement_count}/{total_measurements}] bs={batch_size}...\", end=\" \")\n",
    "            \n",
    "            # Build batch\n",
    "            if batch_size == 1:\n",
    "                batch_texts = [ref_text]\n",
    "            elif batch_size == 2:\n",
    "                batch_texts = [ref_text, ref_dummies[0]]\n",
    "            elif batch_size == 4:\n",
    "                batch_texts = [ref_text] + ref_dummies[:3]\n",
    "            \n",
    "            # Verify reproducibility (3 runs)\n",
    "            runs = []\n",
    "            all_info = []\n",
    "            \n",
    "            for rep in range(3):\n",
    "                key_vectors, extraction_info = extract_last_token_keys(model, tokenizer, batch_texts)\n",
    "                runs.append(key_vectors)\n",
    "                all_info.append(extraction_info)\n",
    "            \n",
    "            # Check reproducibility\n",
    "            reproducible = True\n",
    "            for layer_name in runs[0].keys():\n",
    "                for i in range(1, 3):\n",
    "                    if not torch.equal(runs[0][layer_name], runs[i][layer_name]):\n",
    "                        reproducible = False\n",
    "                        break\n",
    "            \n",
    "            # Verify extraction consistency\n",
    "            extraction_info = all_info[0]\n",
    "            for info in all_info[1:]:\n",
    "                if info['extraction_position'] != extraction_info['extraction_position']:\n",
    "                    print(f\"\\n  ⚠ WARNING: Extraction position varied across runs!\")\n",
    "                if info['truncated_length'] != extraction_info['truncated_length']:\n",
    "                    print(f\"\\n  ⚠ WARNING: Truncated length varied across runs!\")\n",
    "            \n",
    "            if not reproducible:\n",
    "                print(\"⚠ Non-reproducible!\", end=\" \")\n",
    "            \n",
    "            # Use first run\n",
    "            key_vectors = runs[0]\n",
    "            \n",
    "            # Store measurement\n",
    "            measurement = {\n",
    "                'reference_sequence': ref_name,\n",
    "                'batch_size': batch_size,\n",
    "                'extraction_position': extraction_info['extraction_position'],\n",
    "                'original_lengths': extraction_info['original_lengths'],\n",
    "                'truncated_length': extraction_info['truncated_length'],\n",
    "                'reproducible': reproducible,\n",
    "                'layers': {}\n",
    "            }\n",
    "            \n",
    "            for layer_name, layer_keys in key_vectors.items():\n",
    "                # Single token: shape (key_dim,)\n",
    "                measurement['layers'][layer_name] = {\n",
    "                    'key_vector': layer_keys.float().numpy().tolist(),\n",
    "                    'norm': float(torch.norm(layer_keys).item())\n",
    "                }\n",
    "            \n",
    "            results['measurements'][measurement_name] = measurement\n",
    "            \n",
    "            # Print length info\n",
    "            if batch_size > 1:\n",
    "                length_str = f\"[{extraction_info['truncated_length']}/{extraction_info['original_lengths'][0]}]\"\n",
    "                print(f\"{length_str}\", end=\" \")\n",
    "            \n",
    "            print(\"✓\" if reproducible else \"\")\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{hardware_label}_batch_matrix_{timestamp}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    file_size_kb = os.path.getsize(filepath) / 1024\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(f\"✓ Results saved to: {filepath}\")\n",
    "    print(f\"  File size: {file_size_kb:.1f} KB\")\n",
    "    print(f\"  Measurements: {len(results['measurements'])}\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(\"Next steps:\")\n",
    "    print(f\"1. Run on other hardware\")\n",
    "    print(f\"2. Compare: python compare_batch_matrix.py {filename} <other_file>\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Detect environment\n",
    "    try:\n",
    "        get_ipython()\n",
    "        in_notebook = True\n",
    "    except NameError:\n",
    "        in_notebook = False\n",
    "    \n",
    "    if in_notebook:\n",
    "        print(\"Running in notebook mode...\")\n",
    "        print(\"Will collect all batch size configurations automatically.\")\n",
    "        print()\n",
    "        main()\n",
    "    else:\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eb424f-d5fb-41e2-b2e4-2adc3d05851f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
