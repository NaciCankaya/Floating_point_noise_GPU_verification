{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b06b8efd-762d-47c5-a03c-7dc041e5a630",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-HARDWARE BATCH SIZE DETECTABILITY - VERIFICATION (teacher-forcing)\n",
      "================================================================================\n",
      "Log file: /workspace/experiments/experiment_verify_20251124_192512.txt\n",
      "\n",
      "Environment:\n",
      "  hostname: 06768f39097d\n",
      "  platform: Linux-6.14.0-24-generic-x86_64-with-glibc2.39\n",
      "  python_version: 3.12.3\n",
      "  torch_version: 2.8.0+cu128\n",
      "  cuda_version: 12.8\n",
      "  cudnn_version: 91002\n",
      "  transformers_version: 4.57.2\n",
      "  numpy_version: 2.1.2\n",
      "  attn_implementation: flash_attention_2\n",
      "  gpu_name: NVIDIA H100 PCIe\n",
      "  gpu_count: 1\n",
      "  flash_attn_version: 2.8.3\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Layers: [1, 4, 10, 18, 28]\n",
      "  Batch sizes: [4, 5, 8, 9, 16, 17]\n",
      "  Max tokens: 20\n",
      "  Reference file: A100_reference.json\n",
      "\n",
      "Loading model...\n",
      "  flash_attn 2.8.3 available\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad957598862e4c92b6ef6536c988ab8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "584fe36a4f57423cb86f692b02136590",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df04a85a536e4c74aa5eee66a3502f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e13c506fb1745f7a8baec999cc3454d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n",
      "    → 120214 tokens\n",
      "Total tokens: 120214\n",
      "Creating 51 slices of 512 tokens each\n",
      "Created 3 reference sequences\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4725dc9d01f4bc6be0de1125bbf117e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d38b445d4854dddbde8f95b8a409db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3ca740628b34e6dba0bc64276ff2d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25e4892a515449b088f0ffd91c983e67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c36393017c4a5886b6fa5c4a888ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a9aa920ee04edda4b7b8ecfd51a46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0303adf3ab4d48419961c4bbc164ab97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3601d20f704e4d15b5bc8cb3dd90c0c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded (attn_implementation=flash_attention_2)\n",
      "\n",
      "Loading reference file...\n",
      "Reference GPU: NVIDIA A100 80GB PCIe\n",
      "Verifier GPU:  NVIDIA H100 PCIe\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Script configuration:\n",
      "  ✓ attn_implementation: flash_attention_2\n",
      "\n",
      "Container-level dependencies:\n",
      "  ✓ python_version: 3.12.3\n",
      "  ✓ cuda_version: 12.8\n",
      "  ✓ cudnn_version: 91002\n",
      "\n",
      "Pip-installable packages:\n",
      "  ✓ torch_version: 2.8.0+cu128\n",
      "  ✓ transformers_version: 4.57.2\n",
      "  ✓ numpy_version: 2.1.2\n",
      "  ✓ flash_attn_version: 2.8.3\n",
      "\n",
      "Expected differences (hardware):\n",
      "  ✓ gpu_name: reference=NVIDIA A100 80GB PCIe, verifier=NVIDIA H100 PCIe\n",
      "  ✓ hostname: reference=c81492e44ce1, verifier=06768f39097d\n",
      "\n",
      "------------------------------------------------------------\n",
      "✓ ENVIRONMENT VALIDATION PASSED\n",
      "  All critical software versions match.\n",
      "  Only hardware differs - results will be meaningful.\n",
      "\n",
      "✓ Model, batch sizes, and layer indices match\n",
      "\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_0\n",
      "================================================================================\n",
      "\n",
      "  Claimed batch size: 4\n",
      "    Verify bs=4 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=4) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 1.64e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 1.64e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 1.64e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.24e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 3.06e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.50e-01\n",
      "\n",
      "  Claimed batch size: 5\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.80e-01\n",
      "    Verify bs=5 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=5) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.80e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.80e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.50e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.80e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 4.68e-01\n",
      "\n",
      "  Claimed batch size: 8\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.17e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.17e-01\n",
      "    Verify bs=8 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=8) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.17e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.03e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 3.06e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.50e-01\n",
      "\n",
      "  Claimed batch size: 9\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.17e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.17e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.17e-01\n",
      "    Verify bs=9 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=9) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.03e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 3.06e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.50e-01\n",
      "\n",
      "  Claimed batch size: 16\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.50e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.50e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.50e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 2.17e-01\n",
      "    Verify bs=16 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=16) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 3.06e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 5.11e-01, Logprob: 4.15e-01\n",
      "\n",
      "  Claimed batch size: 17\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 5.09e-01, Logprob: 2.21e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 5.09e-01, Logprob: 2.21e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 5.09e-01, Logprob: 2.21e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 5.09e-01, Logprob: 2.50e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 5.09e-01, Logprob: 3.54e-01\n",
      "    Verify bs=17 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=17) → 20 steps\n",
      "      → Key: 5.09e-01, Logprob: 2.50e-01\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_1\n",
      "================================================================================\n",
      "\n",
      "  Claimed batch size: 4\n",
      "    Verify bs=4 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=4) → 20 steps\n",
      "      → Key: 3.50e-01, Logprob: 2.03e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 3.50e-01, Logprob: 2.03e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 3.50e-01, Logprob: 2.03e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 1.44e+00, Logprob: 1.81e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 3.59e-01, Logprob: 2.21e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 3.32e-01, Logprob: 2.13e-01\n",
      "\n",
      "  Claimed batch size: 5\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.37e-01\n",
      "    Verify bs=5 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=5) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.37e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.37e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 1.95e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 1.98e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 1.16e+00, Logprob: 2.15e-01\n",
      "\n",
      "  Claimed batch size: 8\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.06e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.06e-01\n",
      "    Verify bs=8 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=8) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.06e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 1.16e+00, Logprob: 1.61e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 1.86e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 1.16e+00, Logprob: 1.98e-01\n",
      "\n",
      "  Claimed batch size: 9\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.06e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.06e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.06e-01\n",
      "    Verify bs=9 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=9) → 20 steps\n",
      "      → Key: 1.16e+00, Logprob: 1.61e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 1.86e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 1.16e+00, Logprob: 1.98e-01\n",
      "\n",
      "  Claimed batch size: 16\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.33e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.33e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.33e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.00e-01\n",
      "    Verify bs=16 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=16) → 20 steps\n",
      "      → Key: 1.15e+00, Logprob: 2.03e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 1.17e+00, Logprob: 1.71e-01\n",
      "\n",
      "  Claimed batch size: 17\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 1.52e+00, Logprob: 2.03e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 1.52e+00, Logprob: 2.03e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 1.52e+00, Logprob: 2.03e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 5.67e-01, Logprob: 1.91e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 1.52e+00, Logprob: 2.08e-01\n",
      "    Verify bs=17 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=17) → 20 steps\n",
      "      → Key: 1.54e+00, Logprob: 1.53e-01\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_2\n",
      "================================================================================\n",
      "\n",
      "  Claimed batch size: 4\n",
      "    Verify bs=4 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=4) → 20 steps\n",
      "      → Key: 1.05e+00, Logprob: 2.45e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 1.05e+00, Logprob: 2.45e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 1.05e+00, Logprob: 2.45e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 5.68e-01, Logprob: 1.82e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 5.68e-01, Logprob: 1.90e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 6.75e-01, Logprob: 1.84e-01\n",
      "\n",
      "  Claimed batch size: 5\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 5.68e-01, Logprob: 2.14e-01\n",
      "    Verify bs=5 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=5) → 20 steps\n",
      "      → Key: 5.68e-01, Logprob: 2.14e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 5.68e-01, Logprob: 2.14e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 1.06e+00, Logprob: 1.82e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 1.07e+00, Logprob: 2.08e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 1.06e+00, Logprob: 1.59e-01\n",
      "\n",
      "  Claimed batch size: 8\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 1.98e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 1.98e-01\n",
      "    Verify bs=8 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=8) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 1.98e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 2.03e+00, Logprob: 1.94e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 2.03e+00, Logprob: 1.98e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 2.04e+00, Logprob: 1.53e-01\n",
      "\n",
      "  Claimed batch size: 9\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 1.98e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 1.98e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 1.98e-01\n",
      "    Verify bs=9 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=9) → 20 steps\n",
      "      → Key: 2.03e+00, Logprob: 1.94e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 2.03e+00, Logprob: 1.98e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 2.04e+00, Logprob: 1.53e-01\n",
      "\n",
      "  Claimed batch size: 16\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 2.25e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 2.25e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 2.25e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 2.05e+00, Logprob: 1.82e-01\n",
      "    Verify bs=16 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=16) → 20 steps\n",
      "      → Key: 5.68e-01, Logprob: 1.94e-01\n",
      "    Verify bs=17 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=17) → 20 steps\n",
      "      → Key: 6.72e-01, Logprob: 2.10e-01\n",
      "\n",
      "  Claimed batch size: 17\n",
      "    Verify bs=4 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=4) → 20 steps\n",
      "      → Key: 2.04e+00, Logprob: 1.94e-01\n",
      "    Verify bs=5 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=5) → 20 steps\n",
      "      → Key: 2.04e+00, Logprob: 1.94e-01\n",
      "    Verify bs=8 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=8) → 20 steps\n",
      "      → Key: 2.04e+00, Logprob: 1.94e-01\n",
      "    Verify bs=9 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=9) → 20 steps\n",
      "      → Key: 2.04e+00, Logprob: 2.01e-01\n",
      "    Verify bs=16 (off-diag):      Prompt: 512, Gen: 20, arb neighbors (bs=16) → 20 steps\n",
      "      → Key: 5.67e-01, Logprob: 1.94e-01\n",
      "    Verify bs=17 (diagonal):      Prompt: 512, Gen: 20, exact neighbors (bs=17) → 20 steps\n",
      "      → Key: 6.94e-01, Logprob: 1.98e-01\n",
      "\n",
      "================================================================================\n",
      "CROSS-HARDWARE BATCH SIZE DETECTABILITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_0\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (max L2 distance):\n",
      "              Verify bs=  4 Verify bs=  5 Verify bs=  8 Verify bs=  9 Verify bs= 16 Verify bs= 17 \n",
      "Claim bs=4       5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01\n",
      "Claim bs=5       5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01\n",
      "Claim bs=8       5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01\n",
      "Claim bs=9       5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01\n",
      "Claim bs=16      5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01    5.11e-01\n",
      "Claim bs=17      5.09e-01    5.09e-01    5.09e-01    5.09e-01    5.09e-01    5.09e-01\n",
      "\n",
      "Logprobs (max L2 distance):\n",
      "              Verify bs=  4 Verify bs=  5 Verify bs=  8 Verify bs=  9 Verify bs= 16 Verify bs= 17 \n",
      "Claim bs=4       1.64e-01    1.64e-01    1.64e-01    2.24e-01    3.06e-01    2.50e-01\n",
      "Claim bs=5       2.80e-01    2.80e-01    2.80e-01    2.50e-01    2.80e-01    4.68e-01\n",
      "Claim bs=8       2.17e-01    2.17e-01    2.17e-01    2.03e-01    3.06e-01    2.50e-01\n",
      "Claim bs=9       2.17e-01    2.17e-01    2.17e-01    2.03e-01    3.06e-01    2.50e-01\n",
      "Claim bs=16      2.50e-01    2.50e-01    2.50e-01    2.17e-01    3.06e-01    4.15e-01\n",
      "Claim bs=17      2.21e-01    2.21e-01    2.21e-01    2.50e-01    3.54e-01    2.50e-01\n",
      "\n",
      "================================================================================\n",
      "REF_1\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (max L2 distance):\n",
      "              Verify bs=  4 Verify bs=  5 Verify bs=  8 Verify bs=  9 Verify bs= 16 Verify bs= 17 \n",
      "Claim bs=4       3.50e-01    3.50e-01    3.50e-01    1.44e+00    3.59e-01    3.32e-01\n",
      "Claim bs=5       1.15e+00    1.15e+00    1.15e+00    1.15e+00    1.15e+00    1.16e+00\n",
      "Claim bs=8       1.15e+00    1.15e+00    1.15e+00    1.16e+00    1.15e+00    1.16e+00\n",
      "Claim bs=9       1.15e+00    1.15e+00    1.15e+00    1.16e+00    1.15e+00    1.16e+00\n",
      "Claim bs=16      1.15e+00    1.15e+00    1.15e+00    1.15e+00    1.15e+00    1.17e+00\n",
      "Claim bs=17      1.52e+00    1.52e+00    1.52e+00    5.67e-01    1.52e+00    1.54e+00\n",
      "\n",
      "Logprobs (max L2 distance):\n",
      "              Verify bs=  4 Verify bs=  5 Verify bs=  8 Verify bs=  9 Verify bs= 16 Verify bs= 17 \n",
      "Claim bs=4       2.03e-01    2.03e-01    2.03e-01    1.81e-01    2.21e-01    2.13e-01\n",
      "Claim bs=5       2.37e-01    2.37e-01    2.37e-01    1.95e-01    1.98e-01    2.15e-01\n",
      "Claim bs=8       2.06e-01    2.06e-01    2.06e-01    1.61e-01    1.86e-01    1.98e-01\n",
      "Claim bs=9       2.06e-01    2.06e-01    2.06e-01    1.61e-01    1.86e-01    1.98e-01\n",
      "Claim bs=16      2.33e-01    2.33e-01    2.33e-01    2.00e-01    2.03e-01    1.71e-01\n",
      "Claim bs=17      2.03e-01    2.03e-01    2.03e-01    1.91e-01    2.08e-01    1.53e-01\n",
      "\n",
      "================================================================================\n",
      "REF_2\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (max L2 distance):\n",
      "              Verify bs=  4 Verify bs=  5 Verify bs=  8 Verify bs=  9 Verify bs= 16 Verify bs= 17 \n",
      "Claim bs=4       1.05e+00    1.05e+00    1.05e+00    5.68e-01    5.68e-01    6.75e-01\n",
      "Claim bs=5       5.68e-01    5.68e-01    5.68e-01    1.06e+00    1.07e+00    1.06e+00\n",
      "Claim bs=8       2.05e+00    2.05e+00    2.05e+00    2.03e+00    2.03e+00    2.04e+00\n",
      "Claim bs=9       2.05e+00    2.05e+00    2.05e+00    2.03e+00    2.03e+00    2.04e+00\n",
      "Claim bs=16      2.05e+00    2.05e+00    2.05e+00    2.05e+00    5.68e-01    6.72e-01\n",
      "Claim bs=17      2.04e+00    2.04e+00    2.04e+00    2.04e+00    5.67e-01    6.94e-01\n",
      "\n",
      "Logprobs (max L2 distance):\n",
      "              Verify bs=  4 Verify bs=  5 Verify bs=  8 Verify bs=  9 Verify bs= 16 Verify bs= 17 \n",
      "Claim bs=4       2.45e-01    2.45e-01    2.45e-01    1.82e-01    1.90e-01    1.84e-01\n",
      "Claim bs=5       2.14e-01    2.14e-01    2.14e-01    1.82e-01    2.08e-01    1.59e-01\n",
      "Claim bs=8       1.98e-01    1.98e-01    1.98e-01    1.94e-01    1.98e-01    1.53e-01\n",
      "Claim bs=9       1.98e-01    1.98e-01    1.98e-01    1.94e-01    1.98e-01    1.53e-01\n",
      "Claim bs=16      2.25e-01    2.25e-01    2.25e-01    1.82e-01    1.94e-01    2.10e-01\n",
      "Claim bs=17      1.94e-01    1.94e-01    1.94e-01    2.01e-01    1.94e-01    1.98e-01\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE STATISTICS (AVERAGE ACROSS REFERENCES)\n",
      "================================================================================\n",
      "\n",
      "KEY_VECTORS:\n",
      "              Verify bs=  4 Verify bs=  5 Verify bs=  8 Verify bs=  9 Verify bs= 16 Verify bs= 17 \n",
      "Claim bs=4       6.36e-01    6.36e-01    6.36e-01    8.40e-01    4.79e-01    5.06e-01\n",
      "Claim bs=5       7.41e-01    7.41e-01    7.41e-01    9.06e-01    9.11e-01    9.11e-01\n",
      "Claim bs=8       1.24e+00    1.24e+00    1.24e+00    1.23e+00    1.23e+00    1.24e+00\n",
      "Claim bs=9       1.24e+00    1.24e+00    1.24e+00    1.23e+00    1.23e+00    1.24e+00\n",
      "Claim bs=16      1.24e+00    1.24e+00    1.24e+00    1.23e+00    7.44e-01    7.83e-01\n",
      "Claim bs=17      1.36e+00    1.36e+00    1.36e+00    1.04e+00    8.65e-01    9.13e-01\n",
      "\n",
      "  Diagonal (baseline - hardware difference only):\n",
      "    μ = 9.17e-01, σ = 2.39e-01\n",
      "    Values: ['6.36e-01', '7.41e-01', '1.24e+00', '1.23e+00', '7.44e-01', '9.13e-01']\n",
      "\n",
      "  Off-diagonal (signal - hardware + batch size difference):\n",
      "    μ = 1.05e+00, σ = 2.66e-01\n",
      "\n",
      "  SNR (signal/baseline): 1.14×\n",
      "\n",
      "LOGPROBS:\n",
      "              Verify bs=  4 Verify bs=  5 Verify bs=  8 Verify bs=  9 Verify bs= 16 Verify bs= 17 \n",
      "Claim bs=4       2.04e-01    2.04e-01    2.04e-01    1.96e-01    2.39e-01    2.15e-01\n",
      "Claim bs=5       2.43e-01    2.43e-01    2.43e-01    2.09e-01    2.28e-01    2.81e-01\n",
      "Claim bs=8       2.07e-01    2.07e-01    2.07e-01    1.86e-01    2.30e-01    2.00e-01\n",
      "Claim bs=9       2.07e-01    2.07e-01    2.07e-01    1.86e-01    2.30e-01    2.00e-01\n",
      "Claim bs=16      2.36e-01    2.36e-01    2.36e-01    2.00e-01    2.34e-01    2.65e-01\n",
      "Claim bs=17      2.06e-01    2.06e-01    2.06e-01    2.14e-01    2.52e-01    2.00e-01\n",
      "\n",
      "  Diagonal (baseline - hardware difference only):\n",
      "    μ = 2.12e-01, σ = 2.00e-02\n",
      "    Values: ['2.04e-01', '2.43e-01', '2.07e-01', '1.86e-01', '2.34e-01', '2.00e-01']\n",
      "\n",
      "  Off-diagonal (signal - hardware + batch size difference):\n",
      "    μ = 2.20e-01, σ = 2.19e-02\n",
      "\n",
      "  SNR (signal/baseline): 1.04×\n",
      "\n",
      "================================================================================\n",
      "CONCLUSION\n",
      "================================================================================\n",
      "\n",
      "Key Vectors: SNR = 1.14× ✗ NOT DETECTABLE\n",
      "Logprobs:    SNR = 1.04× ✗ NOT DETECTABLE\n",
      "\n",
      "✗ BATCH SIZE NOT RELIABLY DETECTABLE CROSS-HARDWARE\n",
      "  → Batch size signal is comparable to hardware baseline\n",
      "  → Cannot reliably distinguish bs mismatch from hardware difference\n",
      "\n",
      "✓ Verification results saved to: /workspace/experiments/verify_20251124_192602.json\n",
      "File size: 21.7 MB\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Hardware Batch Size Detectability Experiment\n",
    "\n",
    "Tests whether batch size claims can be verified across different GPU architectures\n",
    "using floating-point forensics (key vectors and logprobs).\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A (e.g., A100) with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts signals, saves to JSON\n",
    "2. Copy JSON to Machine B (e.g., H100)\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, extracts signals, compares\n",
    "\n",
    "Matrix interpretation:\n",
    "- Diagonal (claimed_bs == verify_bs): hardware-only difference (baseline)\n",
    "- Off-diagonal (claimed_bs != verify_bs): hardware + batch size difference (signal)\n",
    "- Detectability: Is off-diagonal > diagonal?\n",
    "\n",
    "Usage:\n",
    "    # Machine A: Generate reference\n",
    "    python cross_hardware_batch_size.py\n",
    "    \n",
    "    # Machine B: Verify (edit TEACHER_FORCING and REFERENCE_FILE first)\n",
    "    python cross_hardware_batch_size.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Toggle: False = generate reference, True = verify against reference\n",
    "TEACHER_FORCING = True\n",
    "REFERENCE_FILE = \"A100_reference.json\"  # Used when TEACHER_FORCING=True\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "ATTN_IMPLEMENTATION = \"flash_attention_2\"  # Options: \"eager\", \"sdpa\", \"flash_attention_2\"\n",
    "\n",
    "BATCH_SIZES = [4, 5, 8, 9, 16, 17]\n",
    "LAYER_INDICES = [1, 4, 10, 18, 28]\n",
    "MAX_NEW_TOKENS = 20\n",
    "TOKENS_PER_SLICE = 512\n",
    "NUM_REFERENCES = 3\n",
    "\n",
    "# Will be initialized from PDF in main()\n",
    "REFERENCE_SEQUENCES = None\n",
    "DUMMY_SETS = None\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    \"\"\"Setup logging to file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    \"\"\"Print to both console and log file.\"\"\"\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    \"\"\"Close log file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text content from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def create_sequences_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load all PDFs and split into equal-length slices.\n",
    "    Returns REFERENCE_SEQUENCES and DUMMY_SETS dictionaries.\n",
    "    \"\"\"\n",
    "    # Find PDFs\n",
    "    pdf_files = glob.glob(\"/workspace/*.pdf\")\n",
    "    if not pdf_files:\n",
    "        pdf_files = glob.glob(\"*.pdf\")\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "    \n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    \n",
    "    # Load and tokenize\n",
    "    all_tokens = []\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        all_tokens.extend(tokens)\n",
    "        log_print(f\"    → {len(tokens)} tokens\")\n",
    "    \n",
    "    log_print(f\"Total tokens: {len(all_tokens)}\")\n",
    "    \n",
    "    # Calculate slices needed\n",
    "    max_batch_size = max(BATCH_SIZES)\n",
    "    slices_needed = num_references * max_batch_size\n",
    "    tokens_needed = slices_needed * TOKENS_PER_SLICE\n",
    "    \n",
    "    if len(all_tokens) < tokens_needed:\n",
    "        raise ValueError(f\"Need {tokens_needed} tokens but only have {len(all_tokens)}\")\n",
    "    \n",
    "    log_print(f\"Creating {slices_needed} slices of {TOKENS_PER_SLICE} tokens each\")\n",
    "    \n",
    "    slices = []\n",
    "    for i in range(slices_needed):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        slice_tokens = all_tokens[start:end]\n",
    "        slice_text = tokenizer.decode(slice_tokens)\n",
    "        slices.append(slice_text)\n",
    "    \n",
    "    # Build reference sequences and dummy sets\n",
    "    reference_sequences = {}\n",
    "    dummy_sets = {}\n",
    "    \n",
    "    for ref_idx in range(num_references):\n",
    "        ref_name = f\"ref_{ref_idx}\"\n",
    "        base_idx = ref_idx * max_batch_size\n",
    "        reference_sequences[ref_name] = slices[base_idx]\n",
    "        dummy_sets[ref_name] = slices[base_idx + 1 : base_idx + max_batch_size]\n",
    "    \n",
    "    return reference_sequences, dummy_sets\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"attn_implementation\": ATTN_IMPLEMENTATION,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "    \n",
    "    # Flash attention (optional)\n",
    "    try:\n",
    "        import flash_attn\n",
    "        info[\"flash_attn_version\"] = flash_attn.__version__\n",
    "    except ImportError:\n",
    "        info[\"flash_attn_version\"] = \"N/A\"\n",
    "    \n",
    "    return info\n",
    "\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    If mismatch found, prints exact commands to fix and exits.\n",
    "    \n",
    "    We want ONLY hardware (GPU) to differ. Software differences would confound results.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    # Fields grouped by how to fix them\n",
    "    container_fields = ['python_version', 'cuda_version', 'cudnn_version']  # Need different container\n",
    "    pip_fields = ['torch_version', 'transformers_version', 'numpy_version', 'flash_attn_version']\n",
    "    config_fields = ['attn_implementation']  # Script config - must match\n",
    "    \n",
    "    # Fields that SHOULD differ (the point of the experiment)\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "    \n",
    "    container_mismatches = []\n",
    "    pip_mismatches = []\n",
    "    config_mismatches = []\n",
    "    \n",
    "    log_print(\"\\nScript configuration:\")\n",
    "    for field in config_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        \n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            config_mismatches.append((field, ref_val, ver_val))\n",
    "    \n",
    "    log_print(\"\\nContainer-level dependencies:\")\n",
    "    for field in container_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        \n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            container_mismatches.append((field, ref_val, ver_val))\n",
    "    \n",
    "    log_print(\"\\nPip-installable packages:\")\n",
    "    for field in pip_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        \n",
    "        # flash_attn_version only matters if using flash_attention_2\n",
    "        if field == 'flash_attn_version':\n",
    "            ref_attn = reference_env.get('attn_implementation', '')\n",
    "            ver_attn = verifier_env.get('attn_implementation', '')\n",
    "            \n",
    "            if ref_attn != 'flash_attention_2' and ver_attn != 'flash_attention_2':\n",
    "                log_print(f\"  - {field}: not using flash_attention_2 (skip)\")\n",
    "                continue\n",
    "            \n",
    "            # Using flash_attention_2 - versions MUST match\n",
    "            if ref_val == 'N/A' or ver_val == 'N/A':\n",
    "                log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "                log_print(f\"      (flash_attention_2 requires flash-attn installed on both)\")\n",
    "                pip_mismatches.append((field, ref_val, ver_val))\n",
    "                continue\n",
    "        \n",
    "        # Skip if both N/A (package not used)\n",
    "        if ref_val == 'N/A' and ver_val == 'N/A':\n",
    "            log_print(f\"  - {field}: not installed (OK)\")\n",
    "            continue\n",
    "        \n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            pip_mismatches.append((field, ref_val, ver_val))\n",
    "    \n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        \n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "    \n",
    "    if not container_mismatches and not pip_mismatches and not config_mismatches:\n",
    "        log_print(\"\\n\" + \"-\"*60)\n",
    "        log_print(\"✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        log_print(\"  All critical software versions match.\")\n",
    "        log_print(\"  Only hardware differs - results will be meaningful.\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    \n",
    "    # Mismatches found - print fix commands and exit\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"✗ ENVIRONMENT MISMATCH - FIX REQUIRED\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    cuda_ver = reference_env.get('cuda_version', '')\n",
    "    \n",
    "    # Determine torch index URL based on reference CUDA version\n",
    "    if cuda_ver.startswith('11.8'):\n",
    "        torch_index = 'https://download.pytorch.org/whl/cu118'\n",
    "    elif cuda_ver.startswith('12.1'):\n",
    "        torch_index = 'https://download.pytorch.org/whl/cu121'\n",
    "    elif cuda_ver.startswith('12.4'):\n",
    "        torch_index = 'https://download.pytorch.org/whl/cu124'\n",
    "    else:\n",
    "        torch_index = 'https://download.pytorch.org/whl/cu121'\n",
    "    \n",
    "    if config_mismatches:\n",
    "        log_print(\"\\n--- SCRIPT CONFIG (edit ATTN_IMPLEMENTATION in script) ---\\n\")\n",
    "        \n",
    "        for field, ref_val, ver_val in config_mismatches:\n",
    "            log_print(f\"  {field}: need '{ref_val}', have '{ver_val}'\")\n",
    "            log_print(f\"  Edit the script: ATTN_IMPLEMENTATION = \\\"{ref_val}\\\"\")\n",
    "    \n",
    "    if container_mismatches:\n",
    "        log_print(\"\\n--- CONTAINER-LEVEL (use a different container/image) ---\\n\")\n",
    "        \n",
    "        for field, ref_val, ver_val in container_mismatches:\n",
    "            if field == 'python_version':\n",
    "                log_print(f\"  Python: need {ref_val}, have {ver_val}\")\n",
    "            elif field == 'cuda_version':\n",
    "                log_print(f\"  CUDA: need {ref_val}, have {ver_val}\")\n",
    "            elif field == 'cudnn_version':\n",
    "                log_print(f\"  cuDNN: need {ref_val}, have {ver_val}\")\n",
    "        \n",
    "        log_print(\"\\n  Suggested Docker images (PyTorch NGC containers):\")\n",
    "        log_print(f\"    nvcr.io/nvidia/pytorch:XX.XX-py3\")\n",
    "        log_print(f\"    Or find matching: https://docs.nvidia.com/deeplearning/frameworks/pytorch-release-notes/\")\n",
    "    \n",
    "    if pip_mismatches:\n",
    "        log_print(\"\\n--- PIP-INSTALLABLE (run these commands) ---\\n\")\n",
    "        \n",
    "        for field, ref_val, ver_val in pip_mismatches:\n",
    "            if field == 'torch_version':\n",
    "                log_print(f\"  pip install torch=={ref_val} --index-url {torch_index}\")\n",
    "            elif field == 'transformers_version':\n",
    "                log_print(f\"  pip install transformers=={ref_val}\")\n",
    "            elif field == 'numpy_version':\n",
    "                log_print(f\"  pip install numpy=={ref_val}\")\n",
    "            elif field == 'flash_attn_version':\n",
    "                if ref_val == 'N/A':\n",
    "                    log_print(f\"  pip uninstall flash-attn  # reference didn't use it\")\n",
    "                else:\n",
    "                    log_print(f\"  pip install flash-attn=={ref_val}  # may need to build from source\")\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"Fix the above and re-run this script.\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# SIGNAL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_signals_from_output(outputs, layer_indices, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs from element 0 at specified position.\n",
    "    Returns top-10 token IDs for logprob comparison.\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "    \n",
    "    # Key vectors from element 0\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "    \n",
    "    # Logprobs from element 0 - get top 10\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    top10 = torch.topk(log_probs, k=10)\n",
    "    \n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': top10.indices.cpu().tolist(),\n",
    "        'log_probs': top10.values.cpu().tolist()\n",
    "    }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "\n",
    "def extract_signals_for_token_ids(outputs, layer_indices, token_ids, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs for SPECIFIC token IDs.\n",
    "    Used in verification to compare same tokens as reference.\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "    \n",
    "    # Key vectors from element 0\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "    \n",
    "    # Logprobs for SPECIFIC token IDs\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_ids_tensor = torch.tensor(token_ids, device=logits.device)\n",
    "    selected_logprobs = log_probs[token_ids_tensor]\n",
    "    \n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': token_ids,\n",
    "        'log_probs': selected_logprobs.cpu().tolist()\n",
    "    }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# ============================================================================\n",
    "# DECODE GENERATION (TEACHER_FORCING = False)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_min_length_across_batches(ref_text, ref_name, tokenizer, batch_sizes):\n",
    "    \"\"\"Pre-compute minimum sequence length across all batch configurations.\"\"\"\n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    min_length = float('inf')\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        if batch_size == 1:\n",
    "            batch_texts = [ref_text]\n",
    "        else:\n",
    "            batch_texts = [ref_text] + ref_dummies[:batch_size-1]\n",
    "        \n",
    "        token_lengths = [len(tokenizer.encode(t, add_special_tokens=True)) for t in batch_texts]\n",
    "        min_length = min(min_length, min(token_lengths))\n",
    "    \n",
    "    return min_length\n",
    "\n",
    "\n",
    "def run_decode_with_extraction(model, tokenizer, ref_text, ref_name, batch_size, \n",
    "                                layer_indices, forced_length=None):\n",
    "    \"\"\"\n",
    "    Run decode generation and extract signals from last 3 generation steps.\n",
    "    Works purely at token ID level.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Build batch texts\n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    if batch_size == 1:\n",
    "        batch_texts = [ref_text]\n",
    "    else:\n",
    "        batch_texts = [ref_text] + ref_dummies[:batch_size-1]\n",
    "    \n",
    "    # Tokenize ONCE\n",
    "    all_token_ids = [tokenizer.encode(t, add_special_tokens=True) for t in batch_texts]\n",
    "    \n",
    "    # Truncate at token ID level\n",
    "    if forced_length is not None:\n",
    "        min_length = forced_length\n",
    "    else:\n",
    "        min_length = min(len(ids) for ids in all_token_ids)\n",
    "    \n",
    "    truncated_token_ids = [ids[:min_length] for ids in all_token_ids]\n",
    "    \n",
    "    # Build input tensors DIRECTLY from token IDs\n",
    "    input_ids = torch.tensor(truncated_token_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    log_print(f\"      Prompt: {prompt_length} tokens\", end=\"\")\n",
    "    \n",
    "    # Track generation for ALL batch positions\n",
    "    all_batch_generated_ids = [[] for _ in range(batch_size)]\n",
    "    generation_signals = []\n",
    "    \n",
    "    # FIRST STEP: Prefill with full prompt\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "    \n",
    "    past_kv = outputs.past_key_values\n",
    "    \n",
    "    # Get first token\n",
    "    next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "    for batch_idx in range(batch_size):\n",
    "        all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "    \n",
    "    # Extract signals from element 0\n",
    "    signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    \n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    # Update attention mask\n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((inputs['attention_mask'].shape[0], 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS: True autoregressive with cache\n",
    "    for step in range(1, MAX_NEW_TOKENS):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        # Get next tokens\n",
    "        next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "        for batch_idx in range(batch_size):\n",
    "            all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "        \n",
    "        # Extract signals from element 0\n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        # Update attention mask\n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((attention_mask.shape[0], 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Check for EOS in position 0\n",
    "        if all_batch_generated_ids[0][-1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Extract last 3\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "    \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    final_length = prompt_length + num_generated\n",
    "    log_print(f\" → Final: {final_length} tokens ({num_generated} generated)\")\n",
    "    \n",
    "    return {\n",
    "        'generated_ids': all_batch_generated_ids[0],\n",
    "        'all_batch_generated_ids': all_batch_generated_ids,\n",
    "        'prompt_token_ids': truncated_token_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TEACHER-FORCED DECODE (TEACHER_FORCING = True)\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_decode(model, tokenizer, ref_name, reference_data, \n",
    "                               verify_batch_size, layer_indices, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced decode: feed reference tokens, extract signals.\n",
    "    \n",
    "    Args:\n",
    "        reference_data: Dict with prompt_token_ids, all_batch_generated_ids, signals\n",
    "        verify_batch_size: Batch size for verification\n",
    "        is_diagonal: If True, use exact reference neighbors. If False, use arbitrary neighbors.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ref_prompt_ids = reference_data['prompt_token_ids'][0]\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    ref_batch_size = len(reference_data['prompt_token_ids'])\n",
    "    \n",
    "    log_print(f\"      Prompt: {len(ref_prompt_ids)}, Gen: {len(ref_generated_ids)}\", end=\"\")\n",
    "    \n",
    "    # Build batch for verification\n",
    "    if is_diagonal:\n",
    "        # Use EXACT reference sequences (same batch size, same tokens)\n",
    "        log_print(f\", exact neighbors (bs={ref_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = reference_data['prompt_token_ids']\n",
    "        batch_generated_ids = reference_data['all_batch_generated_ids']\n",
    "        actual_batch_size = ref_batch_size\n",
    "    else:\n",
    "        # Off-diagonal: different batch size, arbitrary neighbors for positions 1+\n",
    "        log_print(f\", arb neighbors (bs={verify_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = [ref_prompt_ids]\n",
    "        batch_generated_ids = [ref_generated_ids]\n",
    "        \n",
    "        # Add arbitrary neighbors\n",
    "        ref_dummies = DUMMY_SETS[ref_name]\n",
    "        for i in range(verify_batch_size - 1):\n",
    "            dummy_ids = tokenizer.encode(ref_dummies[i], add_special_tokens=True)\n",
    "            # Truncate to match reference prompt length\n",
    "            dummy_ids = dummy_ids[:len(ref_prompt_ids)]\n",
    "            # Pad if too short\n",
    "            if len(dummy_ids) < len(ref_prompt_ids):\n",
    "                dummy_ids = dummy_ids + [tokenizer.pad_token_id or 0] * (len(ref_prompt_ids) - len(dummy_ids))\n",
    "            batch_prompt_ids.append(dummy_ids)\n",
    "            # Neighbors generate freely - initialize with empty\n",
    "            batch_generated_ids.append([])\n",
    "        \n",
    "        actual_batch_size = verify_batch_size\n",
    "    \n",
    "    # Build input tensors from prompt IDs\n",
    "    input_ids = torch.tensor(batch_prompt_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask\n",
    "    }\n",
    "    \n",
    "    generation_signals = []\n",
    "    num_steps = len(ref_generated_ids)\n",
    "    \n",
    "    # FIRST STEP: Prefill with full prompt\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "    \n",
    "    past_kv = outputs.past_key_values\n",
    "    \n",
    "    # Extract signals from element 0\n",
    "    ref_step_data = list(reference_data['signals'].values())[0] if reference_data['signals'] else None\n",
    "    if ref_step_data:\n",
    "        ref_token_ids = ref_step_data['signals']['logprobs']['token_ids']\n",
    "        signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "    else:\n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "    \n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    # Prepare next tokens: teacher-force position 0, argmax for others (if off-diagonal)\n",
    "    if is_diagonal:\n",
    "        # All positions teacher-forced\n",
    "        next_tokens = torch.tensor(\n",
    "            [batch_generated_ids[i][0] for i in range(actual_batch_size)],\n",
    "            dtype=torch.long, device='cuda'\n",
    "        )\n",
    "    else:\n",
    "        # Position 0 teacher-forced, others argmax\n",
    "        next_tokens_list = [ref_generated_ids[0]]  # Position 0: teacher-forced\n",
    "        argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)  # Positions 1+: argmax\n",
    "        for i in range(actual_batch_size - 1):\n",
    "            next_tokens_list.append(argmax_tokens[i].item())\n",
    "            batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "        next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "    \n",
    "    # Update attention mask\n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((actual_batch_size, 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS\n",
    "    for step in range(1, num_steps):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        # Extract signals using reference's token IDs for logprob comparison\n",
    "        # Find which reference signal corresponds to this step\n",
    "        ref_signals_list = list(reference_data['signals'].values())\n",
    "        if step < len(ref_signals_list):\n",
    "            ref_token_ids = ref_signals_list[step]['signals']['logprobs']['token_ids']\n",
    "            signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "        else:\n",
    "            signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        \n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        # Prepare next tokens\n",
    "        if step < num_steps - 1:\n",
    "            if is_diagonal:\n",
    "                next_tokens = torch.tensor(\n",
    "                    [batch_generated_ids[i][step] for i in range(actual_batch_size)],\n",
    "                    dtype=torch.long, device='cuda'\n",
    "                )\n",
    "            else:\n",
    "                next_tokens_list = [ref_generated_ids[step]]\n",
    "                argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)\n",
    "                for i in range(actual_batch_size - 1):\n",
    "                    next_tokens_list.append(argmax_tokens[i].item())\n",
    "                    batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "                next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "        \n",
    "        # Update attention mask\n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((actual_batch_size, 1), device='cuda')\n",
    "        ], dim=1)\n",
    "    \n",
    "    # Extract last 3\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "    \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(f\" → {num_generated} steps\")\n",
    "    \n",
    "    return {\n",
    "        'signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(decode_measurements, tokenizer):\n",
    "    \"\"\"\n",
    "    Sanity check: Verify element 0 generates identical tokens across all batch sizes.\n",
    "    If tokens differ, batch composition affects generation and comparison is confounded.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    tokens_by_bs = {}\n",
    "    for bs, data in decode_measurements.items():\n",
    "        tokens_by_bs[bs] = data['generated_ids']\n",
    "    \n",
    "    bs_list = sorted(tokens_by_bs.keys())\n",
    "    reference_tokens = tokens_by_bs[bs_list[0]]\n",
    "    \n",
    "    all_same = True\n",
    "    log_print(\"\\nGenerated tokens by batch size:\")\n",
    "    for bs in bs_list:\n",
    "        tokens = tokens_by_bs[bs]\n",
    "        match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "        decoded_text = tokenizer.decode(tokens)\n",
    "        log_print(f\"  bs={bs}:\")\n",
    "        log_print(f\"    IDs:  {tokens}\")\n",
    "        log_print(f\"    Text: {repr(decoded_text)}\")\n",
    "        log_print(f\"    {match_str}\")\n",
    "        if tokens != reference_tokens:\n",
    "            all_same = False\n",
    "    \n",
    "    if all_same:\n",
    "        log_print(\"\\n✓ Element 0 generates IDENTICAL tokens across all batch sizes\")\n",
    "        log_print(\"  → Can meaningfully compare activations for same token sequence\")\n",
    "    else:\n",
    "        log_print(\"\\n⚠ Element 0 generates DIFFERENT tokens across batch sizes\")\n",
    "        log_print(\"  → Batch composition affects generation - comparison may be confounded\")\n",
    "    \n",
    "    return all_same\n",
    "\n",
    "\n",
    "def compute_l2_distance(vec1, vec2):\n",
    "    \"\"\"Compute L2 distance between two vectors.\"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return float(np.linalg.norm(v1 - v2))\n",
    "\n",
    "\n",
    "def compute_logprob_distance(logprobs1, logprobs2):\n",
    "    \"\"\"Compute L2 distance between logprob distributions.\"\"\"\n",
    "    probs1 = np.array(logprobs1['log_probs'])\n",
    "    probs2 = np.array(logprobs2['log_probs'])\n",
    "    return float(np.linalg.norm(probs1 - probs2))\n",
    "\n",
    "\n",
    "def compare_signals(signals1, signals2, layer_indices):\n",
    "    \"\"\"Compare two signal sets, return distances.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "    \n",
    "    all_key_dists = []\n",
    "    all_logprob_dists = []\n",
    "    \n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]['signals'] if 'signals' in signals1[pos_label] else signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]['signals'] if 'signals' in signals2[pos_label] else signals2[pos_label]\n",
    "        \n",
    "        # Key vectors\n",
    "        for layer_name in sig1['key_vectors'].keys():\n",
    "            dist = compute_l2_distance(\n",
    "                sig1['key_vectors'][layer_name],\n",
    "                sig2['key_vectors'][layer_name]\n",
    "            )\n",
    "            all_key_dists.append(dist)\n",
    "        \n",
    "        # Logprobs\n",
    "        dist = compute_logprob_distance(sig1['logprobs'], sig2['logprobs'])\n",
    "        all_logprob_dists.append(dist)\n",
    "    \n",
    "    return {\n",
    "        'key_vectors_max': max(all_key_dists) if all_key_dists else 0.0,\n",
    "        'key_vectors_mean': np.mean(all_key_dists) if all_key_dists else 0.0,\n",
    "        'logprobs_max': max(all_logprob_dists) if all_logprob_dists else 0.0,\n",
    "        'logprobs_mean': np.mean(all_logprob_dists) if all_logprob_dists else 0.0\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_within_hardware_sanity_check(measurements, batch_sizes, layer_indices):\n",
    "    \"\"\"\n",
    "    Sanity check: Does batch size affect activations on the SAME hardware?\n",
    "    \n",
    "    This validates the premise before cross-hardware comparison.\n",
    "    Expected: YES - different batch sizes should produce different activations.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SANITY CHECK: WITHIN-HARDWARE BATCH SIZE EFFECTS\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(\"\\nQuestion: Does batch size affect activations on the same GPU?\")\n",
    "    log_print(\"Expected: YES - batch size changes computation even for same sequence\\n\")\n",
    "    \n",
    "    # Group by reference\n",
    "    by_ref = {}\n",
    "    for m in measurements:\n",
    "        ref = m['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        by_ref[ref][m['batch_size']] = m['signals']\n",
    "    \n",
    "    all_key_dists = []\n",
    "    all_logprob_dists = []\n",
    "    comparison_details = []\n",
    "    \n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{ref_name.upper()}\")\n",
    "        log_print(\"-\" * 60)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        available_bs = sorted(ref_data.keys())\n",
    "        \n",
    "        for i, bs1 in enumerate(available_bs):\n",
    "            for bs2 in available_bs[i+1:]:\n",
    "                # Compare signals between batch sizes\n",
    "                signals1 = ref_data[bs1]\n",
    "                signals2 = ref_data[bs2]\n",
    "                \n",
    "                distances = compare_signals(signals1, signals2, layer_indices)\n",
    "                \n",
    "                log_print(f\"  bs={bs1} vs bs={bs2}:\")\n",
    "                log_print(f\"    Key vectors: Δ_max = {distances['key_vectors_max']:.2e}\")\n",
    "                log_print(f\"    Logprobs:    Δ_max = {distances['logprobs_max']:.2e}\")\n",
    "                \n",
    "                all_key_dists.append(distances['key_vectors_max'])\n",
    "                all_logprob_dists.append(distances['logprobs_max'])\n",
    "                \n",
    "                comparison_details.append({\n",
    "                    'ref': ref_name,\n",
    "                    'bs1': bs1,\n",
    "                    'bs2': bs2,\n",
    "                    'key_distance': distances['key_vectors_max'],\n",
    "                    'logprob_distance': distances['logprobs_max']\n",
    "                })\n",
    "    \n",
    "    # Summary statistics\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SANITY CHECK SUMMARY\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    key_mean = np.mean(all_key_dists)\n",
    "    key_max = max(all_key_dists)\n",
    "    logprob_mean = np.mean(all_logprob_dists)\n",
    "    logprob_max = max(all_logprob_dists)\n",
    "    \n",
    "    log_print(f\"\\nKey vectors:\")\n",
    "    log_print(f\"  μ = {key_mean:.2e}, max = {key_max:.2e}\")\n",
    "    log_print(f\"\\nLogprobs:\")\n",
    "    log_print(f\"  μ = {logprob_mean:.2e}, max = {logprob_max:.2e}\")\n",
    "    \n",
    "    # Check for zeros (would invalidate experiment)\n",
    "    key_zeros = sum(1 for d in all_key_dists if d == 0.0)\n",
    "    logprob_zeros = sum(1 for d in all_logprob_dists if d == 0.0)\n",
    "    \n",
    "    if key_zeros > 0:\n",
    "        log_print(f\"\\n⚠ WARNING: {key_zeros}/{len(all_key_dists)} key comparisons are EXACTLY ZERO\")\n",
    "    if logprob_zeros > 0:\n",
    "        log_print(f\"⚠ WARNING: {logprob_zeros}/{len(all_logprob_dists)} logprob comparisons are EXACTLY ZERO\")\n",
    "    \n",
    "    # Track which batch pairs produce zeros, grouped by reference\n",
    "    if key_zeros > 0 or logprob_zeros > 0:\n",
    "        zeros_by_ref = {}\n",
    "        for r in comparison_details:\n",
    "            ref = r['ref']\n",
    "            pair = (r['bs1'], r['bs2'])\n",
    "            if ref not in zeros_by_ref:\n",
    "                zeros_by_ref[ref] = {'key': [], 'logprob': []}\n",
    "            if r['key_distance'] == 0.0:\n",
    "                zeros_by_ref[ref]['key'].append(pair)\n",
    "            if r['logprob_distance'] == 0.0:\n",
    "                zeros_by_ref[ref]['logprob'].append(pair)\n",
    "        \n",
    "        log_print(f\"\\n  Zero locations by reference:\")\n",
    "        for ref in sorted(zeros_by_ref.keys()):\n",
    "            key_pairs = zeros_by_ref[ref]['key']\n",
    "            log_pairs = zeros_by_ref[ref]['logprob']\n",
    "            if key_pairs or log_pairs:\n",
    "                log_print(f\"    {ref}:\")\n",
    "                if key_pairs:\n",
    "                    log_print(f\"      key zeros:     {sorted(key_pairs)}\")\n",
    "                if log_pairs:\n",
    "                    log_print(f\"      logprob zeros: {sorted(log_pairs)}\")\n",
    "        \n",
    "        # Check consistency across references\n",
    "        all_refs = sorted(zeros_by_ref.keys())\n",
    "        if len(all_refs) >= 2:\n",
    "            key_sets = [set(zeros_by_ref[ref]['key']) for ref in all_refs]\n",
    "            log_sets = [set(zeros_by_ref[ref]['logprob']) for ref in all_refs]\n",
    "            \n",
    "            key_intersection = set.intersection(*key_sets) if all(key_sets) else set()\n",
    "            log_intersection = set.intersection(*log_sets) if all(log_sets) else set()\n",
    "            \n",
    "            log_print(f\"\\n  Cross-reference consistency:\")\n",
    "            if key_intersection:\n",
    "                log_print(f\"    Key zeros consistent across ALL refs: {sorted(key_intersection)}\")\n",
    "                log_print(f\"    → SYSTEMATIC: These batch pairs produce identical activations!\")\n",
    "            else:\n",
    "                log_print(f\"    Key zeros: NO pairs are zero across all refs (coincidental)\")\n",
    "            if log_intersection:\n",
    "                log_print(f\"    Logprob zeros consistent across ALL refs: {sorted(log_intersection)}\")\n",
    "                log_print(f\"    → SYSTEMATIC: These batch pairs produce identical logprobs!\")\n",
    "            else:\n",
    "                log_print(f\"    Logprob zeros: NO pairs are zero across all refs (coincidental)\")\n",
    "    \n",
    "    # Conclusion\n",
    "    threshold = 1e-10\n",
    "    \n",
    "    log_print(\"\\n\" + \"-\"*60)\n",
    "    if key_mean > threshold and logprob_mean > threshold:\n",
    "        log_print(\"✓ SANITY CHECK PASSED\")\n",
    "        log_print(\"  Batch size DOES affect computation on same hardware\")\n",
    "        log_print(\"  → Proceeding with cross-hardware comparison is meaningful\")\n",
    "    elif key_mean > threshold or logprob_mean > threshold:\n",
    "        log_print(\"~ PARTIAL PASS\")\n",
    "        signal = \"Key vectors\" if key_mean > threshold else \"Logprobs\"\n",
    "        log_print(f\"  Only {signal} show batch size effects\")\n",
    "    else:\n",
    "        log_print(\"✗ SANITY CHECK FAILED\")\n",
    "        log_print(\"  Batch size does NOT affect computation!\")\n",
    "        log_print(\"  → Cross-hardware experiment would be meaningless\")\n",
    "    \n",
    "    return {\n",
    "        'key_vectors_mean': float(key_mean),\n",
    "        'key_vectors_max': float(key_max),\n",
    "        'logprobs_mean': float(logprob_mean),\n",
    "        'logprobs_max': float(logprob_max),\n",
    "        'comparisons': comparison_details\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_cross_hardware_matrix(comparison_results, batch_sizes):\n",
    "    \"\"\"Analyze the comparison matrix and determine detectability.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"CROSS-HARDWARE BATCH SIZE DETECTABILITY ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    # Group by reference\n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_batch_size'], result['verify_batch_size'])\n",
    "        by_ref[ref][key] = result\n",
    "    \n",
    "    all_matrices = {'key_vectors': [], 'logprobs': []}\n",
    "    \n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        n_bs = len(batch_sizes)\n",
    "        matrix_key = np.zeros((n_bs, n_bs))\n",
    "        matrix_logprob = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            for j, verify_bs in enumerate(batch_sizes):\n",
    "                key = (claimed_bs, verify_bs)\n",
    "                if key in ref_data:\n",
    "                    matrix_key[i, j] = ref_data[key]['distances']['key_vectors_max']\n",
    "                    matrix_logprob[i, j] = ref_data[key]['distances']['logprobs_max']\n",
    "        \n",
    "        # Display matrices\n",
    "        header = \"              \" + \"\".join([f\"Verify bs={bs:>3} \" for bs in batch_sizes])\n",
    "        \n",
    "        log_print(\"\\nKey Vectors (max L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"Claim bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:10.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        log_print(\"\\nLogprobs (max L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"Claim bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:10.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        all_matrices['key_vectors'].append(matrix_key)\n",
    "        all_matrices['logprobs'].append(matrix_logprob)\n",
    "    \n",
    "    # Aggregate statistics\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE STATISTICS (AVERAGE ACROSS REFERENCES)\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    results = {}\n",
    "    n_bs = len(batch_sizes)\n",
    "    \n",
    "    for signal_type in ['key_vectors', 'logprobs']:\n",
    "        matrices = all_matrices[signal_type]\n",
    "        avg_matrix = np.mean(matrices, axis=0)\n",
    "        \n",
    "        header = \"              \" + \"\".join([f\"Verify bs={bs:>3} \" for bs in batch_sizes])\n",
    "        \n",
    "        log_print(f\"\\n{signal_type.upper()}:\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"Claim bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {avg_matrix[i,j]:10.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        # Extract diagonal (baseline: hardware-only) and off-diagonal (signal: hardware + bs)\n",
    "        diagonal = np.array([avg_matrix[i, i] for i in range(n_bs)])\n",
    "        off_diagonal = np.array([avg_matrix[i, j] for i in range(n_bs) for j in range(n_bs) if i != j])\n",
    "        \n",
    "        baseline_mean = np.mean(diagonal)\n",
    "        baseline_std = np.std(diagonal)\n",
    "        signal_mean = np.mean(off_diagonal)\n",
    "        signal_std = np.std(off_diagonal)\n",
    "        snr = signal_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "        \n",
    "        log_print(f\"\\n  Diagonal (baseline - hardware difference only):\")\n",
    "        log_print(f\"    μ = {baseline_mean:.2e}, σ = {baseline_std:.2e}\")\n",
    "        log_print(f\"    Values: {[f'{d:.2e}' for d in diagonal]}\")\n",
    "        \n",
    "        log_print(f\"\\n  Off-diagonal (signal - hardware + batch size difference):\")\n",
    "        log_print(f\"    μ = {signal_mean:.2e}, σ = {signal_std:.2e}\")\n",
    "        \n",
    "        log_print(f\"\\n  SNR (signal/baseline): {snr:.2f}×\")\n",
    "        \n",
    "        results[signal_type] = {\n",
    "            'matrix': avg_matrix.tolist(),\n",
    "            'baseline_mean': float(baseline_mean),\n",
    "            'baseline_std': float(baseline_std),\n",
    "            'signal_mean': float(signal_mean),\n",
    "            'signal_std': float(signal_std),\n",
    "            'snr': float(snr)\n",
    "        }\n",
    "    \n",
    "    # Conclusion\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"CONCLUSION\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    threshold = 1.5\n",
    "    \n",
    "    key_snr = results['key_vectors']['snr']\n",
    "    log_snr = results['logprobs']['snr']\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors: SNR = {key_snr:.2f}× {'✓ DETECTABLE' if key_snr >= threshold else '✗ NOT DETECTABLE'}\")\n",
    "    log_print(f\"Logprobs:    SNR = {log_snr:.2f}× {'✓ DETECTABLE' if log_snr >= threshold else '✗ NOT DETECTABLE'}\")\n",
    "    \n",
    "    if key_snr >= threshold and log_snr >= threshold:\n",
    "        log_print(\"\\n✓ BATCH SIZE MISMATCHES ARE DETECTABLE CROSS-HARDWARE\")\n",
    "        log_print(\"  → Verification cluster can detect batch size evasion\")\n",
    "        log_print(\"  → Off-diagonal (bs mismatch) >> diagonal (hardware-only)\")\n",
    "    elif key_snr >= threshold or log_snr >= threshold:\n",
    "        log_print(\"\\n~ PARTIAL DETECTABILITY\")\n",
    "        det = 'Key vectors' if key_snr >= threshold else 'Logprobs'\n",
    "        log_print(f\"  → {det} can detect batch size mismatches\")\n",
    "    else:\n",
    "        log_print(\"\\n✗ BATCH SIZE NOT RELIABLY DETECTABLE CROSS-HARDWARE\")\n",
    "        log_print(\"  → Batch size signal is comparable to hardware baseline\")\n",
    "        log_print(\"  → Cannot reliably distinguish bs mismatch from hardware difference\")\n",
    "    \n",
    "    return {\n",
    "        'matrices': {k: [m.tolist() for m in v] for k, v in all_matrices.items()},\n",
    "        'statistics': results\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    global REFERENCE_SEQUENCES, DUMMY_SETS\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    \n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION (reference)\"\n",
    "    \n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"Log file: {log_path}\")\n",
    "    log_print(f\"\\nEnvironment:\")\n",
    "    for k, v in system_info.items():\n",
    "        log_print(f\"  {k}: {v}\")\n",
    "    \n",
    "    log_print(f\"\\nConfiguration:\")\n",
    "    log_print(f\"  Model: {MODEL_NAME}\")\n",
    "    log_print(f\"  Layers: {LAYER_INDICES}\")\n",
    "    log_print(f\"  Batch sizes: {BATCH_SIZES}\")\n",
    "    log_print(f\"  Max tokens: {MAX_NEW_TOKENS}\")\n",
    "    if TEACHER_FORCING:\n",
    "        log_print(f\"  Reference file: {REFERENCE_FILE}\")\n",
    "    log_print()\n",
    "    \n",
    "    # Load model\n",
    "    log_print(\"Loading model...\")\n",
    "    \n",
    "    # Verify flash_attn is installed if needed\n",
    "    if ATTN_IMPLEMENTATION == \"flash_attention_2\":\n",
    "        try:\n",
    "            import flash_attn\n",
    "            log_print(f\"  flash_attn {flash_attn.__version__} available\")\n",
    "        except ImportError:\n",
    "            log_print(\"\\n✗ ATTN_IMPLEMENTATION='flash_attention_2' but flash_attn not installed\")\n",
    "            log_print(\"  Either install: pip install flash-attn\")\n",
    "            log_print(\"  Or change: ATTN_IMPLEMENTATION = 'eager' or 'sdpa'\")\n",
    "            sys.exit(1)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    \n",
    "    # Initialize sequences from PDF (needed for dummy neighbors in off-diagonal)\n",
    "    REFERENCE_SEQUENCES, DUMMY_SETS = create_sequences_from_pdf(tokenizer)\n",
    "    log_print(f\"Created {len(REFERENCE_SEQUENCES)} reference sequences\\n\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=ATTN_IMPLEMENTATION\n",
    "    )\n",
    "    log_print(f\"✓ Model loaded (attn_implementation={ATTN_IMPLEMENTATION})\\n\")\n",
    "    \n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        # ================================================================\n",
    "        # VERIFICATION MODE\n",
    "        # ================================================================\n",
    "        log_print(\"Loading reference file...\")\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            reference = json.load(f)\n",
    "        \n",
    "        ref_env = reference['metadata']['environment']\n",
    "        ref_gpu = ref_env['gpu_name']\n",
    "        log_print(f\"Reference GPU: {ref_gpu}\")\n",
    "        log_print(f\"Verifier GPU:  {system_info['gpu_name']}\")\n",
    "        \n",
    "        # Validate environments match (except hardware)\n",
    "        env_validation = validate_environment_match(ref_env, system_info)\n",
    "        \n",
    "        # Validate model matches\n",
    "        ref_model = reference['metadata']['model']\n",
    "        if ref_model != MODEL_NAME:\n",
    "            log_print(f\"\\n✗ MODEL MISMATCH: reference={ref_model}, verifier={MODEL_NAME}\")\n",
    "            log_print(\"  Aborting - models must match for meaningful comparison.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # Validate batch sizes match\n",
    "        ref_batch_sizes = reference['metadata']['batch_sizes']\n",
    "        if ref_batch_sizes != BATCH_SIZES:\n",
    "            log_print(f\"\\n✗ BATCH SIZE MISMATCH:\")\n",
    "            log_print(f\"  Reference: {ref_batch_sizes}\")\n",
    "            log_print(f\"  Verifier:  {BATCH_SIZES}\")\n",
    "            log_print(\"  Aborting - batch sizes must match for comparison matrix.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        # Validate layer indices match\n",
    "        ref_layers = reference['metadata']['layer_indices']\n",
    "        if ref_layers != LAYER_INDICES:\n",
    "            log_print(f\"\\n✗ LAYER INDICES MISMATCH:\")\n",
    "            log_print(f\"  Reference: {ref_layers}\")\n",
    "            log_print(f\"  Verifier:  {LAYER_INDICES}\")\n",
    "            log_print(\"  Aborting - layer indices must match for signal comparison.\")\n",
    "            sys.exit(1)\n",
    "        \n",
    "        log_print(\"\\n✓ Model, batch sizes, and layer indices match\\n\")\n",
    "        \n",
    "        comparison_results = []\n",
    "        \n",
    "        # Build lookup for reference measurements\n",
    "        ref_by_key = {}\n",
    "        for m in reference['measurements']:\n",
    "            key = (m['ref_name'], m['batch_size'])\n",
    "            ref_by_key[key] = m\n",
    "        \n",
    "        for ref_name in sorted(REFERENCE_SEQUENCES.keys()):\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            for claimed_bs in BATCH_SIZES:\n",
    "                ref_key = (ref_name, claimed_bs)\n",
    "                if ref_key not in ref_by_key:\n",
    "                    log_print(f\"  ⚠ No reference data for {ref_name} bs={claimed_bs}\")\n",
    "                    continue\n",
    "                \n",
    "                ref_data = ref_by_key[ref_key]\n",
    "                log_print(f\"\\n  Claimed batch size: {claimed_bs}\")\n",
    "                \n",
    "                for verify_bs in BATCH_SIZES:\n",
    "                    is_diagonal = (claimed_bs == verify_bs)\n",
    "                    \n",
    "                    log_print(f\"    Verify bs={verify_bs} ({'diagonal' if is_diagonal else 'off-diag'}):\", end=\"\")\n",
    "                    \n",
    "                    verify_result = run_teacher_forced_decode(\n",
    "                        model, tokenizer, ref_name, ref_data,\n",
    "                        verify_bs, LAYER_INDICES, is_diagonal\n",
    "                    )\n",
    "                    \n",
    "                    # Compare signals\n",
    "                    distances = compare_signals(\n",
    "                        ref_data['signals'],\n",
    "                        verify_result['signals'],\n",
    "                        LAYER_INDICES\n",
    "                    )\n",
    "                    \n",
    "                    log_print(f\"      → Key: {distances['key_vectors_max']:.2e}, Logprob: {distances['logprobs_max']:.2e}\")\n",
    "                    \n",
    "                    comparison_results.append({\n",
    "                        'ref_name': ref_name,\n",
    "                        'claimed_batch_size': claimed_bs,\n",
    "                        'verify_batch_size': verify_bs,\n",
    "                        'is_diagonal': is_diagonal,\n",
    "                        'distances': distances,\n",
    "                        'verify_signals': verify_result['signals']\n",
    "                    })\n",
    "        \n",
    "        # Analyze\n",
    "        analysis = analyze_cross_hardware_matrix(comparison_results, BATCH_SIZES)\n",
    "        \n",
    "        # Save results\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_gpu': ref_gpu,\n",
    "                'verifier_gpu': system_info['gpu_name'],\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'environment_validation': env_validation,\n",
    "                'model': MODEL_NAME,\n",
    "                'layer_indices': LAYER_INDICES,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'analysis': analysis\n",
    "        }\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        log_print(f\"\\n✓ Verification results saved to: {filepath}\")\n",
    "        \n",
    "    else:\n",
    "        # ================================================================\n",
    "        # GENERATION MODE\n",
    "        # ================================================================\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'model': MODEL_NAME,\n",
    "                'layer_indices': LAYER_INDICES,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'max_new_tokens': MAX_NEW_TOKENS,\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'measurements': []\n",
    "        }\n",
    "        \n",
    "        for ref_name, ref_text in REFERENCE_SEQUENCES.items():\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            # Pre-compute minimum length\n",
    "            min_prompt_length = compute_min_length_across_batches(\n",
    "                ref_text, ref_name, tokenizer, BATCH_SIZES\n",
    "            )\n",
    "            log_print(f\"\\nGlobal minimum prompt length: {min_prompt_length} tokens\\n\")\n",
    "            \n",
    "            for batch_size in BATCH_SIZES:\n",
    "                log_print(f\"  bs={batch_size}:\", end=\" \")\n",
    "                \n",
    "                decode_data = run_decode_with_extraction(\n",
    "                    model, tokenizer, ref_text, ref_name, batch_size, LAYER_INDICES,\n",
    "                    forced_length=min_prompt_length\n",
    "                )\n",
    "                \n",
    "                results['measurements'].append({\n",
    "                    'ref_name': ref_name,\n",
    "                    'batch_size': batch_size,\n",
    "                    'generated_ids': decode_data['generated_ids'],\n",
    "                    'all_batch_generated_ids': decode_data['all_batch_generated_ids'],\n",
    "                    'prompt_token_ids': decode_data['prompt_token_ids'],\n",
    "                    'prompt_length': decode_data['prompt_length'],\n",
    "                    'signals': decode_data['signals'],\n",
    "                    'num_generated': decode_data['num_generated']\n",
    "                })\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"decode_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        log_print(f\"\\n✓ Generation results saved to: {filepath}\")\n",
    "        \n",
    "        # Run sanity check\n",
    "        sanity_check = analyze_within_hardware_sanity_check(\n",
    "            results['measurements'], BATCH_SIZES, LAYER_INDICES\n",
    "        )\n",
    "        results['sanity_check'] = sanity_check\n",
    "        \n",
    "        # Re-save with sanity check\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        log_print(f\"\\nNext step: Copy {filepath} to verifier machine\")\n",
    "        log_print(f\"Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\")\n",
    "    \n",
    "    file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "    log_print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "    \n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"EXPERIMENT COMPLETE\")\n",
    "    log_print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    close_logging()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b499e304-3093-42b2-b36b-52ca833adab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
