{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Hardware Verification: Second CUDA Stream\n",
    "\n",
    "## Objective\n",
    "Test whether parallel CUDA stream workloads are detectable across different hardware.\n",
    "\n",
    "## Methodology\n",
    "- Collect key vectors (sparse sampling: every 5th token from end)\n",
    "- Three conditions per hardware:\n",
    "  1. Baseline (no parallel stream)\n",
    "  2. Light parallel workload (short concurrent inference)\n",
    "  3. Heavy parallel workload (long concurrent inference)\n",
    "- 3 repetitions per condition to check reproducibility\n",
    "- Expected: Baseline is reproducible, parallel streams show statistical noise\n",
    "\n",
    "## Usage\n",
    "1. Run this notebook on A100\n",
    "2. Run this notebook on H100\n",
    "3. Use compare_streams.py to analyze cross-hardware differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nos.environ['HF_HOME'] = '/workspace/huggingface_cache'\nos.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\nimport numpy as np\nfrom datetime import datetime\nimport json\nimport socket\nimport threading\nimport time\nimport subprocess"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "LAYER_INDICES = [1, 4, 10, 18, 28]  # Sample layers across model depth\n",
    "NUM_REPETITIONS = 3  # Check reproducibility\n",
    "\n",
    "# Test sequences (10 diverse samples)\n",
    "SEQUENCES = {\n",
    "    \"technical_1\": \"\"\"Large language models have revolutionized natural language processing through their ability to capture complex patterns in text data. The transformer architecture, introduced in 2017, employs self-attention mechanisms that allow the model to weigh the importance of different tokens in the input sequence. During training, these models learn to predict the next token in a sequence by optimizing a cross-entropy loss function across billions of text examples. The attention mechanism computes query, key, and value matrices for each token, enabling the model to establish contextual relationships across long distances in the text. Modern implementations use various optimization techniques including gradient checkpointing, mixed precision training, and careful learning rate scheduling to manage the computational demands of training models with hundreds of billions of parameters. The pre-training phase typically involves processing massive text corpora, often exceeding trillions of tokens, which requires distributed training across hundreds or thousands of GPUs. Researchers have developed various architectural innovations such as sparse attention patterns, sliding window attention, and mixture-of-experts layers to improve efficiency and scalability.\"\"\",\n",
    "    \n",
    "    \"technical_2\": \"\"\"Quantum computing leverages the principles of quantum mechanics to perform computations that would be intractable for classical computers. At the heart of quantum computation lies the qubit, a quantum bit that can exist in a superposition of both 0 and 1 states simultaneously. When multiple qubits are entangled, they form a quantum register capable of representing an exponentially large state space. Quantum gates manipulate these qubits through carefully controlled electromagnetic pulses, implementing quantum algorithms like Shor's algorithm for factoring large numbers or Grover's algorithm for unstructured search. However, quantum systems are extremely fragile and susceptible to decoherence, where environmental interactions cause the quantum state to collapse into a classical state. To combat this, researchers develop quantum error correction codes that encode logical qubits across multiple physical qubits, enabling detection and correction of errors. Current quantum processors utilize various physical implementations including superconducting circuits cooled to millikelvin temperatures, trapped ions manipulated by laser pulses, and topological qubits that exploit exotic quantum states of matter.\"\"\",\n",
    "    \n",
    "    \"narrative_1\": \"\"\"The morning sun filtered through the ancient oak trees as Sarah walked along the forest path, her boots crunching softly on the fallen leaves. She had been coming to these woods since childhood, when her grandmother first taught her to identify the different bird calls echoing through the canopy. Now, decades later, she found herself returning to this same trail whenever life felt overwhelming. The forest had a way of putting things into perspective, reminding her that human concerns were just a small part of a much larger ecosystem. A red-tailed hawk circled overhead, its sharp eyes scanning the underbrush for movement. Sarah paused to watch it, remembering how her grandmother explained that hawks could see mice moving in the grass from hundreds of feet in the air. The forest was full of such marvels, from the intricate networks of fungal mycelia connecting tree roots underground to the complex social structures of ant colonies beneath her feet. She continued walking, feeling the weight of the past week gradually lifting from her shoulders with each step deeper into the woods.\"\"\",\n",
    "    \n",
    "    \"narrative_2\": \"\"\"The small coastal town had changed dramatically over the past fifty years, transforming from a quiet fishing village into a bustling tourist destination. Marcus remembered when there were only a handful of boats in the harbor, each owned by families who had fished these waters for generations. Now the marina was packed with sleek yachts and charter boats, and the waterfront was lined with restaurants and gift shops catering to summer visitors. His father's old fishing boat was long gone, sold when the fishing industry collapsed in the nineties. Marcus had tried to keep the family tradition alive, but the combination of depleted fish stocks, new regulations, and competition from industrial fishing operations made it impossible to earn a living. Eventually, he had taken a job at one of the new hotels, leading nature tours for tourists who wanted to experience the authentic coastal lifestyle. It was ironic, he thought, that he now made more money showing people what the town used to be like than he ever did when it actually was that way.\"\"\",\n",
    "    \n",
    "    \"code_1\": \"\"\"The database migration system implements a sophisticated version control mechanism for schema changes. Each migration file contains both an upgrade and downgrade function, allowing the system to roll forward or backward through schema versions. The migration engine maintains a table tracking which migrations have been applied, using timestamps and hash values to ensure consistency across different environments. When executing migrations, the system wraps each operation in a transaction to maintain atomicity, though some database operations like adding indexes or modifying column types may require careful handling to avoid long table locks. The migration framework supports multiple database backends through a plugin architecture, with each plugin implementing backend-specific SQL generation. For complex migrations involving data transformations, the system provides hooks for custom Python code to manipulate data between schema changes. Performance considerations are critical when migrating large tables, often requiring strategies like batched updates, temporary tables, or online schema change tools that minimize downtime. The system also includes safeguards against dangerous operations, such as preventing destructive changes in production environments without explicit confirmation flags.\"\"\",\n",
    "    \n",
    "    \"code_2\": \"\"\"The distributed cache implementation uses consistent hashing to partition data across multiple nodes, ensuring that adding or removing nodes only requires redistributing a small fraction of keys. Each cache node maintains both hot and cold storage tiers, with frequently accessed items kept in memory while less popular items are stored on SSD. The eviction policy combines LRU with a frequency-based component, tracking both recency and access count to make intelligent decisions about which items to remove when memory pressure increases. To handle cache stampede scenarios where many requests simultaneously attempt to populate the same expired key, the system implements a locking mechanism that allows only one request to perform the expensive computation while others wait for the result. The cache supports automatic replication for high-availability, with configurable consistency levels ranging from eventual consistency to strong consistency depending on the application requirements. Background processes continuously monitor cache hit rates, eviction patterns, and memory usage, automatically adjusting parameters like cache size allocations and replication factors to optimize performance. The implementation includes comprehensive instrumentation exposing metrics through a monitoring endpoint compatible with standard observability platforms.\"\"\",\n",
    "    \n",
    "    \"mixed_1\": \"\"\"The research team analyzed climate data spanning three decades to identify trends in regional temperature patterns. Dr. Martinez presented their findings at the conference, explaining how machine learning models had helped them detect subtle correlations between ocean temperature anomalies and subsequent weather patterns inland. The neural network architecture they developed processed multiple data streams simultaneously: satellite imagery showing cloud formations, oceanic buoy measurements tracking sea surface temperatures, and historical weather station records dating back over a century. By training the model on this diverse dataset, they achieved prediction accuracy that surpassed traditional physics-based models for forecast horizons between two and six weeks. However, the researchers emphasized that their model complemented rather than replaced conventional meteorological approaches, as the neural network could identify patterns but couldn't explain the underlying physical mechanisms. The team planned to collaborate with atmospheric physicists to interpret the model's predictions and potentially discover new relationships between ocean dynamics and terrestrial climate. Their work had implications beyond academic research, with potential applications in agricultural planning, disaster preparedness, and renewable energy grid management.\"\"\",\n",
    "    \n",
    "    \"mixed_2\": \"\"\"The archaeological expedition uncovered evidence of sophisticated astronomical knowledge among the ancient civilization. Stone tablets inscribed with careful observations of celestial movements suggested that the society had developed accurate calendars and could predict astronomical events like eclipses and planetary conjunctions. Dr. Chen's team used 3D scanning technology to create detailed digital models of the artifacts, allowing researchers worldwide to study the inscriptions without risking damage to the fragile originals. The digital models were processed through optical character recognition algorithms adapted for ancient scripts, which helped identify previously unnoticed patterns in the writing system. Computational analysis revealed that certain symbols appeared to encode numerical information using a base-60 system, similar to that used by other ancient cultures for astronomical calculations. The team collaborated with astronomers to reconstruct the night sky as it would have appeared during the civilization's peak, comparing their observations with the records inscribed on the tablets. This interdisciplinary approach combining archaeology, astronomy, and computer science provided new insights into how ancient peoples understood and tracked celestial phenomena, demonstrating a level of scientific sophistication previously unrecognized for this culture.\"\"\",\n",
    "    \n",
    "    \"repetitive\": \"\"\"The algorithm iterates through the data structure, examining each element to determine whether it matches the specified criteria. For each iteration, the function evaluates multiple conditions, checking first whether the element exists, then whether it satisfies the primary constraint, and finally whether it meets any secondary requirements. During each pass through the loop, the system maintains counters tracking how many elements have been processed, how many matched the criteria, and how many were rejected. The iteration continues until all elements have been examined, at which point the function returns a summary of the results. This iterative approach ensures thorough examination of every element while maintaining consistent evaluation criteria throughout the process. The iteration process repeats for each subset of the data, applying the same evaluation logic to ensure consistency across different portions of the dataset. Each iteration produces intermediate results that are aggregated into a final summary. The iterative evaluation guarantees that no elements are skipped and that all elements receive identical treatment during the selection process.\"\"\",\n",
    "    \n",
    "    \"scientific\": \"\"\"Mitochondrial DNA analysis provides unique insights into evolutionary relationships because it is inherited maternally and undergoes minimal recombination. Researchers extract DNA samples from preserved specimens, amplify specific gene regions using polymerase chain reaction, and sequence the resulting fragments to identify genetic variations. The molecular clock hypothesis allows scientists to estimate divergence times between species by assuming that mutations accumulate at a relatively constant rate over evolutionary time. However, this assumption requires careful calibration using fossil evidence and consideration of factors that might affect mutation rates, such as generation time, metabolic rate, and effective population size. Phylogenetic analysis employs computational algorithms to construct evolutionary trees that best explain the observed patterns of genetic similarity and difference among species. Maximum likelihood methods evaluate numerous possible tree topologies, calculating the probability that each tree would produce the observed data under specific models of molecular evolution. Bayesian approaches incorporate prior knowledge about evolutionary processes and provide probability distributions over possible trees rather than single point estimates. These sophisticated computational methods have revolutionized our understanding of evolutionary relationships.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel CUDA Stream Workload"
   ]
  },
  {
   "cell_type": "code",
   "source": "class GPUMonitor:\n    \"\"\"Monitor GPU utilization during inference\"\"\"\n    \n    def __init__(self):\n        self.running = False\n        self.samples = []\n        self.thread = None\n    \n    def _worker(self):\n        while self.running:\n            try:\n                result = subprocess.run(\n                    ['nvidia-smi', '--query-gpu=utilization.gpu',\n                     '--format=csv,noheader,nounits'],\n                    capture_output=True,\n                    text=True,\n                    timeout=1\n                )\n                if result.returncode == 0:\n                    self.samples.append(float(result.stdout.strip()))\n            except:\n                pass\n            time.sleep(0.01)  # Sample every 10ms\n    \n    def start(self):\n        self.samples = []\n        self.running = True\n        self.thread = threading.Thread(target=self._worker, daemon=True)\n        self.thread.start()\n        time.sleep(0.05)  # Let monitoring start\n    \n    def stop(self):\n        self.running = False\n        if self.thread:\n            self.thread.join(timeout=1.0)\n        if self.samples:\n            return {\n                'mean': float(np.mean(self.samples)),\n                'median': float(np.median(self.samples)),\n                'std': float(np.std(self.samples)),\n                'min': float(np.min(self.samples)),\n                'max': float(np.max(self.samples)),\n                'p50': float(np.percentile(self.samples, 50)),\n                'p95': float(np.percentile(self.samples, 95)),\n                'p99': float(np.percentile(self.samples, 99)),\n                'num_samples': len(self.samples)\n            }\n        return None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class ConcurrentInference:\n    \"\"\"Run concurrent inference as hidden workload on separate CUDA stream\"\"\"\n    \n    def __init__(self, model, tokenizer, intensity='light', device='cuda'):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.device = device\n        self.running = False\n        self.thread = None\n        self.stream = None\n        self.ready_event = threading.Event()\n        \n        # MUCH MORE AGGRESSIVE intensities to saturate GPU\n        if intensity == 'light':\n            # Light: ~500 tokens (was 5 tokens before)\n            self.prompt = \"\"\"Large language models have revolutionized natural language processing through their ability to capture complex patterns in text data. The transformer architecture, introduced in 2017, employs self-attention mechanisms that allow the model to weigh the importance of different tokens in the input sequence. During training, these models learn to predict the next token in a sequence by optimizing a cross-entropy loss function across billions of text examples. The attention mechanism computes query, key, and value matrices for each token, enabling the model to establish contextual relationships across long distances in the text. Modern implementations use various optimization techniques including gradient checkpointing, mixed precision training, and careful learning rate scheduling to manage the computational demands of training models with hundreds of billions of parameters. The pre-training phase typically involves processing massive text corpora, often exceeding trillions of tokens, which requires distributed training across hundreds or thousands of GPUs.\"\"\"\n        else:  # heavy\n            # Heavy: ~1500+ tokens (was 250 tokens before)\n            base_text = \"\"\"Large language models have revolutionized natural language processing through their ability to capture complex patterns in text data. The transformer architecture, introduced in 2017, employs self-attention mechanisms that allow the model to weigh the importance of different tokens in the input sequence. During training, these models learn to predict the next token in a sequence by optimizing a cross-entropy loss function across billions of text examples. The attention mechanism computes query, key, and value matrices for each token, enabling the model to establish contextual relationships across long distances in the text. Modern implementations use various optimization techniques including gradient checkpointing, mixed precision training, and careful learning rate scheduling to manage the computational demands of training models with hundreds of billions of parameters.\"\"\"\n            self.prompt = base_text * 3  # Repeat 3x for ~1500 tokens\n        \n        # Pre-tokenize\n        self.inputs = tokenizer([self.prompt], return_tensors=\"pt\")\n        self.inputs = {k: v.to(device) for k, v in self.inputs.items()}\n    \n    def _worker(self):\n        \"\"\"Worker that runs inference CONTINUOUSLY on separate CUDA stream\"\"\"\n        # Use separate stream for concurrent work\n        self.stream = torch.cuda.Stream()\n        \n        # Signal ready\n        self.ready_event.set()\n        \n        with torch.cuda.stream(self.stream):\n            while self.running:\n                # Run inference continuously with NO DELAY\n                # This will saturate GPU and force SM contention\n                with torch.no_grad():\n                    _ = self.model(**self.inputs)\n                # NO SLEEP - run as fast as possible to maximize utilization\n    \n    def start(self):\n        if not self.running:\n            self.ready_event.clear()\n            self.running = True\n            self.thread = threading.Thread(target=self._worker, daemon=True)\n            self.thread.start()\n            # Wait for worker to be ready\n            self.ready_event.wait(timeout=1.0)\n            time.sleep(0.5)  # Let workload stabilize\n    \n    def stop(self):\n        if self.running:\n            self.running = False\n            if self.thread:\n                self.thread.join(timeout=2.0)\n            if self.stream:\n                self.stream.synchronize()\n            time.sleep(0.3)  # Cooldown"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Vector Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_key_vectors_sparse(model, tokenizer, text, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Extract key vectors for SPARSE token positions (every 5th, starting from end).\n",
    "    Uses GQA key heads - much smaller than full hidden states.\n",
    "    \n",
    "    For Qwen2.5-7B: 4 key heads × 128 head_dim = 512 dims (vs 3584 for hidden states)\n",
    "    \n",
    "    Returns:\n",
    "        key_vectors: dict of {layer_name: tensor of shape (num_sampled_positions, key_dim)}\n",
    "        sampled_positions: list of token indices that were sampled\n",
    "        seq_len: total sequence length\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    inputs = tokenizer([text], return_tensors=\"pt\", padding=False)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    # Determine positions to sample: every 5th token starting from the end\n",
    "    sampled_positions = list(range(seq_len - 1, -1, -5))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, use_cache=True)\n",
    "    \n",
    "    # Extract key vectors from KV cache for selected layers\n",
    "    key_vectors = {}\n",
    "    for layer_idx in LAYER_INDICES:\n",
    "        # Get keys for this layer (past_key_values is 0-indexed)\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]  # Get keys (not values)\n",
    "        # layer_keys shape: (1, num_key_heads, seq_len, head_dim)\n",
    "        \n",
    "        # Extract sampled positions and concatenate heads\n",
    "        sampled_keys = layer_keys[0, :, sampled_positions, :]\n",
    "        \n",
    "        # Reshape to (num_sampled, num_key_heads * head_dim)\n",
    "        num_sampled = len(sampled_positions)\n",
    "        key_dim = sampled_keys.shape[0] * sampled_keys.shape[2]  # num_heads * head_dim\n",
    "        key_vectors[f'layer_{layer_idx}'] = sampled_keys.permute(1, 0, 2).reshape(num_sampled, key_dim).cpu().clone()\n",
    "    \n",
    "    del outputs\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return key_vectors, sampled_positions, seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"CROSS-HARDWARE VERIFICATION: SECOND CUDA STREAM\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Auto-detect hardware\n",
    "gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\"\n",
    "if \"H100\" in gpu_name:\n",
    "    hardware_label = \"h100\"\n",
    "elif \"A100\" in gpu_name:\n",
    "    hardware_label = \"a100\"\n",
    "elif \"L40S\" in gpu_name or \"L40s\" in gpu_name:\n",
    "    hardware_label = \"l40s\"\n",
    "elif \"V100\" in gpu_name:\n",
    "    hardware_label = \"v100\"\n",
    "else:\n",
    "    hardware_label = \"gpu\"\n",
    "\n",
    "print(f\"\\nHardware: {hardware_label.upper()}\")\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Layers: {LAYER_INDICES}\")\n",
    "print(f\"Sequences: {len(SEQUENCES)}\")\n",
    "print(f\"Repetitions per condition: {NUM_REPETITIONS}\")\n",
    "\n",
    "hostname = socket.gethostname()\n",
    "print(f\"\\nSystem Info:\")\n",
    "print(f\"  Hostname: {hostname}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA: {torch.version.cuda}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "print(f\"Loading model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"  # Consistent with separate_CUDA_stream experiment\n",
    ")\n",
    "model.eval()\n",
    "print(f\"✓ Model loaded\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize results structure\nresults = {\n    'metadata': {\n        'hardware': hardware_label,\n        'hostname': hostname,\n        'gpu': gpu_name,\n        'pytorch_version': torch.__version__,\n        'cuda_version': torch.version.cuda,\n        'model': MODEL_NAME,\n        'layer_indices': LAYER_INDICES,\n        'num_sequences': len(SEQUENCES),\n        'num_repetitions_per_condition': NUM_REPETITIONS,\n        'timestamp': datetime.now().isoformat()\n    },\n    'sequences': {}\n}\n\n# Run experiments for each sequence\nfor seq_name, text in SEQUENCES.items():\n    print(f\"Processing: {seq_name}\")\n    \n    prompt_length = len(tokenizer.encode(text))\n    print(f\"  Prompt: {prompt_length} tokens\")\n    \n    seq_results = {\n        'sequence_name': seq_name,\n        'prompt_text_preview': text[:100] + '...' if len(text) > 100 else text,\n        'prompt_length_tokens': prompt_length,\n        'conditions': {}\n    }\n    \n    # Test three conditions: baseline, light_concurrent, heavy_concurrent\n    conditions = [\n        ('baseline', None),\n        ('light_concurrent', 'light'),\n        ('heavy_concurrent', 'heavy')\n    ]\n    \n    for condition_name, intensity in conditions:\n        print(f\"  Condition: {condition_name}\")\n        \n        # Start GPU monitoring\n        gpu_monitor = GPUMonitor()\n        gpu_monitor.start()\n        \n        # Start concurrent workload if needed\n        concurrent = None\n        if intensity is not None:\n            concurrent = ConcurrentInference(model, tokenizer, intensity=intensity)\n            concurrent.start()\n            print(f\"    [Parallel stream started: {intensity}]\")\n        \n        # Run multiple repetitions to check reproducibility\n        runs = []\n        all_positions = None\n        all_seq_lens = []\n        \n        for rep in range(NUM_REPETITIONS):\n            key_vectors, sampled_positions, seq_len = extract_key_vectors_sparse(model, tokenizer, text)\n            runs.append(key_vectors)\n            all_seq_lens.append(seq_len)\n            if all_positions is None:\n                all_positions = sampled_positions\n        \n        # Stop concurrent workload\n        if concurrent is not None:\n            concurrent.stop()\n            print(f\"    [Parallel stream stopped]\")\n        \n        # Stop GPU monitoring\n        gpu_stats = gpu_monitor.stop()\n        \n        # Display GPU utilization\n        if gpu_stats:\n            print(f\"    GPU utilization: mean={gpu_stats['mean']:.1f}% \"\n                  f\"p95={gpu_stats['p95']:.1f}% \"\n                  f\"max={gpu_stats['max']:.1f}%\")\n        \n        # Verify all runs have same sequence length\n        if not all(sl == all_seq_lens[0] for sl in all_seq_lens):\n            print(f\"    ⚠ WARNING: Sequence length varied across runs!\")\n        \n        seq_len = all_seq_lens[0]\n        \n        # Check bit-exact reproducibility across all repetitions\n        reproducible = True\n        max_l2_across_runs = {}\n        \n        for layer_name in runs[0].keys():\n            layer_reproducible = True\n            max_l2 = 0.0\n            \n            for i in range(1, NUM_REPETITIONS):\n                if not torch.equal(runs[0][layer_name], runs[i][layer_name]):\n                    layer_reproducible = False\n                    reproducible = False\n                    # Compute L2 distance\n                    l2 = float(torch.norm(runs[0][layer_name].float() - runs[i][layer_name].float()).item())\n                    max_l2 = max(max_l2, l2)\n            \n            max_l2_across_runs[layer_name] = max_l2\n        \n        if not reproducible:\n            print(f\"    ⚠ Non-reproducible! Statistical noise detected\")\n            for layer_name, l2 in max_l2_across_runs.items():\n                if l2 > 0:\n                    print(f\"      {layer_name}: max L2={l2:.2e}\")\n        else:\n            print(f\"    ✓ Reproducible (bit-exact across {NUM_REPETITIONS} runs)\")\n        \n        # Use first run for storage (to save space)\n        key_vectors = runs[0]\n        \n        # Store condition results\n        cond_results = {\n            'condition_name': condition_name,\n            'intensity': intensity,\n            'sequence_length': seq_len,\n            'num_sampled_positions': len(sampled_positions),\n            'sampled_positions': sampled_positions,\n            'sampling_strategy': 'every_5th_from_end',\n            'reproducible': reproducible,\n            'max_l2_across_repetitions': {k: float(v) for k, v in max_l2_across_runs.items()} if not reproducible else None,\n            'gpu_utilization': gpu_stats,\n            'layers': {}\n        }\n        \n        for layer_name, layer_keys in key_vectors.items():\n            cond_results['layers'][layer_name] = {\n                'shape': list(layer_keys.shape),\n                'key_vectors': layer_keys.float().numpy().tolist(),\n                'norms_per_position': [torch.norm(layer_keys[i]).item() \n                                       for i in range(len(sampled_positions))]\n            }\n        \n        seq_results['conditions'][condition_name] = cond_results\n        print(f\"    Sampled: {len(sampled_positions)} positions\\n\")\n    \n    results['sequences'][seq_name] = seq_results\n\nprint(\"=\"*70)\nprint(\"All sequences processed\")\nprint(\"=\"*70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_dir = '/workspace/experiments/cross_hardware_verification/second_CUDA_stream'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "filename = f\"{hardware_label}_cuda_stream_{timestamp}.json\"\n",
    "filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "with open(filepath, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "file_size_mb = os.path.getsize(filepath) / 1024 / 1024\n",
    "print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "print(f\"  File size: {file_size_mb:.1f} MB\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(f\"1. Run on other hardware (different GPU)\")\n",
    "print(f\"2. Compare results: python compare_streams.py {filename} <other_file>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}