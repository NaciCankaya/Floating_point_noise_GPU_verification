{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e955f7-a75b-4b43-8a2b-79e9c041b205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "H100 FP8 QUANTIZATION DETERMINISM TEST\n",
      "Native Tensor Core Support\n",
      "================================================================================\n",
      "\n",
      "Found file: Verification-for-International-AI-Governance.pdf\n",
      "Loading 172 pages from PDF...\n",
      "Loaded 535619 characters from PDF (172 pages)\n",
      "\n",
      "Message content length: 535677 characters\n",
      "\n",
      "Loading tokenizer to validate prompt length...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt statistics:\n",
      "  Characters: 535,825\n",
      "  Tokens: 120,431\n",
      "  Max model length: 130,000\n",
      "  Generation tokens: 100\n",
      "  Total required: 120,531\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen3-32B-FP8\n",
      "  Quantization: fp8 (from model config)\n",
      "  Precision: FP8 weights, FP16 activations\n",
      "  Compute: Native FP8×FP16 tensor core operations\n",
      "  KV cache: FP16\n",
      "  Tensor parallel: 1\n",
      "  Max model len: 130,000 (with YaRN scaling)\n",
      "  YaRN factor: 4.0x (32K -> 128K)\n",
      "  Max tokens: 100\n",
      "  Temperature: 0.0\n",
      "  Seed: 42\n",
      "  Repetitions: 10\n",
      "  Warmup runs: 3\n",
      "\n",
      "IMPORTANT: Using FP16 to match INT8 experiment\n",
      "           This isolates dequantization as the only variable\n",
      "\n",
      "Loading FP8 quantized model (native H100 tensor core, FP16 activations)...\n",
      "Note: FP8 weights × FP16 activations using native tensor cores\n",
      "Note: Using FP16 to match INT8 experiment (controlled comparison)\n",
      "Activations and KV cache: FP16\n",
      "Enabling YaRN rope scaling for 130K context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=870)\u001b[0;0m 2025-11-11 15:17:19,659 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=870)\u001b[0;0m 2025-11-11 15:17:21,939 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 18.33s\n",
      "\n",
      "Running 3 warmup iterations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265fd3e9166c4a5ebc2812381198d9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431165b3580642eeb24149c3e8ba9f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warmup 1: 76.353s (100 tokens, 1.3 tok/s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19a303a537224323915e9a81ddc88edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47a5f64c614453492c358144788edc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warmup 2: 72.708s (100 tokens, 1.4 tok/s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "210688b13d0645f392887dde8fb415fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5c534844fe4fe2b370e9407238e477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warmup 3: 74.296s (100 tokens, 1.3 tok/s)\n",
      "Warmup complete - avg time: 74.452s\n",
      "\n",
      "================================================================================\n",
      "RUNNING EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Repetition 1/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdb3791ad754c2f98d9d3d923a7b8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5aa61b36a0a4b94a9b27ad363a00392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 74.143s (1.3 tok/s)\n",
      "Repetition 2/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b70cc52fc3a406d9894928bbd6573e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efd7c22b02d403589f630ae1bbeacc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 73.604s (1.4 tok/s)\n",
      "Repetition 3/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef58b31d88840ee9b2f655718ad36e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f780a73c03534f4c802b0aa4ed81ad29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 74.915s (1.3 tok/s)\n",
      "Repetition 4/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6d447a7f184c5eabc2b5a7feacc6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384b81a39ab046c0a5a3980dc6f37146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 72.853s (1.4 tok/s)\n",
      "Repetition 5/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66837cf34c404e8cae01de53ac4fc9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a35f7cc28b41f0aaa9122999665fce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 74.602s (1.3 tok/s)\n",
      "Repetition 6/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4fa393c6cca42549c06059631f3412d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fc4b73dd30487c9f3e245ab1d11192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 74.026s (1.4 tok/s)\n",
      "Repetition 7/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd3bd171a1140fa902c9a6f88960493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff21fc2bf59427e8fdbdbd9fe1e466f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 73.068s (1.4 tok/s)\n",
      "Repetition 8/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34e000fc4cc4764ac8769e40d163562",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8990b409f1bd42c08b87be6227de151d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 73.481s (1.4 tok/s)\n",
      "Repetition 9/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4476bd566046139610124862742d38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a4967a8b5e463292bdb8d7c31c4ed4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "H100 FP8 Quantization Determinism Test\n",
    "Tests bit-exact reproducibility with FP8 quantization (native H100 tensor core support)\n",
    "Includes detailed timing measurements for performance comparison\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SUPPRESS VERBOSE LOGGING\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "os.environ['VLLM_LOGGING_LEVEL'] = 'WARNING'\n",
    "os.environ['VLLM_CONFIGURE_LOGGING'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger('vllm').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.engine').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.worker').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.executor').setLevel(logging.ERROR)\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "logging.getLogger('torch').setLevel(logging.ERROR)\n",
    "logging.getLogger('huggingface_hub').setLevel(logging.INFO)\n",
    "logging.getLogger('huggingface_hub.file_download').setLevel(logging.INFO)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration - FP8 quantized version (native H100 tensor core support)\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B-FP8\"\n",
    "QUANTIZATION = \"fp8\"  # Must match model config (not compressed-tensors)\n",
    "TENSOR_PARALLEL_SIZE = 1  # Single GPU for clean timing\n",
    "MAX_MODEL_LEN = 130000\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 100\n",
    "NUM_REPETITIONS = 10\n",
    "TEMPERATURE = 0.0  # Greedy decoding\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Timing configuration\n",
    "NUM_WARMUP_RUNS = 3  # Extra warmup for stable timing\n",
    "\n",
    "# Prompt source\n",
    "AUTO_FIND_FILE = True\n",
    "\n",
    "# User task\n",
    "USER_TASK = \"Please provide a detailed summary of the following text.\"\n",
    "\n",
    "# Hardcoded content\n",
    "HARDCODED_CONTENT = \"\"\"The development of large language models has fundamentally transformed natural language processing and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing. The scaling laws observed in these systems suggest that performance continues to improve with model size, data scale, and compute budget, though with diminishing returns. Recent advances in architecture, training techniques, and inference optimization have made these powerful models increasingly accessible for practical applications.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FILE LOADING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def find_prompt_file():\n",
    "    \"\"\"Find first txt or pdf file in current directory\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    txt_files = glob.glob(os.path.join(cwd, \"*.txt\"))\n",
    "    pdf_files = glob.glob(os.path.join(cwd, \"*.pdf\"))\n",
    "    \n",
    "    if txt_files:\n",
    "        return txt_files[0]\n",
    "    elif pdf_files:\n",
    "        return pdf_files[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_text_from_file(filepath):\n",
    "    \"\"\"Load text from txt or pdf file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    if filepath.endswith('.txt'):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print(f\"Loaded {len(text)} characters from txt file\")\n",
    "        return text\n",
    "    \n",
    "    elif filepath.endswith('.pdf'):\n",
    "        try:\n",
    "            import PyPDF2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"PyPDF2 required for PDF loading. Install with: pip install PyPDF2\")\n",
    "        \n",
    "        text = []\n",
    "        with open(filepath, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Loading {num_pages} pages from PDF...\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text.append(page_text)\n",
    "        \n",
    "        full_text = '\\n'.join(text)\n",
    "        print(f\"Loaded {len(full_text)} characters from PDF ({num_pages} pages)\")\n",
    "        return full_text\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"H100 FP8 QUANTIZATION DETERMINISM TEST\")\n",
    "print(\"Native Tensor Core Support\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load document content\n",
    "prompt_file = None\n",
    "if AUTO_FIND_FILE:\n",
    "    prompt_file = find_prompt_file()\n",
    "    if prompt_file:\n",
    "        print(f\"Found file: {os.path.basename(prompt_file)}\")\n",
    "        DOCUMENT_CONTENT = load_text_from_file(prompt_file)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No txt/pdf files found - using hardcoded content\")\n",
    "        DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "        print()\n",
    "else:\n",
    "    DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "    print(\"Using hardcoded content\")\n",
    "    print()\n",
    "\n",
    "# Prepare messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{USER_TASK}\\n\\n{DOCUMENT_CONTENT}\"}\n",
    "]\n",
    "\n",
    "print(f\"Message content length: {len(messages[0]['content'])} characters\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER PRECHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading tokenizer to validate prompt length...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\",  # Use base Qwen model for tokenizer\n",
    "    cache_dir='/tmp/hf_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply chat template with enable_thinking for Qwen3\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True  # Qwen3 thinking mode\n",
    ")\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_text)\n",
    "prompt_length = len(prompt_tokens)\n",
    "\n",
    "print(f\"Prompt statistics:\")\n",
    "print(f\"  Characters: {len(prompt_text):,}\")\n",
    "print(f\"  Tokens: {prompt_length:,}\")\n",
    "print(f\"  Max model length: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Generation tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Total required: {prompt_length + MAX_TOKENS:,}\")\n",
    "print()\n",
    "\n",
    "if prompt_length + MAX_TOKENS > MAX_MODEL_LEN:\n",
    "    print(f\"[WARNING] May exceed context length\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Quantization: {QUANTIZATION} (from model config)\")\n",
    "print(f\"  Precision: FP8 weights, FP16 activations\")\n",
    "print(f\"  Compute: Native FP8×FP16 tensor core operations\")\n",
    "print(f\"  KV cache: FP16\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {MAX_MODEL_LEN:,} (with YaRN scaling)\")\n",
    "print(f\"  YaRN factor: 4.0x (32K -> 128K)\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print(f\"  Warmup runs: {NUM_WARMUP_RUNS}\")\n",
    "print()\n",
    "print(\"IMPORTANT: Using FP16 to match INT8 experiment\")\n",
    "print(\"           This isolates dequantization as the only variable\")\n",
    "print()\n",
    "\n",
    "print(\"Loading FP8 quantized model (native H100 tensor core, FP16 activations)...\")\n",
    "print(\"Note: FP8 weights × FP16 activations using native tensor cores\")\n",
    "print(\"Note: Using FP16 to match INT8 experiment (controlled comparison)\")\n",
    "print(\"Activations and KV cache: FP16\")\n",
    "print(\"Enabling YaRN rope scaling for 130K context...\")\n",
    "load_start = time.time()\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    quantization=QUANTIZATION,\n",
    "    dtype=\"float16\",  # FP16 to match INT8 experiment (removes FP16 vs BF16 confound)\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    trust_remote_code=True,\n",
    "    seed=SEED,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False,\n",
    "    # YaRN rope scaling for long context (32K -> 130K)\n",
    "    rope_scaling={\n",
    "        \"rope_type\": \"yarn\",\n",
    "        \"factor\": 4.0,\n",
    "        \"original_max_position_embeddings\": 32768\n",
    "    }\n",
    ")\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "print(f\"Model loaded in {load_time:.2f}s\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    "    logprobs=TOP_LOGPROBS,\n",
    "    skip_special_tokens=False\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# WARMUP\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Running {NUM_WARMUP_RUNS} warmup iterations...\")\n",
    "warmup_times = []\n",
    "for i in range(NUM_WARMUP_RUNS):\n",
    "    warmup_start = time.time()\n",
    "    warmup_output = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "    warmup_time = time.time() - warmup_start\n",
    "    warmup_times.append(warmup_time)\n",
    "    num_tokens = len(warmup_output[0].outputs[0].token_ids)\n",
    "    print(f\"  Warmup {i+1}: {warmup_time:.3f}s ({num_tokens} tokens, {num_tokens/warmup_time:.1f} tok/s)\")\n",
    "\n",
    "print(f\"Warmup complete - avg time: {np.mean(warmup_times):.3f}s\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "results_tokens = []\n",
    "results_logprobs = []\n",
    "results_texts = []\n",
    "results_distributions = []\n",
    "timing_data = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Repetition {rep + 1}/{NUM_REPETITIONS}...\")\n",
    "    \n",
    "    # Time the generation\n",
    "    start_time = time.time()\n",
    "    outputs = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed = end_time - start_time\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract token IDs\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    num_tokens = len(token_ids)\n",
    "    tokens_per_sec = num_tokens / elapsed\n",
    "    \n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    # Store timing\n",
    "    timing_data.append({\n",
    "        'repetition': rep + 1,\n",
    "        'elapsed_time': elapsed,\n",
    "        'num_tokens': num_tokens,\n",
    "        'tokens_per_sec': tokens_per_sec,\n",
    "        'time_per_token': elapsed / num_tokens\n",
    "    })\n",
    "    \n",
    "    # Extract generated text\n",
    "    text = output.outputs[0].text\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    # Extract logprobs\n",
    "    logprobs_data = output.outputs[0].logprobs\n",
    "    selected_logprobs = [lp[token_ids[i]].logprob for i, lp in enumerate(logprobs_data)]\n",
    "    results_logprobs.append(np.array(selected_logprobs))\n",
    "    \n",
    "    # Extract distributions\n",
    "    rep_distributions = []\n",
    "    for position_logprobs in logprobs_data:\n",
    "        sorted_items = sorted(position_logprobs.items(), \n",
    "                            key=lambda x: x[1].logprob, \n",
    "                            reverse=True)[:TOP_LOGPROBS]\n",
    "        rep_distributions.append([(tok, lp.logprob) for tok, lp in sorted_items])\n",
    "    results_distributions.append(rep_distributions)\n",
    "    \n",
    "    print(f\"  {num_tokens} tokens in {elapsed:.3f}s ({tokens_per_sec:.1f} tok/s)\")\n",
    "\n",
    "print()\n",
    "print(\"All repetitions complete!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TIMING ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TIMING ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "times = [t['elapsed_time'] for t in timing_data]\n",
    "tps = [t['tokens_per_sec'] for t in timing_data]\n",
    "tpt = [t['time_per_token'] for t in timing_data]\n",
    "\n",
    "print(\"Timing statistics:\")\n",
    "print(f\"  Mean time: {np.mean(times):.3f}s (σ={np.std(times):.4f}s)\")\n",
    "print(f\"  Min/Max: {np.min(times):.3f}s / {np.max(times):.3f}s\")\n",
    "print(f\"  Tokens/sec: {np.mean(tps):.1f} (σ={np.std(tps):.2f})\")\n",
    "print(f\"  Time/token: {np.mean(tpt)*1000:.2f}ms (σ={np.std(tpt)*1000:.3f}ms)\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REPRODUCIBILITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check token sequences\n",
    "print(\"Checking token sequences...\")\n",
    "tokens_identical = all(\n",
    "    results_tokens[0] == results_tokens[i] \n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "if not tokens_identical:\n",
    "    print(\"\\n[WARNING] Token sequences differ!\")\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[0] != results_tokens[i]:\n",
    "            diff_positions = [\n",
    "                j for j in range(min(len(results_tokens[0]), len(results_tokens[i])))\n",
    "                if results_tokens[0][j] != results_tokens[i][j]\n",
    "            ]\n",
    "            print(f\"  Rep 0 vs Rep {i}: {len(diff_positions)} positions differ\")\n",
    "            if diff_positions:\n",
    "                print(f\"    First difference at position {diff_positions[0]}\")\n",
    "                print(f\"      Rep 0: token {results_tokens[0][diff_positions[0]]}\")\n",
    "                print(f\"      Rep {i}: token {results_tokens[i][diff_positions[0]]}\")\n",
    "\n",
    "# Check logprobs\n",
    "print(\"\\nChecking selected token logprobs...\")\n",
    "first_logprobs = results_logprobs[0]\n",
    "logprobs_exact = all(\n",
    "    np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Selected token logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "\n",
    "# Check distributions\n",
    "print(\"\\nChecking full top-k distributions...\")\n",
    "distributions_exact = True\n",
    "distribution_mismatches = []\n",
    "\n",
    "first_dist = results_distributions[0]\n",
    "for rep_idx in range(1, NUM_REPETITIONS):\n",
    "    for pos_idx in range(len(first_dist)):\n",
    "        dist_a = first_dist[pos_idx]\n",
    "        dist_b = results_distributions[rep_idx][pos_idx]\n",
    "        \n",
    "        tokens_match = [t[0] for t in dist_a] == [t[0] for t in dist_b]\n",
    "        \n",
    "        if tokens_match:\n",
    "            logprobs_match = all(\n",
    "                abs(dist_a[i][1] - dist_b[i][1]) < 1e-10 \n",
    "                for i in range(len(dist_a))\n",
    "            )\n",
    "            if not logprobs_match:\n",
    "                distributions_exact = False\n",
    "                distribution_mismatches.append((rep_idx, pos_idx))\n",
    "        else:\n",
    "            distributions_exact = False\n",
    "            distribution_mismatches.append((rep_idx, pos_idx))\n",
    "\n",
    "print(f\"Top-k distributions bit-exact: {distributions_exact}\")\n",
    "\n",
    "if not distributions_exact:\n",
    "    print(f\"\\nFound {len(distribution_mismatches)} mismatches\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERDICT - FP8 QUANTIZATION (NATIVE H100)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact and distributions_exact:\n",
    "    print(\"[PASS] PERFECT REPRODUCIBILITY WITH FP8\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Full top-k distributions: bit-exact\")\n",
    "    print(\"  => FP8 native tensor core operations maintain determinism\")\n",
    "    print(\"  => H100 tensor cores produce reproducible FP8×FP16 results\")\n",
    "elif tokens_identical and logprobs_exact and not distributions_exact:\n",
    "    print(\"[WARNING] SELECTED TOKENS EXACT, DISTRIBUTIONS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Top-k distributions: numerical variation\")\n",
    "    print(\"  => FP8 tensor cores have distribution-level noise\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"[WARNING] TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: numerical variation\")\n",
    "    print(\"  => FP8×FP16 introduces floating-point noise\")\n",
    "else:\n",
    "    print(\"[FAIL] TOKEN SEQUENCES DIFFER - DETERMINISM BROKEN\")\n",
    "    print(\"  - FP8 quantization breaks greedy decoding determinism\")\n",
    "    print(\"  => Native tensor core FP8×FP16 is non-deterministic\")\n",
    "    print(\"  => Same issue as INT4/INT8\")\n",
    "\n",
    "print()\n",
    "print(\"FP8 Implementation Notes:\")\n",
    "print(\"  - FP8 uses native H100 tensor core operations\")\n",
    "print(\"  - E4M3 format: 4-bit exponent, 3-bit mantissa\")\n",
    "print(\"  - FP8×FP16→FP32 is a single tensor core instruction\")\n",
    "print(\"  - Same FP16 precision as INT8 for controlled comparison\")\n",
    "print(\"  - If non-deterministic: likely hardware rounding modes\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"h100_fp8_determinism_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"hardware\": \"H100\",\n",
    "    \"hardware_note\": \"FP8 uses native tensor core operations\",\n",
    "    \"prompt_source\": \"file\" if prompt_file else \"hardcoded\",\n",
    "    \"prompt_file\": os.path.basename(prompt_file) if prompt_file else None,\n",
    "    \"prompt_text\": prompt_text,\n",
    "    \"prompt_length_chars\": len(prompt_text),\n",
    "    \"prompt_length_tokens\": prompt_length,\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"quantization\": QUANTIZATION,\n",
    "        \"precision\": \"FP8 (E4M3)\",\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_model_len\": MAX_MODEL_LEN,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"warmup_runs\": NUM_WARMUP_RUNS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"top_logprobs\": TOP_LOGPROBS\n",
    "    },\n",
    "    \"timing\": {\n",
    "        \"model_load_time\": load_time,\n",
    "        \"warmup_times\": warmup_times,\n",
    "        \"per_repetition\": timing_data,\n",
    "        \"statistics\": {\n",
    "            \"mean_time\": float(np.mean(times)),\n",
    "            \"std_time\": float(np.std(times)),\n",
    "            \"mean_tokens_per_sec\": float(np.mean(tps)),\n",
    "            \"std_tokens_per_sec\": float(np.std(tps)),\n",
    "            \"mean_time_per_token_ms\": float(np.mean(tpt) * 1000),\n",
    "            \"std_time_per_token_ms\": float(np.std(tpt) * 1000)\n",
    "        }\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"distributions_exact\": distributions_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact and distributions_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts\n",
    "}\n",
    "\n",
    "output_file = f\"h100_fp8_determinism_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d5244-e790-4372-bb0f-9500c19a6414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
