{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed060d46-b5b5-4736-8f47-6f0e3c0b1075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "H100 INT8 QUANTIZATION DETERMINISM TEST - Qwen3-32B with YaRN\n",
      "================================================================================\n",
      "\n",
      "Found file: NSA.pdf\n",
      "Loading 24 pages from PDF...\n",
      "  Processed 10/24 pages\n",
      "  Processed 20/24 pages\n",
      "Loaded 59348 characters from PDF (24 pages)\n",
      "\n",
      "Message content length: 59406 characters\n",
      "\n",
      "Loading tokenizer and config with YaRN configuration...\n",
      "YaRN enabled: factor=4.0, extended to 131,072 tokens\n",
      "\n",
      "Prompt statistics:\n",
      "  Characters: 59,456\n",
      "  Tokens: 16,432\n",
      "  Max context (with YaRN): 131,072\n",
      "  Planned generation tokens: 20\n",
      "  Total required: 16,452\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen3-32B\n",
      "  Quantization: bitsandbytes_int8\n",
      "  Precision: INT8 weights via bitsandbytes, higher-precision activations\n",
      "  Compute: INT8-storage weights with dequantization for matmul\n",
      "  KV cache: model default (higher precision)\n",
      "  RoPE scaling: YaRN (factor=4.0, base=32768)\n",
      "  Tensor parallel: 1\n",
      "  Max model len: 131,072\n",
      "  Max new tokens: 20\n",
      "  Temperature: 0.0\n",
      "  Seed: 42\n",
      "  Repetitions: 5\n",
      "  Warmup runs: 1\n",
      "\n",
      "IMPORTANT: Using bitsandbytes INT8 quantization with YaRN extended context\n",
      "           NOTE: Static YaRN may degrade performance on SHORT contexts (<32K)\n",
      "\n",
      "Loading INT8 quantized model with YaRN-extended context...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87fb0f0dc46640c78040d05f641d5044",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "H100 INT8 Quantization Determinism Test - Qwen3-32B with YaRN\n",
    "Tests bit-exact reproducibility with INT8 quantization via bitsandbytes\n",
    "Includes detailed timing measurements for performance comparison\n",
    "Uses YaRN RoPE scaling to extend context to 131K tokens\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SUPPRESS VERBOSE LOGGING\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "logging.getLogger('torch').setLevel(logging.ERROR)\n",
    "logging.getLogger('huggingface_hub').setLevel(logging.INFO)\n",
    "logging.getLogger('huggingface_hub.file_download').setLevel(logging.INFO)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration - INT8 quantized Qwen3-32B via bitsandbytes with YaRN\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"\n",
    "QUANTIZATION = \"bitsandbytes_int8\"\n",
    "TENSOR_PARALLEL_SIZE = 1  # Single GPU for clean timing\n",
    "\n",
    "# YaRN configuration for long context\n",
    "YARN_ENABLED = True\n",
    "YARN_FACTOR = 4.0\n",
    "YARN_ORIGINAL_MAX = 32768\n",
    "MAX_CONTEXT_LENGTH = 131072  # With factor 4.0\n",
    "\n",
    "GPU_MEMORY_UTILIZATION = 0.9  # Kept for consistency; not directly used here\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 20\n",
    "NUM_REPETITIONS = 5\n",
    "TEMPERATURE = 0.0  # Greedy decoding\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Timing configuration\n",
    "NUM_WARMUP_RUNS = 1  # Warmup for stable timing\n",
    "\n",
    "# Prompt source\n",
    "AUTO_FIND_FILE = True\n",
    "\n",
    "# User task\n",
    "USER_TASK = \"Please provide a detailed summary of the following text.\"\n",
    "\n",
    "# Hardcoded content (fallback if no txt/pdf present)\n",
    "HARDCODED_CONTENT = \"\"\"The development of large language models has fundamentally transformed natural language processing and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing. The scaling laws observed in these systems suggest that performance continues to improve with model size, data scale, and compute budget, though with diminishing returns. Recent advances in architecture, training techniques, and inference optimization have made these powerful models increasingly accessible for practical applications.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FILE LOADING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def find_prompt_file():\n",
    "    \"\"\"Find first txt or pdf file in current directory\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    txt_files = glob.glob(os.path.join(cwd, \"*.txt\"))\n",
    "    pdf_files = glob.glob(os.path.join(cwd, \"*.pdf\"))\n",
    "    if txt_files:\n",
    "        return txt_files[0]\n",
    "    elif pdf_files:\n",
    "        return pdf_files[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_text_from_file(filepath):\n",
    "    \"\"\"Load text from txt or pdf file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    if filepath.endswith(\".txt\"):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        print(f\"Loaded {len(text)} characters from txt file\")\n",
    "        return text\n",
    "    elif filepath.endswith(\".pdf\"):\n",
    "        try:\n",
    "            import PyPDF2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"PyPDF2 required for PDF loading. Install with: pip install PyPDF2\")\n",
    "        text = []\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Loading {num_pages} pages from PDF...\")\n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text.append(page_text)\n",
    "                if page_num % 10 == 0:\n",
    "                    print(f\"  Processed {page_num}/{num_pages} pages\")\n",
    "        full_text = \"\\n\".join(text)\n",
    "        print(f\"Loaded {len(full_text)} characters from PDF ({num_pages} pages)\")\n",
    "        return full_text\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath}. Use .txt or .pdf\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"H100 INT8 QUANTIZATION DETERMINISM TEST - Qwen3-32B with YaRN\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "prompt_file = None\n",
    "if AUTO_FIND_FILE:\n",
    "    prompt_file = find_prompt_file()\n",
    "    if prompt_file:\n",
    "        print(f\"Found file: {os.path.basename(prompt_file)}\")\n",
    "        DOCUMENT_CONTENT = load_text_from_file(prompt_file)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No txt/pdf files found in current directory\")\n",
    "        print(\"Using hardcoded content\")\n",
    "        DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "        print()\n",
    "else:\n",
    "    DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "    print(\"Using hardcoded content\")\n",
    "    print()\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{USER_TASK}\\n\\n{DOCUMENT_CONTENT}\"}\n",
    "]\n",
    "\n",
    "print(f\"Message content length: {len(messages[0]['content'])} characters\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER PRECHECK AND CONFIG WITH YARN\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading tokenizer and config with YaRN configuration...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=\"/tmp/hf_cache\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=\"/tmp/hf_cache\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply YaRN RoPE scaling for extended context\n",
    "if YARN_ENABLED:\n",
    "    config.rope_scaling = {\n",
    "        \"rope_type\": \"yarn\",\n",
    "        \"factor\": YARN_FACTOR,\n",
    "        \"original_max_position_embeddings\": YARN_ORIGINAL_MAX\n",
    "    }\n",
    "    max_context_len = MAX_CONTEXT_LENGTH\n",
    "    print(f\"YaRN enabled: factor={YARN_FACTOR}, extended to {max_context_len:,} tokens\")\n",
    "else:\n",
    "    if hasattr(config, \"max_position_embeddings\") and config.max_position_embeddings is not None:\n",
    "        max_context_len = int(config.max_position_embeddings)\n",
    "    else:\n",
    "        max_context_len = 32768\n",
    "    print(f\"YaRN disabled: native context {max_context_len:,} tokens\")\n",
    "\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_text)\n",
    "prompt_length = len(prompt_tokens)\n",
    "\n",
    "print()\n",
    "print(\"Prompt statistics:\")\n",
    "print(f\"  Characters: {len(prompt_text):,}\")\n",
    "print(f\"  Tokens: {prompt_length:,}\")\n",
    "print(f\"  Max context (with YaRN): {max_context_len:,}\")\n",
    "print(f\"  Planned generation tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Total required: {prompt_length + MAX_TOKENS:,}\")\n",
    "print()\n",
    "\n",
    "if prompt_length + MAX_TOKENS > max_context_len:\n",
    "    print(f\"[ERROR] Requested context length ({prompt_length + MAX_TOKENS}) exceeds model max ({max_context_len}).\")\n",
    "    print(\"        Truncate the input document or increase YaRN factor.\")\n",
    "    raise SystemExit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Quantization: {QUANTIZATION}\")\n",
    "print(f\"  Precision: INT8 weights via bitsandbytes, higher-precision activations\")\n",
    "print(f\"  Compute: INT8-storage weights with dequantization for matmul\")\n",
    "print(f\"  KV cache: model default (higher precision)\")\n",
    "print(f\"  RoPE scaling: YaRN (factor={YARN_FACTOR}, base={YARN_ORIGINAL_MAX})\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {max_context_len:,}\")\n",
    "print(f\"  Max new tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print(f\"  Warmup runs: {NUM_WARMUP_RUNS}\")\n",
    "print()\n",
    "print(\"IMPORTANT: Using bitsandbytes INT8 quantization with YaRN extended context\")\n",
    "print(\"           NOTE: Static YaRN may degrade performance on SHORT contexts (<32K)\")\n",
    "print()\n",
    "\n",
    "print(\"Loading INT8 quantized model with YaRN-extended context...\")\n",
    "load_start = time.time()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=\"/tmp/hf_cache\",\n",
    "    config=config,  # Pass modified config with YaRN\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0},  # force all modules onto cuda:0\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "print(f\"Model loaded in {load_time:.2f}s\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "generation_kwargs = {\n",
    "    \"max_new_tokens\": MAX_TOKENS,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": None,\n",
    "    \"return_dict_in_generate\": True,\n",
    "    \"output_scores\": True,\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# WARMUP\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Running {NUM_WARMUP_RUNS} warmup iterations...\")\n",
    "warmup_times = []\n",
    "for i in range(NUM_WARMUP_RUNS):\n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    warmup_start = time.time()\n",
    "    with torch.no_grad():\n",
    "        gen_out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "    warmup_time = time.time() - warmup_start\n",
    "\n",
    "    generated_tokens = gen_out.sequences[0][input_ids.shape[1]:]\n",
    "    num_tokens = generated_tokens.shape[0]\n",
    "    warmup_times.append(warmup_time)\n",
    "    if num_tokens > 0 and warmup_time > 0:\n",
    "        print(f\"  Warmup {i+1}: {warmup_time:.3f}s ({num_tokens / warmup_time:.1f} tok/s)\")\n",
    "    else:\n",
    "        print(f\"  Warmup {i+1}: {warmup_time:.3f}s (0 tok/s)\")\n",
    "\n",
    "if warmup_times:\n",
    "    print(f\"Warmup complete - avg time: {np.mean(warmup_times):.3f}s\")\n",
    "else:\n",
    "    print(\"Warmup complete - no valid timing data\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "results_tokens = []\n",
    "results_logprobs = []\n",
    "results_texts = []\n",
    "results_distributions = []\n",
    "timing_data = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Repetition {rep + 1}/{NUM_REPETITIONS}...\")\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt_text,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(model.device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        gen_out = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed = end_time - start_time\n",
    "\n",
    "    sequences = gen_out.sequences\n",
    "    scores = gen_out.scores  # list of (batch, vocab) tensors, one per generated token\n",
    "\n",
    "    gen_token_ids = sequences[0][input_ids.shape[1]:]\n",
    "    token_ids = gen_token_ids.tolist()\n",
    "    num_tokens = len(token_ids)\n",
    "    tokens_per_sec = (num_tokens / elapsed) if elapsed > 0 else 0.0\n",
    "    \n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    timing_data.append({\n",
    "        \"repetition\": rep + 1,\n",
    "        \"elapsed_time\": elapsed,\n",
    "        \"num_tokens\": num_tokens,\n",
    "        \"tokens_per_sec\": tokens_per_sec,\n",
    "        \"time_per_token\": (elapsed / num_tokens) if num_tokens > 0 else 0.0,\n",
    "    })\n",
    "    \n",
    "    text = tokenizer.decode(gen_token_ids, skip_special_tokens=False)\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    # Compute selected token logprobs and top-k distributions\n",
    "    selected_logprobs = []\n",
    "    per_token_topk = []\n",
    "    for t, score_t in enumerate(scores):\n",
    "        logits = score_t[0]\n",
    "        logprobs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "        tok_id = token_ids[t]\n",
    "        selected_logprobs.append(float(logprobs[tok_id].cpu().item()))\n",
    "\n",
    "        topk = torch.topk(logprobs, k=min(TOP_LOGPROBS, logprobs.shape[-1]))\n",
    "        topk_ids = topk.indices.cpu().tolist()\n",
    "        topk_vals = topk.values.cpu().tolist()\n",
    "        per_token_topk.append(list(zip(topk_ids, topk_vals)))\n",
    "    \n",
    "    results_logprobs.append(np.array(selected_logprobs))\n",
    "    results_distributions.append(per_token_topk)\n",
    "    \n",
    "    print(f\"  {num_tokens} tokens in {elapsed:.3f}s ({tokens_per_sec:.1f} tok/s)\")\n",
    "\n",
    "print()\n",
    "print(\"All repetitions complete!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TIMING ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TIMING ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "times = [t[\"elapsed_time\"] for t in timing_data]\n",
    "tps = [t[\"tokens_per_sec\"] for t in timing_data]\n",
    "tpt = [t[\"time_per_token\"] for t in timing_data if t[\"time_per_token\"] > 0]\n",
    "\n",
    "print(\"Timing statistics:\")\n",
    "if times:\n",
    "    print(f\"  Mean time: {np.mean(times):.3f}s (σ={np.std(times):.4f}s)\")\n",
    "    print(f\"  Min/Max: {np.min(times):.3f}s / {np.max(times):.3f}s\")\n",
    "else:\n",
    "    print(\"  No timing data collected\")\n",
    "\n",
    "if tps:\n",
    "    print(f\"  Tokens/sec: {np.mean(tps):.1f} (σ={np.std(tps):.2f})\")\n",
    "else:\n",
    "    print(\"  Tokens/sec: N/A\")\n",
    "\n",
    "if tpt:\n",
    "    print(f\"  Time/token: {np.mean(tpt) * 1000:.2f}ms (σ={np.std(tpt) * 1000:.3f}ms)\")\n",
    "else:\n",
    "    print(\"  Time/token: N/A\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REPRODUCIBILITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Comparing across repetitions (same config, same seed):\")\n",
    "print(\"  Checking if Run 1 = Run 2 = ... = Run N\")\n",
    "print(\"  Testing: Can we reproduce inference bit-exactly with YaRN?\")\n",
    "print()\n",
    "\n",
    "print(\"Checking token sequences...\")\n",
    "tokens_identical = all(\n",
    "    results_tokens[0] == results_tokens[i]\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "if not tokens_identical:\n",
    "    print(\"\\n[WARNING] Token sequences differ!\")\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[0] != results_tokens[i]:\n",
    "            diff_positions = [\n",
    "                j for j in range(min(len(results_tokens[0]), len(results_tokens[i])))\n",
    "                if results_tokens[0][j] != results_tokens[i][j]\n",
    "            ]\n",
    "            print(f\"  Rep 0 vs Rep {i}: {len(diff_positions)} differing positions\")\n",
    "            if diff_positions:\n",
    "                pos = diff_positions[0]\n",
    "                print(f\"    First difference at position {pos}\")\n",
    "                print(f\"      Rep 0: token {results_tokens[0][pos]}\")\n",
    "                print(f\"      Rep {i}: token {results_tokens[i][pos]}\")\n",
    "\n",
    "print(\"\\nChecking selected token logprobs...\")\n",
    "if results_logprobs:\n",
    "    first_logprobs = results_logprobs[0]\n",
    "    logprobs_exact = all(\n",
    "        np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "        for i in range(1, NUM_REPETITIONS)\n",
    "    )\n",
    "else:\n",
    "    first_logprobs = None\n",
    "    logprobs_exact = False\n",
    "\n",
    "print(f\"Selected token logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "if not logprobs_exact and first_logprobs is not None:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if len(results_logprobs[i]) == len(first_logprobs):\n",
    "            l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "            l2_distances.append(l2)\n",
    "            print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "        else:\n",
    "            print(f\"  Rep 0 vs Rep {i}: length mismatch\")\n",
    "    if l2_distances:\n",
    "        print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "        print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "\n",
    "print(\"\\nChecking full top-k distributions...\")\n",
    "distributions_exact = True\n",
    "distribution_mismatches = []\n",
    "\n",
    "if results_distributions:\n",
    "    first_dist = results_distributions[0]\n",
    "    for rep_idx in range(1, NUM_REPETITIONS):\n",
    "        if len(results_distributions[rep_idx]) != len(first_dist):\n",
    "            distributions_exact = False\n",
    "            distribution_mismatches.append((rep_idx, \"length_mismatch\"))\n",
    "            continue\n",
    "        for pos_idx in range(len(first_dist)):\n",
    "            dist_a = first_dist[pos_idx]\n",
    "            dist_b = results_distributions[rep_idx][pos_idx]\n",
    "            tokens_match = [t[0] for t in dist_a] == [t[0] for t in dist_b]\n",
    "            if tokens_match:\n",
    "                logprobs_match = all(\n",
    "                    abs(dist_a[i][1] - dist_b[i][1]) < 1e-10\n",
    "                    for i in range(len(dist_a))\n",
    "                )\n",
    "                if not logprobs_match:\n",
    "                    distributions_exact = False\n",
    "                    distribution_mismatches.append((rep_idx, pos_idx))\n",
    "            else:\n",
    "                distributions_exact = False\n",
    "                distribution_mismatches.append((rep_idx, pos_idx))\n",
    "\n",
    "print(f\"Top-k distributions bit-exact: {distributions_exact}\")\n",
    "\n",
    "if not distributions_exact:\n",
    "    print(f\"\\nFound {len(distribution_mismatches)} mismatches\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERDICT - INT8 QUANTIZATION + YaRN (bitsandbytes + Qwen3-32B)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact and distributions_exact:\n",
    "    print(\"[PASS] PERFECT REPRODUCIBILITY WITH INT8 + YaRN\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Full top-k distributions: bit-exact\")\n",
    "    print(\"  => INT8 quantization + YaRN maintains determinism\")\n",
    "elif tokens_identical and logprobs_exact and not distributions_exact:\n",
    "    print(\"[WARNING] SELECTED TOKENS EXACT, DISTRIBUTIONS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Top-k distributions: numerical variation\")\n",
    "    print(\"  => INT8 + YaRN introduces distribution-level noise\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"[WARNING] TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: numerical variation\")\n",
    "    print(\"  => INT8 + YaRN introduces FP variation in computations\")\n",
    "else:\n",
    "    print(\"[FAIL] TOKEN SEQUENCES DIFFER - DETERMINISM BROKEN\")\n",
    "    print(\"  - INT8 quantization + YaRN breaks greedy decoding determinism\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"h100_int8_determinism_test_bitsandbytes_qwen3_32b_yarn\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"hardware\": \"H100\",\n",
    "    \"prompt_source\": \"file\" if prompt_file else \"hardcoded\",\n",
    "    \"prompt_file\": os.path.basename(prompt_file) if prompt_file else None,\n",
    "    \"prompt_text\": prompt_text,\n",
    "    \"prompt_length_chars\": len(prompt_text),\n",
    "    \"prompt_length_tokens\": prompt_length,\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"quantization\": QUANTIZATION,\n",
    "        \"precision\": \"INT8\",\n",
    "        \"yarn_enabled\": YARN_ENABLED,\n",
    "        \"yarn_factor\": YARN_FACTOR,\n",
    "        \"yarn_original_max\": YARN_ORIGINAL_MAX,\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_context_len\": max_context_len,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"warmup_runs\": NUM_WARMUP_RUNS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"top_logprobs\": TOP_LOGPROBS,\n",
    "    },\n",
    "    \"timing\": {\n",
    "        \"model_load_time\": load_time,\n",
    "        \"warmup_times\": warmup_times,\n",
    "        \"per_repetition\": timing_data,\n",
    "        \"statistics\": {\n",
    "            \"mean_time\": float(np.mean(times)) if times else None,\n",
    "            \"std_time\": float(np.std(times)) if times else None,\n",
    "            \"mean_tokens_per_sec\": float(np.mean(tps)) if tps else None,\n",
    "            \"std_tokens_per_sec\": float(np.std(tps)) if tps else None,\n",
    "            \"mean_time_per_token_ms\": float(np.mean(tpt) * 1000) if tpt else None,\n",
    "            \"std_time_per_token_ms\": float(np.std(tpt) * 1000) if tpt else None,\n",
    "        },\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"distributions_exact\": distributions_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact and distributions_exact,\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts,\n",
    "}\n",
    "\n",
    "output_file = f\"h100_int8_determinism_qwen3_32b_yarn_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ac5177-3e32-416b-b2dd-055fdeb41abc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
