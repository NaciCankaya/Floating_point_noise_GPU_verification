{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86b2e201-4f26-4916-a6db-b21e93555d74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UNIFIED QUANTIZATION NON-DETERMINISM INVESTIGATION\n",
      "================================================================================\n",
      "\n",
      "Testing 5 variant(s): awq_marlin, awq, gptq_marlin, gptq, pytorch_int4\n",
      "Output directory: .\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STARTING: AWQ_MARLIN\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUANTIZATION INVESTIGATION: AWQ_MARLIN\n",
      "================================================================================\n",
      "\n",
      "Description: AWQ INT4 with Marlin kernel\n",
      "Model: Qwen/Qwen3-8B-AWQ\n",
      "Quantization: awq_marlin\n",
      "Prompt length: 23143 chars\n",
      "Repetitions per mode: 5\n",
      "\n",
      "Investigation plan:\n",
      "  1. Baseline: Standard vLLM generation (non-deterministic expected)\n",
      "  2. With determinism: torch.use_deterministic_algorithms(True)\n",
      "  3. Kernel profiling: Identify which kernels are called\n",
      "  4. Analysis: Compare noise levels and kernel usage\n",
      "\n",
      "Preparing prompt...\n",
      "✓ Prompt prepared: 3790 tokens\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 1: BASELINE (Standard vLLM)\n",
      "================================================================================\n",
      "\n",
      "✓ Deterministic mode DISABLED (standard vLLM)\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0.dev20251112+cu128             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:17<00:17, 17.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:18<00:00,  7.66s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:18<00:00,  9.11s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3491)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=3491)\u001b[0;0m 2025-11-12 15:58:24,673 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3491)\u001b[0;0m 2025-11-12 15:58:25,895 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3491)\u001b[0;0m cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "\n",
      "Warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1c27afb0574c3296ab4308e164936a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e15cfa3f4294ea4aeb7cd7e1f0e8f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb7a9b42b1b4684b73993a5be60102f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ab2a221b944d0894773307cccdbe3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c9277ae71c84e38958cb0b09ff83b7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236128ead15d432a8e4e6941fb20d69b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Warmup complete\n",
      "\n",
      "Running 5 repetitions in BASELINE mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b3bee0f352e4a4bb3390565a0cebdd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0f65efb6ec04061aabac74f87a6e48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6177f5562cdb4cfda884b4875d6fddcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d58dff590e14c27a6ba1c86e1fca081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b63dc68e5041a2991afc308f877fdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56adcbe3f9f74e76ab95fd5c495d85f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98aa1c5c9e84cc99e627c8cff3d1cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b00a952554e4919aa8e484fb8410b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6853738dbe924929bb5b46beba19a431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77eb4beaed774d98a4e22b35d8a4b123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed 5/5 repetitions\n",
      "✓ BASELINE mode complete: 5 repetitions\n",
      "\n",
      "Analysis: BASELINE\n",
      "------------------------------------------------------------\n",
      "Token sequences identical: True\n",
      "Logprobs bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "KERNEL PROFILING\n",
      "================================================================================\n",
      "\n",
      "Profiling kernel calls during generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e2a9728525483f810af557bdf5691e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa00f6e919ef42999a8ad7c85afeecd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 kernels by CUDA time:\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "      cudaDeviceSynchronize        12.21%      21.572us        12.21%      21.572us      21.572us             1  \n",
      "    Activity Buffer Request        87.79%     155.034us        87.79%     155.034us     155.034us             1  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 176.606us\n",
      "\n",
      "\n",
      "Kernel categories detected:\n",
      "  Marlin kernels: 0\n",
      "  GEMM kernels: 0\n",
      "  Dequant kernels: 0\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 2: WITH DETERMINISTIC MODE\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1112 15:58:38.979246809 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deterministic mode ENABLED\n",
      "\n",
      "Reloading model with deterministic settings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0.dev20251112+cu128             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:17<00:17, 17.96s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:18<00:00,  7.80s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:18<00:00,  9.33s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3956)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=3956)\u001b[0;0m 2025-11-12 15:59:10,341 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3956)\u001b[0;0m 2025-11-12 15:59:11,563 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3956)\u001b[0;0m cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded with deterministic mode\n",
      "\n",
      "Warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f32a4211ace40b7a113cc3e4e1ac19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b341fb1eb0249178c56cfbf93fa9451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228114ec74b344d6ba3d728d06759aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1634802ef446b8a9b462de776634a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d88192a92db442ca9618eff6a6bc93c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae3d48e0d854205af662779eb6c6396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Warmup complete\n",
      "\n",
      "Running 5 repetitions in DETERMINISTIC mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "710b1764e7484df5ba146211b1b590f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75dd166076144e12affba7d7fff2dd29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0b92b64151486dae4668052f2853c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69cd3329c10b4e89afb2cb9b394d8eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5c4ca7cb69b4053a103e4106ee4203d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a4f31a69f0545f496657ba5ed8143be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648689070011419389d35dc062316591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f218884c0ba4f27825dc9383b435525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13422394dad84443a4f38b87e9b87ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69756424fd64a8892f6a96cf0fefbf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed 5/5 repetitions\n",
      "✓ DETERMINISTIC mode complete: 5 repetitions\n",
      "\n",
      "Analysis: DETERMINISTIC\n",
      "------------------------------------------------------------\n",
      "Token sequences identical: True\n",
      "Logprobs bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "COMPARATIVE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Baseline vs Deterministic Mode:\n",
      "------------------------------------------------------------\n",
      "Metric                         Baseline             Deterministic       \n",
      "------------------------------------------------------------\n",
      "Tokens identical               True                 True                \n",
      "Logprobs exact                 True                 True                \n",
      "Mean L2 distance               0.000000e+00  0.000000e+00\n",
      "Max L2 distance                0.000000e+00  0.000000e+00\n",
      "Per-token std (mean)           0.000000e+00  0.000000e+00\n",
      "Per-token std (max)            0.000000e+00  0.000000e+00\n",
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "\n",
      "BASELINE IS DETERMINISTIC\n",
      "  Unexpected for awq_marlin! May have been fixed in recent vLLM version\n",
      "\n",
      "Root cause: Fully deterministic behavior\n",
      "\n",
      "Forensic implications:\n",
      "  Perfect reproducibility - any deviation indicates computational change\n",
      "\n",
      "Results saved to: ./awq_marlin_investigation_20251112_155922.json\n",
      "\n",
      "================================================================================\n",
      "INVESTIGATION COMPLETE: AWQ_MARLIN\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1112 15:59:22.321452506 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STARTING: AWQ\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUANTIZATION INVESTIGATION: AWQ\n",
      "================================================================================\n",
      "\n",
      "Description: AWQ INT4 without Marlin kernel\n",
      "Model: Qwen/Qwen3-8B-AWQ\n",
      "Quantization: awq\n",
      "Prompt length: 23143 chars\n",
      "Repetitions per mode: 5\n",
      "\n",
      "Investigation plan:\n",
      "  1. Baseline: Standard vLLM generation (non-deterministic expected)\n",
      "  2. With determinism: torch.use_deterministic_algorithms(True)\n",
      "  3. Kernel profiling: Identify which kernels are called\n",
      "  4. Analysis: Compare noise levels and kernel usage\n",
      "\n",
      "Preparing prompt...\n",
      "✓ Prompt prepared: 3790 tokens\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 1: BASELINE (Standard vLLM)\n",
      "================================================================================\n",
      "\n",
      "✓ Deterministic mode DISABLED (standard vLLM)\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0.dev20251112+cu128             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:17<00:17, 17.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:18<00:00,  7.81s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:18<00:00,  9.30s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4425)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=4425)\u001b[0;0m 2025-11-12 15:59:54,344 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4425)\u001b[0;0m 2025-11-12 15:59:55,013 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4425)\u001b[0;0m cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "\n",
      "Warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378483ee68db4d488ae8b55b64274897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71eab0bbbdf04e139f6b69bcd4c707a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a503e1c7464dc88ac639530efddae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807d437a56074e67819c98d80fc506d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4668fb2eb64c4e62a141b55e55000126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddb4f9bca82246acab99058ab63b5986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Warmup complete\n",
      "\n",
      "Running 5 repetitions in BASELINE mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e55ed3d2b64fe9a92a1f45be4fd371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6dce516f7154f2db02946af2619907a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be93d14863c4bdca27e4c41cca5155d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5937d412927d477a946262e9fed9a470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a4ee9d69d5549479b454bb4b3baec0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066e1bc866c1453fac05620693406569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f5270dd544411b889733e68df0e38b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189783adb8a043909b569d97a9ac090f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99dd61e35add400e8d0c8c5752e5b02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "453e20daafa44abcbc3e386b8befc7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed 5/5 repetitions\n",
      "✓ BASELINE mode complete: 5 repetitions\n",
      "\n",
      "Analysis: BASELINE\n",
      "------------------------------------------------------------\n",
      "Token sequences identical: True\n",
      "Logprobs bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "KERNEL PROFILING\n",
      "================================================================================\n",
      "\n",
      "Profiling kernel calls during generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eab39527ea342879ba77c43ff12e742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8851f18f9687440e93ccd3f83eebf780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 kernels by CUDA time:\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "      cudaDeviceSynchronize        12.98%      21.292us        12.98%      21.292us      21.292us             1  \n",
      "    Activity Buffer Request        87.02%     142.765us        87.02%     142.765us     142.765us             1  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 164.057us\n",
      "\n",
      "\n",
      "Kernel categories detected:\n",
      "  Marlin kernels: 0\n",
      "  GEMM kernels: 0\n",
      "  Dequant kernels: 0\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 2: WITH DETERMINISTIC MODE\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1112 16:00:07.917140625 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deterministic mode ENABLED\n",
      "\n",
      "Reloading model with deterministic settings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0.dev20251112+cu128             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:16<00:16, 16.73s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:17<00:00,  7.35s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:17<00:00,  8.75s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5030)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=5030)\u001b[0;0m 2025-11-12 16:00:37,509 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5030)\u001b[0;0m 2025-11-12 16:00:38,180 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5030)\u001b[0;0m cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded with deterministic mode\n",
      "\n",
      "Warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c7bf4e6ff7a4826b5571b6da6b5c638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78e5321e625143ccb5e49ee01e6d7333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3da5cdf7c64a4d9fbc0b60e29c755a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d13a210c71433bb56a40fc4fce5595",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fcd35f20f94a8695f88a2ce4bbc085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12236310b4344692b3553132cda3a4ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Warmup complete\n",
      "\n",
      "Running 5 repetitions in DETERMINISTIC mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "633bc3e2b0084181a2753138ed6925ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07347e066560412683f7ab007343029b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10918631b90c438fa38eb5b7e527f274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b6d216cded4cf383c017e87a3f31ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f7fb0ce76354e10a4c692bd6e3ea7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79dae19f4c64e4bb438940dc50df02a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b50ae063e5dd425caff1c7f2c1ab1443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6f122f5e1d94eaeb489ff629682139d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58f55957bdec4ddda180f5e07f12a629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e40e9db9374c2399dd9c2e20e856fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed 5/5 repetitions\n",
      "✓ DETERMINISTIC mode complete: 5 repetitions\n",
      "\n",
      "Analysis: DETERMINISTIC\n",
      "------------------------------------------------------------\n",
      "Token sequences identical: True\n",
      "Logprobs bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "COMPARATIVE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Baseline vs Deterministic Mode:\n",
      "------------------------------------------------------------\n",
      "Metric                         Baseline             Deterministic       \n",
      "------------------------------------------------------------\n",
      "Tokens identical               True                 True                \n",
      "Logprobs exact                 True                 True                \n",
      "Mean L2 distance               0.000000e+00  0.000000e+00\n",
      "Max L2 distance                0.000000e+00  0.000000e+00\n",
      "Per-token std (mean)           0.000000e+00  0.000000e+00\n",
      "Per-token std (max)            0.000000e+00  0.000000e+00\n",
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "\n",
      "BASELINE IS DETERMINISTIC\n",
      "  Unexpected for awq! May have been fixed in recent vLLM version\n",
      "\n",
      "Root cause: Fully deterministic behavior\n",
      "\n",
      "Forensic implications:\n",
      "  Perfect reproducibility - any deviation indicates computational change\n",
      "\n",
      "Results saved to: ./awq_investigation_20251112_160049.json\n",
      "\n",
      "================================================================================\n",
      "INVESTIGATION COMPLETE: AWQ\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1112 16:00:49.770949320 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STARTING: GPTQ_MARLIN\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUANTIZATION INVESTIGATION: GPTQ_MARLIN\n",
      "================================================================================\n",
      "\n",
      "Description: GPTQ INT4 with Marlin kernel\n",
      "Model: JunHowie/Qwen3-8B-GPTQ-Int4\n",
      "Quantization: gptq_marlin\n",
      "Prompt length: 23143 chars\n",
      "Repetitions per mode: 5\n",
      "\n",
      "Investigation plan:\n",
      "  1. Baseline: Standard vLLM generation (non-deterministic expected)\n",
      "  2. With determinism: torch.use_deterministic_algorithms(True)\n",
      "  3. Kernel profiling: Identify which kernels are called\n",
      "  4. Analysis: Compare noise levels and kernel usage\n",
      "\n",
      "Preparing prompt...\n",
      "✓ Prompt prepared: 3790 tokens\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 1: BASELINE (Standard vLLM)\n",
      "================================================================================\n",
      "\n",
      "✓ Deterministic mode DISABLED (standard vLLM)\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9844c529dd2d411f95be27f8707238b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0.dev20251112+cu128             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.04it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.16it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5635)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=5635)\u001b[0;0m 2025-11-12 16:01:04,244 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5635)\u001b[0;0m 2025-11-12 16:01:04,823 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5635)\u001b[0;0m cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "\n",
      "Warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13df33bc1e104e21993f9d6f0b379478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cba505243614f1983f812f4021b3e89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b960f1e950034d2ca45fb83752b7c6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17678c7bc3944daf8a0fc28242783ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88fad024f4e04e81b9992eb2ba056b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1552b24f7d7b4cf7a9257934e4094cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Warmup complete\n",
      "\n",
      "Running 5 repetitions in BASELINE mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93a10d37baf456cb2134bacbe3506fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fcf99c39fa74b9390a247f7fee1c6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c2029546c04424a637ba921c42ccce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6abdeba6f144cca1bbe5533fa2bd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb89bcc256348deafedffd16b7d3a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b682f4898c2a4e7683aa4be24dd2cd0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5624ae9abbc14ad8827c34ecb55bb003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc4d6d3e67f4669bd58e98b3ae2394e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de28240cb0af4d3eb15c1fd34fa6a8a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caac12ddeba479c93c624940f5158e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed 5/5 repetitions\n",
      "✓ BASELINE mode complete: 5 repetitions\n",
      "\n",
      "Analysis: BASELINE\n",
      "------------------------------------------------------------\n",
      "Token sequences identical: True\n",
      "Logprobs bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "KERNEL PROFILING\n",
      "================================================================================\n",
      "\n",
      "Profiling kernel calls during generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adec5a4446f54eafa978efdf297d19d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8f116aa9f6a4020a7c32c5480ff462f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 kernels by CUDA time:\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "      cudaDeviceSynchronize        10.75%      17.096us        10.75%      17.096us      17.096us             1  \n",
      "    Activity Buffer Request        89.25%     141.964us        89.25%     141.964us     141.964us             1  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 159.060us\n",
      "\n",
      "\n",
      "Kernel categories detected:\n",
      "  Marlin kernels: 0\n",
      "  GEMM kernels: 0\n",
      "  Dequant kernels: 0\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 2: WITH DETERMINISTIC MODE\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1112 16:01:16.769514758 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deterministic mode ENABLED\n",
      "\n",
      "Reloading model with deterministic settings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d91e68fa0614ed98431fe47514857fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0.dev20251112+cu128             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.00it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6218)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=6218)\u001b[0;0m 2025-11-12 16:01:30,754 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6218)\u001b[0;0m 2025-11-12 16:01:31,349 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6218)\u001b[0;0m cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded with deterministic mode\n",
      "\n",
      "Warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538333a91cc04fd79ecedc78ba0a8dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14f3455b5a2482a86d5a53a395dbc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fbfd17ed7d442c7ba978f1e88469eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8634d1d38d914067b0b17a48f92c1582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "559c23afabaf407aa7b16c097372c3dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d6ee96fa3bc4a488cc347d5f78c2540",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Warmup complete\n",
      "\n",
      "Running 5 repetitions in DETERMINISTIC mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1db183b73e045af9996a9b8fdb19df0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ecef5d10f840f2adc3bbd65bba559b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e248ac1a028e4881819fcacd65f3efe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cd1751328b43f897b67757daaf870d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9a0f139486d4bc998858fc38845448a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a6c7ca22e77490380a87e7c62cdbfff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f6e4d9c5280445cad7ff318b5472236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df1878f6dd61439bb3c7d0bb986bafe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faaf3707318a45138ae5d6a1195d42e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ad67e74c454eeca9bcfbe8b40d2aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed 5/5 repetitions\n",
      "✓ DETERMINISTIC mode complete: 5 repetitions\n",
      "\n",
      "Analysis: DETERMINISTIC\n",
      "------------------------------------------------------------\n",
      "Token sequences identical: True\n",
      "Logprobs bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "COMPARATIVE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Baseline vs Deterministic Mode:\n",
      "------------------------------------------------------------\n",
      "Metric                         Baseline             Deterministic       \n",
      "------------------------------------------------------------\n",
      "Tokens identical               True                 True                \n",
      "Logprobs exact                 True                 True                \n",
      "Mean L2 distance               0.000000e+00  0.000000e+00\n",
      "Max L2 distance                0.000000e+00  0.000000e+00\n",
      "Per-token std (mean)           0.000000e+00  0.000000e+00\n",
      "Per-token std (max)            0.000000e+00  0.000000e+00\n",
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "\n",
      "BASELINE IS DETERMINISTIC\n",
      "  Unexpected for gptq_marlin! May have been fixed in recent vLLM version\n",
      "\n",
      "Root cause: Fully deterministic behavior\n",
      "\n",
      "Forensic implications:\n",
      "  Perfect reproducibility - any deviation indicates computational change\n",
      "\n",
      "Results saved to: ./gptq_marlin_investigation_20251112_160141.json\n",
      "\n",
      "================================================================================\n",
      "INVESTIGATION COMPLETE: GPTQ_MARLIN\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1112 16:01:41.060377516 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STARTING: GPTQ\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUANTIZATION INVESTIGATION: GPTQ\n",
      "================================================================================\n",
      "\n",
      "Description: GPTQ INT4 without Marlin kernel\n",
      "Model: JunHowie/Qwen3-8B-GPTQ-Int4\n",
      "Quantization: gptq\n",
      "Prompt length: 23143 chars\n",
      "Repetitions per mode: 5\n",
      "\n",
      "Investigation plan:\n",
      "  1. Baseline: Standard vLLM generation (non-deterministic expected)\n",
      "  2. With determinism: torch.use_deterministic_algorithms(True)\n",
      "  3. Kernel profiling: Identify which kernels are called\n",
      "  4. Analysis: Compare noise levels and kernel usage\n",
      "\n",
      "Preparing prompt...\n",
      "✓ Prompt prepared: 3790 tokens\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 1: BASELINE (Standard vLLM)\n",
      "================================================================================\n",
      "\n",
      "✓ Deterministic mode DISABLED (standard vLLM)\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b872ac863684c82ba71bfc3a5523bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0.dev20251112+cu128             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.03it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.07it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6785)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=6785)\u001b[0;0m 2025-11-12 16:01:57,393 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6785)\u001b[0;0m 2025-11-12 16:01:58,039 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6785)\u001b[0;0m cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "\n",
      "Warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d50279f35242b4bf408f525c206e29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9049747c4d5d46caaef893715d0c10d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0697768269dc443cb31e32f7a0fddadb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8a00671cf12465cb3dde051c0253184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b965471d90478fbfc1afac38bd33fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0cb1be9104248e6a5f5017fbfb3907d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Warmup complete\n",
      "\n",
      "Running 5 repetitions in BASELINE mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de45da058164ab08ae8f587050875a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306fd3c0771040419f64ce8cffc004fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299160611697437a88392a43eb708d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f2993a504d74d4d9a0030e60f967aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8488b2da1f34634a26df45a3d2469bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca493105bfab45f5a665cdeb1061aadc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "163ef807eee24da19bf40799cb402790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c871fdd46194aecb1cbb0d27817e20d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ffb649fb1945dab935766bdf2af214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15d5ee76d2b64ffbb5f59ed7945f27b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed 5/5 repetitions\n",
      "✓ BASELINE mode complete: 5 repetitions\n",
      "\n",
      "Analysis: BASELINE\n",
      "------------------------------------------------------------\n",
      "Token sequences identical: True\n",
      "Logprobs bit-exact: False\n",
      "\n",
      "Logprob deviations:\n",
      "  Mean L2: 2.969496e-02\n",
      "  Std L2:  9.641845e-04\n",
      "  Min L2:  2.871477e-02\n",
      "  Max L2:  3.109406e-02\n",
      "\n",
      "Per-token std:\n",
      "  Mean: 1.386454e-03\n",
      "  Max:  7.167924e-03\n",
      "  Min:  0.000000e+00\n",
      "\n",
      "================================================================================\n",
      "KERNEL PROFILING\n",
      "================================================================================\n",
      "\n",
      "Profiling kernel calls during generation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0084ebc19bf4d178828a2dc3fc9e1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bfa6887b134792ba5807eeaf2c7ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 kernels by CUDA time:\n",
      "--------------------------------------------------------------------------------\n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                       Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "      cudaDeviceSynchronize        12.69%      20.851us        12.69%      20.851us      20.851us             1  \n",
      "    Activity Buffer Request        87.31%     143.486us        87.31%     143.486us     143.486us             1  \n",
      "---------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 164.337us\n",
      "\n",
      "\n",
      "Kernel categories detected:\n",
      "  Marlin kernels: 0\n",
      "  GEMM kernels: 0\n",
      "  Dequant kernels: 0\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 2: WITH DETERMINISTIC MODE\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1112 16:02:09.821377285 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Deterministic mode ENABLED\n",
      "\n",
      "Reloading model with deterministic settings...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "343791c49b9c4e918c7a58d32890240f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0.dev20251112+cu128             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:01<00:01,  1.00s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.12it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7377)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=7377)\u001b[0;0m 2025-11-12 16:02:24,026 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7377)\u001b[0;0m 2025-11-12 16:02:24,670 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7377)\u001b[0;0m cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded with deterministic mode\n",
      "\n",
      "Warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70394f92f7eb4de1a0ef9099657eaf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4ffb1e7e8b4a869cf175966abe7890",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "648d46b85497434bb9e18e7553d3c31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cf3c49d5f8f4cdfaa4510977bed9497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35db75fda32f4f43b57c5f996a7280f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9df8edd613e44bba2fc526b108891d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Warmup complete\n",
      "\n",
      "Running 5 repetitions in DETERMINISTIC mode...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667e44de9ccd40b3a9883fd739921b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "914103d16d6a482ba708625a07a367ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcf2064527448acb42c5ea9587a4ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e16f43aec7c845a6b42e4f2e0b056bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba7ce295ed648aeaa1f50443c4c5196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fcdb868b958477e971bdb7995a9f243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58c52445745748a8bdc195d7b90b32b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9c8c8a330e24ac692923571e23f0c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a31612a5c284874a8a5b9a4da9c7e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eace2e4024941c7a0cf536de2f613c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Completed 5/5 repetitions\n",
      "✓ DETERMINISTIC mode complete: 5 repetitions\n",
      "\n",
      "Analysis: DETERMINISTIC\n",
      "------------------------------------------------------------\n",
      "Token sequences identical: True\n",
      "Logprobs bit-exact: False\n",
      "\n",
      "Logprob deviations:\n",
      "  Mean L2: 2.405646e-02\n",
      "  Std L2:  4.068370e-03\n",
      "  Min L2:  1.937821e-02\n",
      "  Max L2:  2.974236e-02\n",
      "\n",
      "Per-token std:\n",
      "  Mean: 1.219978e-03\n",
      "  Max:  6.271982e-03\n",
      "  Min:  0.000000e+00\n",
      "\n",
      "================================================================================\n",
      "COMPARATIVE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Baseline vs Deterministic Mode:\n",
      "------------------------------------------------------------\n",
      "Metric                         Baseline             Deterministic       \n",
      "------------------------------------------------------------\n",
      "Tokens identical               True                 True                \n",
      "Logprobs exact                 False                False               \n",
      "Mean L2 distance               2.969496e-02  2.405646e-02\n",
      "Max L2 distance                3.109406e-02  2.974236e-02\n",
      "Per-token std (mean)           1.386454e-03  1.219978e-03\n",
      "Per-token std (max)            7.167924e-03  6.271982e-03\n",
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "\n",
      "BASELINE IS NON-DETERMINISTIC\n",
      "  Mean L2 deviation: 2.969496e-02\n",
      "  Max L2 deviation: 3.109406e-02\n",
      "  ❌ DETERMINISTIC MODE DOESN'T HELP\n",
      "\n",
      "Root cause: Non-determinism NOT from parallel reduction races, likely in quantization/dequantization logic\n",
      "\n",
      "Forensic implications:\n",
      "  Noise level: ~2.97e-02 L2. For detection, need systematic deviation >> noise. Recommend: 3-5 samples for statistical significance\n",
      "\n",
      "Results saved to: ./gptq_investigation_20251112_160234.json\n",
      "\n",
      "================================================================================\n",
      "INVESTIGATION COMPLETE: GPTQ\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1112 16:02:34.328554251 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n",
      "Downloading 'tokenizer_config.json' to '/tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/ddaf69808214a44fdd26d3785b66c1367c78277a.incomplete'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "STARTING: PYTORCH_INT4\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "QUANTIZATION INVESTIGATION: PYTORCH_INT4\n",
      "================================================================================\n",
      "\n",
      "Description: PyTorch TorchAO INT4 (HQQ algorithm)\n",
      "Model: pytorch/Qwen3-8B-INT4\n",
      "Quantization: torchao\n",
      "Prompt length: 23143 chars\n",
      "Repetitions per mode: 5\n",
      "\n",
      "Investigation plan:\n",
      "  1. Baseline: Standard vLLM generation (non-deterministic expected)\n",
      "  2. With determinism: torch.use_deterministic_algorithms(True)\n",
      "  3. Kernel profiling: Identify which kernels are called\n",
      "  4. Analysis: Compare noise levels and kernel usage\n",
      "\n",
      "Preparing prompt...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad6748ff29749d3990bbeb5b56270cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/ddaf69808214a44fdd26d3785b66c1367c78277a\n",
      "Downloading 'vocab.json' to '/tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/4783fe10ac3adce15ac8f358ef5462739852c569.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1884de70274644bd683b17b6a61ff5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/4783fe10ac3adce15ac8f358ef5462739852c569\n",
      "Downloading 'merges.txt' to '/tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac4c65af6424bc2a0dca5cab415b4ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7\n",
      "Downloading 'tokenizer.json' to '/tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd4716281f974706a482fd3d086ce19d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4\n",
      "Downloading 'added_tokens.json' to '/tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/b54f9135e44c1e81047e8d05cb027af8bc039eed.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1158cea1d4e044478f285503ef3e10c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/b54f9135e44c1e81047e8d05cb027af8bc039eed\n",
      "Downloading 'special_tokens_map.json' to '/tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/ac23c0aaa2434523c494330aeb79c58395378103.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41d8a0d3644f4ab8b40694688aad82b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/ac23c0aaa2434523c494330aeb79c58395378103\n",
      "Downloading 'chat_template.jinja' to '/tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/01be9b307daa2d425f7c168c9fb145a286e0afb4.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60084a2b222742f29451b53c0908e4e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/01be9b307daa2d425f7c168c9fb145a286e0afb4\n",
      "Downloading 'config.json' to '/tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/bf3ed02ac7f8e69f6d7f55ebd6dd60d9feab8793.incomplete'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt prepared: 3790 tokens\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 1: BASELINE (Standard vLLM)\n",
      "================================================================================\n",
      "\n",
      "✓ Deterministic mode DISABLED (standard vLLM)\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0648fdd003914428b484b4d580fe1146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/bf3ed02ac7f8e69f6d7f55ebd6dd60d9feab8793\n",
      "Downloading 'generation_config.json' to '/tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/ec7a19e7f3011ec97a691b6533697b418d4e046a.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb430dd042ed460ca2153788ee319575",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--pytorch--Qwen3-8B-INT4/blobs/ec7a19e7f3011ec97a691b6533697b418d4e046a\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Skipping import of cpp extensions due to incompatible torch version 2.8.0+cu128 for torchao version 0.15.0.dev20251112+cu128             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 55, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.collective_rpc(\"load_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 2635, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.model = model_loader.load_model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 45, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     model = initialize_model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py\", line 63, in initialize_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3.py\", line 286, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.model = Qwen3Model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 201, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3.py\", line 258, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     super().__init__(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 201, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 319, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                                                     ^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 630, in make_layers\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 321, in <lambda>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     lambda prefix: decoder_layer_type(config=config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3.py\", line 188, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.self_attn = Qwen3Attention(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3.py\", line 97, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 918, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     super().__init__(input_size=input_size,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 476, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.quant_method.create_weights(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/torchao.py\", line 200, in create_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     weight = torchao_quantize_param_data(weight,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/torchao.py\", line 167, in torchao_quantize_param_data\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     quantize_(dummy_linear, torchao_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 496, in quantize_\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     _replace_with_custom_fn_if_matches_filter(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 212, in _replace_with_custom_fn_if_matches_filter\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     new_child = _replace_with_custom_fn_if_matches_filter(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 207, in _replace_with_custom_fn_if_matches_filter\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     model = replacement_fn(model, *extra_args)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 1273, in _int4_weight_only_transform\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     new_weight = _int4_weight_only_quantize_tensor(module.weight, config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 1193, in _int4_weight_only_quantize_tensor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     new_weight = Int4TilePackedTo4dTensor.from_hp(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quantize_/workflows/int4/int4_tile_packed_to_4d_tensor.py\", line 111, in from_hp\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     assert hp_tensor.dtype == torch.bfloat16, (\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m AssertionError: Only bfloat16 is supported for Int4TilePackedTo4dTensor, got torch.float16\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 55, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.collective_rpc(\"load_model\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 213, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.model_runner.load_model(eep_scale_up=eep_scale_up)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_model_runner.py\", line 2635, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.model = model_loader.load_model(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/base_loader.py\", line 45, in load_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     model = initialize_model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/model_loader/utils.py\", line 63, in initialize_model\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     return model_class(vllm_config=vllm_config, prefix=prefix)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3.py\", line 286, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.model = Qwen3Model(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 201, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3.py\", line 258, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     super().__init__(vllm_config=vllm_config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/compilation/decorators.py\", line 201, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     old_init(self, vllm_config=vllm_config, prefix=prefix, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 319, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.start_layer, self.end_layer, self.layers = make_layers(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                                                     ^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 630, in make_layers\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen2.py\", line 321, in <lambda>\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     lambda prefix: decoder_layer_type(config=config,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3.py\", line 188, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.self_attn = Qwen3Attention(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                      ^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/qwen3.py\", line 97, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.qkv_proj = QKVParallelLinear(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                     ^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 918, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     super().__init__(input_size=input_size,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/linear.py\", line 476, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     self.quant_method.create_weights(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/torchao.py\", line 200, in create_weights\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     weight = torchao_quantize_param_data(weight,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/layers/quantization/torchao.py\", line 167, in torchao_quantize_param_data\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     quantize_(dummy_linear, torchao_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 496, in quantize_\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     _replace_with_custom_fn_if_matches_filter(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 212, in _replace_with_custom_fn_if_matches_filter\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     new_child = _replace_with_custom_fn_if_matches_filter(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 207, in _replace_with_custom_fn_if_matches_filter\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     model = replacement_fn(model, *extra_args)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 1273, in _int4_weight_only_transform\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     new_weight = _int4_weight_only_quantize_tensor(module.weight, config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quant_api.py\", line 1193, in _int4_weight_only_quantize_tensor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     new_weight = Int4TilePackedTo4dTensor.from_hp(\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torchao/quantization/quantize_/workflows/int4/int4_tile_packed_to_4d_tensor.py\", line 111, in from_hp\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m     assert hp_tensor.dtype == torch.bfloat16, (\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7967)\u001b[0;0m AssertionError: Only bfloat16 is supported for Int4TilePackedTo4dTensor, got torch.float16\n",
      "[rank0]:[W1112 16:02:47.087369138 ProcessGroupNCCL.cpp:1538] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Failed to load model: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ALL EXPERIMENTS COMPLETE\n",
      "================================================================================\n",
      "\n",
      "Summary:\n",
      "------------------------------------------------------------\n",
      "  awq_marlin      - ✓ DETERMINISTIC\n",
      "  awq             - ✓ DETERMINISTIC\n",
      "  gptq_marlin     - ✓ DETERMINISTIC\n",
      "  gptq            - ❌ NON-DETERMINISTIC\n",
      "                     Noise level: 2.97e-02 L2\n",
      "  pytorch_int4    - ❌ FAILED: Engine core initialization failed. See root cause above. Failed core proc(s): {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Unified Quantization Non-Determinism Investigation\n",
    "Tests vLLM INT4 quantization reproducibility across different formats with kernel profiling\n",
    "\n",
    "Supports multiple quantization formats:\n",
    "- AWQ (with/without Marlin)\n",
    "- GPTQ (with/without Marlin)\n",
    "- OpenVINO INT4\n",
    "- PyTorch INT4\n",
    "\n",
    "Focus: Identify whether different quantization methods have non-deterministic behavior\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SUPPRESS VERBOSE LOGGING\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "os.environ['VLLM_LOGGING_LEVEL'] = 'WARNING'\n",
    "os.environ['VLLM_CONFIGURE_LOGGING'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger('vllm').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.engine').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.worker').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.executor').setLevel(logging.ERROR)\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "logging.getLogger('torch').setLevel(logging.ERROR)\n",
    "logging.getLogger('huggingface_hub').setLevel(logging.INFO)\n",
    "logging.getLogger('huggingface_hub.file_download').setLevel(logging.INFO)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "import argparse\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Try to import new profiler API, fall back to old if needed\n",
    "try:\n",
    "    from torch.profiler import profile, ProfilerActivity\n",
    "    PROFILER_NEW_API = True\n",
    "except ImportError:\n",
    "    import torch.autograd.profiler as profiler_module\n",
    "    PROFILER_NEW_API = False\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL CONFIGURATIONS\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_CONFIGS = {\n",
    "    \"awq_marlin\": {\n",
    "        \"model_name\": \"Qwen/Qwen3-8B-AWQ\",\n",
    "        \"quantization\": \"awq_marlin\",\n",
    "        \"description\": \"AWQ INT4 with Marlin kernel\"\n",
    "    },\n",
    "    \"awq\": {\n",
    "        \"model_name\": \"Qwen/Qwen3-8B-AWQ\",\n",
    "        \"quantization\": \"awq\",\n",
    "        \"description\": \"AWQ INT4 without Marlin kernel\"\n",
    "    },\n",
    "    \"gptq_marlin\": {\n",
    "        \"model_name\": \"JunHowie/Qwen3-8B-GPTQ-Int4\",\n",
    "        \"quantization\": \"gptq_marlin\",\n",
    "        \"description\": \"GPTQ INT4 with Marlin kernel\"\n",
    "    },\n",
    "    \"gptq\": {\n",
    "        \"model_name\": \"JunHowie/Qwen3-8B-GPTQ-Int4\",\n",
    "        \"quantization\": \"gptq\",\n",
    "        \"description\": \"GPTQ INT4 without Marlin kernel\"\n",
    "    },\n",
    "    \"pytorch_int4\": {\n",
    "        \"model_name\": \"pytorch/Qwen3-8B-INT4\",\n",
    "        \"quantization\": \"torchao\",\n",
    "        \"description\": \"PyTorch TorchAO INT4 (HQQ algorithm)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TENSOR_PARALLEL_SIZE = 1\n",
    "MAX_MODEL_LEN = 8192\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "TORCH_DTYPE = \"float16\"  # Consistent dtype across all experiments\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 50\n",
    "NUM_REPETITIONS = 5\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Test prompt (long prompt for comprehensive testing)\n",
    "TEST_PROMPT = \"\"\"The Evolution of Large Language Models: Technical Foundations and Societal Implications\n",
    "\n",
    "Introduction\n",
    "\n",
    "The field of artificial intelligence has witnessed a remarkable transformation over the past decade, driven primarily by advances in deep learning and the emergence of increasingly sophisticated language models. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing. However, their deployment raises significant challenges related to computational efficiency, interpretability, and safety.\n",
    "\n",
    "This document explores the technical foundations of modern large language models, their architectural innovations, the computational infrastructure required for their deployment, and the broader implications for AI governance and safety. We examine how these systems process information, the sources of variation in their outputs, and the methods being developed to ensure their reliable and safe operation at scale.\n",
    "\n",
    "Part I: Architectural Foundations\n",
    "\n",
    "1.1 The Transformer Architecture\n",
    "\n",
    "At the heart of modern language models lies the transformer architecture, introduced in 2017 by Vaswani et al. The transformer's key innovation was the self-attention mechanism, which allows models to dynamically weigh the importance of different parts of their input when processing each element. Unlike recurrent neural networks, transformers can process sequences in parallel, enabling efficient training on modern hardware.\n",
    "\n",
    "The self-attention mechanism computes attention scores between all pairs of positions in a sequence. For a sequence of length n, this creates an n×n attention matrix, where each entry represents how much focus position i should place on position j when computing its representation. This mechanism enables the model to capture long-range dependencies without the gradient flow problems that plagued earlier sequential architectures.\n",
    "\n",
    "The transformer architecture consists of multiple layers, each containing two main components: a multi-head attention mechanism and a position-wise feed-forward network. The multi-head attention allows the model to attend to different aspects of the input simultaneously, with each \"head\" learning to focus on different patterns. The feed-forward networks apply learned transformations independently to each position, introducing non-linearity and increasing the model's representational capacity.\n",
    "\n",
    "1.2 Scaling Laws and Emergent Capabilities\n",
    "\n",
    "Research has revealed consistent scaling laws that govern language model performance. These laws demonstrate that model capability improves predictably with three key factors: the number of parameters, the size of the training dataset, and the amount of computational resources invested in training. This predictability has enabled researchers to forecast the capabilities of future models and make informed decisions about resource allocation.\n",
    "\n",
    "As models scale beyond certain thresholds, they begin to exhibit emergent capabilities—abilities that were not explicitly trained but arise from the combination of scale and diverse training data. These capabilities include few-shot learning, where models can adapt to new tasks with minimal examples, and chain-of-thought reasoning, where models break down complex problems into intermediate steps.\n",
    "\n",
    "The relationship between model size and capability is not always smooth. Some abilities appear suddenly at particular scales, suggesting that certain computational thresholds must be crossed before specific capabilities emerge. This phenomenon has important implications for AI safety, as it means that scaling up models may lead to unexpected new capabilities that require careful evaluation and monitoring.\n",
    "\n",
    "1.3 Attention Mechanisms and Their Variants\n",
    "\n",
    "While the standard multi-head attention mechanism has proven highly effective, researchers have developed numerous variants to address specific challenges. Grouped query attention (GQA) reduces the computational cost by sharing key and value projections across multiple query heads, maintaining most of the expressiveness while significantly reducing memory requirements.\n",
    "\n",
    "Multi-latent attention (MLA) represents another innovation, particularly valuable for models deployed in memory-constrained environments. MLA compresses the key-value cache through learned projections, achieving dramatic reductions in memory usage—often 90-95% compression—while maintaining model quality. This compression is especially important for inference scenarios with long contexts, where the KV cache would otherwise dominate memory consumption.\n",
    "\n",
    "FlashAttention and its successors have revolutionized attention computation by reorganizing the order of operations to maximize GPU utilization. By computing attention in blocks and carefully managing data movement between GPU memory hierarchies, FlashAttention achieves significant speedups without changing the mathematical operations being performed. This algorithmic innovation demonstrates that substantial performance improvements can come from careful consideration of hardware characteristics rather than changes to the underlying model.\n",
    "\n",
    "Part II: Computational Infrastructure\n",
    "\n",
    "2.1 GPU Architecture and Floating-Point Computation\n",
    "\n",
    "Modern GPUs are highly specialized processors designed to perform massive numbers of parallel floating-point operations. NVIDIA's Hopper and Blackwell architectures, for instance, contain thousands of CUDA cores and specialized tensor cores optimized for matrix multiplication—the fundamental operation in neural network inference and training.\n",
    "\n",
    "Floating-point arithmetic, however, is not exact. IEEE 754 floating-point numbers can only represent a finite subset of real numbers, leading to rounding errors in computation. Moreover, floating-point arithmetic is non-associative: (a + b) + c may yield a different result than a + (b + c) due to rounding at each step. This property has profound implications for reproducibility in distributed computing environments.\n",
    "\n",
    "Different GPU architectures implement floating-point operations with varying degrees of precision and through different execution paths. Even when using the same numerical precision (e.g., bfloat16 or float32), different GPU models may produce slightly different results due to differences in their microarchitecture, instruction scheduling, or specialized hardware accelerators. These hardware-level variations become important when considering verification and reproducibility in production deployments.\n",
    "\n",
    "2.2 Tensor Parallelism and Distributed Inference\n",
    "\n",
    "Large language models often exceed the memory capacity of a single GPU, necessitating distribution across multiple devices. Tensor parallelism splits individual weight matrices across GPUs, requiring careful coordination of matrix multiplications and communication between devices. When a layer is distributed across n GPUs, each GPU computes a portion of the output, which must then be combined through collective communication operations.\n",
    "\n",
    "The specific parallelization strategy affects not just performance but also numerical behavior. Different decompositions of the same computation lead to different orders of floating-point operations, potentially resulting in different final values even when starting from identical weights and inputs. This sensitivity to parallelization strategy has important implications for model verification and monitoring.\n",
    "\n",
    "Pipeline parallelism represents an alternative approach, where different layers of the model reside on different GPUs. Forward passes proceed in a pipelined fashion, with activations flowing from one GPU to the next. While pipeline parallelism typically introduces less numerical variation than tensor parallelism (since each layer's computation remains intact), it requires careful management of micro-batching to maintain efficiency.\n",
    "\n",
    "2.3 Memory Hierarchies and Caching\n",
    "\n",
    "Modern GPUs have complex memory hierarchies, including registers, L1 cache, L2 cache, shared memory, and global memory (VRAM). Efficient inference requires careful orchestration of data movement through these levels, as memory bandwidth often becomes the primary bottleneck rather than computational throughput.\n",
    "\n",
    "The KV (key-value) cache exemplifies memory management challenges in language model inference. During autoregressive generation, previously computed key and value vectors must be retained and accessed at each step. For long contexts, this cache can grow to dominate memory usage. Innovations like paged attention manage the KV cache more efficiently by storing it in non-contiguous memory blocks, similar to how operating systems manage virtual memory.\n",
    "\n",
    "Part III: Quantization and Efficiency\n",
    "\n",
    "3.1 Quantization Techniques\n",
    "\n",
    "Quantization reduces the precision of model weights and activations, trading some accuracy for significant improvements in memory usage and computational efficiency. Early post-training quantization methods simply converted trained models to lower precision, but more sophisticated approaches now integrate quantization awareness into the training process.\n",
    "\n",
    "INT8 quantization represents weights and activations as 8-bit integers rather than 32-bit floating-point numbers, achieving 4× memory reduction and enabling the use of specialized integer arithmetic units on modern processors. More aggressive quantization schemes, including INT4 and even binary networks, push these boundaries further, though with increasing risk to model quality.\n",
    "\n",
    "Weight-only quantization, where activations remain in higher precision while weights are quantized, often provides an attractive trade-off. This approach maintains most of the model's accuracy while still achieving significant memory savings and bandwidth improvements. The asymmetry reflects the fact that weight values are known at deployment time and can be carefully calibrated, while activations vary with each input.\n",
    "\n",
    "3.2 Quantization-Aware Training\n",
    "\n",
    "Quantization-aware training (QAT) incorporates quantization operations into the training process itself, allowing the model to learn weight distributions that are more amenable to low-precision representation. During QAT, the forward pass simulates quantization effects using fake quantization operations, while the backward pass still uses full-precision gradients.\n",
    "\n",
    "The effectiveness of QAT depends critically on the quantization scheme used. Symmetric quantization maps both positive and negative values uniformly, while asymmetric quantization can adapt to weight distributions that don't center on zero. Per-tensor quantization uses a single scale factor for an entire tensor, while per-channel or per-group quantization allows finer-grained adaptation to local statistics.\n",
    "\n",
    "Recent advances in microscaling (MXFP) quantization formats, such as MXFP4 and MXFP6, provide a middle ground between integer and floating-point quantization. These formats maintain a small floating-point exponent while using very few mantissa bits, preserving the dynamic range of floating-point numbers while approaching the memory efficiency of integer quantization.\n",
    "\n",
    "Part IV: Inference Optimization and Kernels\n",
    "\n",
    "4.1 CUDA Kernels and Operator Fusion\n",
    "\n",
    "Inference efficiency depends critically on the implementation of individual operations (kernels) that execute on the GPU. Standard deep learning frameworks provide default kernel implementations, but specialized hand-written kernels can often achieve substantial speedups by exploiting specific hardware features or operation patterns.\n",
    "\n",
    "Operator fusion combines multiple sequential operations into a single kernel, reducing memory traffic by keeping intermediate results in fast memory rather than writing them back to global memory. For instance, fusing a matrix multiplication with its subsequent activation function eliminates the need to store the pre-activation values in VRAM. Modern frameworks use sophisticated graph optimization passes to identify and execute such fusion opportunities automatically.\n",
    "\n",
    "The specific kernel implementation chosen for an operation can affect not just performance but also numerical results. Different kernel implementations may use different algorithms or accumulation orders, leading to different rounding errors. This variation becomes particularly pronounced when comparing implementations across different framework versions or hardware generations.\n",
    "\n",
    "4.2 Compilation and Just-In-Time Optimization\n",
    "\n",
    "Modern deep learning frameworks increasingly employ just-in-time compilation to generate optimized code for specific model configurations and hardware targets. PyTorch's torch.compile, for instance, captures the computational graph and applies a variety of optimizations before generating machine code.\n",
    "\n",
    "These compiler optimizations include operation reordering, dead code elimination, constant folding, and memory layout transformations. While these transformations preserve mathematical correctness in exact arithmetic, they can alter the order and grouping of floating-point operations, potentially affecting numerical outputs. The difference is typically small—often on the order of 10^-6 in relative terms—but may be detectable when comparing against uncompiled baselines.\n",
    "\n",
    "Graph compilation can also unlock new optimization opportunities that aren't available in eager execution mode. For instance, memory planning algorithms can allocate activation buffers more efficiently when they have visibility into the entire computational graph, reducing peak memory usage. Similarly, automatic kernel selection can make globally optimal choices rather than greedy local decisions.\n",
    "\n",
    "Part V: Production Deployment and Batching\n",
    "\n",
    "5.1 Continuous Batching and Request Scheduling\n",
    "\n",
    "Production inference services must handle streams of incoming requests efficiently. Continuous batching, also called iteration-level batching, allows new requests to join a running batch at each generation step. This approach maximizes GPU utilization by ensuring the batch remains full even as individual requests complete at different times.\n",
    "\n",
    "The specific composition of a batch—which particular inputs are grouped together—generally doesn't affect individual outputs in modern attention mechanisms, thanks to the attention mask that prevents information leakage between batch elements. However, the batch size itself does matter: larger batches often employ different computational strategies that may introduce small numerical differences.\n",
    "\n",
    "Memory management for continuous batching requires sophisticated orchestration. The paged attention mechanism treats the KV cache as a virtual memory space, allocating physical memory blocks dynamically as requests arrive and deallocating them as requests complete. This flexibility enables much higher throughput than static batching approaches, though it introduces additional complexity in memory access patterns.\n",
    "\n",
    "5.2 Scheduling and Resource Allocation\n",
    "\n",
    "Inference servers must make real-time decisions about which requests to batch together and how to allocate limited GPU resources. These decisions involve trade-offs between latency (how long individual requests wait) and throughput (how many requests can be served per second). Different scheduling policies lead to different performance characteristics.\n",
    "\n",
    "CUDA streams provide a mechanism for overlapping computation and communication operations. Multiple streams can execute concurrently on the same GPU, potentially improving utilization by allowing I/O operations and computation to proceed in parallel. However, when multiple streams compete for the same compute resources, their interactions can introduce timing variations and potential numerical artifacts from scheduling conflicts.\n",
    "\n",
    "Part VI: Verification and Monitoring\n",
    "\n",
    "6.1 The Challenge of Comprehensive Reporting\n",
    "\n",
    "In scenarios where datacenter operators must report all ML computations performed on their hardware—such as might arise under international AI governance agreements—verification becomes critical. Even if operators provide detailed logs of their inference workloads, how can external auditors verify that these logs are comprehensive?\n",
    "\n",
    "A malicious operator might attempt to hide unauthorized computations by secretly improving the efficiency of their declared workloads, freeing up capacity for off-the-books inference. Traditional monitoring approaches might detect obvious violations, such as periods of idle GPU time being used for hidden work, but wouldn't catch more sophisticated evasion strategies.\n",
    "\n",
    "6.2 Floating-Point Forensics\n",
    "\n",
    "One promising approach to verification leverages the numerical sensitivity we've discussed throughout this document. If an operator claims to have produced certain outputs using a specific computational setup, a verifier can attempt to reproduce those outputs on a trusted verification cluster. Exact numerical matching would provide strong evidence that the claimed setup was indeed used.\n",
    "\n",
    "This forensic approach relies on two key properties we've established: first, that identical setups produce bit-exact or near-identical results (low statistical noise), and second, that different setups produce systematically different outputs (detectable systematic deviations). The combination of these properties creates a unique numerical \"fingerprint\" for each inference configuration.\n",
    "\n",
    "Different aspects of the computational setup leave distinct signatures. Batch size changes typically produce L2 deviations on the order of 0.3-0.6, while changes in attention implementation (e.g., eager attention vs FlashAttention) create much larger deviations, often exceeding 40. CUDA version changes can lead to dramatic differences, with errors growing by several orders of magnitude between major versions. Even compilation optimizations like torch.compile introduce detectable deviations of around 2-5 in L2 norm.\n",
    "\n",
    "6.3 Attack Vectors and Detection Limits\n",
    "\n",
    "An adversary attempting to perform hidden inference in parallel with declared workloads would likely affect the computational environment in detectable ways. Running additional work on separate CUDA streams, for instance, creates resource contention and scheduling conflicts. At high GPU utilization—the regime most relevant for production systems—these conflicts introduce measurable timing variations and numerical artifacts.\n",
    "\n",
    "The economics of different evasion strategies also matter. Small-scale hidden workloads that might evade detection provide minimal benefit to an attacker, as they can't significantly offset the cost of running a datacenter. Larger hidden workloads that would provide meaningful economic value are more likely to create detectable signatures, either through timing anomalies, numerical deviations, or both.\n",
    "\n",
    "However, certain attack vectors remain challenging to detect. For instance, if an adversary can successfully hide parallel work that affects only low-arithmetic-intensity operations (like attention operations over long sequences), they might avoid creating floating-point deviations even while causing measurable slowdowns. The detectability of such scenarios requires careful analysis of both numerical and timing forensics.\n",
    "\n",
    "Part VII: Future Directions\n",
    "\n",
    "7.1 Hardware Evolution\n",
    "\n",
    "Next-generation AI accelerators continue to push the boundaries of performance and efficiency. Blackwell's B200 GPUs, for instance, include native support for FP4 and FP6 microscaling formats, enabling even more aggressive quantization while maintaining model quality. As hardware capabilities evolve, the landscape of numerical behavior and reproducibility characteristics will continue to shift.\n",
    "\n",
    "Specialized AI chips from various vendors introduce additional diversity in computational behavior. Each architecture makes different trade-offs in its implementation of floating-point operations, memory hierarchies, and specialized accelerators. This diversity enriches the space of possible forensic signatures while also complicating verification protocols that must account for legitimate hardware variations.\n",
    "\n",
    "7.2 Software Stack Evolution\n",
    "\n",
    "Deep learning frameworks continue to evolve rapidly, with new optimizations and features appearing in each release. This evolution creates challenges for reproducibility: results from one framework version may differ from another, even when using identical models and inputs. From a forensics perspective, framework version becomes another factor that must be either controlled or characterized.\n",
    "\n",
    "The trend toward greater automation in optimization—such as automatic kernel selection, dynamic batching strategies, and just-in-time compilation—generally improves performance but can make numerical behavior less predictable. Balancing the benefits of these optimizations against the need for reproducibility and verifiability represents an ongoing challenge for production ML systems.\n",
    "\n",
    "7.3 Governance and Policy Implications\n",
    "\n",
    "The techniques discussed in this document have implications beyond technical verification. If floating-point forensics proves reliable for detecting various forms of computational evasion, it could inform the design of AI governance mechanisms and international agreements. The ability to verify claimed computations without requiring complete transparency into internal operations might enable monitoring frameworks that balance accountability with legitimate concerns about intellectual property and competitive advantage.\n",
    "\n",
    "However, realizing this potential requires continued research into the limits and capabilities of forensic approaches. What types of computational changes are reliably detectable? What attack vectors remain? How do detection capabilities scale to different model architectures, sizes, and deployment scenarios? Answering these questions will require systematic experimentation across a wide range of conditions.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The numerical behavior of large language models reflects a complex interplay between model architecture, hardware characteristics, software implementation, and deployment configurations. While this complexity initially appears to pose challenges for reproducibility, it also creates opportunities: the unique numerical fingerprint of each setup can serve as a basis for verification and monitoring in governance contexts.\n",
    "\n",
    "The path forward requires continued research at the intersection of systems optimization, numerical computing, and AI safety. As models grow larger and deployment scenarios more diverse, maintaining the ability to verify and monitor AI systems becomes increasingly important. The forensic approaches discussed here represent one promising direction, but their ultimate viability depends on rigorous empirical validation across realistic production conditions.\n",
    "\n",
    "This document has surveyed the technical foundations necessary to understand these verification challenges and potential solutions. The field continues to evolve rapidly, and many questions remain open. Nevertheless, the convergence of growing AI capabilities, increasing deployment scale, and emerging governance frameworks makes this research area both timely and critical for the safe development of advanced AI systems.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def set_deterministic_mode(enable: bool = True):\n",
    "    \"\"\"Enable/disable PyTorch deterministic algorithms\"\"\"\n",
    "    if enable:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        print(\"✓ Deterministic mode ENABLED\")\n",
    "    else:\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"✓ Deterministic mode DISABLED (standard vLLM)\")\n",
    "    print()\n",
    "\n",
    "def clear_gpu():\n",
    "    \"\"\"Aggressively clear GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def run_repetitions(llm, prompt_text: str, sampling_params, num_reps: int, mode_name: str) -> Tuple:\n",
    "    \"\"\"Run multiple generations and collect results\"\"\"\n",
    "    print(f\"Running {num_reps} repetitions in {mode_name} mode...\")\n",
    "\n",
    "    results_tokens = []\n",
    "    results_logprobs = []\n",
    "    results_distributions = []\n",
    "\n",
    "    for rep in range(num_reps):\n",
    "        clear_gpu()\n",
    "\n",
    "        outputs = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "        output = outputs[0]\n",
    "\n",
    "        # Extract data\n",
    "        token_ids = output.outputs[0].token_ids\n",
    "        results_tokens.append(token_ids)\n",
    "\n",
    "        # Logprobs\n",
    "        logprobs_data = output.outputs[0].logprobs\n",
    "        selected_logprobs = [lp[token_ids[i]].logprob for i, lp in enumerate(logprobs_data)]\n",
    "        results_logprobs.append(np.array(selected_logprobs))\n",
    "\n",
    "        # Distributions\n",
    "        rep_distributions = []\n",
    "        for position_logprobs in logprobs_data:\n",
    "            sorted_items = sorted(position_logprobs.items(),\n",
    "                                key=lambda x: x[1].logprob,\n",
    "                                reverse=True)[:TOP_LOGPROBS]\n",
    "            rep_distributions.append([(tok, lp.logprob) for tok, lp in sorted_items])\n",
    "        results_distributions.append(rep_distributions)\n",
    "\n",
    "        if (rep + 1) % 5 == 0:\n",
    "            print(f\"  Completed {rep + 1}/{num_reps} repetitions\")\n",
    "\n",
    "    print(f\"✓ {mode_name} mode complete: {num_reps} repetitions\")\n",
    "    print()\n",
    "\n",
    "    return results_tokens, results_logprobs, results_distributions\n",
    "\n",
    "def analyze_reproducibility(results_tokens: List, results_logprobs: List, mode_name: str) -> Dict:\n",
    "    \"\"\"Analyze reproducibility statistics\"\"\"\n",
    "    print(f\"Analysis: {mode_name}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Check token sequences\n",
    "    tokens_identical = all(\n",
    "        results_tokens[0] == results_tokens[i]\n",
    "        for i in range(1, len(results_tokens))\n",
    "    )\n",
    "    print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "    # Check logprobs\n",
    "    first_logprobs = results_logprobs[0]\n",
    "    logprobs_exact = all(\n",
    "        np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "        for i in range(1, len(results_logprobs))\n",
    "    )\n",
    "    print(f\"Logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "    if not logprobs_exact:\n",
    "        # Compute L2 distances\n",
    "        l2_distances = []\n",
    "        for i in range(1, len(results_logprobs)):\n",
    "            l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "            l2_distances.append(l2)\n",
    "\n",
    "        print(f\"\\nLogprob deviations:\")\n",
    "        print(f\"  Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "        print(f\"  Std L2:  {np.std(l2_distances):.6e}\")\n",
    "        print(f\"  Min L2:  {np.min(l2_distances):.6e}\")\n",
    "        print(f\"  Max L2:  {np.max(l2_distances):.6e}\")\n",
    "\n",
    "        # Per-token statistics\n",
    "        all_logprobs = np.array(results_logprobs)\n",
    "        std_per_token = all_logprobs.std(axis=0)\n",
    "        print(f\"\\nPer-token std:\")\n",
    "        print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "        print(f\"  Max:  {std_per_token.max():.6e}\")\n",
    "        print(f\"  Min:  {std_per_token.min():.6e}\")\n",
    "\n",
    "        return {\n",
    "            'tokens_identical': tokens_identical,\n",
    "            'logprobs_exact': logprobs_exact,\n",
    "            'l2_mean': float(np.mean(l2_distances)),\n",
    "            'l2_std': float(np.std(l2_distances)),\n",
    "            'l2_max': float(np.max(l2_distances)),\n",
    "            'l2_min': float(np.min(l2_distances)),\n",
    "            'std_per_token_mean': float(std_per_token.mean()),\n",
    "            'std_per_token_max': float(std_per_token.max()),\n",
    "            'std_per_token_min': float(std_per_token.min())\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'tokens_identical': tokens_identical,\n",
    "            'logprobs_exact': logprobs_exact,\n",
    "            'l2_mean': 0.0,\n",
    "            'l2_std': 0.0,\n",
    "            'l2_max': 0.0,\n",
    "            'l2_min': 0.0,\n",
    "            'std_per_token_mean': 0.0,\n",
    "            'std_per_token_max': 0.0,\n",
    "            'std_per_token_min': 0.0\n",
    "        }\n",
    "\n",
    "def generate_verdict(baseline_stats: Dict, det_stats: Optional[Dict],\n",
    "                     deterministic_succeeded: bool, quantization: str) -> Dict:\n",
    "    \"\"\"Generate verdict and interpretation of results\"\"\"\n",
    "    verdict = {\n",
    "        \"is_deterministic\": baseline_stats['logprobs_exact'],\n",
    "        \"deterministic_mode_tested\": deterministic_succeeded,\n",
    "        \"summary\": \"\",\n",
    "        \"root_cause\": \"\",\n",
    "        \"forensic_implications\": \"\",\n",
    "        \"details\": []\n",
    "    }\n",
    "\n",
    "    if not baseline_stats['logprobs_exact']:\n",
    "        verdict[\"summary\"] = \"BASELINE IS NON-DETERMINISTIC\"\n",
    "        verdict[\"details\"].append(f\"Mean L2 deviation: {baseline_stats['l2_mean']:.6e}\")\n",
    "        verdict[\"details\"].append(f\"Max L2 deviation: {baseline_stats['l2_max']:.6e}\")\n",
    "\n",
    "        if deterministic_succeeded and det_stats:\n",
    "            if det_stats['logprobs_exact']:\n",
    "                verdict[\"root_cause\"] = \"Non-deterministic parallel reduction, likely in kernel accumulation logic\"\n",
    "                verdict[\"details\"].append(\"✓ DETERMINISTIC MODE FIXES IT\")\n",
    "                verdict[\"details\"].append(\"Can be fixed but at performance cost\")\n",
    "            else:\n",
    "                improvement = baseline_stats['l2_mean'] / det_stats['l2_mean'] if det_stats['l2_mean'] > 0 else float('inf')\n",
    "                if improvement > 2:\n",
    "                    verdict[\"root_cause\"] = \"Partial non-determinism in parallel reduction\"\n",
    "                    verdict[\"details\"].append(f\"⚠ DETERMINISTIC MODE REDUCES NOISE ({improvement:.1f}x reduction)\")\n",
    "                    verdict[\"details\"].append(\"Partial fix, some non-determinism remains\")\n",
    "                else:\n",
    "                    verdict[\"root_cause\"] = \"Non-determinism NOT from parallel reduction races, likely in quantization/dequantization logic\"\n",
    "                    verdict[\"details\"].append(\"❌ DETERMINISTIC MODE DOESN'T HELP\")\n",
    "        else:\n",
    "            verdict[\"root_cause\"] = \"Unknown - deterministic mode not supported\"\n",
    "            verdict[\"details\"].append(\"⚠ Could not test deterministic mode\")\n",
    "            verdict[\"details\"].append(\"Suggests operations incompatible with deterministic algorithms\")\n",
    "\n",
    "        verdict[\"forensic_implications\"] = f\"Noise level: ~{baseline_stats['l2_mean']:.2e} L2. For detection, need systematic deviation >> noise. Recommend: 3-5 samples for statistical significance\"\n",
    "    else:\n",
    "        verdict[\"summary\"] = \"BASELINE IS DETERMINISTIC\"\n",
    "        verdict[\"root_cause\"] = \"Fully deterministic behavior\"\n",
    "        verdict[\"details\"].append(f\"Unexpected for {quantization}! May have been fixed in recent vLLM version\")\n",
    "        verdict[\"forensic_implications\"] = \"Perfect reproducibility - any deviation indicates computational change\"\n",
    "\n",
    "    return verdict\n",
    "\n",
    "def profile_kernels(llm, prompt_text: str, sampling_params) -> Tuple[bool, List, List, List, List]:\n",
    "    \"\"\"Profile CUDA kernels during generation\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"KERNEL PROFILING\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"Profiling kernel calls during generation...\")\n",
    "\n",
    "    clear_gpu()\n",
    "\n",
    "    try:\n",
    "        if PROFILER_NEW_API:\n",
    "            # New PyTorch profiler API (1.8+)\n",
    "            with profile(\n",
    "                activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "                with_stack=False,\n",
    "                profile_memory=False,\n",
    "                record_shapes=False\n",
    "            ) as prof:\n",
    "                _ = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "        else:\n",
    "            # Old PyTorch profiler API\n",
    "            with profiler_module.profile(\n",
    "                use_cuda=True,\n",
    "                with_stack=False,\n",
    "                profile_memory=False,\n",
    "                record_shapes=False\n",
    "            ) as prof:\n",
    "                _ = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "\n",
    "        print()\n",
    "        print(\"Top 20 kernels by CUDA time:\")\n",
    "        print(\"-\" * 80)\n",
    "        kernel_table = prof.key_averages().table(\n",
    "            sort_by=\"cuda_time_total\",\n",
    "            row_limit=20\n",
    "        )\n",
    "        print(kernel_table)\n",
    "        print()\n",
    "\n",
    "        # Extract kernel names for analysis\n",
    "        kernel_data = []\n",
    "        for evt in prof.key_averages():\n",
    "            # Handle both API versions\n",
    "            is_cuda = False\n",
    "            if PROFILER_NEW_API:\n",
    "                is_cuda = hasattr(evt, 'device_type') and evt.device_type == ProfilerActivity.CUDA\n",
    "            else:\n",
    "                is_cuda = evt.cuda_time_total > 0\n",
    "\n",
    "            if is_cuda and evt.cuda_time_total > 0:\n",
    "                kernel_data.append({\n",
    "                    'name': evt.key,\n",
    "                    'cuda_time_us': evt.cuda_time_total,\n",
    "                    'count': evt.count,\n",
    "                    'avg_time_us': evt.cuda_time_total / evt.count if evt.count > 0 else 0\n",
    "                })\n",
    "\n",
    "        # Categorize kernels\n",
    "        marlin_kernels = [k for k in kernel_data if 'marlin' in k['name'].lower()]\n",
    "        gemm_kernels = [k for k in kernel_data if 'gemm' in k['name'].lower()]\n",
    "        dequant_kernels = [k for k in kernel_data if 'dequant' in k['name'].lower() or 'quant' in k['name'].lower()]\n",
    "\n",
    "        print(\"Kernel categories detected:\")\n",
    "        print(f\"  Marlin kernels: {len(marlin_kernels)}\")\n",
    "        print(f\"  GEMM kernels: {len(gemm_kernels)}\")\n",
    "        print(f\"  Dequant kernels: {len(dequant_kernels)}\")\n",
    "        print()\n",
    "\n",
    "        if marlin_kernels:\n",
    "            print(\"Marlin-specific kernels:\")\n",
    "            for k in marlin_kernels[:5]:\n",
    "                print(f\"  {k['name'][:60]} - {k['cuda_time_us']/1000:.2f}ms\")\n",
    "            print()\n",
    "\n",
    "        return True, kernel_data, marlin_kernels, gemm_kernels, dequant_kernels\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Kernel profiling failed: {e}\")\n",
    "        print(\"Continuing without profiling data...\")\n",
    "        print()\n",
    "        return False, [], [], [], []\n",
    "\n",
    "def run_experiment(variant_name: str, config: Dict, output_dir: str = \".\") -> Dict:\n",
    "    \"\"\"Run complete experiment for a single quantization variant\"\"\"\n",
    "\n",
    "    model_name = config[\"model_name\"]\n",
    "    quantization = config[\"quantization\"]\n",
    "    description = config[\"description\"]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"QUANTIZATION INVESTIGATION: {variant_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Quantization: {quantization}\")\n",
    "    print(f\"Prompt length: {len(TEST_PROMPT)} chars\")\n",
    "    print(f\"Repetitions per mode: {NUM_REPETITIONS}\")\n",
    "    print()\n",
    "\n",
    "    if \"note\" in config:\n",
    "        print(f\"NOTE: {config['note']}\")\n",
    "        print()\n",
    "\n",
    "    print(\"Investigation plan:\")\n",
    "    print(\"  1. Baseline: Standard vLLM generation (non-deterministic expected)\")\n",
    "    print(\"  2. With determinism: torch.use_deterministic_algorithms(True)\")\n",
    "    print(\"  3. Kernel profiling: Identify which kernels are called\")\n",
    "    print(\"  4. Analysis: Compare noise levels and kernel usage\")\n",
    "    print()\n",
    "\n",
    "    # ============================================================================\n",
    "    # PREPARE PROMPT\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"Preparing prompt...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        cache_dir='/tmp/hf_cache',\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": TEST_PROMPT}]\n",
    "    prompt_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    prompt_tokens = tokenizer.encode(prompt_text)\n",
    "    prompt_length = len(prompt_tokens)\n",
    "\n",
    "    print(f\"✓ Prompt prepared: {prompt_length} tokens\")\n",
    "    print()\n",
    "\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=TEMPERATURE,\n",
    "        max_tokens=MAX_TOKENS,\n",
    "        seed=SEED,\n",
    "        logprobs=TOP_LOGPROBS,\n",
    "        skip_special_tokens=False\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # EXPERIMENT 1: BASELINE (NON-DETERMINISTIC MODE)\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXPERIMENT 1: BASELINE (Standard vLLM)\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    set_deterministic_mode(False)\n",
    "\n",
    "    print(\"Loading model...\")\n",
    "    try:\n",
    "        llm_kwargs = {\n",
    "            \"model\": model_name,\n",
    "            \"tensor_parallel_size\": TENSOR_PARALLEL_SIZE,\n",
    "            \"max_model_len\": MAX_MODEL_LEN,\n",
    "            \"gpu_memory_utilization\": GPU_MEMORY_UTILIZATION,\n",
    "            \"dtype\": TORCH_DTYPE,\n",
    "            \"trust_remote_code\": True,\n",
    "            \"seed\": SEED,\n",
    "            \"enforce_eager\": True,\n",
    "            \"enable_prefix_caching\": False\n",
    "        }\n",
    "\n",
    "        # Add quantization parameter if specified\n",
    "        if quantization is not None:\n",
    "            llm_kwargs[\"quantization\"] = quantization\n",
    "\n",
    "        llm = LLM(**llm_kwargs)\n",
    "        print(\"✓ Model loaded\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load model: {e}\")\n",
    "        print()\n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "    # Warmup\n",
    "    print(\"Warmup...\")\n",
    "    try:\n",
    "        for _ in range(3):\n",
    "            _ = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "        clear_gpu()\n",
    "        print(\"✓ Warmup complete\")\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Warmup failed: {e}\")\n",
    "        del llm\n",
    "        clear_gpu()\n",
    "        return {\n",
    "            \"variant\": variant_name,\n",
    "            \"error\": str(e),\n",
    "            \"success\": False\n",
    "        }\n",
    "\n",
    "    # Run baseline\n",
    "    baseline_tokens, baseline_logprobs, baseline_dists = run_repetitions(\n",
    "        llm, prompt_text, sampling_params, NUM_REPETITIONS, \"BASELINE\"\n",
    "    )\n",
    "\n",
    "    baseline_stats = analyze_reproducibility(baseline_tokens, baseline_logprobs, \"BASELINE\")\n",
    "    print()\n",
    "\n",
    "    # ============================================================================\n",
    "    # KERNEL PROFILING\n",
    "    # ============================================================================\n",
    "\n",
    "    profiling_succeeded, kernel_data, marlin_kernels, gemm_kernels, dequant_kernels = profile_kernels(\n",
    "        llm, prompt_text, sampling_params\n",
    "    )\n",
    "\n",
    "    # ============================================================================\n",
    "    # EXPERIMENT 2: WITH DETERMINISTIC MODE\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"EXPERIMENT 2: WITH DETERMINISTIC MODE\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    # Unload model\n",
    "    del llm\n",
    "    clear_gpu()\n",
    "\n",
    "    # Enable deterministic mode\n",
    "    set_deterministic_mode(True)\n",
    "\n",
    "    # Reload model\n",
    "    print(\"Reloading model with deterministic settings...\")\n",
    "    try:\n",
    "        llm = LLM(**llm_kwargs)\n",
    "        print(\"✓ Model loaded with deterministic mode\")\n",
    "        print()\n",
    "\n",
    "        # Warmup\n",
    "        print(\"Warmup...\")\n",
    "        for _ in range(3):\n",
    "            _ = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "        clear_gpu()\n",
    "        print(\"✓ Warmup complete\")\n",
    "        print()\n",
    "\n",
    "        # Run with deterministic mode\n",
    "        det_tokens, det_logprobs, det_dists = run_repetitions(\n",
    "            llm, prompt_text, sampling_params, NUM_REPETITIONS, \"DETERMINISTIC\"\n",
    "        )\n",
    "\n",
    "        det_stats = analyze_reproducibility(det_tokens, det_logprobs, \"DETERMINISTIC\")\n",
    "        print()\n",
    "\n",
    "        deterministic_succeeded = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Deterministic mode failed: {e}\")\n",
    "        print(\"This suggests deterministic algorithms don't support all operations\")\n",
    "        print()\n",
    "        deterministic_succeeded = False\n",
    "        det_stats = None\n",
    "        det_tokens = None\n",
    "        det_logprobs = None\n",
    "\n",
    "    # ============================================================================\n",
    "    # COMPARATIVE ANALYSIS\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    print(\"Baseline vs Deterministic Mode:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Metric':<30} {'Baseline':<20} {'Deterministic':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    if deterministic_succeeded and det_stats:\n",
    "        print(f\"{'Tokens identical':<30} {str(baseline_stats['tokens_identical']):<20} {str(det_stats['tokens_identical']):<20}\")\n",
    "        print(f\"{'Logprobs exact':<30} {str(baseline_stats['logprobs_exact']):<20} {str(det_stats['logprobs_exact']):<20}\")\n",
    "        print(f\"{'Mean L2 distance':<30} {baseline_stats['l2_mean']:.6e}  {det_stats['l2_mean']:.6e}\")\n",
    "        print(f\"{'Max L2 distance':<30} {baseline_stats['l2_max']:.6e}  {det_stats['l2_max']:.6e}\")\n",
    "        print(f\"{'Per-token std (mean)':<30} {baseline_stats['std_per_token_mean']:.6e}  {det_stats['std_per_token_mean']:.6e}\")\n",
    "        print(f\"{'Per-token std (max)':<30} {baseline_stats['std_per_token_max']:.6e}  {det_stats['std_per_token_max']:.6e}\")\n",
    "    else:\n",
    "        print(f\"{'Tokens identical':<30} {str(baseline_stats['tokens_identical']):<20} {'N/A':<20}\")\n",
    "        print(f\"{'Logprobs exact':<30} {str(baseline_stats['logprobs_exact']):<20} {'N/A':<20}\")\n",
    "        print(f\"{'Mean L2 distance':<30} {baseline_stats['l2_mean']:.6e}  {'N/A':<20}\")\n",
    "\n",
    "    print()\n",
    "\n",
    "    # ============================================================================\n",
    "    # VERDICT\n",
    "    # ============================================================================\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"VERDICT\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    verdict = generate_verdict(baseline_stats, det_stats, deterministic_succeeded, quantization or \"unknown\")\n",
    "\n",
    "    print(verdict[\"summary\"])\n",
    "    for detail in verdict[\"details\"]:\n",
    "        print(f\"  {detail}\")\n",
    "    if verdict[\"root_cause\"]:\n",
    "        print()\n",
    "        print(f\"Root cause: {verdict['root_cause']}\")\n",
    "    print()\n",
    "    print(\"Forensic implications:\")\n",
    "    print(f\"  {verdict['forensic_implications']}\")\n",
    "    print()\n",
    "\n",
    "    # ============================================================================\n",
    "    # SAVE RESULTS\n",
    "    # ============================================================================\n",
    "\n",
    "    output_data = {\n",
    "        \"variant\": variant_name,\n",
    "        \"experiment\": \"quantization_nondeterminism_investigation\",\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"model\": model_name,\n",
    "        \"quantization\": quantization,\n",
    "        \"description\": description,\n",
    "        \"config\": {\n",
    "            \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "            \"max_model_len\": MAX_MODEL_LEN,\n",
    "            \"max_tokens\": MAX_TOKENS,\n",
    "            \"repetitions\": NUM_REPETITIONS,\n",
    "            \"temperature\": TEMPERATURE,\n",
    "            \"seed\": SEED\n",
    "        },\n",
    "        \"prompt_length\": prompt_length,\n",
    "        \"baseline\": {\n",
    "            \"stats\": baseline_stats,\n",
    "            \"tokens\": baseline_tokens,\n",
    "            \"logprobs\": [lp.tolist() for lp in baseline_logprobs]\n",
    "        },\n",
    "        \"deterministic\": {\n",
    "            \"succeeded\": deterministic_succeeded,\n",
    "            \"stats\": det_stats if det_stats else None,\n",
    "            \"tokens\": det_tokens if deterministic_succeeded else None,\n",
    "            \"logprobs\": [lp.tolist() for lp in det_logprobs] if deterministic_succeeded and det_logprobs else None\n",
    "        },\n",
    "        \"kernels\": {\n",
    "            \"profiling_succeeded\": profiling_succeeded,\n",
    "            \"all_kernels\": kernel_data if profiling_succeeded else [],\n",
    "            \"marlin_kernels\": marlin_kernels if profiling_succeeded else [],\n",
    "            \"gemm_kernels\": gemm_kernels if profiling_succeeded else [],\n",
    "            \"dequant_kernels\": dequant_kernels if profiling_succeeded else []\n",
    "        },\n",
    "        \"verdict\": verdict,\n",
    "        \"success\": True\n",
    "    }\n",
    "\n",
    "    output_file = os.path.join(output_dir, f\"{variant_name}_investigation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\")\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    print()\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"INVESTIGATION COMPLETE: {variant_name.upper()}\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "    # Cleanup\n",
    "    del llm\n",
    "    clear_gpu()\n",
    "\n",
    "    return output_data\n",
    "\n",
    "# Replace the entire main() function with this simpler version:\n",
    "def main():\n",
    "    \"\"\"Main entry point\"\"\"\n",
    "    \n",
    "    # Direct configuration (no argparse)\n",
    "    variants_to_run = list(MODEL_CONFIGS.keys()) #[\"awq_marlin\"]  # Change this to test different variants\n",
    "    output_dir = \".\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"UNIFIED QUANTIZATION NON-DETERMINISM INVESTIGATION\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(f\"Testing {len(variants_to_run)} variant(s): {', '.join(variants_to_run)}\")\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    print()\n",
    "\n",
    "    # Create output directory if needed\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Run experiments\n",
    "    all_results = {}\n",
    "    for variant_name in variants_to_run:\n",
    "        print()\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"STARTING: {variant_name.upper()}\")\n",
    "        print(\"=\" * 80)\n",
    "        print()\n",
    "\n",
    "        config = MODEL_CONFIGS[variant_name]\n",
    "        result = run_experiment(variant_name, config, output_dir)\n",
    "        all_results[variant_name] = result\n",
    "\n",
    "        print(\"\\n\" * 2)\n",
    "\n",
    "    # Summary\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ALL EXPERIMENTS COMPLETE\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"Summary:\")\n",
    "    print(\"-\" * 60)\n",
    "    for variant_name, result in all_results.items():\n",
    "        if result.get(\"success\", False):\n",
    "            verdict = result[\"verdict\"]\n",
    "            status = \"✓ DETERMINISTIC\" if verdict[\"is_deterministic\"] else \"❌ NON-DETERMINISTIC\"\n",
    "            print(f\"  {variant_name:15} - {status}\")\n",
    "            if not verdict[\"is_deterministic\"]:\n",
    "                print(f\"                     Noise level: {result['baseline']['stats']['l2_mean']:.2e} L2\")\n",
    "        else:\n",
    "            print(f\"  {variant_name:15} - ❌ FAILED: {result.get('error', 'Unknown error')}\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e337fb-0567-41ba-8a09-4adf543bf000",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
