{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f517c2-98c1-4ebf-913b-796a0db3c470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-12 14:35:06 [__init__.py:216] Automatically detected platform cuda.\n",
      "================================================================================\n",
      "AWQ NON-DETERMINISM ROOT CAUSE INVESTIGATION\n",
      "================================================================================\n",
      "\n",
      "Model: Qwen/Qwen3-8B-AWQ\n",
      "Quantization: awq_marlin (INT4)\n",
      "Prompt length: 23143 chars\n",
      "Repetitions per mode: 5\n",
      "\n",
      "Investigation plan:\n",
      "  1. Baseline: Standard vLLM generation (non-deterministic expected)\n",
      "  2. With determinism: torch.use_deterministic_algorithms(True)\n",
      "  3. Kernel profiling: Identify which kernels are called\n",
      "  4. Analysis: Compare noise levels and kernel usage\n",
      "\n",
      "Preparing prompt...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ad9441baa774c19b78682f917f429c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f573e1343c4005bcc48a92b2959148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b61a07def44721a25eb65235ffe1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0807d91b5e148c1911ea945aff62333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt prepared: 3790 tokens\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT 1: BASELINE (Standard vLLM)\n",
      "================================================================================\n",
      "\n",
      "✓ Deterministic mode DISABLED (standard vLLM)\n",
      "\n",
      "Loading model...\n",
      "INFO 11-12 14:35:09 [utils.py:233] non-default args: {'trust_remote_code': True, 'seed': 42, 'max_model_len': 8192, 'enable_prefix_caching': False, 'disable_log_stats': True, 'quantization': 'awq_marlin', 'enforce_eager': True, 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-12 14:35:17 [model.py:547] Resolved architecture: Qwen3ForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-12 14:35:17 [model.py:1510] Using max model len 8192\n",
      "INFO 11-12 14:35:17 [awq_marlin.py:119] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-12 14:35:20 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 11-12 14:35:20 [__init__.py:381] Cudagraph is disabled under eager mode\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m INFO 11-12 14:35:20 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m INFO 11-12 14:35:20 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=42, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=False, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":null,\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":0,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":0,\"local_cache_dir\":null}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708] EngineCore failed to start.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708] Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 54, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 259, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 161, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     current_platform.set_device(self.device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/vllm/platforms/cuda.py\", line 79, in set_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     torch.cuda.set_device(device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]   File \"/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\", line 569, in set_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708]     torch._C._cuda_setDevice(device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708] torch.AcceleratorError: CUDA error: CUDA-capable device(s) is/are busy or unavailable\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708] For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m ERROR 11-12 14:35:37 [core.py:708] \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m Process EngineCore_DP0:\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m Traceback (most recent call last):\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     self.run()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     self._target(*self._args, **self._kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 712, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     raise e\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 699, in run_engine_core\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     engine_core = EngineCoreProc(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 498, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     super().__init__(vllm_config, executor_class, log_stats,\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core.py\", line 83, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     self.model_executor = executor_class(vllm_config)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/executor_base.py\", line 54, in __init__\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     self._init_executor()\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 54, in _init_executor\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     self.collective_rpc(\"init_device\")\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/uniproc_executor.py\", line 83, in collective_rpc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     return [run_method(self.driver_worker, method, args, kwargs)]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/utils/__init__.py\", line 3122, in run_method\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     return func(*args, **kwargs)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 259, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     self.worker.init_device()  # type: ignore\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/v1/worker/gpu_worker.py\", line 161, in init_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     current_platform.set_device(self.device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/vllm/platforms/cuda.py\", line 79, in set_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     torch.cuda.set_device(device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m   File \"/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\", line 569, in set_device\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m     torch._C._cuda_setDevice(device)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m torch.AcceleratorError: CUDA error: CUDA-capable device(s) is/are busy or unavailable\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1649)\u001b[0;0m \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Engine core initialization failed. See root cause above. Failed core proc(s): {}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 423\u001b[39m\n\u001b[32m    420\u001b[39m set_deterministic_mode(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    422\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    424\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mQUANTIZATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTENSOR_PARALLEL_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_MODEL_LEN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mGPU_MEMORY_UTILIZATION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_prefix_caching\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    433\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✓ Model loaded\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    435\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/llm.py:297\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    294\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    301\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:177\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    174\u001b[39m     enable_multiprocessing = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# Create the LLMEngine.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m           \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m           \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m           \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m           \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m           \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43menable_multiprocessing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/llm_engine.py:114\u001b[39m, in \u001b[36mLLMEngine.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats, usage_context, stat_loggers, mm_registry, use_cached_outputs, multiprocess_mode)\u001b[39m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28mself\u001b[39m.output_processor.tracer = tracer\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_core = \u001b[43mEngineCoreClient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmultiprocess_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m    \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m.logger_manager: Optional[StatLoggerManager] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.log_stats:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:80\u001b[39m, in \u001b[36mEngineCoreClient.make_client\u001b[39m\u001b[34m(multiprocess_mode, asyncio_mode, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m EngineCoreClient.make_async_mp_client(\n\u001b[32m     77\u001b[39m         vllm_config, executor_class, log_stats)\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m multiprocess_mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m asyncio_mode:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSyncMPClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m InprocClient(vllm_config, executor_class, log_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:602\u001b[39m, in \u001b[36mSyncMPClient.__init__\u001b[39m\u001b[34m(self, vllm_config, executor_class, log_stats)\u001b[39m\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, vllm_config: VllmConfig, executor_class: \u001b[38;5;28mtype\u001b[39m[Executor],\n\u001b[32m    601\u001b[39m              log_stats: \u001b[38;5;28mbool\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[43masyncio_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    609\u001b[39m     \u001b[38;5;28mself\u001b[39m.is_dp = \u001b[38;5;28mself\u001b[39m.vllm_config.parallel_config.data_parallel_size > \u001b[32m1\u001b[39m\n\u001b[32m    610\u001b[39m     \u001b[38;5;28mself\u001b[39m.outputs_queue = queue.Queue[Union[EngineCoreOutputs, \u001b[38;5;167;01mException\u001b[39;00m]]()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/core_client.py:448\u001b[39m, in \u001b[36mMPClient.__init__\u001b[39m\u001b[34m(self, asyncio_mode, vllm_config, executor_class, log_stats, client_addresses)\u001b[39m\n\u001b[32m    444\u001b[39m     \u001b[38;5;28mself\u001b[39m.stats_update_address = client_addresses.get(\n\u001b[32m    445\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstats_update_address\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    446\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    447\u001b[39m     \u001b[38;5;66;03m# Engines are managed by this client.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlaunch_core_engines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m                             \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m                                            \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresources\u001b[49m\u001b[43m.\u001b[49m\u001b[43mengine_manager\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine_manager\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py:732\u001b[39m, in \u001b[36mlaunch_core_engines\u001b[39m\u001b[34m(vllm_config, executor_class, log_stats, num_api_servers)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m local_engine_manager, coordinator, addresses\n\u001b[32m    731\u001b[39m \u001b[38;5;66;03m# Now wait for engines to start.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m \u001b[43mwait_for_engine_startup\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhandshake_socket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddresses\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengines_to_handshake\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    736\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    737\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvllm_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_engine_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcoordinator\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/dist-packages/vllm/v1/engine/utils.py:785\u001b[39m, in \u001b[36mwait_for_engine_startup\u001b[39m\u001b[34m(handshake_socket, addresses, core_engines, parallel_config, cache_config, proc_manager, coord_process)\u001b[39m\n\u001b[32m    783\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m coord_process \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m coord_process.exitcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    784\u001b[39m         finished[coord_process.name] = coord_process.exitcode\n\u001b[32m--> \u001b[39m\u001b[32m785\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEngine core initialization failed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    786\u001b[39m                        \u001b[33m\"\u001b[39m\u001b[33mSee root cause above. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    787\u001b[39m                        \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed core proc(s): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    789\u001b[39m \u001b[38;5;66;03m# Receive HELLO and READY messages from the input socket.\u001b[39;00m\n\u001b[32m    790\u001b[39m eng_identity, ready_msg_bytes = handshake_socket.recv_multipart()\n",
      "\u001b[31mRuntimeError\u001b[39m: Engine core initialization failed. See root cause above. Failed core proc(s): {}"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "AWQ Non-Determinism Root Cause Investigation\n",
    "Tests vLLM INT4-AWQ quantization reproducibility with kernel profiling\n",
    "\n",
    "Model: Qwen3-8B (AWQ4)\n",
    "Focus: Identify whether Marlin kernel has non-deterministic behavior\n",
    "\"\"\"\n",
    "'''\n",
    "# ============================================================================\n",
    "# SUPPRESS VERBOSE LOGGING\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "os.environ['VLLM_LOGGING_LEVEL'] = 'WARNING'\n",
    "os.environ['VLLM_CONFIGURE_LOGGING'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger('vllm').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.engine').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.worker').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.executor').setLevel(logging.ERROR)\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "logging.getLogger('torch').setLevel(logging.ERROR)\n",
    "logging.getLogger('huggingface_hub').setLevel(logging.INFO)\n",
    "logging.getLogger('huggingface_hub.file_download').setLevel(logging.INFO)\n",
    "'''\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Try to import new profiler API, fall back to old if needed\n",
    "try:\n",
    "    from torch.profiler import profile, ProfilerActivity\n",
    "    PROFILER_NEW_API = True\n",
    "except ImportError:\n",
    "    import torch.autograd.profiler as profiler_module\n",
    "    PROFILER_NEW_API = False\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-8B-AWQ\"\n",
    "QUANTIZATION = \"awq_marlin\"\n",
    "TENSOR_PARALLEL_SIZE = 1\n",
    "MAX_MODEL_LEN = 8192\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 50\n",
    "NUM_REPETITIONS = 5\n",
    "TEMPERATURE = 0.0\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Test prompt\n",
    "TEST_PROMPT = \"\"\"The Evolution of Large Language Models: Technical Foundations and Societal Implications\n",
    "\n",
    "Introduction\n",
    "\n",
    "The field of artificial intelligence has witnessed a remarkable transformation over the past decade, driven primarily by advances in deep learning and the emergence of increasingly sophisticated language models. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing. However, their deployment raises significant challenges related to computational efficiency, interpretability, and safety.\n",
    "\n",
    "This document explores the technical foundations of modern large language models, their architectural innovations, the computational infrastructure required for their deployment, and the broader implications for AI governance and safety. We examine how these systems process information, the sources of variation in their outputs, and the methods being developed to ensure their reliable and safe operation at scale.\n",
    "\n",
    "Part I: Architectural Foundations\n",
    "\n",
    "1.1 The Transformer Architecture\n",
    "\n",
    "At the heart of modern language models lies the transformer architecture, introduced in 2017 by Vaswani et al. The transformer's key innovation was the self-attention mechanism, which allows models to dynamically weigh the importance of different parts of their input when processing each element. Unlike recurrent neural networks, transformers can process sequences in parallel, enabling efficient training on modern hardware.\n",
    "\n",
    "The self-attention mechanism computes attention scores between all pairs of positions in a sequence. For a sequence of length n, this creates an n×n attention matrix, where each entry represents how much focus position i should place on position j when computing its representation. This mechanism enables the model to capture long-range dependencies without the gradient flow problems that plagued earlier sequential architectures.\n",
    "\n",
    "The transformer architecture consists of multiple layers, each containing two main components: a multi-head attention mechanism and a position-wise feed-forward network. The multi-head attention allows the model to attend to different aspects of the input simultaneously, with each \"head\" learning to focus on different patterns. The feed-forward networks apply learned transformations independently to each position, introducing non-linearity and increasing the model's representational capacity.\n",
    "\n",
    "1.2 Scaling Laws and Emergent Capabilities\n",
    "\n",
    "Research has revealed consistent scaling laws that govern language model performance. These laws demonstrate that model capability improves predictably with three key factors: the number of parameters, the size of the training dataset, and the amount of computational resources invested in training. This predictability has enabled researchers to forecast the capabilities of future models and make informed decisions about resource allocation.\n",
    "\n",
    "As models scale beyond certain thresholds, they begin to exhibit emergent capabilities—abilities that were not explicitly trained but arise from the combination of scale and diverse training data. These capabilities include few-shot learning, where models can adapt to new tasks with minimal examples, and chain-of-thought reasoning, where models break down complex problems into intermediate steps.\n",
    "\n",
    "The relationship between model size and capability is not always smooth. Some abilities appear suddenly at particular scales, suggesting that certain computational thresholds must be crossed before specific capabilities emerge. This phenomenon has important implications for AI safety, as it means that scaling up models may lead to unexpected new capabilities that require careful evaluation and monitoring.\n",
    "\n",
    "1.3 Attention Mechanisms and Their Variants\n",
    "\n",
    "While the standard multi-head attention mechanism has proven highly effective, researchers have developed numerous variants to address specific challenges. Grouped query attention (GQA) reduces the computational cost by sharing key and value projections across multiple query heads, maintaining most of the expressiveness while significantly reducing memory requirements.\n",
    "\n",
    "Multi-latent attention (MLA) represents another innovation, particularly valuable for models deployed in memory-constrained environments. MLA compresses the key-value cache through learned projections, achieving dramatic reductions in memory usage—often 90-95% compression—while maintaining model quality. This compression is especially important for inference scenarios with long contexts, where the KV cache would otherwise dominate memory consumption.\n",
    "\n",
    "FlashAttention and its successors have revolutionized attention computation by reorganizing the order of operations to maximize GPU utilization. By computing attention in blocks and carefully managing data movement between GPU memory hierarchies, FlashAttention achieves significant speedups without changing the mathematical operations being performed. This algorithmic innovation demonstrates that substantial performance improvements can come from careful consideration of hardware characteristics rather than changes to the underlying model.\n",
    "\n",
    "Part II: Computational Infrastructure\n",
    "\n",
    "2.1 GPU Architecture and Floating-Point Computation\n",
    "\n",
    "Modern GPUs are highly specialized processors designed to perform massive numbers of parallel floating-point operations. NVIDIA's Hopper and Blackwell architectures, for instance, contain thousands of CUDA cores and specialized tensor cores optimized for matrix multiplication—the fundamental operation in neural network inference and training.\n",
    "\n",
    "Floating-point arithmetic, however, is not exact. IEEE 754 floating-point numbers can only represent a finite subset of real numbers, leading to rounding errors in computation. Moreover, floating-point arithmetic is non-associative: (a + b) + c may yield a different result than a + (b + c) due to rounding at each step. This property has profound implications for reproducibility in distributed computing environments.\n",
    "\n",
    "Different GPU architectures implement floating-point operations with varying degrees of precision and through different execution paths. Even when using the same numerical precision (e.g., bfloat16 or float32), different GPU models may produce slightly different results due to differences in their microarchitecture, instruction scheduling, or specialized hardware accelerators. These hardware-level variations become important when considering verification and reproducibility in production deployments.\n",
    "\n",
    "2.2 Tensor Parallelism and Distributed Inference\n",
    "\n",
    "Large language models often exceed the memory capacity of a single GPU, necessitating distribution across multiple devices. Tensor parallelism splits individual weight matrices across GPUs, requiring careful coordination of matrix multiplications and communication between devices. When a layer is distributed across n GPUs, each GPU computes a portion of the output, which must then be combined through collective communication operations.\n",
    "\n",
    "The specific parallelization strategy affects not just performance but also numerical behavior. Different decompositions of the same computation lead to different orders of floating-point operations, potentially resulting in different final values even when starting from identical weights and inputs. This sensitivity to parallelization strategy has important implications for model verification and monitoring.\n",
    "\n",
    "Pipeline parallelism represents an alternative approach, where different layers of the model reside on different GPUs. Forward passes proceed in a pipelined fashion, with activations flowing from one GPU to the next. While pipeline parallelism typically introduces less numerical variation than tensor parallelism (since each layer's computation remains intact), it requires careful management of micro-batching to maintain efficiency.\n",
    "\n",
    "2.3 Memory Hierarchies and Caching\n",
    "\n",
    "Modern GPUs have complex memory hierarchies, including registers, L1 cache, L2 cache, shared memory, and global memory (VRAM). Efficient inference requires careful orchestration of data movement through these levels, as memory bandwidth often becomes the primary bottleneck rather than computational throughput.\n",
    "\n",
    "The KV (key-value) cache exemplifies memory management challenges in language model inference. During autoregressive generation, previously computed key and value vectors must be retained and accessed at each step. For long contexts, this cache can grow to dominate memory usage. Innovations like paged attention manage the KV cache more efficiently by storing it in non-contiguous memory blocks, similar to how operating systems manage virtual memory.\n",
    "\n",
    "Part III: Quantization and Efficiency\n",
    "\n",
    "3.1 Quantization Techniques\n",
    "\n",
    "Quantization reduces the precision of model weights and activations, trading some accuracy for significant improvements in memory usage and computational efficiency. Early post-training quantization methods simply converted trained models to lower precision, but more sophisticated approaches now integrate quantization awareness into the training process.\n",
    "\n",
    "INT8 quantization represents weights and activations as 8-bit integers rather than 32-bit floating-point numbers, achieving 4× memory reduction and enabling the use of specialized integer arithmetic units on modern processors. More aggressive quantization schemes, including INT4 and even binary networks, push these boundaries further, though with increasing risk to model quality.\n",
    "\n",
    "Weight-only quantization, where activations remain in higher precision while weights are quantized, often provides an attractive trade-off. This approach maintains most of the model's accuracy while still achieving significant memory savings and bandwidth improvements. The asymmetry reflects the fact that weight values are known at deployment time and can be carefully calibrated, while activations vary with each input.\n",
    "\n",
    "3.2 Quantization-Aware Training\n",
    "\n",
    "Quantization-aware training (QAT) incorporates quantization operations into the training process itself, allowing the model to learn weight distributions that are more amenable to low-precision representation. During QAT, the forward pass simulates quantization effects using fake quantization operations, while the backward pass still uses full-precision gradients.\n",
    "\n",
    "The effectiveness of QAT depends critically on the quantization scheme used. Symmetric quantization maps both positive and negative values uniformly, while asymmetric quantization can adapt to weight distributions that don't center on zero. Per-tensor quantization uses a single scale factor for an entire tensor, while per-channel or per-group quantization allows finer-grained adaptation to local statistics.\n",
    "\n",
    "Recent advances in microscaling (MXFP) quantization formats, such as MXFP4 and MXFP6, provide a middle ground between integer and floating-point quantization. These formats maintain a small floating-point exponent while using very few mantissa bits, preserving the dynamic range of floating-point numbers while approaching the memory efficiency of integer quantization.\n",
    "\n",
    "Part IV: Inference Optimization and Kernels\n",
    "\n",
    "4.1 CUDA Kernels and Operator Fusion\n",
    "\n",
    "Inference efficiency depends critically on the implementation of individual operations (kernels) that execute on the GPU. Standard deep learning frameworks provide default kernel implementations, but specialized hand-written kernels can often achieve substantial speedups by exploiting specific hardware features or operation patterns.\n",
    "\n",
    "Operator fusion combines multiple sequential operations into a single kernel, reducing memory traffic by keeping intermediate results in fast memory rather than writing them back to global memory. For instance, fusing a matrix multiplication with its subsequent activation function eliminates the need to store the pre-activation values in VRAM. Modern frameworks use sophisticated graph optimization passes to identify and execute such fusion opportunities automatically.\n",
    "\n",
    "The specific kernel implementation chosen for an operation can affect not just performance but also numerical results. Different kernel implementations may use different algorithms or accumulation orders, leading to different rounding errors. This variation becomes particularly pronounced when comparing implementations across different framework versions or hardware generations.\n",
    "\n",
    "4.2 Compilation and Just-In-Time Optimization\n",
    "\n",
    "Modern deep learning frameworks increasingly employ just-in-time compilation to generate optimized code for specific model configurations and hardware targets. PyTorch's torch.compile, for instance, captures the computational graph and applies a variety of optimizations before generating machine code.\n",
    "\n",
    "These compiler optimizations include operation reordering, dead code elimination, constant folding, and memory layout transformations. While these transformations preserve mathematical correctness in exact arithmetic, they can alter the order and grouping of floating-point operations, potentially affecting numerical outputs. The difference is typically small—often on the order of 10^-6 in relative terms—but may be detectable when comparing against uncompiled baselines.\n",
    "\n",
    "Graph compilation can also unlock new optimization opportunities that aren't available in eager execution mode. For instance, memory planning algorithms can allocate activation buffers more efficiently when they have visibility into the entire computational graph, reducing peak memory usage. Similarly, automatic kernel selection can make globally optimal choices rather than greedy local decisions.\n",
    "\n",
    "Part V: Production Deployment and Batching\n",
    "\n",
    "5.1 Continuous Batching and Request Scheduling\n",
    "\n",
    "Production inference services must handle streams of incoming requests efficiently. Continuous batching, also called iteration-level batching, allows new requests to join a running batch at each generation step. This approach maximizes GPU utilization by ensuring the batch remains full even as individual requests complete at different times.\n",
    "\n",
    "The specific composition of a batch—which particular inputs are grouped together—generally doesn't affect individual outputs in modern attention mechanisms, thanks to the attention mask that prevents information leakage between batch elements. However, the batch size itself does matter: larger batches often employ different computational strategies that may introduce small numerical differences.\n",
    "\n",
    "Memory management for continuous batching requires sophisticated orchestration. The paged attention mechanism treats the KV cache as a virtual memory space, allocating physical memory blocks dynamically as requests arrive and deallocating them as requests complete. This flexibility enables much higher throughput than static batching approaches, though it introduces additional complexity in memory access patterns.\n",
    "\n",
    "5.2 Scheduling and Resource Allocation\n",
    "\n",
    "Inference servers must make real-time decisions about which requests to batch together and how to allocate limited GPU resources. These decisions involve trade-offs between latency (how long individual requests wait) and throughput (how many requests can be served per second). Different scheduling policies lead to different performance characteristics.\n",
    "\n",
    "CUDA streams provide a mechanism for overlapping computation and communication operations. Multiple streams can execute concurrently on the same GPU, potentially improving utilization by allowing I/O operations and computation to proceed in parallel. However, when multiple streams compete for the same compute resources, their interactions can introduce timing variations and potential numerical artifacts from scheduling conflicts.\n",
    "\n",
    "Part VI: Verification and Monitoring\n",
    "\n",
    "6.1 The Challenge of Comprehensive Reporting\n",
    "\n",
    "In scenarios where datacenter operators must report all ML computations performed on their hardware—such as might arise under international AI governance agreements—verification becomes critical. Even if operators provide detailed logs of their inference workloads, how can external auditors verify that these logs are comprehensive?\n",
    "\n",
    "A malicious operator might attempt to hide unauthorized computations by secretly improving the efficiency of their declared workloads, freeing up capacity for off-the-books inference. Traditional monitoring approaches might detect obvious violations, such as periods of idle GPU time being used for hidden work, but wouldn't catch more sophisticated evasion strategies.\n",
    "\n",
    "6.2 Floating-Point Forensics\n",
    "\n",
    "One promising approach to verification leverages the numerical sensitivity we've discussed throughout this document. If an operator claims to have produced certain outputs using a specific computational setup, a verifier can attempt to reproduce those outputs on a trusted verification cluster. Exact numerical matching would provide strong evidence that the claimed setup was indeed used.\n",
    "\n",
    "This forensic approach relies on two key properties we've established: first, that identical setups produce bit-exact or near-identical results (low statistical noise), and second, that different setups produce systematically different outputs (detectable systematic deviations). The combination of these properties creates a unique numerical \"fingerprint\" for each inference configuration.\n",
    "\n",
    "Different aspects of the computational setup leave distinct signatures. Batch size changes typically produce L2 deviations on the order of 0.3-0.6, while changes in attention implementation (e.g., eager attention vs FlashAttention) create much larger deviations, often exceeding 40. CUDA version changes can lead to dramatic differences, with errors growing by several orders of magnitude between major versions. Even compilation optimizations like torch.compile introduce detectable deviations of around 2-5 in L2 norm.\n",
    "\n",
    "6.3 Attack Vectors and Detection Limits\n",
    "\n",
    "An adversary attempting to perform hidden inference in parallel with declared workloads would likely affect the computational environment in detectable ways. Running additional work on separate CUDA streams, for instance, creates resource contention and scheduling conflicts. At high GPU utilization—the regime most relevant for production systems—these conflicts introduce measurable timing variations and numerical artifacts.\n",
    "\n",
    "The economics of different evasion strategies also matter. Small-scale hidden workloads that might evade detection provide minimal benefit to an attacker, as they can't significantly offset the cost of running a datacenter. Larger hidden workloads that would provide meaningful economic value are more likely to create detectable signatures, either through timing anomalies, numerical deviations, or both.\n",
    "\n",
    "However, certain attack vectors remain challenging to detect. For instance, if an adversary can successfully hide parallel work that affects only low-arithmetic-intensity operations (like attention operations over long sequences), they might avoid creating floating-point deviations even while causing measurable slowdowns. The detectability of such scenarios requires careful analysis of both numerical and timing forensics.\n",
    "\n",
    "Part VII: Future Directions\n",
    "\n",
    "7.1 Hardware Evolution\n",
    "\n",
    "Next-generation AI accelerators continue to push the boundaries of performance and efficiency. Blackwell's B200 GPUs, for instance, include native support for FP4 and FP6 microscaling formats, enabling even more aggressive quantization while maintaining model quality. As hardware capabilities evolve, the landscape of numerical behavior and reproducibility characteristics will continue to shift.\n",
    "\n",
    "Specialized AI chips from various vendors introduce additional diversity in computational behavior. Each architecture makes different trade-offs in its implementation of floating-point operations, memory hierarchies, and specialized accelerators. This diversity enriches the space of possible forensic signatures while also complicating verification protocols that must account for legitimate hardware variations.\n",
    "\n",
    "7.2 Software Stack Evolution\n",
    "\n",
    "Deep learning frameworks continue to evolve rapidly, with new optimizations and features appearing in each release. This evolution creates challenges for reproducibility: results from one framework version may differ from another, even when using identical models and inputs. From a forensics perspective, framework version becomes another factor that must be either controlled or characterized.\n",
    "\n",
    "The trend toward greater automation in optimization—such as automatic kernel selection, dynamic batching strategies, and just-in-time compilation—generally improves performance but can make numerical behavior less predictable. Balancing the benefits of these optimizations against the need for reproducibility and verifiability represents an ongoing challenge for production ML systems.\n",
    "\n",
    "7.3 Governance and Policy Implications\n",
    "\n",
    "The techniques discussed in this document have implications beyond technical verification. If floating-point forensics proves reliable for detecting various forms of computational evasion, it could inform the design of AI governance mechanisms and international agreements. The ability to verify claimed computations without requiring complete transparency into internal operations might enable monitoring frameworks that balance accountability with legitimate concerns about intellectual property and competitive advantage.\n",
    "\n",
    "However, realizing this potential requires continued research into the limits and capabilities of forensic approaches. What types of computational changes are reliably detectable? What attack vectors remain? How do detection capabilities scale to different model architectures, sizes, and deployment scenarios? Answering these questions will require systematic experimentation across a wide range of conditions.\n",
    "\n",
    "Conclusion\n",
    "\n",
    "The numerical behavior of large language models reflects a complex interplay between model architecture, hardware characteristics, software implementation, and deployment configurations. While this complexity initially appears to pose challenges for reproducibility, it also creates opportunities: the unique numerical fingerprint of each setup can serve as a basis for verification and monitoring in governance contexts.\n",
    "\n",
    "The path forward requires continued research at the intersection of systems optimization, numerical computing, and AI safety. As models grow larger and deployment scenarios more diverse, maintaining the ability to verify and monitor AI systems becomes increasingly important. The forensic approaches discussed here represent one promising direction, but their ultimate viability depends on rigorous empirical validation across realistic production conditions.\n",
    "\n",
    "This document has surveyed the technical foundations necessary to understand these verification challenges and potential solutions. The field continues to evolve rapidly, and many questions remain open. Nevertheless, the convergence of growing AI capabilities, increasing deployment scale, and emerging governance frameworks makes this research area both timely and critical for the safe development of advanced AI systems.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"AWQ NON-DETERMINISM ROOT CAUSE INVESTIGATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Quantization: {QUANTIZATION} (INT4)\")\n",
    "print(f\"Prompt length: {len(TEST_PROMPT)} chars\")\n",
    "print(f\"Repetitions per mode: {NUM_REPETITIONS}\")\n",
    "print()\n",
    "print(\"Investigation plan:\")\n",
    "print(\"  1. Baseline: Standard vLLM generation (non-deterministic expected)\")\n",
    "print(\"  2. With determinism: torch.use_deterministic_algorithms(True)\")\n",
    "print(\"  3. Kernel profiling: Identify which kernels are called\")\n",
    "print(\"  4. Analysis: Compare noise levels and kernel usage\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def set_deterministic_mode(enable=True):\n",
    "    \"\"\"Enable/disable PyTorch deterministic algorithms\"\"\"\n",
    "    if enable:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        print(\"✓ Deterministic mode ENABLED\")\n",
    "    else:\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "        torch.backends.cudnn.deterministic = False\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        print(\"✓ Deterministic mode DISABLED (standard vLLM)\")\n",
    "    print()\n",
    "\n",
    "def clear_gpu():\n",
    "    \"\"\"Aggressively clear GPU memory\"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def run_repetitions(llm, prompt_text, sampling_params, num_reps, mode_name):\n",
    "    \"\"\"Run multiple generations and collect results\"\"\"\n",
    "    print(f\"Running {num_reps} repetitions in {mode_name} mode...\")\n",
    "    \n",
    "    results_tokens = []\n",
    "    results_logprobs = []\n",
    "    results_distributions = []\n",
    "    \n",
    "    for rep in range(num_reps):\n",
    "        clear_gpu()\n",
    "        \n",
    "        outputs = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "        output = outputs[0]\n",
    "        \n",
    "        # Extract data\n",
    "        token_ids = output.outputs[0].token_ids\n",
    "        results_tokens.append(token_ids)\n",
    "        \n",
    "        # Logprobs\n",
    "        logprobs_data = output.outputs[0].logprobs\n",
    "        selected_logprobs = [lp[token_ids[i]].logprob for i, lp in enumerate(logprobs_data)]\n",
    "        results_logprobs.append(np.array(selected_logprobs))\n",
    "        \n",
    "        # Distributions\n",
    "        rep_distributions = []\n",
    "        for position_logprobs in logprobs_data:\n",
    "            sorted_items = sorted(position_logprobs.items(), \n",
    "                                key=lambda x: x[1].logprob, \n",
    "                                reverse=True)[:TOP_LOGPROBS]\n",
    "            rep_distributions.append([(tok, lp.logprob) for tok, lp in sorted_items])\n",
    "        results_distributions.append(rep_distributions)\n",
    "        \n",
    "        if (rep + 1) % 5 == 0:\n",
    "            print(f\"  Completed {rep + 1}/{num_reps} repetitions\")\n",
    "    \n",
    "    print(f\"✓ {mode_name} mode complete: {num_reps} repetitions\")\n",
    "    print()\n",
    "    \n",
    "    return results_tokens, results_logprobs, results_distributions\n",
    "\n",
    "def analyze_reproducibility(results_tokens, results_logprobs, mode_name):\n",
    "    \"\"\"Analyze reproducibility statistics\"\"\"\n",
    "    print(f\"Analysis: {mode_name}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Check token sequences\n",
    "    tokens_identical = all(\n",
    "        results_tokens[0] == results_tokens[i] \n",
    "        for i in range(1, len(results_tokens))\n",
    "    )\n",
    "    print(f\"Token sequences identical: {tokens_identical}\")\n",
    "    \n",
    "    # Check logprobs\n",
    "    first_logprobs = results_logprobs[0]\n",
    "    logprobs_exact = all(\n",
    "        np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "        for i in range(1, len(results_logprobs))\n",
    "    )\n",
    "    print(f\"Logprobs bit-exact: {logprobs_exact}\")\n",
    "    \n",
    "    if not logprobs_exact:\n",
    "        # Compute L2 distances\n",
    "        l2_distances = []\n",
    "        for i in range(1, len(results_logprobs)):\n",
    "            l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "            l2_distances.append(l2)\n",
    "        \n",
    "        print(f\"\\nLogprob deviations:\")\n",
    "        print(f\"  Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "        print(f\"  Std L2:  {np.std(l2_distances):.6e}\")\n",
    "        print(f\"  Min L2:  {np.min(l2_distances):.6e}\")\n",
    "        print(f\"  Max L2:  {np.max(l2_distances):.6e}\")\n",
    "        \n",
    "        # Per-token statistics\n",
    "        all_logprobs = np.array(results_logprobs)\n",
    "        std_per_token = all_logprobs.std(axis=0)\n",
    "        print(f\"\\nPer-token std:\")\n",
    "        print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "        print(f\"  Max:  {std_per_token.max():.6e}\")\n",
    "        print(f\"  Min:  {std_per_token.min():.6e}\")\n",
    "        \n",
    "        return {\n",
    "            'tokens_identical': tokens_identical,\n",
    "            'logprobs_exact': logprobs_exact,\n",
    "            'l2_mean': float(np.mean(l2_distances)),\n",
    "            'l2_std': float(np.std(l2_distances)),\n",
    "            'l2_max': float(np.max(l2_distances)),\n",
    "            'std_per_token_mean': float(std_per_token.mean()),\n",
    "            'std_per_token_max': float(std_per_token.max())\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            'tokens_identical': tokens_identical,\n",
    "            'logprobs_exact': logprobs_exact,\n",
    "            'l2_mean': 0.0,\n",
    "            'l2_std': 0.0,\n",
    "            'l2_max': 0.0,\n",
    "            'std_per_token_mean': 0.0,\n",
    "            'std_per_token_max': 0.0\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE PROMPT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Preparing prompt...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir='/tmp/hf_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": TEST_PROMPT}]\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_text)\n",
    "prompt_length = len(prompt_tokens)\n",
    "\n",
    "print(f\"✓ Prompt prepared: {prompt_length} tokens\")\n",
    "print()\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    "    logprobs=TOP_LOGPROBS,\n",
    "    skip_special_tokens=False\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 1: BASELINE (NON-DETERMINISTIC MODE)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT 1: BASELINE (Standard vLLM)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "set_deterministic_mode(False)\n",
    "\n",
    "print(\"Loading model...\")\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    quantization=QUANTIZATION,\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    trust_remote_code=True,\n",
    "    seed=SEED,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False\n",
    ")\n",
    "print(\"✓ Model loaded\")\n",
    "print()\n",
    "\n",
    "# Warmup\n",
    "print(\"Warmup...\")\n",
    "for _ in range(3):\n",
    "    _ = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "clear_gpu()\n",
    "print(\"✓ Warmup complete\")\n",
    "print()\n",
    "\n",
    "# Run baseline\n",
    "baseline_tokens, baseline_logprobs, baseline_dists = run_repetitions(\n",
    "    llm, prompt_text, sampling_params, NUM_REPETITIONS, \"BASELINE\"\n",
    ")\n",
    "\n",
    "baseline_stats = analyze_reproducibility(baseline_tokens, baseline_logprobs, \"BASELINE\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# KERNEL PROFILING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"KERNEL PROFILING\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Profiling kernel calls during generation...\")\n",
    "\n",
    "clear_gpu()\n",
    "\n",
    "try:\n",
    "    if PROFILER_NEW_API:\n",
    "        # New PyTorch profiler API (1.8+)\n",
    "        with profile(\n",
    "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "            with_stack=False,\n",
    "            profile_memory=False,\n",
    "            record_shapes=False\n",
    "        ) as prof:\n",
    "            _ = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "    else:\n",
    "        # Old PyTorch profiler API\n",
    "        with profiler_module.profile(\n",
    "            use_cuda=True,\n",
    "            with_stack=False,\n",
    "            profile_memory=False,\n",
    "            record_shapes=False\n",
    "        ) as prof:\n",
    "            _ = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "\n",
    "    print()\n",
    "    print(\"Top 20 kernels by CUDA time:\")\n",
    "    print(\"-\" * 80)\n",
    "    kernel_table = prof.key_averages().table(\n",
    "        sort_by=\"cuda_time_total\",\n",
    "        row_limit=20\n",
    "    )\n",
    "    print(kernel_table)\n",
    "    print()\n",
    "\n",
    "    # Extract kernel names for analysis\n",
    "    kernel_data = []\n",
    "    for evt in prof.key_averages():\n",
    "        # Handle both API versions\n",
    "        is_cuda = False\n",
    "        if PROFILER_NEW_API:\n",
    "            is_cuda = hasattr(evt, 'device_type') and evt.device_type == ProfilerActivity.CUDA\n",
    "        else:\n",
    "            is_cuda = evt.cuda_time_total > 0\n",
    "        \n",
    "        if is_cuda and evt.cuda_time_total > 0:\n",
    "            kernel_data.append({\n",
    "                'name': evt.key,\n",
    "                'cuda_time_us': evt.cuda_time_total,\n",
    "                'count': evt.count,\n",
    "                'avg_time_us': evt.cuda_time_total / evt.count if evt.count > 0 else 0\n",
    "            })\n",
    "    \n",
    "    profiling_succeeded = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Kernel profiling failed: {e}\")\n",
    "    print(\"Continuing without profiling data...\")\n",
    "    print()\n",
    "    kernel_data = []\n",
    "    marlin_kernels = []\n",
    "    gemm_kernels = []\n",
    "    dequant_kernels = []\n",
    "    profiling_succeeded = False\n",
    "\n",
    "# Look for Marlin-specific kernels\n",
    "if profiling_succeeded:\n",
    "    marlin_kernels = [k for k in kernel_data if 'marlin' in k['name'].lower()]\n",
    "    gemm_kernels = [k for k in kernel_data if 'gemm' in k['name'].lower()]\n",
    "    dequant_kernels = [k for k in kernel_data if 'dequant' in k['name'].lower() or 'quant' in k['name'].lower()]\n",
    "\n",
    "    print(\"Kernel categories detected:\")\n",
    "    print(f\"  Marlin kernels: {len(marlin_kernels)}\")\n",
    "    print(f\"  GEMM kernels: {len(gemm_kernels)}\")\n",
    "    print(f\"  Dequant kernels: {len(dequant_kernels)}\")\n",
    "    print()\n",
    "\n",
    "    if marlin_kernels:\n",
    "        print(\"Marlin-specific kernels:\")\n",
    "        for k in marlin_kernels[:5]:\n",
    "            print(f\"  {k['name'][:60]} - {k['cuda_time_us']/1000:.2f}ms\")\n",
    "        print()\n",
    "else:\n",
    "    marlin_kernels = []\n",
    "    gemm_kernels = []\n",
    "    dequant_kernels = []\n",
    "\n",
    "# ============================================================================\n",
    "# EXPERIMENT 2: WITH DETERMINISTIC MODE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXPERIMENT 2: WITH DETERMINISTIC MODE\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Unload model\n",
    "del llm\n",
    "clear_gpu()\n",
    "\n",
    "# Enable deterministic mode\n",
    "set_deterministic_mode(True)\n",
    "\n",
    "# Reload model\n",
    "print(\"Reloading model with deterministic settings...\")\n",
    "try:\n",
    "    llm = LLM(\n",
    "        model=MODEL_NAME,\n",
    "        quantization=QUANTIZATION,\n",
    "        tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "        max_model_len=MAX_MODEL_LEN,\n",
    "        gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "        trust_remote_code=True,\n",
    "        seed=SEED,\n",
    "        enforce_eager=True,\n",
    "        enable_prefix_caching=False\n",
    "    )\n",
    "    print(\"✓ Model loaded with deterministic mode\")\n",
    "    print()\n",
    "    \n",
    "    # Warmup\n",
    "    print(\"Warmup...\")\n",
    "    for _ in range(3):\n",
    "        _ = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "    clear_gpu()\n",
    "    print(\"✓ Warmup complete\")\n",
    "    print()\n",
    "    \n",
    "    # Run with deterministic mode\n",
    "    det_tokens, det_logprobs, det_dists = run_repetitions(\n",
    "        llm, prompt_text, sampling_params, NUM_REPETITIONS, \"DETERMINISTIC\"\n",
    "    )\n",
    "    \n",
    "    det_stats = analyze_reproducibility(det_tokens, det_logprobs, \"DETERMINISTIC\")\n",
    "    print()\n",
    "    \n",
    "    deterministic_succeeded = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Deterministic mode failed: {e}\")\n",
    "    print(\"This suggests deterministic algorithms don't support all operations\")\n",
    "    print()\n",
    "    deterministic_succeeded = False\n",
    "    det_stats = None\n",
    "\n",
    "# ============================================================================\n",
    "# COMPARATIVE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPARATIVE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "print(\"Baseline vs Deterministic Mode:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Metric':<30} {'Baseline':<20} {'Deterministic':<20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if deterministic_succeeded and det_stats:\n",
    "    print(f\"{'Tokens identical':<30} {str(baseline_stats['tokens_identical']):<20} {str(det_stats['tokens_identical']):<20}\")\n",
    "    print(f\"{'Logprobs exact':<30} {str(baseline_stats['logprobs_exact']):<20} {str(det_stats['logprobs_exact']):<20}\")\n",
    "    print(f\"{'Mean L2 distance':<30} {baseline_stats['l2_mean']:.6e}  {det_stats['l2_mean']:.6e}\")\n",
    "    print(f\"{'Max L2 distance':<30} {baseline_stats['l2_max']:.6e}  {det_stats['l2_max']:.6e}\")\n",
    "    print(f\"{'Per-token std (mean)':<30} {baseline_stats['std_per_token_mean']:.6e}  {det_stats['std_per_token_mean']:.6e}\")\n",
    "    print(f\"{'Per-token std (max)':<30} {baseline_stats['std_per_token_max']:.6e}  {det_stats['std_per_token_max']:.6e}\")\n",
    "else:\n",
    "    print(f\"{'Tokens identical':<30} {str(baseline_stats['tokens_identical']):<20} {'N/A':<20}\")\n",
    "    print(f\"{'Logprobs exact':<30} {str(baseline_stats['logprobs_exact']):<20} {'N/A':<20}\")\n",
    "    print(f\"{'Mean L2 distance':<30} {baseline_stats['l2_mean']:.6e}  {'N/A':<20}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if not baseline_stats['logprobs_exact']:\n",
    "    print(\"❌ BASELINE IS NON-DETERMINISTIC\")\n",
    "    print(f\"   Mean L2 deviation: {baseline_stats['l2_mean']:.6e}\")\n",
    "    print(f\"   Max L2 deviation: {baseline_stats['l2_max']:.6e}\")\n",
    "    print()\n",
    "    \n",
    "    if deterministic_succeeded and det_stats:\n",
    "        if det_stats['logprobs_exact']:\n",
    "            print(\"✓ DETERMINISTIC MODE FIXES IT\")\n",
    "            print(\"  → Root cause: Non-deterministic parallel reduction\")\n",
    "            print(\"  → Likely in Marlin kernel accumulation logic\")\n",
    "            print(\"  → Can be fixed but at performance cost\")\n",
    "        else:\n",
    "            improvement = baseline_stats['l2_mean'] / det_stats['l2_mean'] if det_stats['l2_mean'] > 0 else float('inf')\n",
    "            if improvement > 2:\n",
    "                print(\"⚠ DETERMINISTIC MODE REDUCES NOISE\")\n",
    "                print(f\"  → {improvement:.1f}x reduction in mean L2\")\n",
    "                print(\"  → Partial fix, some non-determinism remains\")\n",
    "            else:\n",
    "                print(\"❌ DETERMINISTIC MODE DOESN'T HELP\")\n",
    "                print(\"  → Root cause is NOT parallel reduction races\")\n",
    "                print(\"  → Likely in quantization/dequantization logic itself\")\n",
    "    else:\n",
    "        print(\"⚠ Could not test deterministic mode (not supported)\")\n",
    "        print(\"  → Suggests operations incompatible with deterministic algorithms\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Forensic implications:\")\n",
    "    print(f\"  Noise level: ~{baseline_stats['l2_mean']:.2e} L2\")\n",
    "    print(\"  For detection, need systematic deviation >> noise\")\n",
    "    print(\"  Recommend: 3-5 samples for statistical significance\")\n",
    "else:\n",
    "    print(\"✓ BASELINE IS DETERMINISTIC\")\n",
    "    print(\"  → Unexpected! AWQ should be non-deterministic in vLLM\")\n",
    "    print(\"  → May have been fixed in recent version\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"awq_nondeterminism_investigation\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"quantization\": QUANTIZATION,\n",
    "    \"config\": {\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_model_len\": MAX_MODEL_LEN,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED\n",
    "    },\n",
    "    \"prompt_length\": prompt_length,\n",
    "    \"baseline\": {\n",
    "        \"stats\": baseline_stats,\n",
    "        \"tokens\": baseline_tokens,\n",
    "        \"logprobs\": [lp.tolist() for lp in baseline_logprobs]\n",
    "    },\n",
    "    \"deterministic\": {\n",
    "        \"succeeded\": deterministic_succeeded,\n",
    "        \"stats\": det_stats if det_stats else None,\n",
    "        \"tokens\": det_tokens if deterministic_succeeded else None,\n",
    "        \"logprobs\": [lp.tolist() for lp in det_logprobs] if deterministic_succeeded else None\n",
    "    },\n",
    "    \"kernels\": {\n",
    "        \"profiling_succeeded\": profiling_succeeded,\n",
    "        \"all_kernels\": kernel_data if profiling_succeeded else [],\n",
    "        \"marlin_kernels\": marlin_kernels if profiling_succeeded else [],\n",
    "        \"gemm_kernels\": gemm_kernels if profiling_succeeded else [],\n",
    "        \"dequant_kernels\": dequant_kernels if profiling_succeeded else []\n",
    "    }\n",
    "}\n",
    "\n",
    "output_file = f\"awq_investigation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"INVESTIGATION COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f5c94-f4f4-489d-b91c-627a5b6acdd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
