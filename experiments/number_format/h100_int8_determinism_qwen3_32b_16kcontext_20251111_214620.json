{
  "experiment": "h100_int8_determinism_test_bitsandbytes_qwen3_32b_yarn",
  "timestamp": "2025-11-11T21:46:20.625922",
  "hardware": "H100",
  "prompt_source": "file",
  "prompt_file": "NSA.pdf",
  "prompt_text": "<|im_start|>user\nPlease provide a detailed summary of the following text.\n\nNative Sparse Attention: Hardware-Aligned and Natively\nTrainable Sparse Attention\nJingyang Yuan\u22171,2, Huazuo Gao1, Damai Dai1, Junyu Luo2, Liang Zhao1, Zhengyan Zhang1, Zhenda Xie1,\nY. X. Wei1, Lean Wang1, Zhiping Xiao3, Yuqing Wang1, Chong Ruan1, Ming Zhang2, Wenfeng Liang1,\nWangding Zeng1\n1DeepSeek-AI\n2Key Laboratory for Multimedia Information Processing, Peking University, PKU-Anker LLM Lab\n3University of Washington\n{yuanjy, mzhang_cs}@pku.edu.cn ,{zengwangding, wenfeng.liang}@deepseek.com\nAbstract\nLong-context modeling is crucial for next-generation language models, yet the high compu-\ntational cost of standard attention mechanisms poses significant computational challenges.\nSparse attention offers a promising direction for improving efficiency while maintaining model\ncapabilities. We present NSA, a Natively trainable Sparse Attention mechanism that integrates\nalgorithmic innovations with hardware-aligned optimizations to achieve efficient long-context\nmodeling. NSA employs a dynamic hierarchical sparse strategy, combining coarse-grained\ntoken compression with fine-grained token selection to preserve both global context awareness\nand local precision. Our approach advances sparse attention design with two key innovations:\n(1) We achieve substantial speedups through arithmetic intensity-balanced algorithm design,\nwith implementation optimizations for modern hardware. (2) We enable end-to-end training,\nreducing pretraining computation without sacrificing model performance. As shown in Figure 1,\nexperiments show the model pretrained with NSA maintains or exceeds Full Attention models\nacross general benchmarks, long-context tasks, and instruction-based reasoning. Meanwhile,\nNSA achieves substantial speedups over Full Attention on 64k-length sequences across decod-\ning, forward propagation, and backward propagation, validating its efficiency throughout the\nmodel lifecycle.\n1. Introduction\nThe research community increasingly recognizes long-context modeling as a crucial capability\nfor next-generation large language models, driven by diverse real-world applications ranging\nfrom in-depth reasoning (DeepSeek-AI, 2025; Zelikman et al., 2022), repository-level code gener-\nation (Zhang et al., 2023a; Zhang et al.) and multi-turn autonomous agent systems (Park et al.,\n2023). Recent breakthroughs, including OpenAI\u2019s o-series models, DeepSeek-R1 (DeepSeek-AI,\n2025), and Gemini 1.5 Pro (Google et al., 2024), enabling models to process entire codebases,\nlengthy documents, maintain coherent multi-turn conversations over thousands of tokens, and\nperform complex reasoning across long-range dependencies. However, the high complexity (Za-\nheer et al., 2020) of vanilla Attention (Vaswani et al., 2017) mechanisms emerges as a critical\nlatency bottleneck as sequence length increases. Theoretical estimates indicate that attention\n*Contribution during internship at DeepSeek-AI.arXiv:2502.11089v1  [cs.CL]  16 Feb 2025\nGeneralLongBenchReasoning0.00.10.20.30.40.5ScorePerformanceonBenchmarks\nFullAttention\nNSA\nDecodeForwardBackward1.03.05.07.09.011.013.0SpeedupRatio11.6\u00d7\n9.0\u00d7\n6.0\u00d7SpeedonStagesFigure 1|Comparison of performance and efficiency between Full Attention model and our NSA.\nLeft: Despite being sparse, NSA surpasses Full Attention baseline on average across general\nbenchmarks, long-context tasks, and reasoning evaluation. Right: For 64k-length sequence\nprocessing, NSA achieves substantial computational speedup compared to Full Attention in all\nstages: decoding, forward propagation, and backward propagation.\ncomputation with softmax architectures accounts for 70\u201380% of total latency when decoding\n64k-length contexts, underscoring the urgent need for more efficient attention mechanisms.\nA natural approach to efficient long-context modeling is to take advantage of the inherent\nsparsity of softmax attention (Ge et al., 2023; Jiang et al., 2023), where selectively computing\ncritical query-key pairs can significantly reduce computational overhead while preserving\nperformance. Recent advances demonstrate this potential through diverse strategies: KV-cache\neviction methods (Li et al., 2024; Zhang et al., 2023b; Zhou et al., 2024), blockwise KV-cache\nselection methods (Tang et al., 2024; Xiao et al., 2024), and sampling, clustering or hashing-based\nselection methods (Chen et al., 2024; Desai et al., 2024; Liu et al., 2024). Despite these promising\nstrategies, existing sparse attention methods often fall short in practical deployments. Many\napproaches fail to achieve speedups comparable to their theoretical gains; also, most methods\nmainly focus on inference stage, lacking effective training-time support to fully exploit the\nsparsity patterns of attention.\nTo address these limitations, the deployment of effective sparse attention must tackle two\nkey challenges: (1) Hardware-aligned inference speedup : Converting theoretical computation\nreductions into actual speed improvements requires hardware-friendly algorithm design during\nboth prefilling and decoding stages to mitigate memory access and hardware scheduling bottle-\nnecks; (2) Training-aware algorithm design : Enabling end-to-end computation with trainable\noperators to reduce training costs while maintaining model performance. These requirements\nare crucial for real-world applications to achieve fast long-context inference or training. When\nconsidering both aspects, existing methods still exhibit a noticeable gap.\nTo achieve more effective and efficient sparse attention, we present NSA, a Natively trainable\nSparse Attention architecture that integrates hierarchical token modeling. As shown in Figure 2,\nNSA reduces per-query computation by organizing keys and values into temporal blocks and\nprocessing them through three attention paths: compressed coarse-grained tokens, selectively\nretained fine-grained tokens, and sliding windows for local contextual information. Then\nwe implement specialized kernels to maximize its practical efficiency. NSA introduces two\n2\nConcatTop-n SelectionCompressionSelectionCompress\nCompressed AttentionSliding Attention\nSlidingCompressed Attention Mask  \nSliding Attention MaskSelected AttentionSplit to Continuous Blocks\nAttention ScoreActivated TokenQuery TokenIgnored TokenEvicted TokenNative Sparse Attention Mechanism \nSelected Attention MaskGated OutputOutputOutputOutput...Figure 2|Overview of NSA\u2019s architecture. Left: The framework processes input sequences\nthrough three parallel attention branches: For a given query, preceding keys and values are\nprocessed into compressed attention for coarse-grained patterns, selected attention for important\ntoken blocks, and sliding attention for local context. Right: Visualization of different attention\npatterns produced by each branch. Green areas indicate regions where attention scores need to\nbe computed, while white areas represent regions that can be skipped.\ncore innovations corresponding to the key requirements above: (1) Hardware-aligned system:\nOptimize blockwise sparse attention for Tensor Core utilization and memory access, ensuring\nbalanced arithmetic intensity. (2) Training-aware design: Enable stable end-to-end training\nthrough efficient algorithms and backward operators. This optimization enables NSA to support\nboth efficient deployment and end-to-end training.\nWe evaluate NSA through comprehensive experiments on real-world language corpora.\nPretraining on a 27B-parameter transformer backbone with 260B tokens, we assess NSA\u2019s per-\nformance across general language evaluations, long-context evaluations, and chain-of-thought\nreasoning evaluation. We further compare the kernel speed on A100 GPUs with optimized\nTriton (Tillet et al., 2019) implementations. Experimental results demonstrate that NSA achieves\ncomparable or superior performance to full attention baseline, while outperforming existing\nsparse attention approaches. Additionally, NSA delivers substantial speedups across decoding,\nforward, and backward stages compared to Full Attention, with the speedup ratio increasing for\nlonger sequences. These results validate that our hierarchical sparse attention design effectively\nbalances model capability and computational efficiency.\n2. Rethinking Sparse Attention Methods\nModern sparse attention methods have made significant strides in reducing the theoretical\ncomputational complexity of transformer models. However, most approaches predominantly\napply sparsity during inference while retaining a pretrained Full Attention backbone, poten-\ntially introducing architectural bias that limits their ability to fully exploit sparse attention\u2019s\nadvantages. Before introducing our native sparse architecture, we systematically analyze these\nlimitations through two critical lenses.\n2.1. The Illusion of Efficient Inference\nDespite achieving sparsity in attention computation, many methods fail to achieve correspond-\ning reductions in inference latency, primarily due to two challenges:\nPhase-Restricted Sparsity. Methods such as H2O (Zhang et al., 2023b) apply sparsity\n3\nduring autoregressive decoding while requiring computationally intensive pre-processing\n(e.g. attention map calculation, index building) during prefilling. In contrast, approaches\nlike MInference (Jiang et al., 2024) focus solely on prefilling sparsity. These methods fail to\nachieve acceleration across all inference stages, as at least one phase remains computational\ncosts comparable to Full Attention. The phase specialization reduces the speedup ability of these\nmethods in prefilling-dominated workloads like book summarization and code completion, or\ndecoding-dominated workloads like long chain-of-thought (Wei et al., 2022) reasoning.\nIncompatibility with Advanced Attention Architecture. Some sparse attention methods\nfail to adapt to modern decoding efficient architectures like Mulitiple-Query Attention (MQA)\n(Shazeer, 2019) and Grouped-Query Attention (GQA) (Ainslie et al., 2023), which significantly\nreduced the memory access bottleneck during decoding by sharing KV across multiple query\nheads. For instance, in approaches like Quest (Tang et al., 2024), each attention head indepen-\ndently selects its KV-cache subset. Although it demonstrates consistent computation sparsity\nand memory access sparsity in Multi-Head Attention (MHA) models, it presents a different sce-\nnario in models based on architectures like GQA, where the memory access volume of KV-cache\ncorresponds to the union of selections from all query heads within the same GQA group. This\narchitectural characteristic means that while these methods can reduce computation operations,\nthe required KV-cache memory access remains relatively high. This limitation forces a critical\nchoice: while some sparse attention methods reduce computation, their scattered memory access\npattern conflicts with efficient memory access design from advanced architectures.\nThese limitations arise because many existing sparse attention methods focus on KV-cache\nreduction or theoretical computation reduction, but struggle to achieve significant latency\nreduction in advanced frameworks or backends. This motivates us to develop algorithms that\ncombine both advanced architectural and hardware-efficient implementation to fully leverage\nsparsity for improving model efficiency.\n2.2. The Myth of Trainable Sparsity\nOur pursuit of native trainable sparse attention is motivated by two key insights from analyzing\ninference-only approaches: (1) Performance Degradation : Applying sparsity post-hoc forces\nmodels to deviate from their pretrained optimization trajectory. As demonstrated by Chen et al.\n(2024), top 20% attention can only cover 70% of the total attention scores, rendering structures\nlike retrieval heads in pretrained models vulnerable to pruning during inference. (2) Training\nEfficiency Demands : Efficient handling of long-sequence training is crucial for modern LLM\ndevelopment. This includes both pretraining on longer documents to enhance model capacity,\nand subsequent adaptation phases such as long-context fine-tuning and reinforcement learning.\nHowever, existing sparse attention methods primarily target inference, leaving the computa-\ntional challenges in training largely unaddressed. This limitation hinders the development\nof more capable long-context models through efficient training. Additionally, efforts to adapt\nexisting sparse attention for training also expose challenges:\nNon-Trainable Components. Discrete operations in methods like ClusterKV (Liu et al.,\n2024) (includes k-means clustering) and MagicPIG (Chen et al., 2024) (includes SimHash-based\nselecting) create discontinuities in the computational graph. These non-trainable components\nprevent gradient flow through the token selection process, limiting the model\u2019s ability to learn\noptimal sparse patterns.\nInefficient Back-propagation. Some theoretically trainable sparse attention methods suffer\nfrom practical training inefficiencies. Token-granular selection strategy used in approaches\n4\nlike HashAttention (Desai et al., 2024) leads to the need to load a large number of individual\ntokens from the KV cache during attention computation. This non-contiguous memory access\nprevents efficient adaptation of fast attention techniques like FlashAttention, which rely on\ncontiguous memory access and blockwise computation to achieve high throughput. As a result,\nimplementations are forced to fall back to low hardware utilization, significantly degrading\ntraining efficiency.\n2.3. Native Sparsity as an Imperative\nThese limitations in inference efficiency and training viability motivate our fundamental redesign\nof sparse attention mechanisms. We propose NSA, a natively sparse attention framework that\naddresses both computational efficiency and training requirements. In the following sections,\nwe detail the algorithmic design and operator implementation of NSA.\n3. Methodology\nOur technical approach spans algorithm design and kernel optimization. In the following\nsubsections, we first introduce the background of our methodology. Then we present the\noverall framework of NSA, followed by its key algorithmic components. Finally, we detail our\nhardware-optimized kernel design that maximizes practical efficiency.\n3.1. Background\nAttention Mechanism is widely used in language modeling where each query token q\ud835\udc61computes\nrelevance scores against all preceding keys k:\ud835\udc61to generate a weighted sum of values v:\ud835\udc61. Formally,\nfor an input sequence of length \ud835\udc61, the attention operation is defined as:\no\ud835\udc61=Attn\u0000q\ud835\udc61,k:\ud835\udc61,v:\ud835\udc61\u0001(1)\nwhere Attn denotes the attention function:\nAttn\u0000q\ud835\udc61,k:\ud835\udc61,v:\ud835\udc61\u0001=\ud835\udc61\u2211\ufe01\n\ud835\udc56=1\ud835\udefc\ud835\udc61,\ud835\udc56v\ud835\udc56\u00cd\ud835\udc61\n\ud835\udc57=1\ud835\udefc\ud835\udc61,\ud835\udc57,\ud835\udefc\ud835\udc61,\ud835\udc56=\ud835\udc52q\u22a4\n\ud835\udc61k\ud835\udc56\u221a\n\ud835\udc51\ud835\udc58. (2)\nHere,\ud835\udefc\ud835\udc61,\ud835\udc56represents the attention weight between q\ud835\udc61and k\ud835\udc56, and\ud835\udc51\ud835\udc58is the feature dimension of\nkeys. As sequence length increases, attention computation becomes increasingly dominant in\nthe overall computational cost, presenting significant challenges for long-context processing.\nArithmetic Intensity is the ratio of compute operations to memory accesses. It intrinsically\nshapes algorithm optimization on hardware. Each GPU has a critical arithmetic intensity\ndetermined by its peak compute capability and memory bandwidth, calculated as the ratio\nof these two hardware limits. For computation tasks, arithmetic intensity above this critical\nthreshold becomes compute-bound (limited by GPU FLOPS), while below it becomes memory-\nbound (limited by memory bandwidth).\nSpecifically for causal self-attention mechanism, during training and prefilling phases,\nbatched matrix multiplications and attention computations exhibit high arithmetic intensity,\nmaking these stages compute-bound on modern accelerators. In contrast, auto-regressive de-\ncoding becomes memory-bandwidth constrained because it generates one token per forward\npass while requiring loading the entire key-value cache, resulting in low arithmetic intensity.\nThis leads to different optimization goals \u2014 reducing computation cost during training and\nprefilling, while reducing memory access during decoding.\n5\n3.2. Overall Framework\nTo leverage the potential of attention with natural sparse pattern, we propose replacing the\noriginal key-value pairs k:\ud835\udc61,v:\ud835\udc61in Equation (1) with a more compact and information-dense set\nof representation key-value pairs \u02dc\ud835\udc3e\ud835\udc61,\u02dc\ud835\udc49\ud835\udc61given each query q\ud835\udc61. Specifically, we formally define the\noptimized attention output as follows:\n\u02dc\ud835\udc3e\ud835\udc61=\ud835\udc53\ud835\udc3e(q\ud835\udc61,k:\ud835\udc61,v:\ud835\udc61),\u02dc\ud835\udc49\ud835\udc61=\ud835\udc53\ud835\udc49(q\ud835\udc61,k:\ud835\udc61,v:\ud835\udc61) (3)\no\u2217\n\ud835\udc61=Attn\u0000q\ud835\udc61,\u02dc\ud835\udc3e\ud835\udc61,\u02dc\ud835\udc49\ud835\udc61\u0001(4)\nwhere \u02dc\ud835\udc3e\ud835\udc61,\u02dc\ud835\udc49\ud835\udc61are dynamically constructed based on the current query q\ud835\udc61and the contextual\nmemory k:\ud835\udc61,v:\ud835\udc61. We can design various mapping strategies to get different categories of \u02dc\ud835\udc3e\ud835\udc50\n\ud835\udc61,\u02dc\ud835\udc49\ud835\udc50\n\ud835\udc61,\nand combine them as follows:\no\u2217\n\ud835\udc61=\u2211\ufe01\n\ud835\udc50\u2208C\ud835\udc54\ud835\udc50\n\ud835\udc61\u00b7Attn(q\ud835\udc61,\u02dc\ud835\udc3e\ud835\udc50\n\ud835\udc61,\u02dc\ud835\udc49\ud835\udc50\n\ud835\udc61). (5)\nAs illustrated in Figure 2, NSA have three mapping strategies C={cmp ,slc,win}, representing\ncompression, selection, and sliding window for keys and values. \ud835\udc54\ud835\udc50\n\ud835\udc61\u2208[0, 1]is the gate score for\ncorresponding strategy \ud835\udc50, derived from input features via an MLP and sigmoid activation. Let\n\ud835\udc41\ud835\udc61denote the total number of remapped keys/values:\n\ud835\udc41\ud835\udc61=\u2211\ufe01\n\ud835\udc50\u2208Csize[\u02dc\ud835\udc3e\ud835\udc50\n\ud835\udc61]. (6)\nWe maintain a high sparsity ratio by ensuring \ud835\udc41\ud835\udc61\u226a\ud835\udc61.\n3.3. Algorithm Design\nIn this subsection, we introduce the design of our remapping strategies \ud835\udc53\ud835\udc3eand\ud835\udc53\ud835\udc49: token\ncompression, token selection, and sliding window.\n3.3.1. Token Compression\nBy aggregating sequential blocks of keys or values into block-level representations, we obtain\ncompressed keys and values that capture the information of the entire block. Formally, the\ncompressed key representation is defined as:\n\u02dc\ud835\udc3ecmp\n\ud835\udc61=\ud835\udc53cmp\n\ud835\udc3e(k:\ud835\udc61)=\u001a\n\ud835\udf11(k\ud835\udc56\ud835\udc51+1:\ud835\udc56\ud835\udc51+\ud835\udc59)\f\f\f\f1\u2a7d\ud835\udc56\u2a7d\u0016\ud835\udc61\u2212\ud835\udc59\n\ud835\udc51\u0017\u001b\n(7)\nwhere\ud835\udc59is the block length, \ud835\udc51is the sliding stride between adjacent blocks, and \ud835\udf11is a learnable\nMLP with intra-block position encoding to map keys in a block to a single compressed key.\n\u02dc\ud835\udc3ecmp\n\ud835\udc61\u2208R\ud835\udc51\ud835\udc58\u00d7\u230a\ud835\udc61\u2212\ud835\udc59\n\ud835\udc51\u230bis tensor composed by compresion keys. Usually, we adopt \ud835\udc51 < \ud835\udc59 to mitigate\ninformation fragmentation. An analogous formulation holds for the compressed value rep-\nresentation \u02dc\ud835\udc49cmp\n\ud835\udc61. Compressed representations capture coarser-grained higher-level semantic\ninformation and reduce computational burden of attention.\n6\n3.3.2. Token Selection\nUsing only compressed keys, values might lose important fine-grained information, motivating\nus to selectively preserve individual keys, values. Below we describe our efficient token selection\nmechanism that identifies and preserves the most relevant tokens with low computational\noverhead.\nBlockwise Selection. Our selection strategy processes key and value sequences in spacial\ncontinuous blocks, motivated by two key factors: hardware efficiency considerations and inher-\nent distribution patterns of attention scores. Blockwise selection is crucial to achieve efficient\ncomputation on modern GPUs. That is because modern GPU architectures exhibit significantly\nhigher throughput for continuous block accesses compared to random index-based reads. Also,\nblockwise computation enables optimal utilization of Tensor Cores. This architectural character-\nistic has established blockwise memory access and computation as a fundamental principle in\nhigh-performance attention implementations, as exemplified by FlashAttention\u2019s block-based\ndesign. Blockwise selection follows the inherent distribution patterns of attention scores.\nPrior works (Jiang et al., 2024) have shown that attention scores often exhibit spatial continuity,\nsuggesting that neighboring keys tend to share similar importance levels. Our visualization in\nSection 6.2 also shows this spatial continuous pattern.\nTo implement blockwise selection, we first divide key, value sequences into selection blocks.\nTo identify the most important blocks for attention computation, we need to assign importance\nscores to each block. Below we present our method for computing these block-level importance\nscores.\nImportance Score Computation. Computing block importance scores could introduce\nsignificant overhead. Fortunately, the attention computation of compression tokens produces\nintermediate attention scores that we can leverage to induce selection block importance scores,\nformulated as:\npcmp\n\ud835\udc61=Softmax\u0010\nq\ud835\udc47\n\ud835\udc61\u02dc\ud835\udc3ecmp\n\ud835\udc61\u0011\n, (8)\nwhere pcmp\n\ud835\udc61\u2208R\u230a\ud835\udc61\u2212\ud835\udc59\n\ud835\udc51\u230bis the attention scores between \ud835\udc5e\ud835\udc61and compression keys \u02dc\ud835\udc3ecmp\n\ud835\udc61. Let\ud835\udc59\u2032\ndenote the selection block size. When compression blocks and selection blocks share the same\nblocking scheme, i.e., \ud835\udc59\u2032=\ud835\udc59=\ud835\udc51, we can directly obtain the selection block importance scores pslc\n\ud835\udc61\nbypslc\n\ud835\udc61=pcmp\n\ud835\udc61straightforwardly. For cases where the blocking schemes differ, we derive the\nimportance scores for selection blocks according to their spatial relationship. Given \ud835\udc51|\ud835\udc59and\n\ud835\udc51|\ud835\udc59\u2032, we have:\npslc\n\ud835\udc61[\ud835\udc57]=\ud835\udc59\u2032\n\ud835\udc51\u22121\u2211\ufe01\n\ud835\udc5a=0\ud835\udc59\n\ud835\udc51\u22121\u2211\ufe01\n\ud835\udc5b=0pcmp\n\ud835\udc61\u0014\ud835\udc59\u2032\n\ud835\udc51\ud835\udc57+\ud835\udc5a+\ud835\udc5b\u0015\n, (9)\nwhere[\u00b7]denotes the indexing operator for accessing vector element. For models employing\nGQA or MQA where key-value caches are shared across query heads, consistent block selection\nacross these heads has to be ensured to minimize KV cache loading during decoding. The shared\nimportance scores across heads in a group are formally defined as:\npslc\n\ud835\udc61\u2032=\ud835\udc3b\u2211\ufe01\n\u210e=1pslc,(\u210e)\n\ud835\udc61 , (10)\nwhere(\u210e)in the superscript denotes the head index, and \ud835\udc3bis the number of query heads in each\ngroup. This aggregation ensures consistent block selection across heads within the same group.\n7\nTop-\ud835\udc5b\ud835\udc5b\ud835\udc5bBlock Selection. After obtaining the selection block importance scores, We retain\ntokens within the top- \ud835\udc5bsparse blocks ranked by block importance scores, formulated as:\nI\ud835\udc61={\ud835\udc56|rank(pslc\n\ud835\udc61\u2032[\ud835\udc56])\u2a7d\ud835\udc5b} (11)\n\u02dc\ud835\udc3eslc\n\ud835\udc61=Cat\u0002\n{k\ud835\udc56\ud835\udc59\u2032+1:(\ud835\udc56+1)\ud835\udc59\u2032|\ud835\udc56\u2208I\ud835\udc61}\u0003\n, (12)\nwhere rank(\u00b7)denotes the ranking position in descending order, with rank = 1 corresponding\nto the highest score, I\ud835\udc61is the set of selected blocks\u2019 indices, Catdenotes the concatenation\noperation. \u02dc\ud835\udc3eslc\n\ud835\udc61\u2208R\ud835\udc51\ud835\udc58\u00d7\ud835\udc5b\ud835\udc59\u2032is tensor composed by compresion keys. An analogous formulation\napplies to the fine-grained value \u02dc\ud835\udc49slc\n\ud835\udc61. The selected keys and values then participate in the\nattention computation with q\ud835\udc61as defined in Equation (5).\n3.3.3. Sliding Window\nIn attention mechanisms, local patterns typically adapt faster and can dominate the learning\nprocess, potentially preventing the model from effectively learning from compression and\nselection tokens. To address this issue, we introduce a dedicated sliding window branch that\nexplicitly handles local context, allowing other branches (compression and selection) to focus\non learning their respective features without being shortcutted by local patterns. Specifically,\nwe maintain recent tokens \u02dc\ud835\udc3ewin\n\ud835\udc61=k\ud835\udc61\u2212\ud835\udc64:\ud835\udc61,\u02dc\ud835\udc49win\n\ud835\udc61=v\ud835\udc61\u2212\ud835\udc64:\ud835\udc61in a window \ud835\udc64, and isolate attention\ncomputations of different information sources (compression tokens, and selected tokens, sliding\nwindow) into separate branches. These branch outputs are then aggregated through a learned\ngating mechanism. To further prevent shortcut learning across attention branches with marginal\ncomputational overhead, we provide independent keys and values for three branches. This\narchitectural design enables stable learning by preventing gradient interference between local\nand long-range pattern recognition, while introducing minimal overhead.\nAfter obtaining all three categories of keys and values ( \u02dc\ud835\udc3ecmp\n\ud835\udc61,\u02dc\ud835\udc49cmp\n\ud835\udc61;\u02dc\ud835\udc3eslc\n\ud835\udc61,\u02dc\ud835\udc49slc\n\ud835\udc61; and \u02dc\ud835\udc3ewin\n\ud835\udc61,\u02dc\ud835\udc49win\n\ud835\udc61),\nwe compute the final attention output following Equation (5). Together with the compression,\nselection, and sliding window mechanisms described above, this forms the complete algorithmic\nframework of NSA.\n3.4. Kernel Design\nTo achieve FlashAttention-level speedup during the training and prefilling, we implement\nhardware-aligned sparse attention kernels upon Triton. Given MHA is memory-intensive and\ninefficient for decoding, we focus on architectures with shared KV caches like GQA and MQA\nfollowing the current state-of-the-art LLMs. While compression and sliding window attention\ncomputations are readily compatible with existing FlashAttention-2 kernels, we introduce the\nspecialized kernel design for sparse selection attention. If we were to follow FlashAttention\u2019s\nstrategy of loading temporally continuous query blocks into SRAM, it would result in inefficient\nmemory access since queries within a block may require disjoint KV blocks. To address this,\nour key optimization lies in a different query grouping strategy: for each position on the\nquery sequence, we load all query heads within a GQA group (they share the same sparse KV\nblocks) into SRAM. Figure 3 illustrates our forward pass implementation. The proposed kernel\narchitecture is characterized by the following key features:\n1.Group-Centric Data Loading . For each inner loop, load all heads\u2019 queries \ud835\udc44\u2208R[\u210e,\ud835\udc51\ud835\udc58]in\nthe group at position \ud835\udc61and their shared sparse key/value block indices I\ud835\udc61.\n8\n2.Shared KV Fetching . In the inner loop, Sequentially load continuous key/value blocks\nindexed byI\ud835\udc61into SRAM as \ud835\udc3e\u2208R[\ud835\udc35\ud835\udc58,\ud835\udc51\ud835\udc58],\ud835\udc49\u2208R[\ud835\udc35\ud835\udc58,\ud835\udc51\ud835\udc63]to minimize memory loading, where\n\ud835\udc35\ud835\udc58is the kernel block size satisfying \ud835\udc35\ud835\udc58|\ud835\udc59\u2032.\n3.Outer Loop on Grid . Since the inner-loop length (proportional to the selected block\ncount\ud835\udc5b) remains nearly identical for different query blocks, we put query/output loops in\nTriton\u2019s grid scheduler to simplify and optimize the kernel.\nThis design achieves near-optimal arithmetic intensity by (1) eliminating redundant KV trans-\nfers through group-wise sharing, and (2) balancing compute workloads across GPU streaming\nmultiprocessors.\nOutputInner LoopKSelect InLoadLoadCompute on SRAMOutput to HBMSelect InQGrid LoopVInner Loop\nGrid Loop\nFigure 3|Kernel design for NSA. The kernel loads queries by GQA groups (Grid Loop), fetches\ncorresponding sparse KV blocks (Inner Loop), and performs attention computation on SRAM.\nGreen blocks indicate data on SRAM, while blue indicates data on HBM.\n4. Experiments\nWe evaluate NSA through three lenses: (1) general benchmarks performance, (2) long-context\nbenchmarks performance, and (3) chain-of-thought reasoning performance, comparing against\nFull Attention baseline and state-of-the-art sparse attention methods. We defer the efficiency\nanalysis of our sparse computation paradigm to Section 5, where we provide detailed discussions\non training and inference speed.\n4.1. Pretraining Setup\nFollowing the common practice in state-of-the-art LLMs, our experiments adopt a backbone\ncombining Grouped-Query Attention (GQA) and Mixture-of-Experts (MoE), featuring 27Btotal\nparameters with 3Bactive parameters. The model consists of 30 layers with a hidden dimension\nof 2560. For GQA, we set the number of groups to 4, with a total of 64 attention heads. For each\nhead, the hidden dimensions of the query, key, and value are configured as \ud835\udc51\ud835\udc5e=\ud835\udc51\ud835\udc58=192and\n\ud835\udc51\ud835\udc63=128, respectively. For MoE, we utilize the DeepSeekMoE (Dai et al., 2024; DeepSeek-AI,\n2024) structure, with 72 routed experts and 2 shared experts, and set the top-k experts to 6. To\nensure training stability, the MoE in the first layer is replaced by an MLP in the form of SwiGLU.\n9\n 0 10000 20000 30000 40000 50000 60000\n step 1.5 2.0 2.5 3.0 3.5 4.0 loss TainingLossCurve\n FullAttention\n NSAFigure 4|Pretraining loss comparison between Full Attention and our NSA on 27B-parameter\nmodel. Both models exhibit stable convergence, with NSA achieving lower loss values.\nModelMMLU MMLU-PRO CMMLU BBH GSM8K MATH DROP MBPP HumanEvalAvg.\nAcc. 5-shot Acc. 5-shot Acc. 5-shot Acc. 3-shot Acc. 8-shot Acc. 4-shot F1 1-shot Pass@1 3-shot Pass@1 0-shot\nFull Attn 0.567 0.279 0.576 0.497 0.486 0.263 0.503 0.482 0.335 0.443\nNSA 0.565 0.286 0.587 0.521 0.520 0.264 0.545 0.466 0.348 0.456\nTable 1|Pretraining performance comparison between the full attention baseline and NSA\non general benchmarks, across knowledge (MMLU, MMLU-PRO, CMMLU), reasoning (BBH,\nGSM8K, MATH, DROP), and coding (MBPP , HumanEval) tasks. NSA achieves superior average\nperformance on most benchmarks despite high sparsity.\nThe proposed architecture achieves an effective trade-off between computation cost and model\nperformance. For NSA, we set compression block size \ud835\udc59=32, sliding stride \ud835\udc51=16, selected\nblock size \ud835\udc59\u2032=64, selected block count \ud835\udc5b=16(including fixed activating the 1 initial block and\n2 local blocks), and sliding window size \ud835\udc64=512. Both Full Attention and sparse attention\nmodels are pretrained on 270Btokens of 8k-length texts, followed by continued training and\nsupervised fine-tuning on 32k-length texts with YaRN (Peng et al., 2024) to achieve long-context\nadaptation. Both models are trained to full convergence to ensure fair comparison. As shown in\nFigure 4, the pretraining loss curve of our NSA and Full Attention baseline demonstrates stable\nand smooth decline, with NSA consistently outperforming the Full Attention model.\n4.2. Baselines Methods\nIn addition to comparing with Full Attention, we evaluate several state-of-the-art inference-stage\nsparse attention methods: H2O (Zhang et al., 2023b), infLLM (Xiao et al., 2024), Quest (Tang et al.,\n2024), and Exact-Top, which first computes full attention score and select the top- \ud835\udc5bscores keys\ncorresponding to each query and then calculates attention on these positions. These methods\n10\nModelSQA MQA Synthetic CodeAvg.\nMFQA-en MFQA-zh Qasper HPQ 2Wiki GovRpt Dur PassR-en PassR-zh LCC\nH2O 0.428 0.429 0.308 0.112 0.101 0.231 0.208 0.704 0.421 0.092 0.303\nInfLLM 0.474 0.517 0.356 0.306 0.250 0.277 0.257 0.766 0.486 0.143 0.383\nQuest 0.495 0.561 0.365 0.295 0.245 0.293 0.257 0.792 0.478 0.135 0.392\nExact-Top 0.502 0.605 0.397 0.321 0.288 0.316 0.291 0.810 0.548 0.156 0.423\nFull Attn 0.512 0.623 0.409 0.350 0.305 0.324 0.294 0.830 0.560 0.163 0.437\nNSA 0.503 0.624 0.432 0.437 0.356 0.307 0.341 0.905 0.550 0.232 0.469\nTable 2|Performance comparison between our NSA and baselines on LongBench, including\nsubsets in single document QA, multi-document QA, synthetic and code task categories. NSA\noutperformed most of the baselines including Full Attention.\nspan diverse sparse attention paradigms, including KV-cache eviction, query-aware selection,\nand exact top- \ud835\udc5bsparse selection.\nFor general evaluation, where most samples have lengths within the local context window of\nsparse attention baselines, these methods are effectively equivalent to Full Attention. Therefore,\nwe present only the comparison results between NSA and Full Attention baseline in this setting.\nIn the long-context evaluation, we conduct comparisons across all baseline methods, with the\nsparsity of all sparse attention methods set to the same to ensure a fair comparison. For chain-\nof-thought reasoning evaluation, which requires long-text supervised fine-tuning, we limit our\ncomparison to Full Attention, as sparse attention baselines do not support training.\n4.3. Performance Comparison\nGeneral Evaluation. We evaluated the pretrained NSA and Full Attention baseline, on a\ncomprehensive suite of benchmarks spanning knowledge, reasoning, and coding capabilities,\nincluding MMLU (Hendrycks et al., 2020), MMLU-PRO (Wang et al., 2024), CMMLU (Li et al.,\n2023), BBH (Suzgun et al., 2022), GSM8K (Cobbe et al., 2021), MATH (Hendrycks et al., 2020),\nDROP (Dua et al., 2019), MBPP (Austin et al., 2021), and HumanEval (Chen et al., 2021). The\nresults are shown in Table 1. Despite its sparsity, NSA achieves superior overall performance,\noutperforming all baselines including Full Attention on 7 out of 9 metrics. This indicates\nthat although NSA may not fully leverage its efficiency advantages on shorter sequences, it\nshows strong performance. Notably, NSA demonstrates significant gains in reasoning-related\nbenchmarks (DROP: +0.042, GSM8K: +0.034), suggesting that our pretraining helps models\nto develop specialized attention mechanisms. This sparse attention pretraining mechanism\nforces model to focus on the most important information, potentially enhancing performance\nby filtering out noise from irrelevant attention pathways. The consistent performance across\ndiverse evaluations also validates NSA\u2019s robustness as a general-purpose architecture.\nLong-Context Evaluation. As shown in Figure 5, NSA achieves perfect retrieval accuracy\nacross all positions in 64k-context needle-in-a-haystack (Kamradt, 2023) test. This performance\nstems from our hierarchical sparse attention design, which combines compression tokens for\nefficient global context scanning, and selection tokens for precise local information retrieval. The\ncoarse-grained compression identifies relevant context blocks at low computational cost, while\nthe token-level attention on selected tokens ensures the preservation of critical fine-grained\ninformation. This design enables NSA to maintain both global awareness and local precision.\nWe also evaluate NSA on LongBench (Bai et al., 2023) against state-of-the-art sparse attention\n11\n 1K 8K 15K 22K 29K 36K 43K 50K 57K 64K\n ContextLength 0\n 11\n 22\n 33\n 44\n 55\n 66\n 77\n 88\n 100 DepthPercent(%) NeedleinAHaystack27BNSA64kContext\n 0.0 0.2 0.4 0.6 0.8 1.0Figure 5|Needle-in-a-Haystack retrieval accuracy across context positions with 64k context\nlength. NSA achieves perfect accuracy through its hierarchical sparse attention design.\nmethods and Full Attention baseline. To ensure consistent sparsity, we set the token activated\nby each query in all sparse attention baselines to 2560 tokens, which corresponds to the average\nnumber of tokens activated in NSA when handling 32k sequence lengths. Following Stream-\nLLM (Xiao et al., 2023), this token budget includes the leading 128 tokens and 512 local tokens.\nWe exclude certain subsets from LongBench due to their low scores across all models, which\nmay not provide meaningful comparisons. As shown in Table 2, NSA achieves the highest\naverage score 0.469, outperforming all baselines (+0.032 over Full Attention and +0.046 over\nExact-Top). This improvement arises from two key innovations: (1) our native sparse attention\ndesign, which enables end-to-end optimization of sparse patterns during pretraining, facilitates\nsynchronized adaptation between the sparse attention module and other model components;\nand (2) the hierarchical sparse attention mechanism achieves a balance between local and global\ninformation processing.\nNotably, NSA demonstrates exceptional performance on tasks requiring complex reasoning\nover long contexts, achieving +0.087 and +0.051 improvements over Full Attention on multi-hop\nQA tasks (HPQ and 2Wiki), exceeding the performance of baselines on code understanding\n(LCC: +0.069), and outperforming other methods on passage retrieval (PassR-en: +0.075). These\nresults validate NSA\u2019s capability to handle diverse long-context challenges, with its natively\npretrained sparse attention providing additional benefits in learning task-optimal patterns.\nChain-of-Thought Reasoning Evaluation. To evaluate NSA\u2019s compatibility with advanced\ndownstream training paradigms, we investigate its capacity to acquire chain-of-thought mathe-\nmatical reasoning abilities via post-training. Given the limited effectiveness of reinforcement\nlearning on smaller-scale models, we employ knowledge distillation from DeepSeek-R1, con-\nducting supervised fine-tuning (SFT) with 10B tokens of 32k-length mathematical reasoning\ntraces. This produces two comparable models: Full Attention-R (Full Attention baseline) and\nNSA-R (our sparse variant). We assess both models on the challenging American Invitational\nMathematics Examination (AIME 24) benchmark. We use a sampling temperature of 0.7 and a\ntop-\ud835\udc5dvalue of 0.95 to generate 16 responses for each question and obtain the average score. To\nvalidate the impact of reasoning depth, we conduct experiments with two generation context\nlimits: 8k and 16k tokens, measuring whether extended reasoning chains improve accuracy.\nExample comparisons of model predictions are provided in Appendix A.\n12\nGeneration Token Limit 8192 16384\nFull Attention-R 0.046 0.092\nNSA-R 0.121 0.146\nTable 3|AIME Instruction-based Evaluating after supervised fine-tuning. Our NSA-R demon-\nstrates better performance than Full Attention-R at both 8k and 16k sequence lengths\n8k 16k 32k 64k02004006008001000Time (ms)\n2.1 \u00d7 3.8 \u00d7 \n6.3 \u00d7 \n9.0 \u00d7Forward Time Comparison\nFlashAttention- 2\nSparse Attention Kernel\nCompression and\nWindow Attention\nSpeedup Ratio\n8k 16k 32k 64k05001000150020002500\n1.1 \u00d72.0 \u00d7 \n3.4 \u00d7 \n6.0 \u00d7Backward Time Comparison\n9.0 \n\u00d7\nFigure 6|Comparison of Triton-based NSA kernel with Triton-based FlashAttention-2 kernel.\nOur implementation significantly reduces latency across all context lengths, with the improve-\nment becoming more pronounced as input length increases.\nAs shown in Table 3, NSA-R achieves significantly higher accuracy than Full Attention-R\nunder the 8k context setting (+0.075), with this advantage persisting at 16k contexts (+0.054).\nThese results validate two key benefits of native sparse attention: (1) The pretrained sparse\nattention patterns enable efficient capture of long-range logical dependencies critical for complex\nmathematical derivations; (2) Our architecture\u2019s hardware-aligned design maintains sufficient\ncontext density to support growing reasoning depth without catastrophic forgetting. The consis-\ntent outperformance across context lengths confirms sparse attention\u2019s viability for advanced\nreasoning tasks when natively integrated into the training pipeline.\n5. Efficiency Analysis\nWe evaluate the computational efficiency of NSA against Full Attention on an 8-GPU A100\nsystem. In efficiency analysis, we also configure the model with GQA group \ud835\udc54=4, heads per\ngroup\u210e=16, query/key dimension \ud835\udc51\ud835\udc58=192, and value dimension \ud835\udc51\ud835\udc63=128. Following the same\nsettings in Section 4, we set NSA compression block size \ud835\udc59=32, sliding stride \ud835\udc51=16, selected\nblock size \ud835\udc59\u2032=64, selected block count \ud835\udc5b=16, and sliding window size \ud835\udc64=512.\n5.1. Training Speed\nWe compare the Triton-based implementations of our NSA attention and Full Attention with\nTriton-based FlashAttention-2 to ensure fair speed comparison across the same backend. As\nshown in Figure 6, our NSA achieves progressively greater speedups as context length increases,\nup to 9.0\u00d7forward and 6.0\u00d7backward speedup at 64k context-length. Notably, the speed\nadvantage becomes more pronounced with longer sequences. This speedup stems from our\n13\nContext Length 8192 16384 32768 65536\nFull Attention 8192 16384 32768 65536\nNSA 2048 2560 3584 5632\nExpected Speedup 4\u00d7 6.4\u00d7 9.1\u00d7 11.6\u00d7\nTable 4|Memory access volume (in equivalent number of tokens) per attention operation during\ndecoding. Due to the low arithmetic intensity and memory-bound nature of decoding, the\nexpected speedup is approximately linear with the volume of memory access.\nhardware-aligned algorithm design to maximize the efficiency of sparse attention architecture:\n(1) The Blockwise memory access pattern maximizes Tensor Core utilization through coalesced\nloads, (2) The delicate loop scheduling in the kernel eliminates redundant KV transfers.\n5.2. Decoding Speed\nThe decoding speed of Attention is primarily determined by the memory access bottleneck,\nwhich is closely tied to the amount of KV cache loading. In each decoding step, Our NSA just\nneeds to load at most\u0004\ud835\udc60\u2212\ud835\udc59\n\ud835\udc51\u0005compression tokens, \ud835\udc5b\ud835\udc59\u2032selected tokens, and \ud835\udc64neighbor tokens,\nwhere\ud835\udc60is the cached sequence length. As shown in Table 4, our method exhibits a signifi-\ncant reduction in latency as the decoding length increases, achieving up to 11.6 \u00d7speedup at\n64k context-length. This advantage in memory access efficiency also amplifies with longer\nsequences.\n6. Discussion\nIn this section, we reflect on the development process of NSA and discuss key insights gained\nfrom our exploration of different sparse attention strategies. While our approach demonstrates\npromising results, understanding the challenges encountered with alternative strategies and\nanalyzing attention patterns provides valuable context for future research directions. We first\nexamine challenges with alternative token selection strategies that motivated our design choices,\nfollowed by visualizations that offer insights into attention distribution patterns.\n6.1. Challenges with Alternative Token Selection Strategies\nBefore designing NSA, we explored adapting existing sparse attention methods to the training\nstage. However, these attempts encountered various challenges, prompting us to design a\ndifferent sparse attention architecture:\nKey-Clustering Based Strategies. We examined clustering-based strategies like Clus-\nterKV (Liu et al., 2024). These methods store Keys and Values from the same cluster in contiguous\nmemory regions. While theoretically feasible for training and inference, they face three signif-\nicant challenges: (1) Non-trivial computational overhead introduced by dynamic clustering\nmechanisms; (2) Operator optimization difficulties exacerbated by inter-cluster imbalances,\nespecially in Mixture-of-Experts (MoE) systems, where skewed Expert Parallelism (EP) group\nexecution times lead to persistent load imbalances; (3) Implementation constraints arising from\nthe need for mandatory periodic reclustering and chunk-sequential training protocols. These\ncombined factors create substantial bottlenecks, significantly limiting their effectiveness for\nreal-world deployment.\n14\n050001000015000200002500030000\nstep2.22.42.62.83.03.2lossTrainingLossCurve\nFullAttention\nAuxiliaryLoss-basedLearnableSelection\nHeuristicParameter-freeSelection\nNSAFigure 7|Compare training loss on a 3B-\nparameter model with Full Attention and different\ntoken selection strategies and. Our NSA achieves\nbetter performance.\nVisualization f or A tt ention Scor e Block Patt ern\npositionpositionFigure 8|Visualization of Attention Map\non a Full Attention transformer. Light-\ncolored regions indicate higher attention\nvalues. As shown in the figure, attention\nscores exhibit blockwise clustering distri-\nbution.\nOther Blockwise Selection Strategies. We also considered blockwise key, value selection\nstrategies different from NSA, such as Quest (Tang et al., 2024) and InfLLM (Xiao et al., 2024).\nThese methods rely on computing an importance score for each block and selecting the top- \ud835\udc5b\nblocks based on their similarity with \ud835\udc5e\ud835\udc61. However, existing methods face two critical issues:\n(1) Since the selection operation is non-differentiable, importance score computation based on\nneural networks relies on auxiliary loss, which increases operator overhead and often degrades\nmodel performance; (2) Heuristic parameter-free importance score computation strategy suffer\nfrom low recall rates, leading to suboptimal performance. We evaluate both approaches on\na 3B-parameter model with similar architecture and compare their loss curve with NSA and\nFull Attention. For the auxiliary loss-based selection method, we introduce additional queries\nand representative keys for each block to estimate the block importance scores. These scores\nare supervised by the mean attention scores between the original queries and keys within each\nblock. For the heuristic parameter-free selection method, following the strategy of Quest, we\nimplement direct selection using the product between queries and coordinate-wise min-max of\nthe key chunks, without introducing additional parameters. We also explore a cold-start training\napproach where Full Attention is applied for the initial 1000 steps before transitioning to the\nheuristic blockwise selection. As shown in Figure 7, both methods exhibited inferior loss.\n6.2. Visualization\nTo explore potential patterns in transformer attention distributions and seek inspiration for\nour design, we visualize the attention map from our pretrained 27B Full Attention model in\nFigure 8. The visualization reveals interesting patterns where attention scores tend to exhibit\nblockwise clustering characteristics, with nearby keys often showing similar attention scores.\nThis observation inspired our design of NSA, suggesting that selecting key blocks based on\nspatial continuity might be a promising approach. The blockwise clustering phenomenon\nindicates that tokens adjacent in the sequence may share certain semantic relationships with\nquery tokens, though the exact nature of these relationships requires further investigation. This\nobservation motivated us to explore a sparse attention mechanism that operates on continuous\ntoken blocks rather than individual tokens, aiming to enhance computational efficiency and\npreserve high-attention patterns.\n15\n7. Related Works\nWe review existing approaches that improve the efficiency of attention computation through\nsparse attention. These methods can be broadly categorized into three groups based on their core\nstrategies: (1) fixed sparse pattern, (2) dynamic token pruning, and (3) query-aware selection.\nWe introduce several representative works from each category.\n7.1. Fixed Sparse Pattern\nSlidingWindow is a commonly used approach that allows the query to compute attention only\nwithin a fixed window. StreamingLLM (Xiao et al., 2023) addresses the challenges of processing\nlong text streams by maintaining two critical portions of the context: an attention sink (early\ntokens) and a local context window. While these approaches effectively reduce memory and\ncomputation costs, their rigid pattern of ignoring contexts limits their performance on tasks\nrequiring full context understanding.\n7.2. Dynamic Token Pruning\nH2O (Zhang et al., 2023b) implements an adaptive approach to reduce KV-cache memory usage\nduring decoding. This method dynamically evicts tokens deemed less important for future\npredictions based on their recent utility according to attention score. SnapKV (Li et al., 2024)\nalso introduces a token pruning strategy that reduces the KV cache by selectively retaining\nonly the most crucial features, enabling efficient memory usage. SnapKV identifies important\nfeatures through attention weight analysis and voting during prefilling, then updates KV cache\nby combining selected compressed features with recent context to maintain prompt consistency.\n7.3. Query-Aware Selection\nQuest (Tang et al., 2024) employs a blockwise selection strategy where each chunk\u2019s importance\nis estimated by product between query and coordinate-wise min-max of the key chunks. The\nresults scores help to select top \u2212\ud835\udc5bimportant key-value chunks for attention. InfLLM (Xiao\net al., 2024) combines fixed patterns with retrieval by maintaining attention sinks, local context,\nand retrievable chunks. This method selects representative keys from each chunk to estimate\nchunk importance. HashAttention (Desai et al., 2024) formulates pivotal token identification\nas a recommendation problem by mapping queries and keys to Hamming space using learned\nfunctions. ClusterKV (Liu et al., 2024) achieves sparsity by firstly clustering keys and then\nselecting the most relevant clusters for attention computation based on query-cluster similarity.\n8. Conclusion\nWe present NSA, a hardware-aligned sparse attention architecture for efficient long-context\nmodeling. By integrating hierarchical token compression with blockwise token selection within\na trainable architecture, our architecture achieves accelerated training and inference while\nmaintaining Full Attention performance. NSA advances the state-of-the-art by demonstrating\ngeneral benchmark performance matches full-attention baselines, exceeding modeling capability\nin long-context evaluations, and enhanced reasoning ability, all accompanied by measurable\nreductions in computational latency and achieving significant speedup.\n16\nReferences\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr\u00f3n, and S. Sanghai. Gqa: Training\ngeneralized multi-query transformer models from multi-head checkpoints. arXiv preprint\narXiv:2305.13245, 2023.\nJ. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,\nQ. Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 ,\n2021.\nY. Bai, X. Lv, J. Zhang, H. Lyu, J. Tang, Z. Huang, Z. Du, X. Liu, A. Zeng, L. Hou, et al.\nLongbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint\narXiv:2308.14508, 2023.\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P . D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,\nN. Joseph, G. Brockman, et al. Evaluating large language models trained on code. arXiv\npreprint arXiv:2107.03374, 2021.\nZ. Chen, R. Sadhukhan, Z. Ye, Y. Zhou, J. Zhang, N. Nolte, Y. Tian, M. Douze, L. Bottou, Z. Jia,\net al. Magicpig: Lsh sampling for efficient llm generation. arXiv preprint arXiv:2410.16179 ,\n2024.\nK. Cobbe, V . Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,\nR. Nakano, et al. Training verifiers to solve math word problems, 2021. URL https://arxiv.\norg/abs/2110.14168, 2021.\nD. Dai, C. Deng, C. Zhao, R. Xu, H. Gao, D. Chen, J. Li, W. Zeng, X. Yu, Y. Wu, et al. Deepseekmoe:\nTowards ultimate expert specialization in mixture-of-experts language models. arXiv preprint\narXiv:2401.06066, 2024.\nDeepSeek-AI. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language\nmodel. 2024. URL https://arxiv.org/abs/2405.04434 .\nDeepSeek-AI. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,\n2025. URL https://arxiv.org/abs/2501.12948 .\nA. Desai, S. Yang, A. Cuadron, A. Klimovic, M. Zaharia, J. E. Gonzalez, and I. Stoica. Hashatten-\ntion: Semantic sparsity for faster inference. arXiv preprint arXiv:2412.14468, 2024.\nD. Dua, Y. Wang, P . Dasigi, G. Stanovsky, S. Singh, and M. Gardner. Drop: A reading\ncomprehension benchmark requiring discrete reasoning over paragraphs. arXiv preprint\narXiv:1903.00161, 2019.\nS. Ge, Y. Zhang, L. Liu, M. Zhang, J. Han, and J. Gao. Model tells you what to discard: Adaptive\nkv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023.\nG. T. Google, P . Georgiev, V . I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan,\nS. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of\ncontext. arXiv preprint arXiv:2403.05530, 2024.\nD. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt. Measuring\nmassive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.\nH. Jiang, Q. Wu, C.-Y. Lin, Y. Yang, and L. Qiu. Llmlingua: Compressing prompts for accelerated\ninference of large language models. arXiv preprint arXiv:2310.05736, 2023.\n17\nH. Jiang, Y. Li, C. Zhang, Q. Wu, X. Luo, S. Ahn, Z. Han, A. H. Abdi, D. Li, C.-Y. Lin, et al.\nMinference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention.\narXiv preprint arXiv:2407.02490, 2024.\nG. Kamradt. LLMTest NeedleInAHaystack. GitHub repository, 2023. URL https://github.c\nom/gkamradt/LLMTest_NeedleInAHaystack . Accessed: [Insert Access Date Here].\nH. Li, Y. Zhang, F. Koto, Y. Yang, H. Zhao, Y. Gong, N. Duan, and T. Baldwin. Cmmlu: Measuring\nmassive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212 , 2023.\nY. Li, Y. Huang, B. Yang, B. Venkitesh, A. Locatelli, H. Ye, T. Cai, P . Lewis, and D. Chen. Snapkv:\nLlm knows what you are looking for before generation. arXiv preprint arXiv:2404.14469 , 2024.\nG. Liu, C. Li, J. Zhao, C. Zhang, and M. Guo. Clusterkv: Manipulating llm kv cache in semantic\nspace for recallable compression. arXiv preprint arXiv:2412.03213, 2024.\nJ. S. Park, J. C. O\u2019Brien, C. J. Cai, M. R. Morris, P . Liang, and M. S. Bernstein. Generative agents:\nInteractive simulacra of human behavior. In S. Follmer, J. Han, J. Steimle, and N. H. Riche,\neditors, Proceedings ofthe36th Annual ACM Symposium onUser Interface Software and\nTechnology, UIST 2023, San Francisco, CA, USA, 29October 2023\u2013 1November 2023 , pages\n2:1\u20132:22. ACM, 2023.\nB. Peng, J. Quesnelle, H. Fan, and E. Shippole. Yarn: Efficient context window extension of large\nlanguage models. In ICLR. OpenReview.net, 2024.\nN. Shazeer. Fast transformer decoding: One write-head is all you need. CoRR , abs/1911.02150,\n2019.\nM. Suzgun, N. Scales, N. Sch\u00e4rli, S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. V . Le,\nE. H. Chi, D. Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve\nthem. arXiv preprint arXiv:2210.09261, 2022.\nJ. Tang, Y. Zhao, K. Zhu, G. Xiao, B. Kasikci, and S. Han. Quest: Query-aware sparsity for\nefficient long-context llm inference. arXiv preprint arXiv:2406.10774, 2024.\nP . Tillet, H.-T. Kung, and D. Cox. Triton: an intermediate language and compiler for tiled neural\nnetwork computations. In Proceedings ofthe3rdACM SIGPLAN International Workshop\nonMachine Learning and Programming Languages, pages 10\u201319, 2019.\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. u. Kaiser, and\nI. Polosukhin. Attention is all you need. Advances inNeural Information Processing Systems ,\n2017.\nY. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, et al.\nMmlu-pro: A more robust and challenging multi-task language understanding benchmark.\narXiv preprint arXiv:2406.01574, 2024.\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V . Le, D. Zhou, et al. Chain-\nof-thought prompting elicits reasoning in large language models. Advances inneural\ninformation processing systems, 35:24824\u201324837, 2022.\nC. Xiao, P . Zhang, X. Han, G. Xiao, Y. Lin, Z. Zhang, Z. Liu, and M. Sun. Infllm: Training-free\nlong-context extrapolation for llms with an efficient context memory. In The Thirty-eighth\nAnnual Conference onNeural Information Processing Systems, 2024.\n18\nG. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis. Efficient streaming language models with\nattention sinks. arXiv preprint arXiv:2309.17453, 2023.\nM. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti, S. Ontanon, P . Pham, A. Ravula,\nQ. Wang, L. Yang, et al. Big bird: Transformers for longer sequences. Advances inneural\ninformation processing systems, 33:17283\u201317297, 2020.\nE. Zelikman, Y. Wu, J. Mu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning.\nIn S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances\ninNeural Information Processing Systems 35:Annual Conference onNeural Information\nProcessing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28\u2013December\n9,2022, 2022.\nF. Zhang, B. Chen, Y. Zhang, J. Keung, J. Liu, D. Zan, Y. Mao, J. Lou, and W. Chen. Repocoder:\nRepository-level code completion through iterative retrieval and generation. In H. Bouamor,\nJ. Pino, and K. Bali, editors, Proceedings ofthe2023 Conference onEmpirical Methods in\nNatural Language Processing, EMNLP 2023, Singapore, December 6\u201310, 2023 , pages 2471\u2013\n2484. Association for Computational Linguistics, 2023a.\nK. Zhang, J. Li, G. Li, X. Shi, and Z. Jin. Codeagent: Enhancing code generation with tool-\nintegrated agent systems for real-world repo-level coding challenges. In L. Ku, A. Martins,\nand V . Srikumar, editors, Proceedings ofthe62nd Annual Meeting oftheAssociation for\nComputational Linguistics (Volume 1:Long Papers), ACL 2024, Bangkok, Thailand, August\n11\u201316, 2024, pages 13643\u201313658.\nZ. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song, Y. Tian, C. R\u00e9, C. Barrett,\net al. H2o: Heavy-hitter oracle for efficient generative inference of large language models.\nAdvances inNeural Information Processing Systems, 36:34661\u201334710, 2023b.\nZ. Zhou, C. Li, X. Chen, S. Wang, Y. Chao, Z. Li, H. Wang, R. An, Q. Shi, Z. Tan, et al. Llm \u00d7\nmapreduce: Simplified long-sequence processing using large language models. arXiv preprint\narXiv:2410.09342, 2024.\n19\nA. Example of AIME Results\nPrompt:\n\u201c|User|Let \ud835\udc65,\ud835\udc66and\ud835\udc67be positive real numbers that satisfy the following system of equa-\ntions:\nlog2\u0012\ud835\udc65\n\ud835\udc66\ud835\udc67\u0013\n=1\n2\nlog2\u0010\ud835\udc66\n\ud835\udc65\ud835\udc67\u0011\n=1\n3\nlog2\u0012\ud835\udc67\n\ud835\udc65\ud835\udc66\u0013\n=1\n4\nThen the value of\f\flog2(\ud835\udc654\ud835\udc663\ud835\udc672)\f\fis\ud835\udc5a\n\ud835\udc5bwhere\ud835\udc5aand\ud835\udc5bare relatively prime positive integers.\nFind\ud835\udc5a+\ud835\udc5b. Please reason step by step, and put your final answer within .|Assistant|\u201d\nOur Result:\n\u27e8think\u27e9...\u27e8/think\u27e9(2275 tokens)\nGiven the system of equations:\n1. log2\u0010\n\ud835\udc65\n\ud835\udc66\ud835\udc67\u0011\n=1\n22. log2\u0000\ud835\udc66\n\ud835\udc65\ud835\udc67\u0001=1\n33. log2\u0010\n\ud835\udc67\n\ud835\udc65\ud835\udc66\u0011\n=1\n4\nWe convert these logarithmic equations to their exponential forms:\n1.\ud835\udc65\n\ud835\udc66\ud835\udc67=\u221a\n2 2.\ud835\udc66\n\ud835\udc65\ud835\udc67=21/33.\ud835\udc67\n\ud835\udc65\ud835\udc66=21/4\nExpressing each variable in terms of \ud835\udc65,\ud835\udc66, and\ud835\udc67:\n1.\ud835\udc65=\u221a\n2\u00b7\ud835\udc66\ud835\udc672.\ud835\udc66=21/3\u00b7\ud835\udc65\ud835\udc673.\ud835\udc67=21/4\u00b7\ud835\udc65\ud835\udc66\nSubstituting these expressions into each other, we find:\n- From\ud835\udc65=\u221a\n2\u00b7\ud835\udc66\ud835\udc67and\ud835\udc66=21/3\u00b7\ud835\udc65\ud835\udc67, we get\ud835\udc65=\u221a\n2\u00b7(21/3\u00b7\ud835\udc65\ud835\udc67)\u00b7\ud835\udc67, leading to\n\ud835\udc67=2\u22125/12. - Substituting \ud835\udc67into the equation for \ud835\udc66, we get\ud835\udc66=2\u22123/8. - Substituting\n\ud835\udc67and\ud835\udc66into the equation for \ud835\udc65, we get\ud835\udc65=2\u22127/24.\nNext, we compute \ud835\udc654\ud835\udc663\ud835\udc672:\n-\ud835\udc654=(2\u22127/24)4=2\u22127/6-\ud835\udc663=(2\u22123/8)3=2\u22129/8-\ud835\udc672=(2\u22125/12)2=2\u22125/6\nMultiplying these together:\n\ud835\udc654\ud835\udc663\ud835\udc672=2\u22127/6\u00b72\u22129/8\u00b72\u22125/6=2\u221275/24=2\u221225/8\nTaking the logarithm base 2:\nlog2(\ud835\udc654\ud835\udc663\ud835\udc672)=\u221225/8\nThe absolute value is:\n|log2(\ud835\udc654\ud835\udc663\ud835\udc672)|=25/8\n20\nThus,\ud835\udc5a=25 and\ud835\udc5b=8, giving\ud835\udc5a+\ud835\udc5b=33.\n33\nBaseline Result:\n\u27e8think\u27e9...\u27e8/think\u27e9(9392 tokens)\nGiven the system of logarithmic equations:\nlog2\u0012\ud835\udc65\n\ud835\udc66\ud835\udc67\u0013\n=1\n2\nlog2\u0010\ud835\udc66\n\ud835\udc65\ud835\udc67\u0011\n=1\n3\nlog2\u0012\ud835\udc67\n\ud835\udc65\ud835\udc66\u0013\n=1\n4\nWe convert these equations to exponential form:\n\u221a\n2=\ud835\udc65\n\ud835\udc66\ud835\udc67=\u21d2\ud835\udc65=\u221a\n2\u00b7\ud835\udc66\ud835\udc67\n21\n3=\ud835\udc66\n\ud835\udc65\ud835\udc67=\u21d2\ud835\udc66=21\n3\u00b7\ud835\udc65\ud835\udc67\n21\n4=\ud835\udc67\n\ud835\udc65\ud835\udc66=\u21d2\ud835\udc67=21\n4\u00b7\ud835\udc65\ud835\udc66\nSubstituting these into each other, we find:\n1.\ud835\udc65=\u221a\n2\u00b7\ud835\udc66\ud835\udc672.\ud835\udc66=21\n3\u00b7\ud835\udc65\ud835\udc673.\ud835\udc67=21\n4\u00b7\ud835\udc65\ud835\udc66\nBy solving these equations step-by-step, we find:\n-\ud835\udc67=2\u22125/12-\ud835\udc65\ud835\udc66=2\u22122/3-\ud835\udc66=2\u22121/24-\ud835\udc65=21/24\nHowever, upon verifying the third equation, we realize the system might not\nhave a solution. Therefore, we use matrix inversion to solve the system:\n\uf8f1\uf8f4\uf8f4\uf8f4 \uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3log2(\ud835\udc65)\u2212log2(\ud835\udc66)\u2212log2(\ud835\udc67)=1\n2\nlog2(\ud835\udc66)\u2212log2(\ud835\udc65)\u2212log2(\ud835\udc67)=1\n3\nlog2(\ud835\udc67)\u2212log2(\ud835\udc65)\u2212log2(\ud835\udc66)=1\n4\nLet\ud835\udc34=log2(\ud835\udc65),\ud835\udc35=log2(\ud835\udc66),\ud835\udc36=log2(\ud835\udc67). The system becomes:\n\uf8f1\uf8f4\uf8f4\uf8f4 \uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\ud835\udc34\u2212\ud835\udc35\u2212\ud835\udc36=1\n2\n\ud835\udc35\u2212\ud835\udc34\u2212\ud835\udc36=1\n3\n\ud835\udc36\u2212\ud835\udc34\u2212\ud835\udc35=1\n4\n21\nWe solve this system using Cramer\u2019s rule and matrix inversion. The determinant\nof the matrix is -2, and the solutions are:\n\ud835\udc34=61\n48,\ud835\udc35=13\n16,\ud835\udc36=\u22121\n6\nThus, we have:\n\ud835\udc65=261/48,\ud835\udc66=213/16,\ud835\udc67=2\u22121/6\nWe need to find\f\flog2(\ud835\udc654\ud835\udc663\ud835\udc672)\f\f:\n4 log2\ud835\udc65+3 log2\ud835\udc66+2 log2\ud835\udc67=4\u00b761\n48+3\u00b713\n16+2\u00b7\u0012\n\u22121\n6\u0013\nConverting to a common denominator (48):\n4\u00b761\n48=244\n48, 3\u00b713\n16=117\n48, 2\u00b7\u0012\n\u22121\n6\u0013\n=\u221216\n48\nSumming these:\n244\n48+117\n48\u221216\n48=345\n48=115\n16\nThe fraction115\n16is in simplest form, so \ud835\udc5a=115and\ud835\udc5b=16. Therefore, \ud835\udc5a+\ud835\udc5b=\n115+16=131.\n131\nPrompt:\n\u201c|User |Real numbers \ud835\udc65and\ud835\udc66with\ud835\udc65,\ud835\udc66 >1satisfy log\ud835\udc65(\ud835\udc66\ud835\udc65)=log\ud835\udc66(\ud835\udc654\ud835\udc66)=10.\nWhat is the value of \ud835\udc65\ud835\udc66?\nPlease reason step by step, and put your final answer within . |Assistant|\u201d\nOur Result:\n\u27e8think\u27e9...\u27e8/think\u27e9(15147 tokens)\nGiven the equations:\n22\n1. log\ud835\udc65(\ud835\udc66\ud835\udc65)=10\n2. log\ud835\udc66(\ud835\udc654\ud835\udc66)=10\nWe start by applying the logarithm power rule to both equations.\nFor the first equation:\nlog\ud835\udc65(\ud835\udc66\ud835\udc65)=\ud835\udc65\u00b7log\ud835\udc65(\ud835\udc66)=10\nLet\ud835\udc67=log\ud835\udc66(\ud835\udc65). Then, log\ud835\udc65(\ud835\udc66)=1\n\ud835\udc67. Substituting this into the equation, we get:\n\ud835\udc65\u00b71\n\ud835\udc67=10=\u21d2\ud835\udc65=10\ud835\udc67\nFor the second equation:\nlog\ud835\udc66(\ud835\udc654\ud835\udc66)=\ud835\udc66\u00b7log\ud835\udc66(\ud835\udc654)=10\nUsing the power rule, we get:\n\ud835\udc66\u00b74 log\ud835\udc66(\ud835\udc65)=10=\u21d24\ud835\udc66log\ud835\udc66(\ud835\udc65)=10\nSince log\ud835\udc66(\ud835\udc65)=\ud835\udc67, this simplifies to:\n4\ud835\udc66\ud835\udc67=10=\u21d2\ud835\udc66\ud835\udc67=5\n2\nWe now have two expressions:\n1.\ud835\udc65=10\ud835\udc67\n2.\ud835\udc66=5/2\n\ud835\udc67\nSubstituting \ud835\udc65=10\ud835\udc67and\ud835\udc66=5/2\n\ud835\udc67into the product \ud835\udc65\ud835\udc66, we get:\n\ud835\udc65\ud835\udc66=(10\ud835\udc67)\u00b7\u00125/2\n\ud835\udc67\u0013\n=10\ud835\udc67\u00b75\n2\ud835\udc67=10\u00b75\n2=25\nThus, the value of \ud835\udc65\ud835\udc66is 25 .\nBaseline Result:\n\u27e8think\u27e9...\u27e8/think\u27e9(16223 tokens)\nGiven the problem where real numbers \ud835\udc65and\ud835\udc66with\ud835\udc65,\ud835\udc66 >1satisfy the equa-\ntions log\ud835\udc65(\ud835\udc66\ud835\udc65)=10 and log\ud835\udc66(\ud835\udc654\ud835\udc66)=10, we need to find the value of \ud835\udc65\ud835\udc66.\nFirst, we convert the logarithmic equations to their exponential forms using the\nchange of base formula.\n23\n1. log\ud835\udc65(\ud835\udc66\ud835\udc65)=10\n\ud835\udc65log\ud835\udc65(\ud835\udc66)=10\nUsing the change of base formula, log\ud835\udc65(\ud835\udc66)=ln\ud835\udc66\nln\ud835\udc65. Since log\ud835\udc66\ud835\udc65=9\n4, we have\nlog\ud835\udc65\ud835\udc66=4\n9. Thus,\n\ud835\udc65\u00b74\n9=10=\u21d2\ud835\udc65=10\u00b79\n4=90\n4=45\n2\n2. log\ud835\udc66(\ud835\udc654\ud835\udc66)=10\n4 log\ud835\udc66\ud835\udc65+1=10=\u21d24 log\ud835\udc66\ud835\udc65=9=\u21d2log\ud835\udc66\ud835\udc65=9\n4\nAgain, using the change of base formula, log\ud835\udc65\ud835\udc66=4\n9.\nNext, we express \ud835\udc65and\ud835\udc66in terms of each other:\n\ud835\udc65=45\n2and\ud835\udc66=\u001245\n2\u00134/9\nWe then compute the product \ud835\udc65\ud835\udc66:\n\ud835\udc65\ud835\udc66=\u001245\n2\u0013\n\u00b7\u001245\n2\u00134/9\n=\u001245\n2\u00131+4/9\n=\u001245\n2\u001313/9\nHowever, upon re-evaluating and using another approach by setting \ud835\udc65=2\ud835\udc4eand\n\ud835\udc66=2\ud835\udc4f, we find:\nlog\ud835\udc65(\ud835\udc66\ud835\udc65)=10=\u21d2\ud835\udc65log\ud835\udc65\ud835\udc66\nlog\ud835\udc65\ud835\udc66=10=\u21d2\ud835\udc65\u00b74\n9=10=\u21d2\ud835\udc65=45\n2\nlog\ud835\udc66(\ud835\udc654\ud835\udc66)=10=\u21d24 log\ud835\udc66\ud835\udc65+1\nlog\ud835\udc66\ud835\udc65=10=\u21d24\u00b79\n4+1=10=\u21d29+1=10 consistent\nFinally, we find that the product \ud835\udc65\ud835\udc66simplifies to:\n\ud835\udc65\ud835\udc66=\u001245\n2\u0013\n24<|im_end|>\n<|im_start|>assistant\n",
  "prompt_length_chars": 59456,
  "prompt_length_tokens": 16432,
  "config": {
    "model": "Qwen/Qwen3-32B",
    "quantization": "bitsandbytes_int8",
    "precision": "INT8",
    "yarn_enabled": true,
    "yarn_factor": 4.0,
    "yarn_original_max": 32768,
    "tensor_parallel": 1,
    "max_context_len": 131072,
    "max_tokens": 20,
    "repetitions": 5,
    "warmup_runs": 1,
    "temperature": 0.0,
    "seed": 42,
    "top_logprobs": 10
  },
  "timing": {
    "model_load_time": 46.613378047943115,
    "warmup_times": [
      8.971736907958984
    ],
    "per_repetition": [
      {
        "repetition": 1,
        "elapsed_time": 8.717336416244507,
        "num_tokens": 20,
        "tokens_per_sec": 2.294278784828193,
        "time_per_token": 0.43586682081222533
      },
      {
        "repetition": 2,
        "elapsed_time": 8.708871603012085,
        "num_tokens": 20,
        "tokens_per_sec": 2.296508768493351,
        "time_per_token": 0.43544358015060425
      },
      {
        "repetition": 3,
        "elapsed_time": 8.727273941040039,
        "num_tokens": 20,
        "tokens_per_sec": 2.2916663479474297,
        "time_per_token": 0.43636369705200195
      },
      {
        "repetition": 4,
        "elapsed_time": 8.747863531112671,
        "num_tokens": 20,
        "tokens_per_sec": 2.2862725200122243,
        "time_per_token": 0.43739317655563353
      },
      {
        "repetition": 5,
        "elapsed_time": 8.766433238983154,
        "num_tokens": 20,
        "tokens_per_sec": 2.2814295683063754,
        "time_per_token": 0.4383216619491577
      }
    ],
    "statistics": {
      "mean_time": 8.73355574607849,
      "std_time": 0.020964145134051988,
      "mean_tokens_per_sec": 2.2900311979175147,
      "std_tokens_per_sec": 0.00549163586396589,
      "mean_time_per_token_ms": 436.6777873039246,
      "std_time_per_token_ms": 1.048207256702603
    }
  },
  "results": {
    "tokens_identical": true,
    "logprobs_exact": true,
    "distributions_exact": true,
    "perfect_reproducibility": true
  },
  "token_sequences": [
    [
      151667,
      198,
      32313,
      11,
      1077,
      752,
      1430,
      311,
      3535,
      419,
      5567,
      389,
      16703,
      71794,
      62020,
      320,
      2448,
      32,
      568,
      576
    ],
    [
      151667,
      198,
      32313,
      11,
      1077,
      752,
      1430,
      311,
      3535,
      419,
      5567,
      389,
      16703,
      71794,
      62020,
      320,
      2448,
      32,
      568,
      576
    ],
    [
      151667,
      198,
      32313,
      11,
      1077,
      752,
      1430,
      311,
      3535,
      419,
      5567,
      389,
      16703,
      71794,
      62020,
      320,
      2448,
      32,
      568,
      576
    ],
    [
      151667,
      198,
      32313,
      11,
      1077,
      752,
      1430,
      311,
      3535,
      419,
      5567,
      389,
      16703,
      71794,
      62020,
      320,
      2448,
      32,
      568,
      576
    ],
    [
      151667,
      198,
      32313,
      11,
      1077,
      752,
      1430,
      311,
      3535,
      419,
      5567,
      389,
      16703,
      71794,
      62020,
      320,
      2448,
      32,
      568,
      576
    ]
  ],
  "logprobs_vectors": [
    [
      0.0,
      -1.0013530300057027e-05,
      -0.022240620106458664,
      -6.186770770000294e-05,
      -0.6364033818244934,
      -0.6038233041763306,
      -0.0626058503985405,
      -6.9141146923357155e-06,
      -0.6923182010650635,
      -0.10519725829362869,
      -0.3135235905647278,
      -0.9849625825881958,
      -0.028313709422945976,
      -0.0002451834443490952,
      -4.9828242481453344e-05,
      -0.0855126678943634,
      -8.702239938429557e-06,
      -2.1934269170742482e-05,
      -0.14816683530807495,
      -0.20087304711341858
    ],
    [
      0.0,
      -1.0013530300057027e-05,
      -0.022240620106458664,
      -6.186770770000294e-05,
      -0.6364033818244934,
      -0.6038233041763306,
      -0.0626058503985405,
      -6.9141146923357155e-06,
      -0.6923182010650635,
      -0.10519725829362869,
      -0.3135235905647278,
      -0.9849625825881958,
      -0.028313709422945976,
      -0.0002451834443490952,
      -4.9828242481453344e-05,
      -0.0855126678943634,
      -8.702239938429557e-06,
      -2.1934269170742482e-05,
      -0.14816683530807495,
      -0.20087304711341858
    ],
    [
      0.0,
      -1.0013530300057027e-05,
      -0.022240620106458664,
      -6.186770770000294e-05,
      -0.6364033818244934,
      -0.6038233041763306,
      -0.0626058503985405,
      -6.9141146923357155e-06,
      -0.6923182010650635,
      -0.10519725829362869,
      -0.3135235905647278,
      -0.9849625825881958,
      -0.028313709422945976,
      -0.0002451834443490952,
      -4.9828242481453344e-05,
      -0.0855126678943634,
      -8.702239938429557e-06,
      -2.1934269170742482e-05,
      -0.14816683530807495,
      -0.20087304711341858
    ],
    [
      0.0,
      -1.0013530300057027e-05,
      -0.022240620106458664,
      -6.186770770000294e-05,
      -0.6364033818244934,
      -0.6038233041763306,
      -0.0626058503985405,
      -6.9141146923357155e-06,
      -0.6923182010650635,
      -0.10519725829362869,
      -0.3135235905647278,
      -0.9849625825881958,
      -0.028313709422945976,
      -0.0002451834443490952,
      -4.9828242481453344e-05,
      -0.0855126678943634,
      -8.702239938429557e-06,
      -2.1934269170742482e-05,
      -0.14816683530807495,
      -0.20087304711341858
    ],
    [
      0.0,
      -1.0013530300057027e-05,
      -0.022240620106458664,
      -6.186770770000294e-05,
      -0.6364033818244934,
      -0.6038233041763306,
      -0.0626058503985405,
      -6.9141146923357155e-06,
      -0.6923182010650635,
      -0.10519725829362869,
      -0.3135235905647278,
      -0.9849625825881958,
      -0.028313709422945976,
      -0.0002451834443490952,
      -4.9828242481453344e-05,
      -0.0855126678943634,
      -8.702239938429557e-06,
      -2.1934269170742482e-05,
      -0.14816683530807495,
      -0.20087304711341858
    ]
  ],
  "generated_texts": [
    "<think>\nOkay, let me try to understand this paper on Native Sparse Attention (NSA). The",
    "<think>\nOkay, let me try to understand this paper on Native Sparse Attention (NSA). The",
    "<think>\nOkay, let me try to understand this paper on Native Sparse Attention (NSA). The",
    "<think>\nOkay, let me try to understand this paper on Native Sparse Attention (NSA). The",
    "<think>\nOkay, let me try to understand this paper on Native Sparse Attention (NSA). The"
  ]
}