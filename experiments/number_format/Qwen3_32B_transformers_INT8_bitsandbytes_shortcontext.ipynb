{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec67b34a-61ce-47a0-ac72-f640391f6d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dependencies...\n",
      "  bitsandbytes: OK\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRANSFORMERS INT8-BITSANDBYTES DETERMINISM TEST\n",
      "================================================================================\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen3-32B\n",
      "  Library: HuggingFace Transformers + bitsandbytes\n",
      "  Quantization: BitsAndBytes INT8\n",
      "  Precision: INT8 with FP16 compute\n",
      "  Max new tokens: 50\n",
      "  Temperature: 0.0 (greedy decoding)\n",
      "  Seed: 42\n",
      "  Repetitions: 5\n",
      "\n",
      "Loading model with bitsandbytes INT8 quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1b02e41be841d18006c946f83693a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 48.97s\n",
      "Device: cuda:0\n",
      "\n",
      "Prompt length: 622 characters\n",
      "Prompt tokens: 100\n",
      "\n",
      "Running warmup...\n",
      "Warmup complete - generated 50 tokens\n",
      "\n",
      "================================================================================\n",
      "RUNNING EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Repetition 1/5...\n",
      "  50 tokens in 7.247s (6.9 tok/s)\n",
      "Repetition 2/5...\n",
      "  50 tokens in 7.423s (6.7 tok/s)\n",
      "Repetition 3/5...\n",
      "  50 tokens in 7.313s (6.8 tok/s)\n",
      "Repetition 4/5...\n",
      "  50 tokens in 7.237s (6.9 tok/s)\n",
      "Repetition 5/5...\n",
      "  50 tokens in 7.291s (6.9 tok/s)\n",
      "\n",
      "All repetitions complete!\n",
      "\n",
      "================================================================================\n",
      "TIMING ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Timing statistics:\n",
      "  Mean time: 7.302s (σ=0.0667s)\n",
      "  Min/Max: 7.237s / 7.423s\n",
      "  Tokens/sec: 6.8 (σ=0.06)\n",
      "  Time/token: 146.05ms (σ=1.334ms)\n",
      "\n",
      "================================================================================\n",
      "REPRODUCIBILITY ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Comparing across repetitions (same config, same seed):\n",
      "  Checking if Run 1 = Run 2 = Run 3 = ... = Run 10\n",
      "  Testing: Can we reproduce inference bit-exactly?\n",
      "\n",
      "Checking token sequences...\n",
      "Token sequences identical: True\n",
      "\n",
      "Checking selected token logprobs...\n",
      "Selected token logprobs bit-exact: True\n",
      "\n",
      "================================================================================\n",
      "VERDICT - TRANSFORMERS INT8-BITSANDBYTES\n",
      "================================================================================\n",
      "\n",
      "[PASS] PERFECT REPRODUCIBILITY WITH TRANSFORMERS INT8-BITSANDBYTES\n",
      "  - Token sequences: bit-exact\n",
      "  - Selected token logprobs: bit-exact\n",
      "  => Transformers + bitsandbytes maintains determinism\n",
      "  => INT8 quantization does not break forensic verification\n",
      "\n",
      "Comparison with BF16 baseline:\n",
      "  BF16: Perfect bit-exact reproducibility\n",
      "  INT8-BitsAndBytes: [see above]\n",
      "\n",
      "\n",
      "Results saved to: transformers_int8_bnb_determinism_20251111_201226.json\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Transformers INT8-BitsAndBytes Determinism Test\n",
    "Tests bit-exact reproducibility using HuggingFace Transformers with BitsAndBytes quantization\n",
    "Compares with BF16 baseline to isolate quantization-specific reproducibility issues\n",
    "\n",
    "Dependencies:\n",
    "- transformers\n",
    "- torch\n",
    "- bitsandbytes (INT8 quantization backend)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "\n",
    "# Check dependencies\n",
    "print(\"Checking dependencies...\")\n",
    "missing = []\n",
    "\n",
    "try:\n",
    "    import bitsandbytes\n",
    "    print(\"  bitsandbytes: OK\")\n",
    "except ImportError:\n",
    "    print(\"  bitsandbytes: MISSING\")\n",
    "    missing.append(\"bitsandbytes\")\n",
    "\n",
    "if missing:\n",
    "    print()\n",
    "    print(\"ERROR: Missing required packages!\")\n",
    "    print()\n",
    "    print(\"Please install manually in a separate cell:\")\n",
    "    print()\n",
    "    for pkg in missing:\n",
    "        print(f\"  !pip install {pkg}\")\n",
    "    print()\n",
    "    print(\"Then re-run this script.\")\n",
    "    import sys\n",
    "    sys.exit(1)\n",
    "\n",
    "print()\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-32B\"\n",
    "MAX_NEW_TOKENS = 50\n",
    "NUM_REPETITIONS = 5\n",
    "TEMPERATURE = 0.0  # Greedy (but we'll use do_sample=False)\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10  # Will extract manually from model output\n",
    "\n",
    "# Test prompt\n",
    "USER_TASK = \"Please provide a detailed summary of the following text.\"\n",
    "DOCUMENT_CONTENT = \"\"\"The development of large language models has fundamentally transformed natural language processing and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing. The scaling laws observed in these systems suggest that performance continues to improve with model size, data scale, and compute budget, though with diminishing returns.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRANSFORMERS INT8-BITSANDBYTES DETERMINISM TEST\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Library: HuggingFace Transformers + bitsandbytes\")\n",
    "print(f\"  Quantization: BitsAndBytes INT8\")\n",
    "print(f\"  Precision: INT8 with FP16 compute\")\n",
    "print(f\"  Max new tokens: {MAX_NEW_TOKENS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE} (greedy decoding)\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD MODEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading model with bitsandbytes INT8 quantization...\")\n",
    "load_start = time.time()\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir='/tmp/hf_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Configure INT8 quantization\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "# Load model with BitsAndBytes INT8\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quantization_config,\n",
    "    cache_dir='/tmp/hf_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.eval()  # Evaluation mode\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "print(f\"Model loaded in {load_time:.2f}s\")\n",
    "print(f\"Device: {model.device}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# PREPARE INPUT\n",
    "# ============================================================================\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{USER_TASK}\\n\\n{DOCUMENT_CONTENT}\"}\n",
    "]\n",
    "\n",
    "# Apply chat template\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(f\"Prompt length: {len(prompt_text)} characters\")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
    "prompt_length = inputs.input_ids.shape[1]\n",
    "print(f\"Prompt tokens: {prompt_length}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# WARMUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Running warmup...\")\n",
    "with torch.no_grad():\n",
    "    warmup_output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=MAX_NEW_TOKENS,\n",
    "        do_sample=False,  # Greedy decoding\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        output_scores=True,\n",
    "        return_dict_in_generate=True\n",
    "    )\n",
    "print(f\"Warmup complete - generated {len(warmup_output.sequences[0]) - prompt_length} tokens\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "results_tokens = []\n",
    "results_logprobs = []\n",
    "results_texts = []\n",
    "timing_data = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Repetition {rep + 1}/{NUM_REPETITIONS}...\")\n",
    "    \n",
    "    # Reset seeds for each repetition\n",
    "    torch.manual_seed(SEED)\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    \n",
    "    # Time generation\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,  # Greedy decoding\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            output_scores=True,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    # Extract tokens (remove prompt)\n",
    "    generated_ids = outputs.sequences[0][prompt_length:].cpu().tolist()\n",
    "    num_tokens = len(generated_ids)\n",
    "    \n",
    "    results_tokens.append(generated_ids)\n",
    "    \n",
    "    # Decode text\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    results_texts.append(generated_text)\n",
    "    \n",
    "    # Extract logprobs from scores\n",
    "    # scores is tuple of tensors, one per generated token\n",
    "    # shape: (batch=1, vocab_size)\n",
    "    logprobs_selected = []\n",
    "    for i, (token_id, scores_tensor) in enumerate(zip(generated_ids, outputs.scores)):\n",
    "        # Get log probabilities\n",
    "        log_probs = torch.nn.functional.log_softmax(scores_tensor[0], dim=-1)\n",
    "        # Get logprob of selected token\n",
    "        selected_logprob = log_probs[token_id].item()\n",
    "        logprobs_selected.append(selected_logprob)\n",
    "    \n",
    "    results_logprobs.append(np.array(logprobs_selected))\n",
    "    \n",
    "    # Timing\n",
    "    tokens_per_sec = num_tokens / elapsed\n",
    "    timing_data.append({\n",
    "        'repetition': rep + 1,\n",
    "        'elapsed_time': elapsed,\n",
    "        'num_tokens': num_tokens,\n",
    "        'tokens_per_sec': tokens_per_sec,\n",
    "        'time_per_token': elapsed / num_tokens\n",
    "    })\n",
    "    \n",
    "    print(f\"  {num_tokens} tokens in {elapsed:.3f}s ({tokens_per_sec:.1f} tok/s)\")\n",
    "\n",
    "print()\n",
    "print(\"All repetitions complete!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TIMING ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TIMING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "times = [t['elapsed_time'] for t in timing_data]\n",
    "tps = [t['tokens_per_sec'] for t in timing_data]\n",
    "tpt = [t['time_per_token'] for t in timing_data]\n",
    "\n",
    "print(\"Timing statistics:\")\n",
    "print(f\"  Mean time: {np.mean(times):.3f}s (σ={np.std(times):.4f}s)\")\n",
    "print(f\"  Min/Max: {np.min(times):.3f}s / {np.max(times):.3f}s\")\n",
    "print(f\"  Tokens/sec: {np.mean(tps):.1f} (σ={np.std(tps):.2f})\")\n",
    "print(f\"  Time/token: {np.mean(tpt)*1000:.2f}ms (σ={np.std(tpt)*1000:.3f}ms)\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"REPRODUCIBILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "print(\"Comparing across repetitions (same config, same seed):\")\n",
    "print(\"  Checking if Run 1 = Run 2 = Run 3 = ... = Run 10\")\n",
    "print(\"  Testing: Can we reproduce inference bit-exactly?\")\n",
    "print()\n",
    "\n",
    "# Check token sequences\n",
    "print(\"Checking token sequences...\")\n",
    "tokens_identical = all(\n",
    "    results_tokens[0] == results_tokens[i]\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "if not tokens_identical:\n",
    "    print(\"\\n[WARNING] Token sequences differ!\")\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[0] != results_tokens[i]:\n",
    "            diff_positions = [\n",
    "                j for j in range(min(len(results_tokens[0]), len(results_tokens[i])))\n",
    "                if results_tokens[0][j] != results_tokens[i][j]\n",
    "            ]\n",
    "            print(f\"  Rep 0 vs Rep {i}: {len(diff_positions)} positions differ\")\n",
    "            if diff_positions:\n",
    "                print(f\"    First difference at position {diff_positions[0]}\")\n",
    "\n",
    "# Check logprobs\n",
    "print(\"\\nChecking selected token logprobs...\")\n",
    "first_logprobs = results_logprobs[0]\n",
    "logprobs_exact = all(\n",
    "    np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Selected token logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "    \n",
    "    # Element-wise statistics\n",
    "    all_logprobs = np.array(results_logprobs)\n",
    "    std_per_token = all_logprobs.std(axis=0)\n",
    "    print(f\"\\nPer-token std statistics:\")\n",
    "    print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "    print(f\"  Max: {std_per_token.max():.6e}\")\n",
    "    print(f\"  Median: {np.median(std_per_token):.6e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"VERDICT - TRANSFORMERS INT8-BITSANDBYTES\")\n",
    "print(\"=\"*80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact:\n",
    "    print(\"[PASS] PERFECT REPRODUCIBILITY WITH TRANSFORMERS INT8-BITSANDBYTES\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  => Transformers + bitsandbytes maintains determinism\")\n",
    "    print(\"  => INT8 quantization does not break forensic verification\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"[WARNING] TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: numerical variation\")\n",
    "    max_l2 = max(l2_distances) if not logprobs_exact else 0.0\n",
    "    print(f\"  - L2 distance: {max_l2:.6e}\")\n",
    "    print(\"  => Greedy decoding unaffected by quantization noise\")\n",
    "    print(\"  => BitsAndBytes INT8 has non-deterministic dequantization\")\n",
    "else:\n",
    "    print(\"[FAIL] TOKEN SEQUENCES DIFFER\")\n",
    "    print(\"  - Greedy decoding produces different tokens\")\n",
    "    print(\"  => Severe non-determinism in Transformers BitsAndBytes\")\n",
    "\n",
    "print()\n",
    "print(\"Comparison with BF16 baseline:\")\n",
    "print(\"  BF16: Perfect bit-exact reproducibility\")\n",
    "print(\"  INT8-BitsAndBytes: [see above]\")\n",
    "print()\n",
    "\n",
    "if not tokens_identical or not logprobs_exact:\n",
    "    print(\"Hypothesis:\")\n",
    "    if not logprobs_exact and tokens_identical:\n",
    "        print(\"  INT8 dequantization introduces numerical variation\")\n",
    "        print(\"  Variation is small enough to not affect argmax selection\")\n",
    "        print(\"  Hidden state forensics may still detect quantization\")\n",
    "    else:\n",
    "        print(\"  Quantization introduces significant non-determinism\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"transformers_int8_bitsandbytes_determinism\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"library\": \"transformers + bitsandbytes\",\n",
    "    \"prompt_text\": prompt_text,\n",
    "    \"prompt_length_tokens\": prompt_length,\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"quantization\": \"bitsandbytes\",\n",
    "        \"precision\": \"INT8 with FP16 compute\",\n",
    "        \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"do_sample\": False\n",
    "    },\n",
    "    \"timing\": {\n",
    "        \"model_load_time\": load_time,\n",
    "        \"per_repetition\": timing_data,\n",
    "        \"statistics\": {\n",
    "            \"mean_time\": float(np.mean(times)),\n",
    "            \"std_time\": float(np.std(times)),\n",
    "            \"mean_tokens_per_sec\": float(np.mean(tps)),\n",
    "            \"std_tokens_per_sec\": float(np.std(tps)),\n",
    "            \"mean_time_per_token_ms\": float(np.mean(tpt) * 1000),\n",
    "            \"std_time_per_token_ms\": float(np.std(tpt) * 1000)\n",
    "        }\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts\n",
    "}\n",
    "\n",
    "output_file = f\"transformers_int8_bnb_determinism_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d19eebb-995f-4bb1-baf6-c5468ae54cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
