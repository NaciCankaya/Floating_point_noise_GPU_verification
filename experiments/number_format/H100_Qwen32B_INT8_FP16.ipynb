{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c44877-1cba-4867-bfdb-fdd51903baf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "H100 INT8 QUANTIZATION DETERMINISM TEST\n",
      "================================================================================\n",
      "\n",
      "Found file: Verification-for-International-AI-Governance.pdf\n",
      "Loading 172 pages from PDF...\n",
      "Loaded 535619 characters from PDF (172 pages)\n",
      "\n",
      "Message content length: 535677 characters\n",
      "\n",
      "Loading tokenizer to validate prompt length...\n",
      "Prompt statistics:\n",
      "  Characters: 535,825\n",
      "  Tokens: 120,431\n",
      "  Max model length: 130,000\n",
      "  Generation tokens: 100\n",
      "  Total required: 120,531\n",
      "\n",
      "Configuration:\n",
      "  Model: JunHowie/Qwen3-32B-GPTQ-Int8\n",
      "  Quantization: gptq (from model config)\n",
      "  Precision: INT8 weights, FP16 activations\n",
      "  Compute: INT8 dequantized to FP16, then FP16 matmul\n",
      "  KV cache: FP16\n",
      "  Tensor parallel: 1\n",
      "  Max model len: 130,000 (with YaRN scaling)\n",
      "  YaRN factor: 4.0x (32K -> 128K)\n",
      "  Max tokens: 100\n",
      "  Temperature: 0.0\n",
      "  Seed: 42\n",
      "  Repetitions: 10\n",
      "  Warmup runs: 3\n",
      "\n",
      "IMPORTANT: GPTQ quantization in vLLM requires FP16\n",
      "           BF16 is not supported for GPTQ\n",
      "           FP8 experiment will use BF16 (native support)\n",
      "\n",
      "Loading INT8-GPTQ quantized model (FP16 activations)...\n",
      "Note: INT8 weights will be dequantized to FP16 for computation\n",
      "Note: GPTQ in vLLM requires FP16 (BF16 not supported)\n",
      "Activations and KV cache: FP16\n",
      "Enabling YaRN rope scaling for 130K context...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 'model.safetensors.index.json' to '/tmp/hf_cache/hub/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/a215754d8ddfb760dcc6c6893bc7e887ba4552c2.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "288e5434aa814c2d92e3b613c0612828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/hub/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/a215754d8ddfb760dcc6c6893bc7e887ba4552c2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e547210b274448dbb7d1642d313c4b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 'tokenizer_config.json' to '/tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/7ce73cd447d91f15fe113e8eefbf74c254fd9210.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3da75263208496c964031a8d0210a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/7ce73cd447d91f15fe113e8eefbf74c254fd9210\n",
      "Downloading 'vocab.json' to '/tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/4783fe10ac3adce15ac8f358ef5462739852c569.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c79641e334c24ef7b4245a911deebc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/4783fe10ac3adce15ac8f358ef5462739852c569\n",
      "Downloading 'merges.txt' to '/tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5838adb4ee5a408c84b492f48ef75d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/31349551d90c7606f325fe0f11bbb8bd5fa0d7c7\n",
      "Downloading 'tokenizer.json' to '/tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b10facc62fa467082b2657053c9b7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/aeb13307a71acd8fe81861d94ad54ab689df773318809eed3cbe794b4492dae4\n",
      "Downloading 'added_tokens.json' to '/tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/b54f9135e44c1e81047e8d05cb027af8bc039eed.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806fe4b92c7346378e00606ae54ed543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/b54f9135e44c1e81047e8d05cb027af8bc039eed\n",
      "Downloading 'special_tokens_map.json' to '/tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/8034efea782cf5b066f55bb0b75d3062b4175dec.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "849e7b3f0ede4f87a9e51ee6f53f1954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/8034efea782cf5b066f55bb0b75d3062b4175dec\n",
      "Downloading 'chat_template.jinja' to '/tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/01be9b307daa2d425f7c168c9fb145a286e0afb4.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9c2d9bdb57e4021bfa40727724740fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/01be9b307daa2d425f7c168c9fb145a286e0afb4\n",
      "Downloading 'generation_config.json' to '/tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/ec7a19e7f3011ec97a691b6533697b418d4e046a.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4fee23e9a84401a825a26e19fe81728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download complete. Moving file to /tmp/hf_cache/models--JunHowie--Qwen3-32B-GPTQ-Int8/blobs/ec7a19e7f3011ec97a691b6533697b418d4e046a\n",
      "/venv/main/lib/python3.10/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/9 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  11% Completed | 1/9 [00:00<00:05,  1.53it/s]\n",
      "Loading safetensors checkpoint shards:  22% Completed | 2/9 [00:01<00:04,  1.66it/s]\n",
      "Loading safetensors checkpoint shards:  33% Completed | 3/9 [00:01<00:03,  1.71it/s]\n",
      "Loading safetensors checkpoint shards:  44% Completed | 4/9 [00:02<00:02,  1.82it/s]\n",
      "Loading safetensors checkpoint shards:  56% Completed | 5/9 [00:02<00:02,  1.97it/s]\n",
      "Loading safetensors checkpoint shards:  67% Completed | 6/9 [00:03<00:01,  2.07it/s]\n",
      "Loading safetensors checkpoint shards:  78% Completed | 7/9 [00:03<00:00,  2.14it/s]\n",
      "Loading safetensors checkpoint shards:  89% Completed | 8/9 [00:04<00:00,  2.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  2.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 9/9 [00:04<00:00,  2.00it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1419)\u001b[0;0m \n",
      "\u001b[1;36m(EngineCore_DP0 pid=1419)\u001b[0;0m 2025-11-11 15:12:33,825 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1419)\u001b[0;0m 2025-11-11 15:12:35,600 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1419)\u001b[0;0m cudagraph dispatching keys are not initialized. No cudagraph will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded in 147.97s\n",
      "\n",
      "Running 3 warmup iterations...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dccbaea113a4e4f992e6e60bfb42374",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109f86cef33447d787052942e5f6fd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warmup 1: 45.340s (100 tokens, 2.2 tok/s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a404ac78e54d89b20c0f43a550c198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94fe7f1a09b40c2a793e46fbe2440e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warmup 2: 44.164s (100 tokens, 2.3 tok/s)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "091f18bd648b4c7ab2439db611001e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e579cdea51b4abf8f0e32c580f437f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Warmup 3: 44.120s (100 tokens, 2.3 tok/s)\n",
      "Warmup complete - avg time: 44.542s\n",
      "\n",
      "================================================================================\n",
      "RUNNING EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Repetition 1/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "546ad0a0d60646fa9b3199475b2a4c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2cc42bf84ee495fb07b1afca5c69689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 44.131s (2.3 tok/s)\n",
      "Repetition 2/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ca9dfb04ed4ecbab2b321380d978ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7839c38982147aa92a585fec716f5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 44.064s (2.3 tok/s)\n",
      "Repetition 3/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42851189dcb44555bc44bf4ba4c41b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8628c2744ace440b81ff9a63b3f0e224",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 43.988s (2.3 tok/s)\n",
      "Repetition 4/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d3d1c975a345acb9113cbadfe764dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60c938179d924653b445b9d0a7b376e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 43.998s (2.3 tok/s)\n",
      "Repetition 5/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c0e9c543c264b419740f94026407bbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7bacb151624be1a47ed0b8cfb920fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 44.058s (2.3 tok/s)\n",
      "Repetition 6/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b967c4e1b45242a0a4c8a233da83d987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d13d5d23fb8c4f6ba8c3ee43ea13fb6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 44.028s (2.3 tok/s)\n",
      "Repetition 7/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e3064ab84fd4dd1abc13eb9dd2acb9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafd91ef52f745e7b7f1267696c3384b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 44.016s (2.3 tok/s)\n",
      "Repetition 8/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039c736c95e5435090cbc41598db0fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be32aac597604804a8e0b7a84dee1da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 44.051s (2.3 tok/s)\n",
      "Repetition 9/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810de2569e8c443ba0b037c307832926",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2387c5928b954145b1369d14244634a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  100 tokens in 44.067s (2.3 tok/s)\n",
      "Repetition 10/10...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9777b0795745fc9748036110e8c9d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f8f786488f545c990b6fdb695f71371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "H100 INT8 Quantization Determinism Test\n",
    "Tests bit-exact reproducibility with INT8-GPTQ quantization\n",
    "Includes detailed timing measurements for performance comparison\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SUPPRESS VERBOSE LOGGING\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "os.environ['VLLM_LOGGING_LEVEL'] = 'WARNING'\n",
    "os.environ['VLLM_CONFIGURE_LOGGING'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "logging.getLogger('vllm').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.engine').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.worker').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.executor').setLevel(logging.ERROR)\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "logging.getLogger('torch').setLevel(logging.ERROR)\n",
    "logging.getLogger('huggingface_hub').setLevel(logging.INFO)\n",
    "logging.getLogger('huggingface_hub.file_download').setLevel(logging.INFO)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import time\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration - INT8-GPTQ quantized version\n",
    "MODEL_NAME = \"JunHowie/Qwen3-32B-GPTQ-Int8\"\n",
    "QUANTIZATION = \"gptq\"  # Must match model config (not compressed-tensors)\n",
    "TENSOR_PARALLEL_SIZE = 1  # Single GPU for clean timing\n",
    "MAX_MODEL_LEN = 130000\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 100\n",
    "NUM_REPETITIONS = 10\n",
    "TEMPERATURE = 0.0  # Greedy decoding\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Timing configuration\n",
    "NUM_WARMUP_RUNS = 3  # Extra warmup for stable timing\n",
    "\n",
    "# Prompt source\n",
    "AUTO_FIND_FILE = True\n",
    "\n",
    "# User task\n",
    "USER_TASK = \"Please provide a detailed summary of the following text.\"\n",
    "\n",
    "# Hardcoded content\n",
    "HARDCODED_CONTENT = \"\"\"The development of large language models has fundamentally transformed natural language processing and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing. The scaling laws observed in these systems suggest that performance continues to improve with model size, data scale, and compute budget, though with diminishing returns. Recent advances in architecture, training techniques, and inference optimization have made these powerful models increasingly accessible for practical applications.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FILE LOADING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def find_prompt_file():\n",
    "    \"\"\"Find first txt or pdf file in current directory\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    txt_files = glob.glob(os.path.join(cwd, \"*.txt\"))\n",
    "    pdf_files = glob.glob(os.path.join(cwd, \"*.pdf\"))\n",
    "    \n",
    "    if txt_files:\n",
    "        return txt_files[0]\n",
    "    elif pdf_files:\n",
    "        return pdf_files[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_text_from_file(filepath):\n",
    "    \"\"\"Load text from txt or pdf file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    if filepath.endswith('.txt'):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print(f\"Loaded {len(text)} characters from txt file\")\n",
    "        return text\n",
    "    \n",
    "    elif filepath.endswith('.pdf'):\n",
    "        try:\n",
    "            import PyPDF2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"PyPDF2 required for PDF loading. Install with: pip install PyPDF2\")\n",
    "        \n",
    "        text = []\n",
    "        with open(filepath, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Loading {num_pages} pages from PDF...\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text.append(page_text)\n",
    "        \n",
    "        full_text = '\\n'.join(text)\n",
    "        print(f\"Loaded {len(full_text)} characters from PDF ({num_pages} pages)\")\n",
    "        return full_text\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"H100 INT8 QUANTIZATION DETERMINISM TEST\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load document content\n",
    "prompt_file = None\n",
    "if AUTO_FIND_FILE:\n",
    "    prompt_file = find_prompt_file()\n",
    "    if prompt_file:\n",
    "        print(f\"Found file: {os.path.basename(prompt_file)}\")\n",
    "        DOCUMENT_CONTENT = load_text_from_file(prompt_file)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No txt/pdf files found - using hardcoded content\")\n",
    "        DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "        print()\n",
    "else:\n",
    "    DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "    print(\"Using hardcoded content\")\n",
    "    print()\n",
    "\n",
    "# Prepare messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{USER_TASK}\\n\\n{DOCUMENT_CONTENT}\"}\n",
    "]\n",
    "\n",
    "print(f\"Message content length: {len(messages[0]['content'])} characters\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER PRECHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading tokenizer to validate prompt length...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-32B-Instruct\",  # Use base Qwen model for tokenizer\n",
    "    cache_dir='/tmp/hf_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply chat template with enable_thinking for Qwen3\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True  # Qwen3 thinking mode\n",
    ")\n",
    "\n",
    "prompt_tokens = tokenizer.encode(prompt_text)\n",
    "prompt_length = len(prompt_tokens)\n",
    "\n",
    "print(f\"Prompt statistics:\")\n",
    "print(f\"  Characters: {len(prompt_text):,}\")\n",
    "print(f\"  Tokens: {prompt_length:,}\")\n",
    "print(f\"  Max model length: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Generation tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Total required: {prompt_length + MAX_TOKENS:,}\")\n",
    "print()\n",
    "\n",
    "if prompt_length + MAX_TOKENS > MAX_MODEL_LEN:\n",
    "    print(f\"[WARNING] May exceed context length\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Quantization: {QUANTIZATION} (from model config)\")\n",
    "print(f\"  Precision: INT8 weights, FP16 activations\")\n",
    "print(f\"  Compute: INT8 dequantized to FP16, then FP16 matmul\")\n",
    "print(f\"  KV cache: FP16\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {MAX_MODEL_LEN:,} (with YaRN scaling)\")\n",
    "print(f\"  YaRN factor: 4.0x (32K -> 128K)\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print(f\"  Warmup runs: {NUM_WARMUP_RUNS}\")\n",
    "print()\n",
    "print(\"IMPORTANT: GPTQ quantization in vLLM requires FP16\")\n",
    "print(\"           BF16 is not supported for GPTQ\")\n",
    "print(\"           FP8 experiment will use BF16 (native support)\")\n",
    "print()\n",
    "\n",
    "print(\"Loading INT8-GPTQ quantized model (FP16 activations)...\")\n",
    "print(\"Note: INT8 weights will be dequantized to FP16 for computation\")\n",
    "print(\"Note: GPTQ in vLLM requires FP16 (BF16 not supported)\")\n",
    "print(\"Activations and KV cache: FP16\")\n",
    "print(\"Enabling YaRN rope scaling for 130K context...\")\n",
    "load_start = time.time()\n",
    "\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    quantization=QUANTIZATION,\n",
    "    dtype=\"float16\",  # GPTQ requires FP16 (BF16 not supported by vLLM GPTQ)\n",
    "    tensor_parallel_size=TENSOR_PARALLEL_SIZE,\n",
    "    max_model_len=MAX_MODEL_LEN,\n",
    "    gpu_memory_utilization=GPU_MEMORY_UTILIZATION,\n",
    "    trust_remote_code=True,\n",
    "    seed=SEED,\n",
    "    enforce_eager=True,\n",
    "    enable_prefix_caching=False,\n",
    "    # YaRN rope scaling for long context (32K -> 130K)\n",
    "    rope_scaling={\n",
    "        \"rope_type\": \"yarn\",\n",
    "        \"factor\": 4.0,\n",
    "        \"original_max_position_embeddings\": 32768\n",
    "    }\n",
    ")\n",
    "\n",
    "load_time = time.time() - load_start\n",
    "print(f\"Model loaded in {load_time:.2f}s\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    "    logprobs=TOP_LOGPROBS,\n",
    "    skip_special_tokens=False\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# WARMUP\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"Running {NUM_WARMUP_RUNS} warmup iterations...\")\n",
    "warmup_times = []\n",
    "for i in range(NUM_WARMUP_RUNS):\n",
    "    warmup_start = time.time()\n",
    "    warmup_output = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "    warmup_time = time.time() - warmup_start\n",
    "    warmup_times.append(warmup_time)\n",
    "    num_tokens = len(warmup_output[0].outputs[0].token_ids)\n",
    "    print(f\"  Warmup {i+1}: {warmup_time:.3f}s ({num_tokens} tokens, {num_tokens/warmup_time:.1f} tok/s)\")\n",
    "\n",
    "print(f\"Warmup complete - avg time: {np.mean(warmup_times):.3f}s\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "results_tokens = []\n",
    "results_logprobs = []\n",
    "results_texts = []\n",
    "results_distributions = []\n",
    "timing_data = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Repetition {rep + 1}/{NUM_REPETITIONS}...\")\n",
    "    \n",
    "    # Time the generation\n",
    "    start_time = time.time()\n",
    "    outputs = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed = end_time - start_time\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract token IDs\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    num_tokens = len(token_ids)\n",
    "    tokens_per_sec = num_tokens / elapsed\n",
    "    \n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    # Store timing\n",
    "    timing_data.append({\n",
    "        'repetition': rep + 1,\n",
    "        'elapsed_time': elapsed,\n",
    "        'num_tokens': num_tokens,\n",
    "        'tokens_per_sec': tokens_per_sec,\n",
    "        'time_per_token': elapsed / num_tokens\n",
    "    })\n",
    "    \n",
    "    # Extract generated text\n",
    "    text = output.outputs[0].text\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    # Extract logprobs\n",
    "    logprobs_data = output.outputs[0].logprobs\n",
    "    selected_logprobs = [lp[token_ids[i]].logprob for i, lp in enumerate(logprobs_data)]\n",
    "    results_logprobs.append(np.array(selected_logprobs))\n",
    "    \n",
    "    # Extract distributions\n",
    "    rep_distributions = []\n",
    "    for position_logprobs in logprobs_data:\n",
    "        sorted_items = sorted(position_logprobs.items(), \n",
    "                            key=lambda x: x[1].logprob, \n",
    "                            reverse=True)[:TOP_LOGPROBS]\n",
    "        rep_distributions.append([(tok, lp.logprob) for tok, lp in sorted_items])\n",
    "    results_distributions.append(rep_distributions)\n",
    "    \n",
    "    print(f\"  {num_tokens} tokens in {elapsed:.3f}s ({tokens_per_sec:.1f} tok/s)\")\n",
    "\n",
    "print()\n",
    "print(\"All repetitions complete!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TIMING ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TIMING ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "times = [t['elapsed_time'] for t in timing_data]\n",
    "tps = [t['tokens_per_sec'] for t in timing_data]\n",
    "tpt = [t['time_per_token'] for t in timing_data]\n",
    "\n",
    "print(\"Timing statistics:\")\n",
    "print(f\"  Mean time: {np.mean(times):.3f}s (σ={np.std(times):.4f}s)\")\n",
    "print(f\"  Min/Max: {np.min(times):.3f}s / {np.max(times):.3f}s\")\n",
    "print(f\"  Tokens/sec: {np.mean(tps):.1f} (σ={np.std(tps):.2f})\")\n",
    "print(f\"  Time/token: {np.mean(tpt)*1000:.2f}ms (σ={np.std(tpt)*1000:.3f}ms)\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"REPRODUCIBILITY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check token sequences\n",
    "print(\"Checking token sequences...\")\n",
    "tokens_identical = all(\n",
    "    results_tokens[0] == results_tokens[i] \n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "if not tokens_identical:\n",
    "    print(\"\\n[WARNING] Token sequences differ!\")\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[0] != results_tokens[i]:\n",
    "            diff_positions = [\n",
    "                j for j in range(min(len(results_tokens[0]), len(results_tokens[i])))\n",
    "                if results_tokens[0][j] != results_tokens[i][j]\n",
    "            ]\n",
    "            print(f\"  Rep 0 vs Rep {i}: {len(diff_positions)} positions differ\")\n",
    "            if diff_positions:\n",
    "                print(f\"    First difference at position {diff_positions[0]}\")\n",
    "                print(f\"      Rep 0: token {results_tokens[0][diff_positions[0]]}\")\n",
    "                print(f\"      Rep {i}: token {results_tokens[i][diff_positions[0]]}\")\n",
    "\n",
    "# Check logprobs\n",
    "print(\"\\nChecking selected token logprobs...\")\n",
    "first_logprobs = results_logprobs[0]\n",
    "logprobs_exact = all(\n",
    "    np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Selected token logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "\n",
    "# Check distributions\n",
    "print(\"\\nChecking full top-k distributions...\")\n",
    "distributions_exact = True\n",
    "distribution_mismatches = []\n",
    "\n",
    "first_dist = results_distributions[0]\n",
    "for rep_idx in range(1, NUM_REPETITIONS):\n",
    "    for pos_idx in range(len(first_dist)):\n",
    "        dist_a = first_dist[pos_idx]\n",
    "        dist_b = results_distributions[rep_idx][pos_idx]\n",
    "        \n",
    "        tokens_match = [t[0] for t in dist_a] == [t[0] for t in dist_b]\n",
    "        \n",
    "        if tokens_match:\n",
    "            logprobs_match = all(\n",
    "                abs(dist_a[i][1] - dist_b[i][1]) < 1e-10 \n",
    "                for i in range(len(dist_a))\n",
    "            )\n",
    "            if not logprobs_match:\n",
    "                distributions_exact = False\n",
    "                distribution_mismatches.append((rep_idx, pos_idx))\n",
    "        else:\n",
    "            distributions_exact = False\n",
    "            distribution_mismatches.append((rep_idx, pos_idx))\n",
    "\n",
    "print(f\"Top-k distributions bit-exact: {distributions_exact}\")\n",
    "\n",
    "if not distributions_exact:\n",
    "    print(f\"\\nFound {len(distribution_mismatches)} mismatches\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERDICT - INT8 QUANTIZATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact and distributions_exact:\n",
    "    print(\"[PASS] PERFECT REPRODUCIBILITY WITH INT8\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Full top-k distributions: bit-exact\")\n",
    "    print(\"  => INT8 quantization maintains determinism\")\n",
    "elif tokens_identical and logprobs_exact and not distributions_exact:\n",
    "    print(\"[WARNING] SELECTED TOKENS EXACT, DISTRIBUTIONS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Top-k distributions: numerical variation\")\n",
    "    print(\"  => INT8 has distribution-level noise\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"[WARNING] TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: numerical variation\")\n",
    "    print(\"  => INT8 introduces FP noise in dequantization\")\n",
    "else:\n",
    "    print(\"[FAIL] TOKEN SEQUENCES DIFFER - DETERMINISM BROKEN\")\n",
    "    print(\"  - INT8 quantization breaks greedy decoding determinism\")\n",
    "    print(\"  => Same issue as INT4\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"h100_int8_determinism_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"hardware\": \"H100\",\n",
    "    \"prompt_source\": \"file\" if prompt_file else \"hardcoded\",\n",
    "    \"prompt_file\": os.path.basename(prompt_file) if prompt_file else None,\n",
    "    \"prompt_text\": prompt_text,\n",
    "    \"prompt_length_chars\": len(prompt_text),\n",
    "    \"prompt_length_tokens\": prompt_length,\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"quantization\": QUANTIZATION,\n",
    "        \"precision\": \"INT8\",\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_model_len\": MAX_MODEL_LEN,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"warmup_runs\": NUM_WARMUP_RUNS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"top_logprobs\": TOP_LOGPROBS\n",
    "    },\n",
    "    \"timing\": {\n",
    "        \"model_load_time\": load_time,\n",
    "        \"warmup_times\": warmup_times,\n",
    "        \"per_repetition\": timing_data,\n",
    "        \"statistics\": {\n",
    "            \"mean_time\": float(np.mean(times)),\n",
    "            \"std_time\": float(np.std(times)),\n",
    "            \"mean_tokens_per_sec\": float(np.mean(tps)),\n",
    "            \"std_tokens_per_sec\": float(np.std(tps)),\n",
    "            \"mean_time_per_token_ms\": float(np.mean(tpt) * 1000),\n",
    "            \"std_time_per_token_ms\": float(np.std(tpt) * 1000)\n",
    "        }\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"distributions_exact\": distributions_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact and distributions_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts\n",
    "}\n",
    "\n",
    "output_file = f\"h100_int8_determinism_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b51b50-7115-4cef-a512-2fc9fdb85c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
