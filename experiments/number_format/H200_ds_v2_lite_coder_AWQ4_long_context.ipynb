{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b564e0ff-a2b6-43dd-ad9b-f598ba9edc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "vLLM LOGPROBS EXTRACTION TEST - INT4-AWQ QUANTIZATION\n",
      "================================================================================\n",
      "\n",
      "Found file: Verification-for-International-AI-Governance.pdf\n",
      "Loading 172 pages from PDF...\n",
      "  Processed 10/172 pages\n",
      "  Processed 20/172 pages\n",
      "  Processed 30/172 pages\n",
      "  Processed 40/172 pages\n",
      "  Processed 50/172 pages\n",
      "  Processed 60/172 pages\n",
      "  Processed 70/172 pages\n",
      "  Processed 80/172 pages\n",
      "  Processed 90/172 pages\n",
      "  Processed 100/172 pages\n",
      "  Processed 110/172 pages\n",
      "  Processed 120/172 pages\n",
      "  Processed 130/172 pages\n",
      "  Processed 140/172 pages\n",
      "  Processed 150/172 pages\n",
      "  Processed 160/172 pages\n",
      "  Processed 170/172 pages\n",
      "Loaded 535619 characters from PDF (172 pages)\n",
      "\n",
      "Using standard OpenAI-compatible message format\n",
      "Message content length: 535677 characters\n",
      "Note: Chat template will be applied to convert messages to string prompt\n",
      "\n",
      "Loading tokenizer to validate prompt length...\n",
      "Chat template applied successfully\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (124366 > 16384). Running this sequence through the model will result in indexing errors\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt statistics:\n",
      "  Characters: 535,716\n",
      "  Tokens: 124,366\n",
      "  Max model length: 128,000\n",
      "  Generation tokens: 40\n",
      "  Total required: 124,406\n",
      "\n",
      "✓ Prompt length validation passed\n",
      "  Remaining capacity: 3,594 tokens\n",
      "\n",
      "Configuration:\n",
      "  Model: TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ\n",
      "  Quantization: awq\n",
      "  Tensor parallel: 1\n",
      "  Max model len: 128,000\n",
      "  Max tokens: 40\n",
      "  Temperature: 0.0\n",
      "  Seed: 42\n",
      "  Repetitions: 20\n",
      "\n",
      "Loading INT4-AWQ quantized model...\n",
      "Explicitly setting quantization: awq\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ:\n",
      "- configuration_deepseek.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2998)\u001b[0;0m Downloading 'model-00001-of-00002.safetensors' to '/tmp/hf_cache/hub/models--TechxGenus--DeepSeek-Coder-V2-Lite-Instruct-AWQ/blobs/cb7b6c687bc40160addf9d9b152504ad4a6502662952bc8d6c6b45d2455430a2.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "107dba30a703486db81c5bc10220ab03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2998)\u001b[0;0m Download complete. Moving file to /tmp/hf_cache/hub/models--TechxGenus--DeepSeek-Coder-V2-Lite-Instruct-AWQ/blobs/cb7b6c687bc40160addf9d9b152504ad4a6502662952bc8d6c6b45d2455430a2\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2998)\u001b[0;0m Downloading 'model-00002-of-00002.safetensors' to '/tmp/hf_cache/hub/models--TechxGenus--DeepSeek-Coder-V2-Lite-Instruct-AWQ/blobs/6c1ac251e05b6cc35df953b0ca011c3ca7e8e35e9ad655746de8bf09026b83bc.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635384b5fa964884ba2d2f76dcd0e101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/4.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2998)\u001b[0;0m Download complete. Moving file to /tmp/hf_cache/hub/models--TechxGenus--DeepSeek-Coder-V2-Lite-Instruct-AWQ/blobs/6c1ac251e05b6cc35df953b0ca011c3ca7e8e35e9ad655746de8bf09026b83bc\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2998)\u001b[0;0m Downloading 'model.safetensors.index.json' to '/tmp/hf_cache/hub/models--TechxGenus--DeepSeek-Coder-V2-Lite-Instruct-AWQ/blobs/98a61aa7bac860866a84d88736bdc33d0dc03620.incomplete'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c77d70fda3450dafe4852526918e3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2998)\u001b[0;0m Download complete. Moving file to /tmp/hf_cache/hub/models--TechxGenus--DeepSeek-Coder-V2-Lite-Instruct-AWQ/blobs/98a61aa7bac860866a84d88736bdc33d0dc03620\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe1c7fab1e8425c99fd4ebed5526be6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2998)\u001b[0;0m 2025-11-10 16:59:38,367 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2998)\u001b[0;0m 2025-11-10 16:59:38,634 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "\n",
      "Sampling parameters:\n",
      "  Temperature: 0.0\n",
      "  Max tokens: 40\n",
      "  Seed: 42\n",
      "  Top logprobs: 10\n",
      "\n",
      "Running warmup...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95b6cd1bc2a471299f87fd00be46823",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (124366 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f2a0e28dd3249648429ebfb13d8f095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warmup complete - generated 5 tokens\n",
      "\n",
      "================================================================================\n",
      "RUNNING EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Repetition 1/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66545e02ebe540dbb025483c0b93c485",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a8ddc838994e2f8a1ca1b6442c643d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 2/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dacf18689d04fb794023c9d8432d7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed22f9f3b51c4ae28ccae85476ceba35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 3/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097e17b81d184f18a98f1dfaa03c6b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43558572f87c46e39b8cdb2d395b380d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 4/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ad2e7fd6b84d83b509600be6cebe5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1655d0749114652abc32b0d74355a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 5/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0312922a8cc2478ead7f0cc3a9176a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e24f86ec29634e919c903c5edf599921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 6/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e23faa535c4603b7302a25842e76dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5508ea5c5164cbaafe9b0896f0cf91d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 7/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "905ce183f14449a3bd9c8382a5bf6a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc3cc2c6a341484f9f0a2aa66809de32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 8/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07748665c0b4483daaae4c8de5362423",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aca306c9f0f4c32bd20bfbfab32b444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 9/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "979f50e91fe341049c495275912fdb4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc06dc526dfc422b9e64d8cf8757f6ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 10/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b627ade0d4f74324bf8b1ba2f9143159",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19b83c227e734477ad767717d132bc2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 11/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5164f2146f49b2a798ed590b953db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e195569dad1453293f4b6eab03477da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 12/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c470bfefa040dd8cd95b6873a486f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "348f1ea1cbd845fcaa438aee9de5d3ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 13/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f291cf426b49df95d5897a49788867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9539951a06fa4540b11a342d5e577460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 14/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ade0c55fd104c7383ad3b99371c8504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ada27b901f545ceae0d9c65c744fe48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 15/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4610c2976f6f451f802c0072adb8857b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "667579a6321d4b0ba7809b06c25bb0a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 16/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7e853acd890425cbeb51d24ed7e6cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71e79ca330734e969ec422fb2ba17e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 17/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14e7fe82a7674e379e2b2caf0b130734",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a174cf6a7a448abe907cb222c27371",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 18/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109b7897ce07497f836e0f8a57e29854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc191ff454b040b4a733ebd970a6d14c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 19/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f6c6f2b1364be5b7f0b1bbca6ce00d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4de538c0e6a4ff68ecea15d13642983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "Repetition 20/20...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e094fc431d7453a97a20124ee8f9482",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea3f8b2ba4c476fa3052f99fbc43894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Generated 5 tokens\n",
      "\n",
      "All repetitions complete!\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Checking token sequences...\n",
      "Token sequences identical: True\n",
      "\n",
      "Checking selected token logprobs...\n",
      "Selected token logprobs bit-exact: False\n",
      "\n",
      "Checking full top-k distributions...\n",
      "Top-k distributions bit-exact: False\n",
      "\n",
      "⚠️  Found 95 position mismatches in distributions\n",
      "  First 5: [(1, 0), (1, 1), (1, 2), (1, 3), (1, 4)]\n",
      "\n",
      "L2 distances:\n",
      "  Rep 0 vs Rep 1: L2 = 1.108155e-01\n",
      "  Rep 0 vs Rep 2: L2 = 8.145385e-02\n",
      "  Rep 0 vs Rep 3: L2 = 1.362306e-01\n",
      "  Rep 0 vs Rep 4: L2 = 7.424670e-02\n",
      "  Rep 0 vs Rep 5: L2 = 3.751740e-02\n",
      "  Rep 0 vs Rep 6: L2 = 1.949225e-02\n",
      "  Rep 0 vs Rep 7: L2 = 9.125113e-02\n",
      "  Rep 0 vs Rep 8: L2 = 1.400322e-02\n",
      "  Rep 0 vs Rep 9: L2 = 8.200484e-02\n",
      "  Rep 0 vs Rep 10: L2 = 8.734348e-02\n",
      "  Rep 0 vs Rep 11: L2 = 9.233576e-02\n",
      "  Rep 0 vs Rep 12: L2 = 8.593806e-02\n",
      "  Rep 0 vs Rep 13: L2 = 8.975907e-02\n",
      "  Rep 0 vs Rep 14: L2 = 1.051396e-01\n",
      "  Rep 0 vs Rep 15: L2 = 2.938185e-02\n",
      "  Rep 0 vs Rep 16: L2 = 8.847586e-02\n",
      "  Rep 0 vs Rep 17: L2 = 7.072383e-02\n",
      "  Rep 0 vs Rep 18: L2 = 8.182420e-02\n",
      "  Rep 0 vs Rep 19: L2 = 1.265410e-01\n",
      "\n",
      "Max L2: 1.362306e-01\n",
      "Mean L2: 7.918306e-02\n",
      "\n",
      "Per-token std statistics:\n",
      "  Mean: 1.692877e-02\n",
      "  Max: 3.707217e-02\n",
      "  Median: 7.938867e-03\n",
      "\n",
      "================================================================================\n",
      "VERDICT\n",
      "================================================================================\n",
      "\n",
      "⚠️  TOKENS IDENTICAL, LOGPROBS VARY\n",
      "  - Token sequences: bit-exact\n",
      "  - Logprobs: small numerical variation\n",
      "  - Variation notable (L2=1.36e-01)\n",
      "  → INT4 quantization introduces numerical noise\n",
      "\n",
      "Results saved to: vllm_deepseek_coder_v2_lite_int4_test_20251110_170203.json\n",
      "\n",
      "================================================================================\n",
      "TEST COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM Logprobs Extraction Test - DeepSeek-Coder-V2-Lite-Instruct INT4-AWQ\n",
    "Tests bit-exact reproducibility across multiple runs with INT4 quantization\n",
    "Automatically finds txt/pdf files in current directory\n",
    "Uses standard OpenAI-compatible message format\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# SUPPRESS VERBOSE LOGGING\n",
    "# ============================================================================\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/tmp/hf_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/tmp/hf_cache'\n",
    "\n",
    "# Suppress vLLM verbose output\n",
    "os.environ['VLLM_LOGGING_LEVEL'] = 'WARNING'\n",
    "os.environ['VLLM_CONFIGURE_LOGGING'] = '0'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import logging\n",
    "# Suppress vLLM and related libraries\n",
    "logging.getLogger('vllm').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.engine').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.worker').setLevel(logging.ERROR)\n",
    "logging.getLogger('vllm.executor').setLevel(logging.ERROR)\n",
    "logging.getLogger('transformers').setLevel(logging.ERROR)\n",
    "logging.getLogger('torch').setLevel(logging.ERROR)\n",
    "\n",
    "# Keep download progress - only allow INFO for huggingface_hub downloads\n",
    "logging.getLogger('huggingface_hub').setLevel(logging.INFO)\n",
    "logging.getLogger('huggingface_hub.file_download').setLevel(logging.INFO)\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Model configuration - INT4-AWQ quantized version\n",
    "MODEL_NAME = \"TechxGenus/DeepSeek-Coder-V2-Lite-Instruct-AWQ\"\n",
    "QUANTIZATION = \"awq\"  # Optional: vLLM auto-detects from model config. Set to None for auto-detection.\n",
    "TENSOR_PARALLEL_SIZE = 1\n",
    "MAX_MODEL_LEN = 128000\n",
    "GPU_MEMORY_UTILIZATION = 0.9\n",
    "\n",
    "# Generation configuration\n",
    "MAX_TOKENS = 40\n",
    "NUM_REPETITIONS = 20\n",
    "TEMPERATURE = 0.0  # Greedy decoding\n",
    "SEED = 42\n",
    "TOP_LOGPROBS = 10\n",
    "\n",
    "# Prompt source - finds first txt or pdf in current directory\n",
    "AUTO_FIND_FILE = True  # Set to False to use hardcoded content\n",
    "\n",
    "# User task\n",
    "USER_TASK = \"Please provide a detailed summary of the following text.\"\n",
    "\n",
    "# Hardcoded content (used if AUTO_FIND_FILE=False or no files found)\n",
    "HARDCODED_CONTENT = \"\"\"The development of large language models has fundamentally transformed natural language processing and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated remarkable capabilities across a wide range of tasks, from translation and summarization to question answering and creative writing.\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# FILE LOADING UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def find_prompt_file():\n",
    "    \"\"\"Find first txt or pdf file in current directory\"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # Look for txt files first, then pdf\n",
    "    txt_files = glob.glob(os.path.join(cwd, \"*.txt\"))\n",
    "    pdf_files = glob.glob(os.path.join(cwd, \"*.pdf\"))\n",
    "    \n",
    "    if txt_files:\n",
    "        return txt_files[0]\n",
    "    elif pdf_files:\n",
    "        return pdf_files[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def load_text_from_file(filepath):\n",
    "    \"\"\"Load text from txt or pdf file\"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "    \n",
    "    if filepath.endswith('.txt'):\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "        print(f\"Loaded {len(text)} characters from txt file\")\n",
    "        return text\n",
    "    \n",
    "    elif filepath.endswith('.pdf'):\n",
    "        try:\n",
    "            import PyPDF2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"PyPDF2 required for PDF loading. Install with: pip install PyPDF2\")\n",
    "        \n",
    "        text = []\n",
    "        with open(filepath, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            num_pages = len(pdf_reader.pages)\n",
    "            print(f\"Loading {num_pages} pages from PDF...\")\n",
    "            \n",
    "            for page_num, page in enumerate(pdf_reader.pages, 1):\n",
    "                page_text = page.extract_text()\n",
    "                text.append(page_text)\n",
    "                if page_num % 10 == 0:\n",
    "                    print(f\"  Processed {page_num}/{num_pages} pages\")\n",
    "        \n",
    "        full_text = '\\n'.join(text)\n",
    "        print(f\"Loaded {len(full_text)} characters from PDF ({num_pages} pages)\")\n",
    "        return full_text\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file type: {filepath}. Use .txt or .pdf\")\n",
    "\n",
    "# ============================================================================\n",
    "# PROMPT LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"vLLM LOGPROBS EXTRACTION TEST - INT4-AWQ QUANTIZATION\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Load document content\n",
    "prompt_file = None\n",
    "if AUTO_FIND_FILE:\n",
    "    prompt_file = find_prompt_file()\n",
    "    if prompt_file:\n",
    "        print(f\"Found file: {os.path.basename(prompt_file)}\")\n",
    "        DOCUMENT_CONTENT = load_text_from_file(prompt_file)\n",
    "        print()\n",
    "    else:\n",
    "        print(\"No txt/pdf files found in current directory\")\n",
    "        print(\"Using hardcoded content\")\n",
    "        DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "        print()\n",
    "else:\n",
    "    DOCUMENT_CONTENT = HARDCODED_CONTENT\n",
    "    print(\"Using hardcoded content\")\n",
    "    print()\n",
    "\n",
    "# Prepare messages in standard OpenAI format\n",
    "# vLLM's tokenizer will apply DeepSeek-Coder-V2-Lite-Instruct's chat template\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": f\"{USER_TASK}\\n\\n{DOCUMENT_CONTENT}\"}\n",
    "]\n",
    "\n",
    "print(\"Using standard OpenAI-compatible message format\")\n",
    "print(f\"Message content length: {len(messages[0]['content'])} characters\")\n",
    "print(\"Note: Chat template will be applied to convert messages to string prompt\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# TOKENIZER PRECHECK\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Loading tokenizer to validate prompt length...\")\n",
    "# Use base model tokenizer (AWQ model should have same tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct\",\n",
    "    cache_dir='/tmp/hf_cache',\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Apply chat template to convert messages to string prompt\n",
    "# This is the actual prompt that will be used for generation\n",
    "prompt_text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(\"Chat template applied successfully\")\n",
    "print()\n",
    "\n",
    "# Tokenize prompt\n",
    "prompt_tokens = tokenizer.encode(prompt_text)\n",
    "prompt_length = len(prompt_tokens)\n",
    "\n",
    "print(f\"Prompt statistics:\")\n",
    "print(f\"  Characters: {len(prompt_text):,}\")\n",
    "print(f\"  Tokens: {prompt_length:,}\")\n",
    "print(f\"  Max model length: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Generation tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Total required: {prompt_length + MAX_TOKENS:,}\")\n",
    "print()\n",
    "\n",
    "# Validate length\n",
    "if prompt_length > MAX_MODEL_LEN:\n",
    "    print(f\"❌ ERROR: Prompt is too long!\")\n",
    "    print(f\"  Prompt has {prompt_length:,} tokens\")\n",
    "    print(f\"  Model max is {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Exceeds by {prompt_length - MAX_MODEL_LEN:,} tokens\")\n",
    "    print()\n",
    "    print(\"Solutions:\")\n",
    "    print(f\"  1. Increase MAX_MODEL_LEN to at least {prompt_length + MAX_TOKENS}\")\n",
    "    print(f\"  2. Truncate/reduce the prompt\")\n",
    "    exit(1)\n",
    "\n",
    "if prompt_length + MAX_TOKENS > MAX_MODEL_LEN:\n",
    "    print(f\"⚠️  WARNING: Prompt + generation may exceed context\")\n",
    "    print(f\"  Prompt: {prompt_length:,} tokens\")\n",
    "    print(f\"  Generation: {MAX_TOKENS} tokens\")\n",
    "    print(f\"  Total: {prompt_length + MAX_TOKENS:,} tokens\")\n",
    "    print(f\"  Model max: {MAX_MODEL_LEN:,} tokens\")\n",
    "    print(f\"  Consider increasing MAX_MODEL_LEN to {prompt_length + MAX_TOKENS + 100}\")\n",
    "    print()\n",
    "else:\n",
    "    print(f\"✓ Prompt length validation passed\")\n",
    "    print(f\"  Remaining capacity: {MAX_MODEL_LEN - prompt_length - MAX_TOKENS:,} tokens\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  Quantization: {QUANTIZATION if QUANTIZATION else 'auto-detect'}\")\n",
    "print(f\"  Tensor parallel: {TENSOR_PARALLEL_SIZE}\")\n",
    "print(f\"  Max model len: {MAX_MODEL_LEN:,}\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Repetitions: {NUM_REPETITIONS}\")\n",
    "print()\n",
    "\n",
    "print(\"Loading INT4-AWQ quantized model...\")\n",
    "if QUANTIZATION:\n",
    "    print(f\"Explicitly setting quantization: {QUANTIZATION}\")\n",
    "else:\n",
    "    print(\"Using auto-detection for quantization\")\n",
    "print()\n",
    "\n",
    "# Build LLM kwargs - only include quantization if explicitly set\n",
    "llm_kwargs = {\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"tensor_parallel_size\": TENSOR_PARALLEL_SIZE,\n",
    "    \"max_model_len\": MAX_MODEL_LEN,\n",
    "    \"gpu_memory_utilization\": GPU_MEMORY_UTILIZATION,\n",
    "    \"trust_remote_code\": True,\n",
    "    \"seed\": SEED,\n",
    "    \"enforce_eager\": True,  # Disable cudagraph for determinism\n",
    "    \"enable_prefix_caching\": False  # Disable prefix caching for clean experiment\n",
    "}\n",
    "\n",
    "if QUANTIZATION:\n",
    "    llm_kwargs[\"quantization\"] = QUANTIZATION\n",
    "\n",
    "llm = LLM(**llm_kwargs)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAMPLING CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    seed=SEED,\n",
    "    logprobs=TOP_LOGPROBS,\n",
    "    skip_special_tokens=False\n",
    ")\n",
    "\n",
    "print(\"Sampling parameters:\")\n",
    "print(f\"  Temperature: {TEMPERATURE}\")\n",
    "print(f\"  Max tokens: {MAX_TOKENS}\")\n",
    "print(f\"  Seed: {SEED}\")\n",
    "print(f\"  Top logprobs: {TOP_LOGPROBS}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# WARMUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"Running warmup...\")\n",
    "# vLLM's LLM.generate() expects string prompts, not message dicts\n",
    "# The chat template was already applied above during validation\n",
    "warmup_output = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "print(f\"Warmup complete - generated {len(warmup_output[0].outputs[0].token_ids)} tokens\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RUNNING EXPERIMENT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "results_tokens = []\n",
    "results_logprobs = []\n",
    "results_texts = []\n",
    "results_distributions = []\n",
    "\n",
    "for rep in range(NUM_REPETITIONS):\n",
    "    print(f\"Repetition {rep + 1}/{NUM_REPETITIONS}...\")\n",
    "    \n",
    "    # Use the string prompt (chat template already applied)\n",
    "    outputs = llm.generate(prompt_text, sampling_params=sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract token IDs\n",
    "    token_ids = output.outputs[0].token_ids\n",
    "    results_tokens.append(token_ids)\n",
    "    \n",
    "    # Extract generated text\n",
    "    text = output.outputs[0].text\n",
    "    results_texts.append(text)\n",
    "    \n",
    "    # Extract logprobs for selected tokens\n",
    "    logprobs_data = output.outputs[0].logprobs\n",
    "    selected_logprobs = [lp[token_ids[i]].logprob for i, lp in enumerate(logprobs_data)]\n",
    "    results_logprobs.append(np.array(selected_logprobs))\n",
    "    \n",
    "    # Extract full top-k distributions\n",
    "    rep_distributions = []\n",
    "    for position_logprobs in logprobs_data:\n",
    "        # Get top-k sorted by logprob (descending)\n",
    "        sorted_items = sorted(position_logprobs.items(), \n",
    "                            key=lambda x: x[1].logprob, \n",
    "                            reverse=True)[:TOP_LOGPROBS]\n",
    "        rep_distributions.append([(tok, lp.logprob) for tok, lp in sorted_items])\n",
    "    results_distributions.append(rep_distributions)\n",
    "    \n",
    "    print(f\"  Generated {len(token_ids)} tokens\")\n",
    "\n",
    "print()\n",
    "print(\"All repetitions complete!\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Check token sequence identity\n",
    "print(\"Checking token sequences...\")\n",
    "tokens_identical = all(\n",
    "    results_tokens[0] == results_tokens[i] \n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Token sequences identical: {tokens_identical}\")\n",
    "\n",
    "if not tokens_identical:\n",
    "    print(\"\\n⚠️  Token sequences differ!\")\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        if results_tokens[0] != results_tokens[i]:\n",
    "            diff_positions = [\n",
    "                j for j in range(min(len(results_tokens[0]), len(results_tokens[i])))\n",
    "                if results_tokens[0][j] != results_tokens[i][j]\n",
    "            ]\n",
    "            print(f\"  Rep 0 vs Rep {i}: {len(diff_positions)} positions differ\")\n",
    "            if diff_positions:\n",
    "                print(f\"    First difference at position {diff_positions[0]}\")\n",
    "\n",
    "# Check logprobs for selected tokens\n",
    "print(\"\\nChecking selected token logprobs...\")\n",
    "first_logprobs = results_logprobs[0]\n",
    "logprobs_exact = all(\n",
    "    np.allclose(first_logprobs, results_logprobs[i], rtol=0, atol=1e-10)\n",
    "    for i in range(1, NUM_REPETITIONS)\n",
    ")\n",
    "print(f\"Selected token logprobs bit-exact: {logprobs_exact}\")\n",
    "\n",
    "# Check top-k distributions\n",
    "print(\"\\nChecking full top-k distributions...\")\n",
    "distributions_exact = True\n",
    "distribution_mismatches = []\n",
    "\n",
    "first_dist = results_distributions[0]\n",
    "for rep_idx in range(1, NUM_REPETITIONS):\n",
    "    for pos_idx in range(len(first_dist)):\n",
    "        dist_a = first_dist[pos_idx]\n",
    "        dist_b = results_distributions[rep_idx][pos_idx]\n",
    "        \n",
    "        # Check if token IDs match in same order\n",
    "        tokens_match = [t[0] for t in dist_a] == [t[0] for t in dist_b]\n",
    "        \n",
    "        # Check if logprobs are bit-exact\n",
    "        if tokens_match:\n",
    "            logprobs_match = all(\n",
    "                abs(dist_a[i][1] - dist_b[i][1]) < 1e-10 \n",
    "                for i in range(len(dist_a))\n",
    "            )\n",
    "            if not logprobs_match:\n",
    "                distributions_exact = False\n",
    "                distribution_mismatches.append((rep_idx, pos_idx))\n",
    "        else:\n",
    "            distributions_exact = False\n",
    "            distribution_mismatches.append((rep_idx, pos_idx))\n",
    "\n",
    "print(f\"Top-k distributions bit-exact: {distributions_exact}\")\n",
    "\n",
    "if not distributions_exact:\n",
    "    print(f\"\\n⚠️  Found {len(distribution_mismatches)} position mismatches in distributions\")\n",
    "    if len(distribution_mismatches) <= 5:\n",
    "        for rep_idx, pos_idx in distribution_mismatches:\n",
    "            print(f\"  Rep 0 vs Rep {rep_idx}, position {pos_idx}\")\n",
    "    else:\n",
    "        print(f\"  First 5: {distribution_mismatches[:5]}\")\n",
    "\n",
    "if not logprobs_exact:\n",
    "    print(\"\\nL2 distances:\")\n",
    "    l2_distances = []\n",
    "    for i in range(1, NUM_REPETITIONS):\n",
    "        l2 = np.linalg.norm(first_logprobs - results_logprobs[i])\n",
    "        l2_distances.append(l2)\n",
    "        print(f\"  Rep 0 vs Rep {i}: L2 = {l2:.6e}\")\n",
    "    \n",
    "    print(f\"\\nMax L2: {max(l2_distances):.6e}\")\n",
    "    print(f\"Mean L2: {np.mean(l2_distances):.6e}\")\n",
    "    \n",
    "    # Element-wise statistics\n",
    "    all_logprobs = np.array(results_logprobs)\n",
    "    std_per_token = all_logprobs.std(axis=0)\n",
    "    print(f\"\\nPer-token std statistics:\")\n",
    "    print(f\"  Mean: {std_per_token.mean():.6e}\")\n",
    "    print(f\"  Max: {std_per_token.max():.6e}\")\n",
    "    print(f\"  Median: {np.median(std_per_token):.6e}\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# VERDICT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "if tokens_identical and logprobs_exact and distributions_exact:\n",
    "    print(\"✓ PERFECT REPRODUCIBILITY WITH INT4-AWQ\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Full top-k distributions: bit-exact\")\n",
    "    print(\"  - INT4 quantization does NOT break reproducibility\")\n",
    "elif tokens_identical and logprobs_exact and not distributions_exact:\n",
    "    print(\"⚠️  SELECTED TOKENS EXACT, DISTRIBUTIONS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Selected token logprobs: bit-exact\")\n",
    "    print(\"  - Top-k distributions: numerical variation detected\")\n",
    "    print(\"  → May indicate computational instability in non-selected paths\")\n",
    "    print(\"  → INT4 quantization may introduce distribution noise\")\n",
    "elif tokens_identical and not logprobs_exact:\n",
    "    print(\"⚠️  TOKENS IDENTICAL, LOGPROBS VARY\")\n",
    "    print(\"  - Token sequences: bit-exact\")\n",
    "    print(\"  - Logprobs: small numerical variation\")\n",
    "    max_l2 = max(l2_distances) if not logprobs_exact else 0.0\n",
    "    if max_l2 < 1e-6:\n",
    "        print(f\"  - Variation very small (L2={max_l2:.2e})\")\n",
    "        print(\"  → Likely acceptable for forensics\")\n",
    "    else:\n",
    "        print(f\"  - Variation notable (L2={max_l2:.2e})\")\n",
    "        print(\"  → INT4 quantization introduces numerical noise\")\n",
    "else:\n",
    "    print(\"❌ TOKEN SEQUENCES DIFFER\")\n",
    "    print(\"  - This should NOT happen with temperature=0\")\n",
    "    print(\"  → INT4 quantization breaks determinism\")\n",
    "    print(\"  → This is the culprit for Kimi K2 non-reproducibility!\")\n",
    "\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output_data = {\n",
    "    \"experiment\": \"deepseek_coder_v2_lite_instruct_int4_awq_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"prompt_source\": \"file\" if prompt_file else \"hardcoded\",\n",
    "    \"prompt_file\": os.path.basename(prompt_file) if prompt_file else None,\n",
    "    \"prompt_text\": prompt_text,  # Save actual prompt for reproducibility\n",
    "    \"prompt_length_chars\": len(prompt_text),\n",
    "    \"prompt_length_tokens\": prompt_length,\n",
    "    \"message_format\": \"openai_compatible\",\n",
    "    \"config\": {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"quantization\": QUANTIZATION if QUANTIZATION else \"auto-detected\",\n",
    "        \"tensor_parallel\": TENSOR_PARALLEL_SIZE,\n",
    "        \"max_model_len\": MAX_MODEL_LEN,\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"repetitions\": NUM_REPETITIONS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"seed\": SEED,\n",
    "        \"warmup_enabled\": True,\n",
    "        \"prefix_caching_disabled\": True,\n",
    "        \"enforce_eager\": True,\n",
    "        \"top_logprobs\": TOP_LOGPROBS\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"tokens_identical\": tokens_identical,\n",
    "        \"logprobs_exact\": logprobs_exact,\n",
    "        \"distributions_exact\": distributions_exact,\n",
    "        \"perfect_reproducibility\": tokens_identical and logprobs_exact and distributions_exact\n",
    "    },\n",
    "    \"token_sequences\": results_tokens,\n",
    "    \"logprobs_vectors\": [lp.tolist() for lp in results_logprobs],\n",
    "    \"generated_texts\": results_texts,\n",
    "    \"top_k_distributions\": [\n",
    "        [[(int(tok), float(prob)) for tok, prob in dist] for dist in rep_dists]\n",
    "        for rep_dists in results_distributions\n",
    "    ]\n",
    "}\n",
    "\n",
    "output_file = f\"vllm_deepseek_coder_v2_lite_int4_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")\n",
    "print()\n",
    "print(\"=\" * 80)\n",
    "print(\"TEST COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62eb8acd-dc7d-4e1d-8005-196e36b99c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
