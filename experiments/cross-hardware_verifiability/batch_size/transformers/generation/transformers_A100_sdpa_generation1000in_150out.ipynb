{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98fb9e08-8680-473f-b415-54312ed3069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-HARDWARE BATCH SIZE DETECTABILITY - GENERATION (reference)\n",
      "================================================================================\n",
      "\n",
      "System: c6c6242ae438\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "Attention: sdpa\n",
      "Layers: [28]\n",
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n",
      "Total source tokens: 120215\n",
      "Prompt structure: 33 prefix + 1000 snippet + 34 suffix = 1067 tokens\n",
      "All 51 prompts: 1067 tokens each\n",
      "\n",
      "Sample prompt ending:\n",
      "' . . . . . .\"\\n\\nBased on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\\n<|im_start|>assistant\\n'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74fc5eb835a74fe787051aea5ba01231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_0\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 1067 tokens\n",
      "\n",
      "  bs=1:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=2:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=3:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=4:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=5:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=8:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=9:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=16:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=17:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_1\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 1067 tokens\n",
      "\n",
      "  bs=1:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=2:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=3:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=4:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=5:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=8:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=9:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=16:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=17:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_2\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 1067 tokens\n",
      "\n",
      "  bs=1:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=2:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=3:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=4:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=5:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=8:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=9:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=16:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "  bs=17:      Prompt: 1067 tokens → Final: 1217 tokens (150 generated)\n",
      "\n",
      "--- Token consistency for ref_0 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 1895, 476, 264, 4158, 5567, 10735, 389, 279, 8544, 315, 6489, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 19473, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 3851, 323, 24124, 95518, 576, 2197, 374, 24849, 330, 62339, 369, 7179, 15235, 80589, 1, 323, 11469, 5248, 12014, 504, 5257, 14250, 323, 3412, 14336, 11, 18860, 264, 39706, 3412, 5041, 382, 17, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 374, 32930, 448, 458, 8115, 11, 8794, 11, 323, 14158, 429, 4263, 279, 2739, 12919, 2897, 315, 6489, 15235, 19473, 11, 279, 11513, 304, 68863, 54480, 5601, 323, 7375, 11, 323, 4948, 2606, 323, 6559, 32483, 13, 1096, 5944, 374, 14260, 315, 264, 3412, 1895, 476, 4158, 5567, 382, 18, 13, 3070, 13019, 33364, 95518, 576, 2197, 1594, 2342, 1119, 279, 68443]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research report or a white paper focused on the topic of international governance and verification of artificial intelligence (AI) agreements. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Title and Authors**: The document is titled \"Verification for International AI Governance\" and lists multiple authors from various academic and research institutions, indicating a collaborative research effort.\\n\\n2. **Content and Structure**: The document is structured with an abstract, contents, and sections that discuss the verifiability of international AI agreements, the challenges in verifying computational rules and tools, and political options and tradeoffs. This structure is typical of a research report or white paper.\\n\\n3. **Subject Matter**: The document delves into the feasibility'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 1895, 476, 4158, 5567, 10735, 389, 279, 8544, 315, 6489, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 19473, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 3851, 323, 24124, 95518, 576, 2197, 374, 24849, 330, 62339, 369, 7179, 15235, 80589, 1, 323, 11469, 5248, 12014, 504, 5257, 14250, 323, 3412, 14336, 11, 18860, 264, 39706, 3412, 5041, 382, 17, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 374, 32930, 448, 458, 8115, 11, 8794, 11, 323, 14158, 429, 4263, 279, 2739, 12919, 2897, 315, 6489, 15235, 19473, 11, 279, 11513, 304, 22901, 11, 323, 4650, 19827, 311, 22279, 1493, 11513, 13, 1096, 5944, 374, 14260, 315, 3412, 6682, 476, 4158, 15689, 382, 18, 13, 3070, 13019, 33364, 95518, 576, 2197, 1594, 2342, 1119, 279, 68443, 315, 6489, 19473, 311, 10091]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research report or white paper focused on the topic of international governance and verification of artificial intelligence (AI) agreements. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Title and Authors**: The document is titled \"Verification for International AI Governance\" and lists multiple authors from various academic and research institutions, indicating a collaborative research effort.\\n\\n2. **Content and Structure**: The document is structured with an abstract, contents, and sections that discuss the verifiability of international AI agreements, the challenges in verification, and potential approaches to overcome these challenges. This structure is typical of research reports or white papers.\\n\\n3. **Subject Matter**: The document delves into the feasibility of international agreements to manage'\n",
      "    ✗ DIFFERENT\n",
      "  bs=3:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 1895, 476, 4158, 5567, 10735, 389, 279, 8544, 315, 6489, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 19473, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 3851, 323, 24124, 95518, 576, 2197, 374, 24849, 330, 62339, 369, 7179, 15235, 80589, 1, 323, 11469, 5248, 12014, 504, 5257, 14250, 323, 3412, 14336, 11, 18860, 264, 39706, 3412, 5041, 382, 17, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 374, 32930, 448, 458, 8115, 11, 8794, 11, 323, 14158, 429, 4263, 279, 2739, 12919, 2897, 315, 6489, 15235, 19473, 11, 279, 11513, 304, 22901, 11, 323, 4650, 19827, 311, 22279, 1493, 11513, 13, 1096, 5944, 374, 14260, 315, 3412, 6682, 476, 4158, 15689, 382, 18, 13, 3070, 13019, 33364, 95518, 576, 2197, 1594, 2342, 1119, 279, 68443, 315, 6489, 19473, 311, 10091]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research report or white paper focused on the topic of international governance and verification of artificial intelligence (AI) agreements. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Title and Authors**: The document is titled \"Verification for International AI Governance\" and lists multiple authors from various academic and research institutions, indicating a collaborative research effort.\\n\\n2. **Content and Structure**: The document is structured with an abstract, contents, and sections that discuss the verifiability of international AI agreements, the challenges in verification, and potential approaches to overcome these challenges. This structure is typical of research reports or white papers.\\n\\n3. **Subject Matter**: The document delves into the feasibility of international agreements to manage'\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 1895, 476, 264, 4158, 5567, 10735, 389, 279, 8544, 315, 6489, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 19473, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 3851, 323, 24124, 95518, 576, 2197, 374, 24849, 330, 62339, 369, 7179, 15235, 80589, 1, 323, 11469, 5248, 12014, 504, 5257, 14250, 323, 3412, 14336, 11, 18860, 264, 39706, 3412, 5041, 382, 17, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 374, 32930, 448, 458, 8115, 11, 8794, 11, 323, 14158, 429, 4263, 279, 2739, 12919, 2897, 315, 6489, 15235, 19473, 11, 279, 11513, 304, 22901, 11, 323, 4650, 19827, 311, 22279, 1493, 11513, 13, 1096, 5944, 374, 14260, 315, 3412, 6682, 476, 4158, 15689, 382, 18, 13, 3070, 13019, 33364, 95518, 576, 2197, 1594, 2342, 1119, 279, 68443, 315, 6489, 19473, 311]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research report or a white paper focused on the topic of international governance and verification of artificial intelligence (AI) agreements. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Title and Authors**: The document is titled \"Verification for International AI Governance\" and lists multiple authors from various academic and research institutions, indicating a collaborative research effort.\\n\\n2. **Content and Structure**: The document is structured with an abstract, contents, and sections that discuss the verifiability of international AI agreements, the challenges in verification, and potential approaches to overcome these challenges. This structure is typical of research reports or white papers.\\n\\n3. **Subject Matter**: The document delves into the feasibility of international agreements to'\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 1895, 476, 264, 4158, 5567, 10735, 389, 279, 8544, 315, 6489, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 19473, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 3851, 323, 24124, 95518, 576, 2197, 374, 24849, 330, 62339, 369, 7179, 15235, 80589, 1, 323, 11469, 5248, 12014, 504, 5257, 14250, 323, 3412, 14336, 11, 18860, 264, 39706, 3412, 5041, 382, 17, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 374, 32930, 448, 458, 8115, 11, 8794, 11, 323, 14158, 429, 4263, 279, 2739, 12919, 2897, 315, 6489, 15235, 19473, 11, 279, 11513, 304, 22901, 11, 323, 4650, 19827, 311, 22279, 1493, 11513, 13, 1096, 5944, 374, 14260, 315, 3412, 6682, 476, 4158, 15689, 382, 18, 13, 3070, 13019, 33364, 95518, 576, 2197, 1594, 2342, 1119, 279, 68443, 315, 6489, 19473, 311]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research report or a white paper focused on the topic of international governance and verification of artificial intelligence (AI) agreements. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Title and Authors**: The document is titled \"Verification for International AI Governance\" and lists multiple authors from various academic and research institutions, indicating a collaborative research effort.\\n\\n2. **Content and Structure**: The document is structured with an abstract, contents, and sections that discuss the verifiability of international AI agreements, the challenges in verification, and potential approaches to overcome these challenges. This structure is typical of research reports or white papers.\\n\\n3. **Subject Matter**: The document delves into the feasibility of international agreements to'\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 1895, 476, 4158, 5567, 10735, 389, 279, 8544, 315, 6489, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 19473, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 3851, 323, 24124, 95518, 576, 2197, 374, 24849, 330, 62339, 369, 7179, 15235, 80589, 1, 323, 11469, 5248, 12014, 504, 5257, 14250, 323, 3412, 14336, 11, 18860, 264, 39706, 3412, 5041, 382, 17, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 374, 32930, 448, 458, 8115, 11, 8794, 11, 323, 14158, 429, 4263, 279, 2739, 12919, 2897, 315, 6489, 15235, 19473, 11, 279, 11513, 304, 22901, 11, 323, 4650, 19827, 311, 22279, 1493, 11513, 13, 1096, 5944, 374, 14260, 315, 3412, 6682, 476, 4158, 15689, 382, 18, 13, 3070, 13019, 33364, 95518, 576, 2197, 1594, 2342, 1119, 279, 68443, 315, 6489, 19473, 311, 10091]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research report or white paper focused on the topic of international governance and verification of artificial intelligence (AI) agreements. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Title and Authors**: The document is titled \"Verification for International AI Governance\" and lists multiple authors from various academic and research institutions, indicating a collaborative research effort.\\n\\n2. **Content and Structure**: The document is structured with an abstract, contents, and sections that discuss the verifiability of international AI agreements, the challenges in verification, and potential approaches to overcome these challenges. This structure is typical of research reports or white papers.\\n\\n3. **Subject Matter**: The document delves into the feasibility of international agreements to manage'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 1895, 476, 4158, 5567, 10735, 389, 279, 8544, 315, 6489, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 19473, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 3851, 323, 24124, 95518, 576, 2197, 374, 24849, 330, 62339, 369, 7179, 15235, 80589, 1, 323, 11469, 5248, 12014, 504, 5257, 14250, 323, 3412, 14336, 11, 18860, 264, 39706, 3412, 5041, 382, 17, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 374, 32930, 448, 458, 8115, 11, 8794, 11, 323, 14158, 429, 4263, 279, 2739, 12919, 2897, 315, 6489, 15235, 19473, 11, 279, 11513, 304, 22901, 11, 323, 4650, 19827, 311, 22279, 1493, 11513, 13, 1096, 5944, 374, 14260, 315, 3412, 6682, 476, 4158, 15689, 382, 18, 13, 3070, 13019, 33364, 95518, 576, 2197, 1594, 2342, 1119, 279, 68443, 315, 6489, 19473, 311, 10091]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research report or white paper focused on the topic of international governance and verification of artificial intelligence (AI) agreements. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Title and Authors**: The document is titled \"Verification for International AI Governance\" and lists multiple authors from various academic and research institutions, indicating a collaborative research effort.\\n\\n2. **Content and Structure**: The document is structured with an abstract, contents, and sections that discuss the verifiability of international AI agreements, the challenges in verification, and potential approaches to overcome these challenges. This structure is typical of research reports or white papers.\\n\\n3. **Subject Matter**: The document delves into the feasibility of international agreements to manage'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 1895, 476, 4158, 5567, 10735, 389, 279, 8544, 315, 6489, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 19473, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 3851, 323, 24124, 95518, 576, 2197, 374, 24849, 330, 62339, 369, 7179, 15235, 80589, 1, 323, 11469, 5248, 12014, 504, 5257, 14250, 323, 3412, 14336, 11, 18860, 264, 39706, 3412, 5041, 382, 17, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 374, 32930, 448, 458, 8115, 11, 8794, 11, 323, 14158, 429, 4263, 279, 2739, 12919, 2897, 315, 6489, 15235, 19473, 11, 279, 11513, 304, 68863, 54480, 5601, 323, 7375, 11, 323, 4948, 2606, 323, 6559, 32483, 13, 1096, 5944, 374, 14260, 315, 264, 3412, 1895, 476, 4158, 5567, 382, 18, 13, 3070, 13019, 33364, 95518, 576, 2197, 1594, 2342, 1119, 279, 68443, 315]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research report or white paper focused on the topic of international governance and verification of artificial intelligence (AI) agreements. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Title and Authors**: The document is titled \"Verification for International AI Governance\" and lists multiple authors from various academic and research institutions, indicating a collaborative research effort.\\n\\n2. **Content and Structure**: The document is structured with an abstract, contents, and sections that discuss the verifiability of international AI agreements, the challenges in verifying computational rules and tools, and political options and tradeoffs. This structure is typical of a research report or white paper.\\n\\n3. **Subject Matter**: The document delves into the feasibility of'\n",
      "    ✗ DIFFERENT\n",
      "  bs=17:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 1895, 476, 4158, 5567, 10735, 389, 279, 8544, 315, 6489, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 19473, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 3851, 323, 24124, 95518, 576, 2197, 374, 24849, 330, 62339, 369, 7179, 15235, 80589, 1, 323, 11469, 5248, 12014, 504, 5257, 14250, 323, 3412, 14336, 11, 18860, 264, 39706, 3412, 5041, 382, 17, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 374, 32930, 448, 458, 8115, 11, 8794, 11, 323, 14158, 429, 4263, 279, 2739, 12919, 2897, 315, 6489, 15235, 19473, 11, 279, 11513, 304, 22901, 11, 323, 4650, 19827, 311, 22279, 1493, 11513, 13, 1096, 5944, 374, 14260, 315, 264, 3412, 1895, 476, 4158, 5567, 382, 18, 13, 3070, 13019, 33364, 95518, 576, 2197, 1594, 2342, 1119, 279, 68443, 315, 6489, 19473, 311]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research report or white paper focused on the topic of international governance and verification of artificial intelligence (AI) agreements. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Title and Authors**: The document is titled \"Verification for International AI Governance\" and lists multiple authors from various academic and research institutions, indicating a collaborative research effort.\\n\\n2. **Content and Structure**: The document is structured with an abstract, contents, and sections that discuss the verifiability of international AI agreements, the challenges in verification, and potential approaches to overcome these challenges. This structure is typical of a research report or white paper.\\n\\n3. **Subject Matter**: The document delves into the feasibility of international agreements to'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "--- Token consistency for ref_1 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 62581, 476, 14250, 16599, 11, 4363, 264, 3412, 5567, 476, 264, 12453, 304, 264, 2311, 11, 10735, 389, 279, 18580, 24154, 315, 20443, 11229, 320, 15469, 8, 304, 279, 2266, 315, 6489, 4300, 323, 4763, 13, 576, 32711, 369, 419, 16688, 374, 438, 11017, 1447, 16, 13, 3070, 34, 7556, 11913, 323, 15042, 95518, 576, 1467, 5646, 51846, 304, 264, 15908, 14250, 1707, 11, 1667, 5109, 323, 56516, 3151, 6816, 323, 8173, 13, 1096, 374, 14260, 315, 14250, 4378, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2213, 34334, 279, 18580, 10707, 304, 279, 4231, 315, 15235, 11, 279, 4650, 15276, 323, 10488, 5815, 448, 279, 6277, 990, 315, 15235, 11, 323, 279, 26829, 24154, 369, 6489, 4300, 323, 4763, 13, 1084, 28627, 389, 13347, 1741, 438, 279, 16775, 57698, 8172, 11, 279, 4763, 54767, 11, 11249]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a scholarly or academic publication, likely a research paper or a chapter in a book, focused on the strategic implications of artificial intelligence (AI) in the context of international relations and security. The reasoning for this conclusion is as follows:\\n\\n1. **Citation Style and Format**: The text includes citations in a formal academic style, using numbers and referencing specific pages and sources. This is typical of academic writing.\\n\\n2. **Subject Matter**: The content discusses the strategic competition in the age of AI, the potential risks and opportunities associated with the military use of AI, and the broader implications for international relations and security. It touches on topics such as the offense-defense balance, the security dilemma, nuclear'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 62581, 476, 14250, 16599, 11, 4363, 264, 3412, 5567, 476, 264, 12453, 304, 264, 2311, 13, 576, 32711, 369, 419, 16688, 374, 438, 11017, 1447, 16, 13, 3070, 34, 7556, 11913, 95518, 576, 1467, 5646, 11682, 51846, 448, 2150, 5109, 323, 3151, 15057, 311, 12014, 323, 4278, 11, 892, 374, 14260, 315, 14250, 4378, 13, 1752, 3110, 11, 51846, 311, 330, 79046, 11, 220, 17, 15, 17, 19, 1335, 330, 28418, 30497, 9965, 10294, 220, 23, 22, 11, 902, 13, 220, 16, 320, 16, 24, 24, 18, 35393, 323, 330, 785, 27745, 315, 58585, 320, 26490, 21601, 3822, 8445, 11, 220, 16, 24, 21, 15, 9940, 525, 23126, 304, 264, 1616, 429, 374, 4185, 304, 14250, 15689, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2213, 34334, 18580, 10707, 304, 279, 2266, 315, 20443, 11229, 320, 15469]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a scholarly or academic publication, likely a research paper or a chapter in a book. The reasoning for this conclusion is as follows:\\n\\n1. **Citation Style**: The text includes detailed citations with page numbers and specific references to authors and works, which is typical of academic writing. For example, citations to \"RAND, 2024,\" \"American Political Science Review 87, no. 1 (1993),\" and \"The Strategy of Conflict (Harvard University Press, 1960)\" are formatted in a way that is common in academic papers.\\n\\n2. **Subject Matter**: The content discusses strategic competition in the context of artificial intelligence (AI'\n",
      "    ✗ DIFFERENT\n",
      "  bs=3:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 62581, 476, 14250, 16599, 11, 4363, 264, 3412, 5567, 476, 264, 12453, 304, 264, 2311, 11, 10735, 389, 279, 18580, 24154, 315, 20443, 11229, 320, 15469, 8, 304, 279, 2266, 315, 6489, 4300, 323, 4763, 13, 576, 32711, 369, 419, 16688, 374, 438, 11017, 1447, 16, 13, 3070, 34, 7556, 11913, 323, 15042, 95518, 576, 1467, 5646, 51846, 304, 264, 15908, 14250, 1707, 11, 1667, 5109, 323, 15057, 311, 3151, 6816, 476, 2860, 8173, 13, 1096, 374, 14260, 315, 14250, 4378, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2213, 34334, 279, 18580, 10707, 304, 279, 4231, 315, 15235, 11, 279, 4650, 15276, 323, 10488, 5815, 448, 279, 6277, 990, 315, 15235, 11, 323, 279, 24154, 369, 6489, 4300, 323, 4763, 13, 1084, 28627, 389, 13347, 1741, 438, 279, 16775, 57698, 8172, 11, 279, 4763, 54767, 11]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a scholarly or academic publication, likely a research paper or a chapter in a book, focused on the strategic implications of artificial intelligence (AI) in the context of international relations and security. The reasoning for this conclusion is as follows:\\n\\n1. **Citation Style and Format**: The text includes citations in a formal academic style, using numbers and references to specific pages or online sources. This is typical of academic writing.\\n\\n2. **Subject Matter**: The content discusses the strategic competition in the age of AI, the potential risks and opportunities associated with the military use of AI, and the implications for international relations and security. It touches on topics such as the offense-defense balance, the security dilemma,'\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 62581, 476, 14250, 16599, 11, 4363, 264, 3412, 5567, 476, 264, 12453, 304, 264, 2311, 11, 10735, 389, 279, 18580, 24154, 315, 20443, 11229, 320, 15469, 8, 304, 279, 2266, 315, 6489, 4300, 323, 4763, 13, 576, 32711, 369, 419, 16688, 374, 438, 11017, 1447, 16, 13, 3070, 34, 7556, 11913, 323, 15042, 95518, 576, 1467, 5646, 51846, 304, 264, 15908, 14250, 1707, 11, 1667, 5109, 323, 15057, 311, 3151, 6816, 476, 2860, 8173, 13, 1096, 374, 14260, 315, 14250, 4378, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2213, 34334, 279, 18580, 10707, 304, 279, 4231, 315, 15235, 11, 21080, 389, 279, 15276, 323, 10488, 5815, 448, 279, 6277, 990, 315, 15235, 13, 1084, 1083, 28627, 389, 13656, 323, 31787, 38455, 389, 3170, 5302, 2578, 728, 311, 4116, 11, 279, 24154, 315, 15235, 389, 279, 16775]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a scholarly or academic publication, likely a research paper or a chapter in a book, focused on the strategic implications of artificial intelligence (AI) in the context of international relations and security. The reasoning for this conclusion is as follows:\\n\\n1. **Citation Style and Format**: The text includes citations in a formal academic style, using numbers and references to specific pages or online sources. This is typical of academic writing.\\n\\n2. **Subject Matter**: The content discusses the strategic competition in the age of AI, focusing on the risks and opportunities associated with the military use of AI. It also touches on historical and theoretical perspectives on why states might go to war, the implications of AI on the offense'\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 62581, 476, 14250, 16599, 11, 4363, 264, 3412, 5567, 476, 264, 12453, 304, 264, 2311, 11, 10735, 389, 279, 18580, 24154, 315, 20443, 11229, 320, 15469, 8, 304, 279, 2266, 315, 6489, 4300, 323, 4763, 13, 576, 32711, 369, 419, 16688, 374, 438, 11017, 1447, 16, 13, 3070, 34, 7556, 11913, 323, 15042, 95518, 576, 1467, 5646, 51846, 304, 264, 15908, 14250, 1707, 11, 1667, 5109, 323, 15057, 311, 3151, 6816, 476, 2860, 8173, 13, 1096, 374, 14260, 315, 14250, 4378, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2213, 34334, 279, 18580, 10707, 304, 279, 4231, 315, 15235, 11, 21080, 389, 279, 15276, 323, 10488, 5815, 448, 279, 6277, 990, 315, 15235, 13, 1084, 1083, 28627, 389, 13656, 323, 31787, 38455, 389, 3170, 5302, 2578, 728, 311, 4116, 11, 279, 24154, 315, 15235, 389, 279, 16775]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a scholarly or academic publication, likely a research paper or a chapter in a book, focused on the strategic implications of artificial intelligence (AI) in the context of international relations and security. The reasoning for this conclusion is as follows:\\n\\n1. **Citation Style and Format**: The text includes citations in a formal academic style, using numbers and references to specific pages or online sources. This is typical of academic writing.\\n\\n2. **Subject Matter**: The content discusses the strategic competition in the age of AI, focusing on the risks and opportunities associated with the military use of AI. It also touches on historical and theoretical perspectives on why states might go to war, the implications of AI on the offense'\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 62581, 476, 14250, 16599, 11, 4363, 264, 3412, 5567, 476, 264, 12453, 304, 264, 2311, 13, 576, 32711, 369, 419, 16688, 374, 438, 11017, 1447, 16, 13, 3070, 34, 7556, 11913, 95518, 576, 1467, 5646, 11682, 51846, 448, 2150, 5109, 323, 3151, 15057, 311, 12014, 323, 4278, 11, 892, 374, 14260, 315, 14250, 4378, 13, 1752, 3110, 11, 51846, 311, 330, 79046, 11, 220, 17, 15, 17, 19, 1335, 330, 28418, 30497, 9965, 10294, 220, 23, 22, 11, 902, 13, 220, 16, 320, 16, 24, 24, 18, 35393, 323, 330, 785, 27745, 315, 58585, 320, 26490, 21601, 3822, 8445, 11, 220, 16, 24, 21, 15, 9940, 525, 23126, 304, 264, 1616, 429, 374, 4185, 304, 14250, 15689, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2213, 34334, 18580, 10707, 304, 279, 2266, 315, 20443, 11229, 320, 15469]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a scholarly or academic publication, likely a research paper or a chapter in a book. The reasoning for this conclusion is as follows:\\n\\n1. **Citation Style**: The text includes detailed citations with page numbers and specific references to authors and works, which is typical of academic writing. For example, citations to \"RAND, 2024,\" \"American Political Science Review 87, no. 1 (1993),\" and \"The Strategy of Conflict (Harvard University Press, 1960)\" are formatted in a way that is common in academic papers.\\n\\n2. **Subject Matter**: The content discusses strategic competition in the context of artificial intelligence (AI'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 62581, 476, 14250, 16599, 11, 4363, 264, 3412, 5567, 476, 264, 12453, 304, 264, 2311, 13, 576, 32711, 369, 419, 16688, 374, 438, 11017, 1447, 16, 13, 3070, 34, 7556, 11913, 95518, 576, 1467, 5646, 11682, 51846, 448, 2150, 5109, 323, 3151, 15057, 311, 12014, 323, 4278, 11, 892, 374, 14260, 315, 14250, 4378, 13, 1752, 3110, 11, 51846, 311, 330, 79046, 11, 220, 17, 15, 17, 19, 1335, 330, 28418, 30497, 9965, 10294, 220, 23, 22, 11, 902, 13, 220, 16, 320, 16, 24, 24, 18, 35393, 323, 330, 785, 27745, 315, 58585, 320, 26490, 21601, 3822, 8445, 11, 220, 16, 24, 21, 15, 9940, 525, 23126, 304, 264, 1616, 429, 374, 4185, 304, 14250, 15689, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2213, 34334, 18580, 10707, 304, 279, 2266, 315, 20443, 11229, 320, 15469]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a scholarly or academic publication, likely a research paper or a chapter in a book. The reasoning for this conclusion is as follows:\\n\\n1. **Citation Style**: The text includes detailed citations with page numbers and specific references to authors and works, which is typical of academic writing. For example, citations to \"RAND, 2024,\" \"American Political Science Review 87, no. 1 (1993),\" and \"The Strategy of Conflict (Harvard University Press, 1960)\" are formatted in a way that is common in academic papers.\\n\\n2. **Subject Matter**: The content discusses strategic competition in the context of artificial intelligence (AI'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 62581, 476, 14250, 16599, 11, 4363, 264, 3412, 5567, 476, 264, 12453, 304, 264, 2311, 13, 576, 32711, 369, 419, 16688, 374, 438, 11017, 1447, 16, 13, 3070, 34, 7556, 11913, 95518, 576, 1467, 5646, 11682, 51846, 448, 2150, 5109, 323, 3151, 15057, 311, 12014, 323, 4278, 11, 892, 374, 14260, 315, 14250, 4378, 13, 1752, 3110, 11, 51846, 311, 330, 79046, 11, 220, 17, 15, 17, 19, 1335, 330, 28418, 30497, 9965, 10294, 220, 23, 22, 11, 902, 13, 220, 16, 320, 16, 24, 24, 18, 35393, 323, 330, 785, 27745, 315, 58585, 320, 26490, 21601, 3822, 8445, 11, 220, 16, 24, 21, 15, 9940, 525, 23126, 304, 264, 1616, 429, 374, 4185, 304, 14250, 15689, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2213, 34334, 18580, 10707, 304, 279, 2266, 315, 20443, 11229, 320, 15469]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a scholarly or academic publication, likely a research paper or a chapter in a book. The reasoning for this conclusion is as follows:\\n\\n1. **Citation Style**: The text includes detailed citations with page numbers and specific references to authors and works, which is typical of academic writing. For example, citations to \"RAND, 2024,\" \"American Political Science Review 87, no. 1 (1993),\" and \"The Strategy of Conflict (Harvard University Press, 1960)\" are formatted in a way that is common in academic papers.\\n\\n2. **Subject Matter**: The content discusses strategic competition in the context of artificial intelligence (AI'\n",
      "    ✗ DIFFERENT\n",
      "  bs=17:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 62581, 476, 14250, 16599, 11, 4363, 264, 3412, 5567, 476, 264, 12453, 304, 264, 2311, 11, 10735, 389, 279, 18580, 24154, 315, 20443, 11229, 320, 15469, 8, 304, 279, 2266, 315, 6489, 4300, 323, 4763, 13, 576, 32711, 369, 419, 16688, 374, 438, 11017, 1447, 16, 13, 3070, 34, 30667, 323, 44078, 95518, 576, 1467, 5646, 12114, 51846, 323, 15057, 311, 14250, 15689, 11, 6467, 11, 323, 6682, 11, 1741, 438, 330, 79046, 11, 220, 17, 15, 17, 19, 1335, 330, 28418, 30497, 9965, 10294, 1335, 323, 330, 38066, 23051, 1189, 1096, 14807, 429, 279, 2197, 374, 44345, 304, 14250, 3412, 323, 6358, 382, 17, 13, 3070, 71021, 315, 576, 90767, 23752, 82, 95518, 576, 49465, 34334, 31787, 48025, 504, 6489, 4300, 11, 1741, 438, 279, 7286, 315, 20337, 279, 2639, 40508, 1660, 2238, 33248, 369, 5302]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a scholarly or academic publication, likely a research paper or a chapter in a book, focused on the strategic implications of artificial intelligence (AI) in the context of international relations and security. The reasoning for this conclusion is as follows:\\n\\n1. **Citations and References**: The text includes numerous citations and references to academic papers, books, and reports, such as \"RAND, 2024,\" \"American Political Science Review,\" and \"Lawfare.\" This indicates that the document is grounded in academic research and analysis.\\n\\n2. **Discussion of Theoretical Frameworks**: The excerpt discusses theoretical frameworks from international relations, such as the concept of maintaining the status quo being too costly for states'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "--- Token consistency for ref_2 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 11, 4363, 10735, 389, 279, 11513, 323, 24154, 315, 68863, 10847, 7377, 11773, 323, 5234, 1924, 1058, 1087, 13, 576, 2197, 1594, 2342, 1119, 3151, 10916, 4714, 5435, 311, 279, 13420, 2628, 2022, 315, 7377, 11773, 323, 279, 22901, 315, 10847, 5234, 1924, 1058, 1087, 11, 892, 13230, 432, 374, 949, 315, 264, 8131, 10219, 389, 5440, 22901, 11, 4763, 11, 323, 10767, 61442, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 62226, 45945, 2449, 323, 75772, 25, 1019, 256, 481, 576, 1467, 5711, 3151, 10916, 3793, 1741, 438, 330, 36411, 2628, 2022, 1335, 330, 57269, 11773, 1335, 330, 6998, 1924, 1058, 1087, 1335, 330, 1458, 380, 1087, 1335, 330, 3509, 12282, 1335, 330, 15105, 4019, 1335, 323, 330, 29662, 25812, 1189, 1096, 14807, 264, 2197, 429, 374, 7548, 10916, 304, 6993, 624]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report, likely focused on the challenges and implications of verifying advanced digital hardware and semiconductors. The document delves into specific technical issues related to the miniaturization of digital hardware and the verification of advanced semiconductors, which suggests it is part of a larger discussion on technology verification, security, and possibly cybersecurity.\\n\\n### Reasoning:\\n\\n1. **Technical Terminology and Concepts:**\\n   - The text uses specific technical terms such as \"miniaturization,\" \"digital hardware,\" \"semiconductors,\" \"transistors,\" \"node sizes,\" \"foundries,\" and \"chip designers.\" This indicates a document that is highly technical in nature.\\n'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 11, 4363, 10735, 389, 279, 11513, 323, 24154, 315, 68863, 10847, 7377, 11773, 323, 5234, 1924, 1058, 1087, 13, 576, 2197, 1594, 2342, 1119, 3151, 10916, 4714, 5435, 311, 279, 13420, 2628, 2022, 315, 7377, 11773, 323, 279, 22901, 315, 10847, 5234, 1924, 1058, 1087, 11, 892, 13230, 432, 374, 949, 315, 264, 8131, 10219, 389, 22901, 14310, 11, 4763, 11, 323, 10767, 61442, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 62226, 45945, 2449, 323, 75772, 25, 1019, 256, 481, 576, 1467, 5711, 3151, 10916, 3793, 1741, 438, 330, 36411, 2628, 2022, 1335, 330, 57269, 11773, 1335, 330, 6998, 1924, 1058, 1087, 1335, 330, 1458, 380, 1087, 1335, 330, 3509, 12282, 1335, 330, 15105, 4019, 1335, 323, 330, 29662, 25812, 1189, 1096, 14807, 264, 2197, 429, 374, 7548, 10916, 304, 6993, 624]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report, likely focused on the challenges and implications of verifying advanced digital hardware and semiconductors. The document delves into specific technical issues related to the miniaturization of digital hardware and the verification of advanced semiconductors, which suggests it is part of a larger discussion on verification technologies, security, and possibly cybersecurity.\\n\\n### Reasoning:\\n\\n1. **Technical Terminology and Concepts:**\\n   - The text uses specific technical terms such as \"miniaturization,\" \"digital hardware,\" \"semiconductors,\" \"transistors,\" \"node sizes,\" \"foundries,\" and \"chip designers.\" This indicates a document that is highly technical in nature.\\n'\n",
      "    ✗ DIFFERENT\n",
      "  bs=3:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 11, 4363, 10735, 389, 279, 11513, 323, 24154, 315, 68863, 10847, 7377, 11773, 323, 5234, 1924, 1058, 1087, 13, 576, 2197, 1594, 2342, 1119, 3151, 10916, 4714, 5435, 311, 279, 13420, 2628, 2022, 315, 7377, 11773, 323, 279, 22901, 315, 10847, 5234, 1924, 1058, 1087, 11, 892, 13230, 432, 374, 949, 315, 264, 8131, 10219, 389, 5440, 22901, 11, 4763, 11, 323, 10767, 61442, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 62226, 45945, 2449, 323, 75772, 25, 1019, 256, 481, 576, 1467, 5711, 3151, 10916, 3793, 1741, 438, 330, 36411, 2628, 2022, 1335, 330, 57269, 11773, 1335, 330, 6998, 1924, 1058, 1087, 1335, 330, 1458, 380, 1087, 1335, 330, 3509, 12282, 1335, 330, 15105, 4019, 1335, 323, 330, 29662, 25812, 1189, 1096, 14807, 264, 2197, 429, 374, 10735, 389, 10916, 13566, 315]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report, likely focused on the challenges and implications of verifying advanced digital hardware and semiconductors. The document delves into specific technical issues related to the miniaturization of digital hardware and the verification of advanced semiconductors, which suggests it is part of a larger discussion on technology verification, security, and possibly cybersecurity.\\n\\n### Reasoning:\\n\\n1. **Technical Terminology and Concepts:**\\n   - The text uses specific technical terms such as \"miniaturization,\" \"digital hardware,\" \"semiconductors,\" \"transistors,\" \"node sizes,\" \"foundries,\" and \"chip designers.\" This indicates a document that is focused on technical aspects of'\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 11, 4363, 10735, 389, 279, 11513, 323, 24154, 315, 68863, 10847, 7377, 11773, 323, 5234, 1924, 1058, 1087, 13, 576, 2197, 1594, 2342, 1119, 3151, 10916, 4714, 5435, 311, 279, 13420, 2628, 2022, 315, 7377, 11773, 323, 279, 22901, 315, 10847, 5234, 1924, 1058, 1087, 11, 892, 13230, 432, 374, 949, 315, 264, 8131, 10219, 389, 5440, 22901, 11, 4763, 11, 323, 10767, 61442, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 62226, 45945, 2449, 323, 75772, 25, 1019, 256, 481, 576, 2197, 5711, 10916, 3793, 1741, 438, 330, 36411, 2628, 2022, 1335, 330, 57269, 11773, 1335, 330, 6998, 1924, 1058, 1087, 1335, 330, 1458, 380, 1087, 1335, 330, 3509, 12282, 1335, 330, 15105, 4019, 1335, 323, 330, 29662, 25812, 1189, 1096, 14807, 264, 7548, 10916, 3832, 4925, 624, 256, 481, 1084, 34334]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report, likely focused on the challenges and implications of verifying advanced digital hardware and semiconductors. The document delves into specific technical issues related to the miniaturization of digital hardware and the verification of advanced semiconductors, which suggests it is part of a larger discussion on technology verification, security, and possibly cybersecurity.\\n\\n### Reasoning:\\n\\n1. **Technical Terminology and Concepts:**\\n   - The document uses technical terms such as \"miniaturization,\" \"digital hardware,\" \"semiconductors,\" \"transistors,\" \"node sizes,\" \"foundries,\" and \"chip designers.\" This indicates a highly technical subject matter.\\n   - It discusses'\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 11, 4363, 10735, 389, 279, 11513, 323, 24154, 315, 68863, 10847, 7377, 11773, 323, 5234, 1924, 1058, 1087, 13, 576, 2197, 1594, 2342, 1119, 3151, 10916, 4714, 5435, 311, 279, 13420, 2628, 2022, 315, 7377, 11773, 323, 279, 22901, 315, 10847, 5234, 1924, 1058, 1087, 11, 892, 13230, 432, 374, 949, 315, 264, 8131, 10219, 389, 5440, 22901, 11, 4763, 11, 323, 10767, 61442, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 62226, 45945, 2449, 323, 75772, 25, 1019, 256, 481, 576, 2197, 5711, 10916, 3793, 1741, 438, 330, 36411, 2628, 2022, 1335, 330, 57269, 11773, 1335, 330, 6998, 1924, 1058, 1087, 1335, 330, 1458, 380, 1087, 1335, 330, 3509, 12282, 1335, 330, 15105, 4019, 1335, 323, 330, 29662, 25812, 1189, 1096, 14807, 264, 7548, 10916, 3832, 4925, 624, 256, 481, 1084, 34334]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report, likely focused on the challenges and implications of verifying advanced digital hardware and semiconductors. The document delves into specific technical issues related to the miniaturization of digital hardware and the verification of advanced semiconductors, which suggests it is part of a larger discussion on technology verification, security, and possibly cybersecurity.\\n\\n### Reasoning:\\n\\n1. **Technical Terminology and Concepts:**\\n   - The document uses technical terms such as \"miniaturization,\" \"digital hardware,\" \"semiconductors,\" \"transistors,\" \"node sizes,\" \"foundries,\" and \"chip designers.\" This indicates a highly technical subject matter.\\n   - It discusses'\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 11, 4363, 10735, 389, 279, 11513, 323, 24154, 315, 68863, 10847, 7377, 11773, 323, 5234, 1924, 1058, 1087, 13, 576, 2197, 1594, 2342, 1119, 3151, 10916, 4714, 5435, 311, 279, 13420, 2628, 2022, 315, 7377, 11773, 323, 279, 22901, 315, 10847, 5234, 1924, 1058, 1087, 11, 892, 13230, 432, 374, 949, 315, 264, 8131, 10219, 389, 5440, 22901, 11, 4763, 11, 323, 10767, 61442, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 62226, 45945, 2449, 323, 75772, 95518, 576, 49465, 5711, 10916, 3793, 1741, 438, 330, 36411, 2628, 2022, 1335, 330, 57269, 11773, 1335, 330, 6998, 1924, 1058, 1087, 1335, 330, 1458, 380, 1087, 1335, 330, 3509, 12282, 1335, 323, 330, 50632, 1189, 1096, 14807, 429, 279, 2197, 374, 14550, 448, 7548, 27076, 323, 10916, 14878, 382, 17, 13, 3070, 47514, 25311, 323]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report, likely focused on the challenges and implications of verifying advanced digital hardware and semiconductors. The document delves into specific technical issues related to the miniaturization of digital hardware and the verification of advanced semiconductors, which suggests it is part of a larger discussion on technology verification, security, and possibly cybersecurity.\\n\\n### Reasoning:\\n\\n1. **Technical Terminology and Concepts**: The excerpt uses technical terms such as \"miniaturization,\" \"digital hardware,\" \"semiconductors,\" \"transistors,\" \"node sizes,\" and \"verification.\" This indicates that the document is dealing with highly specialized and technical subjects.\\n\\n2. **Specific Examples and'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 11, 4363, 10735, 389, 279, 11513, 323, 24154, 315, 68863, 10847, 7377, 11773, 323, 5234, 1924, 1058, 1087, 13, 576, 2197, 1594, 2342, 1119, 3151, 10916, 4714, 5435, 311, 279, 13420, 2628, 2022, 315, 7377, 11773, 323, 279, 22901, 315, 10847, 5234, 1924, 1058, 1087, 11, 892, 13230, 432, 374, 949, 315, 264, 8131, 10219, 389, 5440, 22901, 11, 4763, 11, 323, 10767, 61442, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 62226, 45945, 2449, 323, 75772, 95518, 576, 49465, 5711, 10916, 3793, 1741, 438, 330, 36411, 2628, 2022, 1335, 330, 57269, 11773, 1335, 330, 6998, 1924, 1058, 1087, 1335, 330, 1458, 380, 1087, 1335, 330, 3509, 12282, 1335, 323, 330, 50632, 1189, 1096, 14807, 429, 279, 2197, 374, 14550, 448, 7548, 27076, 323, 10916, 14878, 382, 17, 13, 3070, 47514, 25311, 323]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report, likely focused on the challenges and implications of verifying advanced digital hardware and semiconductors. The document delves into specific technical issues related to the miniaturization of digital hardware and the verification of advanced semiconductors, which suggests it is part of a larger discussion on technology verification, security, and possibly cybersecurity.\\n\\n### Reasoning:\\n\\n1. **Technical Terminology and Concepts**: The excerpt uses technical terms such as \"miniaturization,\" \"digital hardware,\" \"semiconductors,\" \"transistors,\" \"node sizes,\" and \"verification.\" This indicates that the document is dealing with highly specialized and technical subjects.\\n\\n2. **Specific Examples and'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 11, 4363, 10735, 389, 279, 11513, 323, 24154, 315, 68863, 10847, 7377, 11773, 323, 5234, 1924, 1058, 1087, 13, 576, 2197, 1594, 2342, 1119, 3151, 10916, 4714, 5435, 311, 279, 13420, 2628, 2022, 315, 7377, 11773, 323, 279, 22901, 315, 10847, 5234, 1924, 1058, 1087, 11, 892, 13230, 432, 374, 949, 315, 264, 8131, 10219, 389, 5440, 22901, 11, 4763, 11, 323, 10767, 61442, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 62226, 45945, 2449, 323, 75772, 25, 1019, 256, 481, 576, 1467, 5711, 3151, 10916, 3793, 1741, 438, 330, 36411, 2628, 2022, 1335, 330, 57269, 11773, 1335, 330, 6998, 1924, 1058, 1087, 1335, 330, 1458, 380, 1087, 1335, 330, 3509, 12282, 1335, 330, 15105, 4019, 1335, 323, 330, 29662, 25812, 1189, 1096, 14807, 264, 2197, 429, 374, 7548, 10916, 323, 4363, 5326]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report, likely focused on the challenges and implications of verifying advanced digital hardware and semiconductors. The document delves into specific technical issues related to the miniaturization of digital hardware and the verification of advanced semiconductors, which suggests it is part of a larger discussion on technology verification, security, and possibly cybersecurity.\\n\\n### Reasoning:\\n\\n1. **Technical Terminology and Concepts:**\\n   - The text uses specific technical terms such as \"miniaturization,\" \"digital hardware,\" \"semiconductors,\" \"transistors,\" \"node sizes,\" \"foundries,\" and \"chip designers.\" This indicates a document that is highly technical and likely written'\n",
      "    ✗ DIFFERENT\n",
      "  bs=17:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 11, 4363, 10735, 389, 279, 11513, 323, 24154, 315, 68863, 10847, 7377, 11773, 323, 5234, 1924, 1058, 1087, 13, 576, 2197, 1594, 2342, 1119, 3151, 10916, 4714, 5435, 311, 279, 13420, 2628, 2022, 315, 7377, 11773, 323, 279, 22901, 315, 10847, 5234, 1924, 1058, 1087, 11, 892, 13230, 432, 374, 949, 315, 264, 8131, 10219, 389, 5440, 22901, 11, 4763, 11, 323, 10767, 61442, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 62226, 45945, 2449, 323, 75772, 25, 1019, 256, 481, 576, 1467, 5711, 3151, 10916, 3793, 1741, 438, 330, 36411, 2628, 2022, 1335, 330, 57269, 11773, 1335, 330, 6998, 1924, 1058, 1087, 1335, 330, 1458, 380, 1087, 1335, 330, 3509, 12282, 1335, 323, 330, 50632, 1189, 1096, 14807, 264, 2197, 429, 374, 7548, 10916, 304, 6993, 382, 17, 13, 3070, 71021, 315]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report, likely focused on the challenges and implications of verifying advanced digital hardware and semiconductors. The document delves into specific technical issues related to the miniaturization of digital hardware and the verification of advanced semiconductors, which suggests it is part of a larger discussion on technology verification, security, and possibly cybersecurity.\\n\\n### Reasoning:\\n\\n1. **Technical Terminology and Concepts:**\\n   - The text uses specific technical terms such as \"miniaturization,\" \"digital hardware,\" \"semiconductors,\" \"transistors,\" \"node sizes,\" and \"verification.\" This indicates a document that is highly technical in nature.\\n\\n2. **Discussion of'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_0\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  5.88e-01  8.91e-01  8.91e-01  8.91e-01  8.91e-01  8.91e-01  8.91e-01  8.91e-01\n",
      "bs=2    5.88e-01  0.00e+00  1.19e+00  1.19e+00  1.19e+00  1.19e+00  1.19e+00  1.19e+00  1.19e+00\n",
      "bs=3    8.91e-01  1.19e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    8.91e-01  1.19e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    8.91e-01  1.19e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    8.91e-01  1.19e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    8.91e-01  1.19e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   8.91e-01  1.19e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   8.91e-01  1.19e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  5.20e-01  4.06e-01  4.06e-01  4.06e-01  4.06e-01  4.06e-01  4.06e-01  4.06e-01\n",
      "bs=2    5.20e-01  0.00e+00  4.43e-01  4.43e-01  4.43e-01  4.43e-01  4.43e-01  4.43e-01  4.43e-01\n",
      "bs=3    4.06e-01  4.43e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    4.06e-01  4.43e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    4.06e-01  4.43e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    4.06e-01  4.43e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    4.06e-01  4.43e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   4.06e-01  4.43e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   4.06e-01  4.43e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_1\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  3.94e-01  3.12e-01  3.12e-01  3.12e-01  3.12e-01  3.12e-01  3.12e-01  3.12e-01\n",
      "bs=2    3.94e-01  0.00e+00  3.72e-01  3.72e-01  3.72e-01  3.72e-01  3.72e-01  3.72e-01  3.72e-01\n",
      "bs=3    3.12e-01  3.72e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    3.12e-01  3.72e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    3.12e-01  3.72e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    3.12e-01  3.72e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    3.12e-01  3.72e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   3.12e-01  3.72e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   3.12e-01  3.72e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  3.14e-01  7.13e-01  7.13e-01  7.13e-01  7.13e-01  7.13e-01  7.13e-01  7.13e-01\n",
      "bs=2    3.14e-01  0.00e+00  4.61e-01  4.61e-01  4.61e-01  4.61e-01  4.61e-01  4.61e-01  4.61e-01\n",
      "bs=3    7.13e-01  4.61e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    7.13e-01  4.61e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    7.13e-01  4.61e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    7.13e-01  4.61e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    7.13e-01  4.61e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   7.13e-01  4.61e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   7.13e-01  4.61e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_2\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  3.38e-01  5.99e-01  5.99e-01  5.99e-01  5.99e-01  5.99e-01  5.99e-01  5.99e-01\n",
      "bs=2    3.38e-01  0.00e+00  5.64e-01  5.64e-01  5.64e-01  5.64e-01  5.64e-01  5.64e-01  5.64e-01\n",
      "bs=3    5.99e-01  5.64e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    5.99e-01  5.64e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    5.99e-01  5.64e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    5.99e-01  5.64e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    5.99e-01  5.64e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   5.99e-01  5.64e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   5.99e-01  5.64e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  1.60e-01  2.14e-01  2.14e-01  2.14e-01  2.14e-01  2.14e-01  2.14e-01  2.14e-01\n",
      "bs=2    1.60e-01  0.00e+00  2.82e-01  2.82e-01  2.82e-01  2.82e-01  2.82e-01  2.82e-01  2.82e-01\n",
      "bs=3    2.14e-01  2.82e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    2.14e-01  2.82e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    2.14e-01  2.82e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    2.14e-01  2.82e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    2.14e-01  2.82e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   2.14e-01  2.82e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   2.14e-01  2.82e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  4.40e-01  6.01e-01  6.01e-01  6.01e-01  6.01e-01  6.01e-01  6.01e-01  6.01e-01\n",
      "bs=2    4.40e-01  0.00e+00  7.08e-01  7.08e-01  7.08e-01  7.08e-01  7.08e-01  7.08e-01  7.08e-01\n",
      "bs=3    6.01e-01  7.08e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    6.01e-01  7.08e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    6.01e-01  7.08e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    6.01e-01  7.08e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    6.01e-01  7.08e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   6.01e-01  7.08e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   6.01e-01  7.08e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  3.31e-01  4.44e-01  4.44e-01  4.44e-01  4.44e-01  4.44e-01  4.44e-01  4.44e-01\n",
      "bs=2    3.31e-01  0.00e+00  3.95e-01  3.95e-01  3.95e-01  3.95e-01  3.95e-01  3.95e-01  3.95e-01\n",
      "bs=3    4.44e-01  3.95e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    4.44e-01  3.95e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    4.44e-01  3.95e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    4.44e-01  3.95e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    4.44e-01  3.95e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   4.44e-01  3.95e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   4.44e-01  3.95e-01  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Key vectors - Mean: 2.67e-01, Range: [0.00e+00, 7.08e-01]\n",
      "  Logprobs - Mean: 1.72e-01, Range: [0.00e+00, 4.44e-01]\n",
      "\n",
      "⚠ NOTE: 21/36 comparisons are equivalent (< 1e-09)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1]\n",
      "  Class 2: [2]\n",
      "  Class 3: [3, 4, 5, 8, 9, 16, 17]\n",
      "\n",
      "Equivalent pairs (will be excluded from cross-hardware signal):\n",
      "  (3, 4)\n",
      "  (3, 5)\n",
      "  (3, 8)\n",
      "  (3, 9)\n",
      "  (3, 16)\n",
      "  (3, 17)\n",
      "  (4, 5)\n",
      "  (4, 8)\n",
      "  (4, 9)\n",
      "  (4, 16)\n",
      "  (4, 17)\n",
      "  (5, 8)\n",
      "  (5, 9)\n",
      "  (5, 16)\n",
      "  (5, 17)\n",
      "  (8, 9)\n",
      "  (8, 16)\n",
      "  (8, 17)\n",
      "  (9, 16)\n",
      "  (9, 17)\n",
      "  (16, 17)\n",
      "\n",
      "✓ SANITY CHECK PASSED\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (DECODE)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_0\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.50e+01  2.50e+01  2.75e+01  2.75e+01  2.50e+01  2.50e+01  2.40e+01  2.75e+01\n",
      "bs=2    2.50e+01  0.00e+00  2.73e-01  2.35e+01  2.35e+01  3.55e-01  3.55e-01  2.62e+01  2.35e+01\n",
      "bs=3    2.50e+01  2.73e-01  0.00e+00  2.35e+01  2.35e+01  3.59e-01  3.59e-01  2.62e+01  2.34e+01\n",
      "bs=4    2.75e+01  2.35e+01  2.35e+01  0.00e+00  0.00e+00  2.35e+01  2.35e+01  2.28e+01  4.50e-01\n",
      "bs=5    2.75e+01  2.35e+01  2.35e+01  0.00e+00  0.00e+00  2.35e+01  2.35e+01  2.28e+01  4.50e-01\n",
      "bs=8    2.50e+01  3.55e-01  3.59e-01  2.35e+01  2.35e+01  0.00e+00  0.00e+00  2.62e+01  2.35e+01\n",
      "bs=9    2.50e+01  3.55e-01  3.59e-01  2.35e+01  2.35e+01  0.00e+00  0.00e+00  2.62e+01  2.35e+01\n",
      "bs=16   2.40e+01  2.62e+01  2.62e+01  2.28e+01  2.28e+01  2.62e+01  2.62e+01  0.00e+00  2.27e+01\n",
      "bs=17   2.75e+01  2.35e+01  2.34e+01  4.50e-01  4.50e-01  2.35e+01  2.35e+01  2.27e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00  1.21e-01       inf       inf  1.20e-01  1.20e-01       inf       inf\n",
      "bs=3         inf  1.21e-01  0.00e+00       inf       inf  1.33e-01  1.33e-01       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00  0.00e+00       inf       inf       inf  2.14e-01\n",
      "bs=5         inf       inf       inf  0.00e+00  0.00e+00       inf       inf       inf  2.14e-01\n",
      "bs=8         inf  1.20e-01  1.33e-01       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf  1.20e-01  1.33e-01       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf  2.14e-01  2.14e-01       inf       inf       inf  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_1\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.74e+01  2.94e+01  2.50e+01  2.50e+01  2.74e+01  2.74e+01  2.74e+01  2.54e+01\n",
      "bs=2    2.74e+01  0.00e+00  3.25e+01  2.30e+01  2.30e+01  9.00e-01  9.00e-01  8.98e-01  2.69e+01\n",
      "bs=3    2.94e+01  3.25e+01  0.00e+00  2.76e+01  2.76e+01  3.25e+01  3.25e+01  3.25e+01  2.97e+01\n",
      "bs=4    2.50e+01  2.30e+01  2.76e+01  0.00e+00  0.00e+00  2.30e+01  2.30e+01  2.30e+01  2.57e+01\n",
      "bs=5    2.50e+01  2.30e+01  2.76e+01  0.00e+00  0.00e+00  2.30e+01  2.30e+01  2.30e+01  2.57e+01\n",
      "bs=8    2.74e+01  9.00e-01  3.25e+01  2.30e+01  2.30e+01  0.00e+00  0.00e+00  2.49e-01  2.69e+01\n",
      "bs=9    2.74e+01  9.00e-01  3.25e+01  2.30e+01  2.30e+01  0.00e+00  0.00e+00  2.49e-01  2.69e+01\n",
      "bs=16   2.74e+01  8.98e-01  3.25e+01  2.30e+01  2.30e+01  2.49e-01  2.49e-01  0.00e+00  2.69e+01\n",
      "bs=17   2.54e+01  2.69e+01  2.97e+01  2.57e+01  2.57e+01  2.69e+01  2.69e+01  2.69e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00       inf       inf       inf  2.65e-01  2.65e-01  2.99e-01       inf\n",
      "bs=3         inf       inf  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00  0.00e+00       inf       inf       inf       inf\n",
      "bs=5         inf       inf       inf  0.00e+00  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf  2.65e-01       inf       inf       inf  0.00e+00  0.00e+00  4.17e-02       inf\n",
      "bs=9         inf  2.65e-01       inf       inf       inf  0.00e+00  0.00e+00  4.17e-02       inf\n",
      "bs=16        inf  2.99e-01       inf       inf       inf  4.17e-02  4.17e-02  0.00e+00       inf\n",
      "bs=17        inf       inf       inf       inf       inf       inf       inf       inf  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_2\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  3.60e-01  2.27e+01  2.93e+01  2.93e+01  2.97e+01  2.97e+01  1.41e+01  2.93e+01\n",
      "bs=2    3.60e-01  0.00e+00  2.27e+01  2.93e+01  2.93e+01  2.97e+01  2.97e+01  1.41e+01  2.93e+01\n",
      "bs=3    2.27e+01  2.27e+01  0.00e+00  2.60e+01  2.60e+01  2.35e+01  2.35e+01  2.56e+01  2.58e+01\n",
      "bs=4    2.93e+01  2.93e+01  2.60e+01  0.00e+00  0.00e+00  2.25e+01  2.25e+01  2.53e+01  1.48e+01\n",
      "bs=5    2.93e+01  2.93e+01  2.60e+01  0.00e+00  0.00e+00  2.25e+01  2.25e+01  2.53e+01  1.48e+01\n",
      "bs=8    2.97e+01  2.97e+01  2.35e+01  2.25e+01  2.25e+01  0.00e+00  0.00e+00  3.00e+01  1.93e+01\n",
      "bs=9    2.97e+01  2.97e+01  2.35e+01  2.25e+01  2.25e+01  0.00e+00  0.00e+00  3.00e+01  1.93e+01\n",
      "bs=16   1.41e+01  1.41e+01  2.56e+01  2.53e+01  2.53e+01  3.00e+01  3.00e+01  0.00e+00  2.66e+01\n",
      "bs=17   2.93e+01  2.93e+01  2.58e+01  1.48e+01  1.48e+01  1.93e+01  1.93e+01  2.66e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.49e-01       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2    2.49e-01  0.00e+00       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=3         inf       inf  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00  0.00e+00       inf       inf       inf       inf\n",
      "bs=5         inf       inf       inf  0.00e+00  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf       inf       inf       inf       inf       inf  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  1.76e+01  2.57e+01  2.73e+01  2.73e+01  2.74e+01  2.74e+01  2.18e+01  2.74e+01\n",
      "bs=2    1.76e+01  0.00e+00  1.85e+01  2.53e+01  2.53e+01  1.03e+01  1.03e+01  1.37e+01  2.65e+01\n",
      "bs=3    2.57e+01  1.85e+01  0.00e+00  2.57e+01  2.57e+01  1.88e+01  1.88e+01  2.81e+01  2.63e+01\n",
      "bs=4    2.73e+01  2.53e+01  2.57e+01  0.00e+00  0.00e+00  2.30e+01  2.30e+01  2.37e+01  1.37e+01\n",
      "bs=5    2.73e+01  2.53e+01  2.57e+01  0.00e+00  0.00e+00  2.30e+01  2.30e+01  2.37e+01  1.37e+01\n",
      "bs=8    2.74e+01  1.03e+01  1.88e+01  2.30e+01  2.30e+01  0.00e+00  0.00e+00  1.88e+01  2.32e+01\n",
      "bs=9    2.74e+01  1.03e+01  1.88e+01  2.30e+01  2.30e+01  0.00e+00  0.00e+00  1.88e+01  2.32e+01\n",
      "bs=16   2.18e+01  1.37e+01  2.81e+01  2.37e+01  2.37e+01  1.88e+01  1.88e+01  0.00e+00  2.54e+01\n",
      "bs=17   2.74e+01  2.65e+01  2.63e+01  1.37e+01  1.37e+01  2.32e+01  2.32e+01  2.54e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=3         inf       inf  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00  0.00e+00       inf       inf       inf       inf\n",
      "bs=5         inf       inf       inf  0.00e+00  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf       inf       inf       inf       inf       inf  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Key vectors - Mean: 2.09e+01, Range: [0.00e+00, 2.81e+01]\n",
      "  Logprobs - Mean: 0.00e+00, Range: [0.00e+00, inf]\n",
      "\n",
      "⚠ NOTE: 2/36 comparisons are equivalent (< 1e-09)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1]\n",
      "  Class 2: [2]\n",
      "  Class 3: [3]\n",
      "  Class 4: [4, 5]\n",
      "  Class 5: [8, 9]\n",
      "  Class 6: [16]\n",
      "  Class 7: [17]\n",
      "\n",
      "Equivalent pairs (will be excluded from cross-hardware signal):\n",
      "  (4, 5)\n",
      "  (8, 9)\n",
      "\n",
      "✗ SANITY CHECK FAILED (No variation)\n",
      "\n",
      "✓ Saved to: /workspace/experiments/decode_20251126_163114.json\n",
      "\n",
      "Next step: Copy to verifier machine, set TEACHER_FORCING=True\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Hardware Prefill vs Decode Detectability Experiment (Transformers)\n",
    "\n",
    "Tests whether batch size claims can be verified across different GPU architectures\n",
    "using floating-point forensics (key vectors and logprobs).\n",
    "\n",
    "UPDATED: Uses chat-formatted prompts for more realistic/higher-entropy responses.\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A (e.g., A100) with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts prefill + decode signals, saves to JSON\n",
    "2. Copy JSON to Machine B (e.g., H100)\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, extracts signals, compares\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = False\n",
    "REFERENCE_FILE = \"/workspace/experiments/decode_reference.json\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "ATTN_IMPLEMENTATION = \"sdpa\"  # Options: \"eager\", \"sdpa\", \"flash_attention_2\"\n",
    "\n",
    "BATCH_SIZES = [1, 2, 3, 4, 5, 8, 9, 16, 17]\n",
    "LAYER_INDICES = [28]  # Last layer only\n",
    "MAX_NEW_TOKENS = 150\n",
    "TOKENS_PER_SLICE = 1000  \n",
    "NUM_REFERENCES = 3\n",
    "\n",
    "# Threshold for considering two batch sizes \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Will be initialized from PDF in main()\n",
    "REFERENCE_SEQUENCES = None  # Now contains token IDs, not text\n",
    "DUMMY_SETS = None           # Now contains token IDs, not text\n",
    "\n",
    "# ============================================================================\n",
    "# CHAT TEMPLATE CONFIGURATION (Qwen recommended)\n",
    "# ============================================================================\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Here is an excerpt from a document:\n",
    "\n",
    "\"{snippet}\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.\"\"\"\n",
    "\n",
    "\n",
    "def create_chat_messages(snippet_text):\n",
    "    \"\"\"Create chat messages list for the tokenizer.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(snippet=snippet_text.strip())}\n",
    "    ]\n",
    "\n",
    "\n",
    "def tokenize_chat_prompt(tokenizer, snippet_text):\n",
    "    \"\"\"\n",
    "    Tokenize a chat prompt using the tokenizer's apply_chat_template.\n",
    "    Returns token IDs ready for model input.\n",
    "    \"\"\"\n",
    "    messages = create_chat_messages(snippet_text)\n",
    "    token_ids = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    return token_ids\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    \"\"\"Setup logging to file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    \"\"\"Print to both console and log file.\"\"\"\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    \"\"\"Close log file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING - UPDATED TO RETURN TOKEN IDS WITH CHAT TEMPLATE\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text content from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def create_sequences_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load all PDFs, create chat-formatted prompts, and return token IDs.\n",
    "    All assembly done at token level - guaranteed identical lengths.\n",
    "    \"\"\"\n",
    "    pdf_files = glob.glob(\"/workspace/*.pdf\")\n",
    "    if not pdf_files:\n",
    "        pdf_files = glob.glob(\"*.pdf\")\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    # Tokenize PDF content\n",
    "    content_tokens = tokenizer.encode(all_text, add_special_tokens=False)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    max_batch_size = max(BATCH_SIZES)\n",
    "    slices_needed = num_references * max_batch_size\n",
    "\n",
    "    if len(content_tokens) < slices_needed * TOKENS_PER_SLICE:\n",
    "        raise ValueError(f\"Need {slices_needed * TOKENS_PER_SLICE} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    # Tokenize template parts (prefix before snippet, suffix after)\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "    \n",
    "    suffix = f\"\"\"\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "    \n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    # Assemble prompts at token level\n",
    "    all_prompts = []\n",
    "    for i in range(slices_needed):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        all_prompts.append(prompt)\n",
    "    \n",
    "    # Verify\n",
    "    lengths = set(len(p) for p in all_prompts)\n",
    "    assert len(lengths) == 1, f\"Length mismatch: {lengths}\"\n",
    "    log_print(f\"All {slices_needed} prompts: {total_len} tokens each\")\n",
    "    \n",
    "    # Show sample ending\n",
    "    log_print(f\"\\nSample prompt ending:\")\n",
    "    log_print(repr(tokenizer.decode(all_prompts[0][-40:])))\n",
    "\n",
    "    reference_sequences = {}\n",
    "    dummy_sets = {}\n",
    "\n",
    "    for ref_idx in range(num_references):\n",
    "        ref_name = f\"ref_{ref_idx}\"\n",
    "        base_idx = ref_idx * max_batch_size\n",
    "        reference_sequences[ref_name] = all_prompts[base_idx]\n",
    "        dummy_sets[ref_name] = all_prompts[base_idx + 1 : base_idx + max_batch_size]\n",
    "\n",
    "    return reference_sequences, dummy_sets\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"attn_implementation\": ATTN_IMPLEMENTATION,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import flash_attn\n",
    "        info[\"flash_attn_version\"] = flash_attn.__version__\n",
    "    except ImportError:\n",
    "        info[\"flash_attn_version\"] = \"N/A\"\n",
    "        \n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    container_fields = ['python_version', 'cuda_version', 'cudnn_version']\n",
    "    pip_fields = ['torch_version', 'transformers_version', 'numpy_version', 'flash_attn_version']\n",
    "    config_fields = ['attn_implementation']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "    \n",
    "    container_mismatches = []\n",
    "    pip_mismatches = []\n",
    "    config_mismatches = []\n",
    "    \n",
    "    log_print(\"\\nScript configuration:\")\n",
    "    for field in config_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            config_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nContainer-level dependencies:\")\n",
    "    for field in container_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            container_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nPip-installable packages:\")\n",
    "    for field in pip_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        \n",
    "        if field == 'flash_attn_version':\n",
    "            ref_attn = reference_env.get('attn_implementation', '')\n",
    "            ver_attn = verifier_env.get('attn_implementation', '')\n",
    "            if ref_attn != 'flash_attention_2' and ver_attn != 'flash_attention_2':\n",
    "                log_print(f\"  - {field}: not using flash_attention_2 (skip)\")\n",
    "                continue\n",
    "            if ref_val == 'N/A' or ver_val == 'N/A':\n",
    "                log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "                pip_mismatches.append((field, ref_val, ver_val))\n",
    "                continue\n",
    "        \n",
    "        if ref_val == 'N/A' and ver_val == 'N/A':\n",
    "            log_print(f\"  - {field}: not installed (OK)\")\n",
    "            continue\n",
    "            \n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            pip_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not container_mismatches and not pip_mismatches and not config_mismatches:\n",
    "        log_print(\"\\n\" + \"-\"*60)\n",
    "        log_print(\"✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"✗ ENVIRONMENT MISMATCH - FIX REQUIRED\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# SIGNAL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_signals_from_output(outputs, layer_indices, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs from element 0 at specified position.\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    top_k = torch.topk(log_probs, k=20)\n",
    "\n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': top_k.indices.cpu().tolist(),\n",
    "        'log_probs': top_k.values.cpu().tolist()\n",
    "    }\n",
    "\n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract signals from multiple positions during prefill.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        prefill_signals[pos_label] = extract_signals_from_output(outputs, layer_indices, position=pos)\n",
    "    return prefill_signals\n",
    "\n",
    "def extract_signals_for_token_ids(outputs, layer_indices, token_ids, position=-1):\n",
    "    \"\"\"Extract signals for SPECIFIC token IDs (used in verification).\"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_ids_tensor = torch.tensor(token_ids, device=logits.device)\n",
    "    selected_logprobs = log_probs[token_ids_tensor]\n",
    "\n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': token_ids,\n",
    "        'log_probs': selected_logprobs.cpu().tolist()\n",
    "    }\n",
    "\n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals_for_token_ids(outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract prefill signals using reference token IDs.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        if pos_label in ref_prefill_signals:\n",
    "            ref_token_ids = ref_prefill_signals[pos_label]['logprobs']['token_ids']\n",
    "            prefill_signals[pos_label] = extract_signals_for_token_ids(\n",
    "                outputs, layer_indices, ref_token_ids, position=pos\n",
    "            )\n",
    "    return prefill_signals\n",
    "\n",
    "# ============================================================================\n",
    "# DECODE GENERATION (TEACHER_FORCING = False) - UPDATED FOR TOKEN IDS\n",
    "# ============================================================================\n",
    "\n",
    "def run_decode_with_extraction(model, tokenizer, ref_token_ids, ref_name, batch_size, \n",
    "                               layer_indices):\n",
    "    \"\"\"\n",
    "    Run decode generation and extract signals from last 3 generation steps.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    if batch_size == 1:\n",
    "        batch_token_ids = [ref_token_ids]\n",
    "    else:\n",
    "        batch_token_ids = [ref_token_ids] + ref_dummies[:batch_size-1]\n",
    "    \n",
    "    input_ids = torch.tensor(batch_token_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    log_print(f\"      Prompt: {prompt_length} tokens\", end=\"\")\n",
    "    \n",
    "    all_batch_generated_ids = [[] for _ in range(batch_size)]\n",
    "    generation_signals = []\n",
    "    \n",
    "    # FIRST STEP: Prefill\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "        \n",
    "    past_kv = outputs.past_key_values\n",
    "    prefill_signals = extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1])\n",
    "    \n",
    "    next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "    for batch_idx in range(batch_size):\n",
    "        all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "        \n",
    "    signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    \n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((inputs['attention_mask'].shape[0], 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS\n",
    "    for step in range(1, MAX_NEW_TOKENS):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "        for batch_idx in range(batch_size):\n",
    "            all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "            \n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((attention_mask.shape[0], 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "        if all_batch_generated_ids[0][-1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "    # Extract last 3 decode signals\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "        \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    final_length = prompt_length + num_generated\n",
    "    log_print(f\" → Final: {final_length} tokens ({num_generated} generated)\")\n",
    "    \n",
    "    return {\n",
    "        'generated_ids': all_batch_generated_ids[0],\n",
    "        'all_batch_generated_ids': all_batch_generated_ids,\n",
    "        'prompt_token_ids': batch_token_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TEACHER-FORCED DECODE (TEACHER_FORCING = True) - UPDATED FOR TOKEN IDS\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_decode(model, tokenizer, ref_name, reference_data, \n",
    "                              verify_batch_size, layer_indices, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced decode: feed reference tokens, extract signals.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ref_prompt_ids = reference_data['prompt_token_ids'][0]\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    ref_batch_size = len(reference_data['prompt_token_ids'])\n",
    "    \n",
    "    log_print(f\"      Prompt: {len(ref_prompt_ids)}, Gen: {len(ref_generated_ids)}\", end=\"\")\n",
    "    \n",
    "    if is_diagonal:\n",
    "        log_print(f\", exact neighbors (bs={ref_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = reference_data['prompt_token_ids']\n",
    "        batch_generated_ids = reference_data['all_batch_generated_ids']\n",
    "        actual_batch_size = ref_batch_size\n",
    "    else:\n",
    "        log_print(f\", arb neighbors (bs={verify_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = [ref_prompt_ids]\n",
    "        batch_generated_ids = [ref_generated_ids]\n",
    "        \n",
    "        ref_dummies = DUMMY_SETS[ref_name]\n",
    "        for i in range(verify_batch_size - 1):\n",
    "            batch_prompt_ids.append(ref_dummies[i])\n",
    "            batch_generated_ids.append([])\n",
    "        \n",
    "        actual_batch_size = verify_batch_size\n",
    "        \n",
    "    input_ids = torch.tensor(batch_prompt_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    generation_signals = []\n",
    "    num_steps = len(ref_generated_ids)\n",
    "    \n",
    "    # FIRST STEP: Prefill\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "        \n",
    "    past_kv = outputs.past_key_values\n",
    "    ref_prefill_signals = reference_data['prefill_signals']\n",
    "    prefill_signals = extract_prefill_signals_for_token_ids(\n",
    "        outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]\n",
    "    )\n",
    "    \n",
    "    ref_decode_signals = reference_data['decode_signals']\n",
    "    ref_step_data = list(ref_decode_signals.values())[0] if ref_decode_signals else None\n",
    "    if ref_step_data:\n",
    "        ref_token_ids = ref_step_data['signals']['logprobs']['token_ids']\n",
    "        signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "    else:\n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        \n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    # Prepare next tokens\n",
    "    if is_diagonal:\n",
    "        next_tokens = torch.tensor(\n",
    "            [batch_generated_ids[i][0] for i in range(actual_batch_size)],\n",
    "            dtype=torch.long, device='cuda'\n",
    "        )\n",
    "    else:\n",
    "        next_tokens_list = [ref_generated_ids[0]]\n",
    "        argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)\n",
    "        for i in range(actual_batch_size - 1):\n",
    "            next_tokens_list.append(argmax_tokens[i].item())\n",
    "            batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "        next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "        \n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((actual_batch_size, 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS\n",
    "    for step in range(1, num_steps):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        ref_signals_list = list(ref_decode_signals.values())\n",
    "        if step < len(ref_signals_list):\n",
    "            ref_token_ids = ref_signals_list[step]['signals']['logprobs']['token_ids']\n",
    "            signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "        else:\n",
    "            signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "            \n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        if step < num_steps - 1:\n",
    "            if is_diagonal:\n",
    "                next_tokens = torch.tensor(\n",
    "                    [batch_generated_ids[i][step] for i in range(actual_batch_size)],\n",
    "                    dtype=torch.long, device='cuda'\n",
    "                )\n",
    "            else:\n",
    "                next_tokens_list = [ref_generated_ids[step]]\n",
    "                argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)\n",
    "                for i in range(actual_batch_size - 1):\n",
    "                    next_tokens_list.append(argmax_tokens[i].item())\n",
    "                    batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "                next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((actual_batch_size, 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "    # Extract last 3\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "        \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(f\" → {num_generated} steps\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(decode_measurements, tokenizer):\n",
    "    \"\"\"Verify element 0 generates identical tokens across all batch sizes.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    tokens_by_bs = {}\n",
    "    for bs, data in decode_measurements.items():\n",
    "        tokens_by_bs[bs] = data['generated_ids']\n",
    "        \n",
    "    bs_list = sorted(tokens_by_bs.keys())\n",
    "    reference_tokens = tokens_by_bs[bs_list[0]]\n",
    "    \n",
    "    all_same = True\n",
    "    log_print(\"\\nGenerated tokens by batch size:\")\n",
    "    for bs in bs_list:\n",
    "        tokens = tokens_by_bs[bs]\n",
    "        match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "        decoded_text = tokenizer.decode(tokens)\n",
    "        log_print(f\"  bs={bs}:\")\n",
    "        log_print(f\"    IDs:  {tokens}\")\n",
    "        log_print(f\"    Text: {repr(decoded_text)}\")\n",
    "        log_print(f\"    {match_str}\")\n",
    "        if tokens != reference_tokens:\n",
    "            all_same = False\n",
    "            \n",
    "    if all_same:\n",
    "        log_print(\"\\n✓ Element 0 generates IDENTICAL tokens across all batch sizes\")\n",
    "    else:\n",
    "        log_print(\"\\n⚠ Element 0 generates DIFFERENT tokens across batch sizes\")\n",
    "        \n",
    "    return all_same\n",
    "\n",
    "def compute_l2_distance(vec1, vec2):\n",
    "    \"\"\"Compute L2 distance between two vectors.\"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return float(np.linalg.norm(v1 - v2))\n",
    "\n",
    "def compute_logprob_distance(logprobs1, logprobs2):\n",
    "    \"\"\"\n",
    "    Compute L2 distance between logprob distributions.\n",
    "    Uses top 5 token IDs from first signal as canonical (stored top 20 as buffer).\n",
    "    \"\"\"\n",
    "    canonical_ids = logprobs1['token_ids'][:5]\n",
    "    vec1 = logprobs1['log_probs'][:5]\n",
    "    \n",
    "    map2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "    \n",
    "    vec2 = []\n",
    "    for tid in canonical_ids:\n",
    "        if tid in map2:\n",
    "            vec2.append(map2[tid])\n",
    "        else:\n",
    "            return float('inf')\n",
    "    \n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2, layer_indices):\n",
    "    \"\"\"Compare two signal sets, return distances.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "    \n",
    "    all_key_dists = []\n",
    "    all_logprob_dists = []\n",
    "    \n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]['signals'] if 'signals' in signals1[pos_label] else signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]['signals'] if 'signals' in signals2[pos_label] else signals2[pos_label]\n",
    "        \n",
    "        for layer_name in sig1['key_vectors'].keys():\n",
    "            dist = compute_l2_distance(\n",
    "                sig1['key_vectors'][layer_name],\n",
    "                sig2['key_vectors'][layer_name]\n",
    "            )\n",
    "            all_key_dists.append(dist)\n",
    "            \n",
    "        dist = compute_logprob_distance(sig1['logprobs'], sig2['logprobs'])\n",
    "        all_logprob_dists.append(dist)\n",
    "        \n",
    "    return {\n",
    "        'key_vectors_max': max(all_key_dists) if all_key_dists else 0.0,\n",
    "        'key_vectors_mean': np.mean(all_key_dists) if all_key_dists else 0.0,\n",
    "        'logprobs_max': max(all_logprob_dists) if all_logprob_dists else 0.0,\n",
    "        'logprobs_mean': np.mean(all_logprob_dists) if all_logprob_dists else 0.0\n",
    "    }\n",
    "\n",
    "def find_equivalent_pairs(matrix, batch_sizes, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"Find pairs of batch sizes that produce equivalent results (same kernel).\"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n_bs = len(batch_sizes)\n",
    "    \n",
    "    for i in range(n_bs):\n",
    "        for j in range(i + 1, n_bs):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((batch_sizes[i], batch_sizes[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, batch_sizes):\n",
    "    \"\"\"Group batch sizes into kernel equivalence classes.\"\"\"\n",
    "    parent = {bs: bs for bs in batch_sizes}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        union(bs1, bs2)\n",
    "    \n",
    "    groups = {}\n",
    "    for bs in batch_sizes:\n",
    "        root = find(bs)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(bs)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "def analyze_within_hardware(measurements, batch_sizes, layer_indices, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware batch size effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE BATCH SIZE EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    by_ref = {}\n",
    "    for m in measurements:\n",
    "        ref = m['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "        by_ref[ref][m['batch_size']] = m[signals_key]\n",
    "\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix_key = np.zeros((n_bs, n_bs))\n",
    "        matrix_logprob = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, bs1 in enumerate(batch_sizes):\n",
    "            for j, bs2 in enumerate(batch_sizes):\n",
    "                if bs1 in ref_data and bs2 in ref_data:\n",
    "                    distances = compare_signals(ref_data[bs1], ref_data[bs2], layer_indices)\n",
    "                    matrix_key[i, j] = distances['key_vectors_mean']\n",
    "                    matrix_logprob[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "        \n",
    "        log_print(f\"\\nKey Vectors (mean L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, bs in enumerate(batch_sizes):\n",
    "            row_str = f\"bs={bs:<3}\"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        log_print(f\"\\nLogprobs (mean L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, bs in enumerate(batch_sizes):\n",
    "            row_str = f\"bs={bs:<3}\"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors (mean L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        row_str = f\"bs={bs:<3}\"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_key_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    log_print(f\"\\nLogprobs (mean L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        row_str = f\"bs={bs:<3}\"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_logprob_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    off_diag_key = avg_key_matrix[np.triu_indices(n_bs, k=1)]\n",
    "    off_diag_logprob = avg_logprob_matrix[np.triu_indices(n_bs, k=1)]\n",
    "    \n",
    "    key_mean = np.mean(off_diag_key[np.isfinite(off_diag_key)]) if np.any(np.isfinite(off_diag_key)) else 0\n",
    "    logprob_mean = np.mean(off_diag_logprob[np.isfinite(off_diag_logprob)]) if np.any(np.isfinite(off_diag_logprob)) else 0\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    log_print(f\"  Key vectors - Mean: {key_mean:.2e}, Range: [{np.min(off_diag_key):.2e}, {np.max(off_diag_key):.2e}]\")\n",
    "    log_print(f\"  Logprobs - Mean: {logprob_mean:.2e}, Range: [{np.min(off_diag_logprob):.2e}, {np.max(off_diag_logprob):.2e}]\")\n",
    "\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_key_matrix, batch_sizes)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, batch_sizes)\n",
    "    \n",
    "    zero_count = np.sum(off_diag_key < EQUIVALENCE_THRESHOLD)\n",
    "    total_count = len(off_diag_key)\n",
    "    \n",
    "    if zero_count == total_count:\n",
    "        log_print(f\"\\n⚠ WARNING: {zero_count}/{total_count} comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All batch sizes produce identical results (single kernel class)\")\n",
    "    elif zero_count > 0:\n",
    "        log_print(f\"\\n⚠ NOTE: {zero_count}/{total_count} comparisons are equivalent (< {EQUIVALENCE_THRESHOLD})\")\n",
    "    \n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs (will be excluded from cross-hardware signal):\")\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2})\")\n",
    "    \n",
    "    threshold = 1e-10\n",
    "    if key_mean > threshold and logprob_mean > threshold:\n",
    "        log_print(\"\\n✓ SANITY CHECK PASSED\")\n",
    "    else:\n",
    "        log_print(\"\\n✗ SANITY CHECK FAILED (No variation)\")\n",
    "        \n",
    "    return {\n",
    "        'key_matrix': avg_key_matrix.tolist(),\n",
    "        'logprob_matrix': avg_logprob_matrix.tolist(),\n",
    "        'per_reference_key_matrices': [m.tolist() for m in all_key_matrices],\n",
    "        'per_reference_logprob_matrices': [m.tolist() for m in all_logprob_matrices],\n",
    "        'key_vectors_mean': float(key_mean),\n",
    "        'logprobs_mean': float(logprob_mean),\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes]\n",
    "    }\n",
    "\n",
    "def analyze_cross_hardware_matrix(comparison_results, batch_sizes, layer_indices, \n",
    "                                   signal_source='decode', equivalent_pairs=None):\n",
    "    \"\"\"\n",
    "    Analyze the comparison matrix and determine detectability.\n",
    "    Excludes equivalent pairs from SNR signal calculation.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    equiv_set = set()\n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        equiv_set.add((bs1, bs2))\n",
    "        equiv_set.add((bs2, bs1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_batch_size'], result['verify_batch_size'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix_key = np.zeros((n_bs, n_bs))\n",
    "        matrix_logprob = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            for j, verify_bs in enumerate(batch_sizes):\n",
    "                key = (claimed_bs, verify_bs)\n",
    "                if key in ref_data:\n",
    "                    matrix_key[i, j] = ref_data[key][dist_key]['key_vectors_mean']\n",
    "                    matrix_logprob[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "        \n",
    "        log_print(f\"\\nKey Vectors (mean L2):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        log_print(f\"\\nLogprobs (mean L2):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "\n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors (mean L2):\")\n",
    "    log_print(header)\n",
    "    for i, claimed_bs in enumerate(batch_sizes):\n",
    "        row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_key_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    log_print(f\"\\nLogprobs (mean L2):\")\n",
    "    log_print(header)\n",
    "    for i, claimed_bs in enumerate(batch_sizes):\n",
    "        row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_logprob_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    diag_key = np.diag(avg_key_matrix)\n",
    "    diag_logprob = np.diag(avg_logprob_matrix)\n",
    "    diag_key_mean = np.mean(diag_key)\n",
    "    diag_logprob_mean = np.mean(diag_logprob)\n",
    "\n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same batch size):\")\n",
    "    log_print(f\"  Key vectors: {diag_key_mean:.2e}\")\n",
    "    log_print(f\"  Logprobs: {diag_logprob_mean:.2e}\")\n",
    "\n",
    "    off_diag_key = []\n",
    "    off_diag_logprob = []\n",
    "    for i in range(n_bs):\n",
    "        for j in range(n_bs):\n",
    "            if i != j:\n",
    "                off_diag_key.append(avg_key_matrix[i, j])\n",
    "                off_diag_logprob.append(avg_logprob_matrix[i, j])\n",
    "    \n",
    "    off_diag_key = np.array(off_diag_key)\n",
    "    off_diag_logprob = np.array(off_diag_logprob)\n",
    "    \n",
    "    off_key_mean = np.mean(off_diag_key)\n",
    "    off_logprob_mean = np.mean(off_diag_logprob)\n",
    "    \n",
    "    snr_key = off_key_mean / diag_key_mean if diag_key_mean > 0 else float('inf')\n",
    "    snr_logprob = off_logprob_mean / diag_logprob_mean if diag_logprob_mean > 0 else float('inf')\n",
    "\n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Key vectors - Mean: {off_key_mean:.2e}, SNR: {snr_key:.2f}×\")\n",
    "    log_print(f\"  Logprobs - Mean: {off_logprob_mean:.2e}, SNR: {snr_logprob:.2f}×\")\n",
    "\n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        excluded_count = 0\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2}) and ({bs2}, {bs1})\")\n",
    "            excluded_count += 2\n",
    "        log_print(f\"  Total excluded: {excluded_count} cells\")\n",
    "        \n",
    "        meaningful_key = []\n",
    "        meaningful_logprob = []\n",
    "        for i in range(n_bs):\n",
    "            for j in range(n_bs):\n",
    "                if i != j:\n",
    "                    bs_i, bs_j = batch_sizes[i], batch_sizes[j]\n",
    "                    if (bs_i, bs_j) not in equiv_set:\n",
    "                        meaningful_key.append(avg_key_matrix[i, j])\n",
    "                        meaningful_logprob.append(avg_logprob_matrix[i, j])\n",
    "        \n",
    "        if meaningful_key:\n",
    "            meaningful_key = np.array(meaningful_key)\n",
    "            meaningful_logprob = np.array(meaningful_logprob)\n",
    "            meaningful_key_mean = np.mean(meaningful_key)\n",
    "            meaningful_logprob_mean = np.mean(meaningful_logprob)\n",
    "            \n",
    "            snr_key_meaningful = meaningful_key_mean / diag_key_mean if diag_key_mean > 0 else float('inf')\n",
    "            snr_logprob_meaningful = meaningful_logprob_mean / diag_logprob_mean if diag_logprob_mean > 0 else float('inf')\n",
    "            \n",
    "            log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "            log_print(f\"  Count: {len(meaningful_key)}\")\n",
    "            log_print(f\"  Key vectors - Mean: {meaningful_key_mean:.2e}, SNR: {snr_key_meaningful:.2f}×\")\n",
    "            log_print(f\"  Logprobs - Mean: {meaningful_logprob_mean:.2e}, SNR: {snr_logprob_meaningful:.2f}×\")\n",
    "        else:\n",
    "            snr_key_meaningful = snr_key\n",
    "            snr_logprob_meaningful = snr_logprob\n",
    "    else:\n",
    "        snr_key_meaningful = snr_key\n",
    "        snr_logprob_meaningful = snr_logprob\n",
    "\n",
    "    return {\n",
    "        'key_vectors': {\n",
    "            'diagonal_mean': float(diag_key_mean),\n",
    "            'off_diagonal_mean': float(off_key_mean),\n",
    "            'snr': float(snr_key),\n",
    "            'snr_meaningful': float(snr_key_meaningful)\n",
    "        },\n",
    "        'logprobs': {\n",
    "            'diagonal_mean': float(diag_logprob_mean),\n",
    "            'off_diagonal_mean': float(off_logprob_mean),\n",
    "            'snr': float(snr_logprob),\n",
    "            'snr_meaningful': float(snr_logprob_meaningful)\n",
    "        },\n",
    "        'matrices': {\n",
    "            'key_vectors': avg_key_matrix.tolist(),\n",
    "            'logprobs': avg_logprob_matrix.tolist()\n",
    "        },\n",
    "        'excluded_pairs': equivalent_pairs\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    global REFERENCE_SEQUENCES, DUMMY_SETS\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    \n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION (reference)\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"Attention: {ATTN_IMPLEMENTATION}\")\n",
    "    log_print(f\"Layers: {LAYER_INDICES}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    REFERENCE_SEQUENCES, DUMMY_SETS = create_sequences_from_pdf(tokenizer)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=ATTN_IMPLEMENTATION\n",
    "    )\n",
    "    \n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        # VERIFICATION MODE\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            reference = json.load(f)\n",
    "            \n",
    "        ref_env = reference['metadata']['environment']\n",
    "        validate_environment_match(ref_env, system_info)\n",
    "        \n",
    "        prefill_sanity = reference.get('prefill_sanity_check', {})\n",
    "        decode_sanity = reference.get('decode_sanity_check', {})\n",
    "        \n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_sanity.get('equivalent_pairs', [])]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_sanity.get('equivalent_pairs', [])]\n",
    "        \n",
    "        log_print(f\"\\nLoaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "        \n",
    "        comparison_results = []\n",
    "        ref_by_key = {}\n",
    "        for m in reference['measurements']:\n",
    "            key = (m['ref_name'], m['batch_size'])\n",
    "            ref_by_key[key] = m\n",
    "            \n",
    "        for ref_name in sorted(REFERENCE_SEQUENCES.keys()):\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            for claimed_bs in BATCH_SIZES:\n",
    "                ref_key = (ref_name, claimed_bs)\n",
    "                if ref_key not in ref_by_key:\n",
    "                    log_print(f\"  ⚠ No reference data for {ref_name} bs={claimed_bs}\")\n",
    "                    continue\n",
    "                ref_data = ref_by_key[ref_key]\n",
    "                \n",
    "                log_print(f\"\\n  Claimed batch size: {claimed_bs}\")\n",
    "                \n",
    "                for verify_bs in BATCH_SIZES:\n",
    "                    is_diagonal = (claimed_bs == verify_bs)\n",
    "                    \n",
    "                    log_print(f\"    Verify bs={verify_bs} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "                    \n",
    "                    verify_result = run_teacher_forced_decode(\n",
    "                        model, tokenizer, ref_name, ref_data,\n",
    "                        verify_bs, LAYER_INDICES, is_diagonal\n",
    "                    )\n",
    "                    \n",
    "                    prefill_distances = compare_signals(\n",
    "                        ref_data['prefill_signals'], verify_result['prefill_signals'], LAYER_INDICES\n",
    "                    )\n",
    "                    decode_distances = compare_signals(\n",
    "                        ref_data['decode_signals'], verify_result['decode_signals'], LAYER_INDICES\n",
    "                    )\n",
    "                    \n",
    "                    log_print(f\"      Key: {decode_distances['key_vectors_mean']:.2e}, LP: {decode_distances['logprobs_mean']:.2e}\")\n",
    "                    \n",
    "                    comparison_results.append({\n",
    "                        'ref_name': ref_name,\n",
    "                        'claimed_batch_size': claimed_bs,\n",
    "                        'verify_batch_size': verify_bs,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances,\n",
    "                    })\n",
    "\n",
    "        prefill_analysis = analyze_cross_hardware_matrix(\n",
    "            comparison_results, BATCH_SIZES, LAYER_INDICES, \n",
    "            signal_source='prefill', equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware_matrix(\n",
    "            comparison_results, BATCH_SIZES, LAYER_INDICES,\n",
    "            signal_source='decode', equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "        \n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON (meaningful SNR)\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill - Key: {prefill_analysis['key_vectors']['snr_meaningful']:.2f}×, LP: {prefill_analysis['logprobs']['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode  - Key: {decode_analysis['key_vectors']['snr_meaningful']:.2f}×, LP: {decode_analysis['logprobs']['snr_meaningful']:.2f}×\")\n",
    "        \n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'layer_indices': LAYER_INDICES,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "        filepath = os.path.join(output_dir, f\"verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "            \n",
    "    else:\n",
    "        # GENERATION MODE\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'layer_indices': LAYER_INDICES\n",
    "            },\n",
    "            'measurements': []\n",
    "        }\n",
    "        \n",
    "        for ref_name, ref_token_ids in REFERENCE_SEQUENCES.items():\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            log_print(f\"\\nPrompt length: {len(ref_token_ids)} tokens\\n\")\n",
    "            \n",
    "            for batch_size in BATCH_SIZES:\n",
    "                log_print(f\"  bs={batch_size}:\", end=\"\")\n",
    "                decode_data = run_decode_with_extraction(\n",
    "                    model, tokenizer, ref_token_ids, ref_name, batch_size, LAYER_INDICES\n",
    "                )\n",
    "                results['measurements'].append({\n",
    "                    'ref_name': ref_name,\n",
    "                    'batch_size': batch_size,\n",
    "                    'generated_ids': decode_data['generated_ids'],\n",
    "                    'prompt_token_ids': decode_data['prompt_token_ids'],\n",
    "                    'prefill_signals': decode_data['prefill_signals'],\n",
    "                    'decode_signals': decode_data['decode_signals'],\n",
    "                    'all_batch_generated_ids': decode_data['all_batch_generated_ids']\n",
    "                })\n",
    "        \n",
    "        # Token consistency check\n",
    "        for ref_name in REFERENCE_SEQUENCES.keys():\n",
    "            log_print(f\"\\n--- Token consistency for {ref_name} ---\")\n",
    "            ref_measurements = {m['batch_size']: m for m in results['measurements'] if m['ref_name'] == ref_name}\n",
    "            check_token_consistency(ref_measurements, tokenizer)\n",
    "                \n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, LAYER_INDICES, 'prefill'\n",
    "        )\n",
    "        decode_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, LAYER_INDICES, 'decode'\n",
    "        )\n",
    "        \n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"decode_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        log_print(f\"\\n✓ Saved to: {filepath}\")\n",
    "        log_print(f\"\\nNext step: Copy to verifier machine, set TEACHER_FORCING=True\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4face-f066-4ac2-b379-5d6c1a46e210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
