{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c2d5bb1-7aaa-4826-a27f-b77bb9217ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-HARDWARE BATCH SIZE DETECTABILITY - GENERATION (reference)\n",
      "================================================================================\n",
      "\n",
      "System: c6ba2aea0d91\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "Attention: eager\n",
      "Layers: [28]\n",
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    → 120214 tokens\n",
      "Total tokens: 120214\n",
      "Creating 51 slices of 1000 tokens each\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2e0cf50bcc452eae8b45922f4da991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_0\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 1000 tokens\n",
      "\n",
      "  bs=1:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=2:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=3:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=4:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=5:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=8:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=9:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=16:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=17:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_1\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 1000 tokens\n",
      "\n",
      "  bs=1:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=2:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=3:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=4:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=5:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=8:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=9:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=16:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=17:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_2\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 1000 tokens\n",
      "\n",
      "  bs=1:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=2:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=3:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=4:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=5:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=8:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=9:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=16:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "  bs=17:      Prompt: 1000 tokens → Final: 1020 tokens (20 generated)\n",
      "\n",
      "--- Token consistency for ref_0 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=3:\n",
      "    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=4:\n",
      "    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=5:\n",
      "    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=8:\n",
      "    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=9:\n",
      "    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=16:\n",
      "    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=17:\n",
      "    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "\n",
      "✓ Element 0 generates IDENTICAL tokens across all batch sizes\n",
      "\n",
      "--- Token consistency for ref_1 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 2687, 15, 15, 17, 20, 198, 15, 15, 16]\n",
      "    Text: '://doi.org/10.1017/s0025\\n001'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 2687, 15, 15, 17, 20, 198, 15, 15, 16]\n",
      "    Text: '://doi.org/10.1017/s0025\\n001'\n",
      "    ✓\n",
      "  bs=3:\n",
      "    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 2687, 15, 15, 17, 20, 198, 15, 15, 16]\n",
      "    Text: '://doi.org/10.1017/s0025\\n001'\n",
      "    ✓\n",
      "  bs=4:\n",
      "    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 2687, 15, 15, 17, 20, 198, 15, 15, 16]\n",
      "    Text: '://doi.org/10.1017/s0025\\n001'\n",
      "    ✓\n",
      "  bs=5:\n",
      "    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 15, 20, 198, 15, 15, 15]\n",
      "    Text: '://doi.org/10.1017/S0005\\n000'\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 2687, 15, 15, 17, 20, 198, 15, 15, 16]\n",
      "    Text: '://doi.org/10.1017/s0025\\n001'\n",
      "    ✓\n",
      "  bs=9:\n",
      "    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 2687, 15, 15, 17, 20, 198, 15, 15, 16]\n",
      "    Text: '://doi.org/10.1017/s0025\\n001'\n",
      "    ✓\n",
      "  bs=16:\n",
      "    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 2687, 15, 15, 17, 20, 198, 15, 15, 16]\n",
      "    Text: '://doi.org/10.1017/s0025\\n001'\n",
      "    ✓\n",
      "  bs=17:\n",
      "    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 15, 20, 198, 15, 15, 15]\n",
      "    Text: '://doi.org/10.1017/S0005\\n000'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "--- Token consistency for ref_2 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [311, 387, 82004, 11, 714, 432, 1035, 198, 4310, 264, 3460, 3311, 315, 882, 323, 4963, 11, 323, 1035, 537]\n",
      "    Text: ' to be reconstructed, but it would\\nrequire a large amount of time and resources, and would not'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [311, 387, 82004, 11, 714, 432, 1035, 198, 4310, 264, 3460, 3311, 315, 882, 323, 4963, 11, 323, 1035, 537]\n",
      "    Text: ' to be reconstructed, but it would\\nrequire a large amount of time and resources, and would not'\n",
      "    ✓\n",
      "  bs=3:\n",
      "    IDs:  [311, 387, 82004, 11, 714, 432, 1035, 198, 4310, 264, 2244, 3484, 315, 882, 323, 5041, 11, 323, 1035, 537]\n",
      "    Text: ' to be reconstructed, but it would\\nrequire a great deal of time and effort, and would not'\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [311, 387, 82004, 11, 714, 432, 1035, 198, 4310, 264, 2244, 3484, 315, 882, 323, 5041, 11, 323, 1035, 537]\n",
      "    Text: ' to be reconstructed, but it would\\nrequire a great deal of time and effort, and would not'\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [311, 387, 82004, 11, 714, 432, 1035, 198, 4310, 264, 3460, 3311, 315, 882, 323, 4963, 11, 323, 1035, 6921]\n",
      "    Text: ' to be reconstructed, but it would\\nrequire a large amount of time and resources, and would destroy'\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [311, 387, 82004, 11, 714, 432, 1035, 198, 4310, 264, 3460, 3311, 315, 882, 323, 4963, 11, 323, 1035, 6921]\n",
      "    Text: ' to be reconstructed, but it would\\nrequire a large amount of time and resources, and would destroy'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [311, 387, 82004, 11, 714, 432, 1035, 198, 4310, 264, 3460, 3311, 315, 882, 323, 4963, 11, 323, 1035, 6921]\n",
      "    Text: ' to be reconstructed, but it would\\nrequire a large amount of time and resources, and would destroy'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [311, 387, 82004, 11, 714, 432, 1035, 198, 4310, 264, 3460, 3311, 315, 882, 323, 4963, 11, 323, 1035, 537]\n",
      "    Text: ' to be reconstructed, but it would\\nrequire a large amount of time and resources, and would not'\n",
      "    ✓\n",
      "  bs=17:\n",
      "    IDs:  [311, 387, 82004, 11, 714, 432, 1035, 198, 4310, 264, 2244, 3484, 315, 882, 323, 5041, 11, 323, 1035, 537]\n",
      "    Text: ' to be reconstructed, but it would\\nrequire a great deal of time and effort, and would not'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_0\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_1\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_2\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Key vectors - Mean: 0.00e+00, Range: [0.00e+00, 0.00e+00]\n",
      "  Logprobs - Mean: 0.00e+00, Range: [0.00e+00, 0.00e+00]\n",
      "\n",
      "⚠ WARNING: 36/36 comparisons are EXACTLY ZERO\n",
      "  All batch sizes produce identical results (single kernel class)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1, 2, 3, 4, 5, 8, 9, 16, 17]\n",
      "\n",
      "Equivalent pairs (will be excluded from cross-hardware signal):\n",
      "  (1, 2)\n",
      "  (1, 3)\n",
      "  (1, 4)\n",
      "  (1, 5)\n",
      "  (1, 8)\n",
      "  (1, 9)\n",
      "  (1, 16)\n",
      "  (1, 17)\n",
      "  (2, 3)\n",
      "  (2, 4)\n",
      "  (2, 5)\n",
      "  (2, 8)\n",
      "  (2, 9)\n",
      "  (2, 16)\n",
      "  (2, 17)\n",
      "  (3, 4)\n",
      "  (3, 5)\n",
      "  (3, 8)\n",
      "  (3, 9)\n",
      "  (3, 16)\n",
      "  (3, 17)\n",
      "  (4, 5)\n",
      "  (4, 8)\n",
      "  (4, 9)\n",
      "  (4, 16)\n",
      "  (4, 17)\n",
      "  (5, 8)\n",
      "  (5, 9)\n",
      "  (5, 16)\n",
      "  (5, 17)\n",
      "  (8, 9)\n",
      "  (8, 16)\n",
      "  (8, 17)\n",
      "  (9, 16)\n",
      "  (9, 17)\n",
      "  (16, 17)\n",
      "\n",
      "✗ SANITY CHECK FAILED (No variation)\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (DECODE)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_0\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  4.08e-01  3.87e-01  3.87e-01  3.87e-01  4.25e-01  4.00e-01  3.95e-01  4.15e-01\n",
      "bs=2    4.08e-01  0.00e+00  3.34e-01  3.34e-01  2.81e-01  3.04e-01  3.15e-01  3.46e-01  3.31e-01\n",
      "bs=3    3.87e-01  3.34e-01  0.00e+00  0.00e+00  3.05e-01  3.29e-01  3.04e-01  2.87e-01  3.59e-01\n",
      "bs=4    3.87e-01  3.34e-01  0.00e+00  0.00e+00  3.05e-01  3.29e-01  3.04e-01  2.87e-01  3.59e-01\n",
      "bs=5    3.87e-01  2.81e-01  3.05e-01  3.05e-01  0.00e+00  2.71e-01  2.87e-01  3.29e-01  2.97e-01\n",
      "bs=8    4.25e-01  3.04e-01  3.29e-01  3.29e-01  2.71e-01  0.00e+00  2.87e-01  3.51e-01  3.13e-01\n",
      "bs=9    4.00e-01  3.15e-01  3.04e-01  3.04e-01  2.87e-01  2.87e-01  0.00e+00  3.41e-01  3.26e-01\n",
      "bs=16   3.95e-01  3.46e-01  2.87e-01  2.87e-01  3.29e-01  3.51e-01  3.41e-01  0.00e+00  3.38e-01\n",
      "bs=17   4.15e-01  3.31e-01  3.59e-01  3.59e-01  2.97e-01  3.13e-01  3.26e-01  3.38e-01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.56e-01  2.84e-01  2.84e-01  2.37e-01  1.74e-01  2.82e-01  3.03e-01  2.46e-01\n",
      "bs=2    2.56e-01  0.00e+00  2.71e-01  2.71e-01  2.37e-01  2.43e-01  2.87e-01  1.73e-01  2.03e-01\n",
      "bs=3    2.84e-01  2.71e-01  0.00e+00  0.00e+00  1.77e-01  1.77e-01  1.90e-01  2.35e-01  2.01e-01\n",
      "bs=4    2.84e-01  2.71e-01  0.00e+00  0.00e+00  1.77e-01  1.77e-01  1.90e-01  2.35e-01  2.01e-01\n",
      "bs=5    2.37e-01  2.37e-01  1.77e-01  1.77e-01  0.00e+00  1.56e-01  1.94e-01  2.76e-01  1.25e-01\n",
      "bs=8    1.74e-01  2.43e-01  1.77e-01  1.77e-01  1.56e-01  0.00e+00  1.44e-01  2.71e-01  1.73e-01\n",
      "bs=9    2.82e-01  2.87e-01  1.90e-01  1.90e-01  1.94e-01  1.44e-01  0.00e+00  2.56e-01  1.60e-01\n",
      "bs=16   3.03e-01  1.73e-01  2.35e-01  2.35e-01  2.76e-01  2.71e-01  2.56e-01  0.00e+00  2.33e-01\n",
      "bs=17   2.46e-01  2.03e-01  2.01e-01  2.01e-01  1.25e-01  1.73e-01  1.60e-01  2.33e-01  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_1\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  8.53e-01  7.19e-01  7.19e-01  1.04e+01  7.86e-01  6.54e-01  7.75e-01  1.04e+01\n",
      "bs=2    8.53e-01  0.00e+00  6.95e-01  6.95e-01  1.03e+01  6.09e-01  8.25e-01  8.57e-01  1.04e+01\n",
      "bs=3    7.19e-01  6.95e-01  0.00e+00  0.00e+00  1.04e+01  7.91e-01  7.07e-01  7.08e-01  1.04e+01\n",
      "bs=4    7.19e-01  6.95e-01  0.00e+00  0.00e+00  1.04e+01  7.91e-01  7.07e-01  7.08e-01  1.04e+01\n",
      "bs=5    1.04e+01  1.03e+01  1.04e+01  1.04e+01  0.00e+00  1.04e+01  1.03e+01  1.04e+01  1.14e+00\n",
      "bs=8    7.86e-01  6.09e-01  7.91e-01  7.91e-01  1.04e+01  0.00e+00  9.86e-01  9.11e-01  1.05e+01\n",
      "bs=9    6.54e-01  8.25e-01  7.07e-01  7.07e-01  1.03e+01  9.86e-01  0.00e+00  7.14e-01  1.04e+01\n",
      "bs=16   7.75e-01  8.57e-01  7.08e-01  7.08e-01  1.04e+01  9.11e-01  7.14e-01  0.00e+00  1.04e+01\n",
      "bs=17   1.04e+01  1.04e+01  1.04e+01  1.04e+01  1.14e+00  1.05e+01  1.04e+01  1.04e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.17e-01  1.63e-01  1.63e-01  1.86e+00  1.28e-01  2.59e-01  2.13e-01  1.91e+00\n",
      "bs=2    2.17e-01  0.00e+00  1.38e-01  1.38e-01  1.85e+00  2.20e-01  1.35e-01  1.12e-01  1.90e+00\n",
      "bs=3    1.63e-01  1.38e-01  0.00e+00  0.00e+00  1.85e+00  1.62e-01  1.98e-01  1.76e-01  1.90e+00\n",
      "bs=4    1.63e-01  1.38e-01  0.00e+00  0.00e+00  1.85e+00  1.62e-01  1.98e-01  1.76e-01  1.90e+00\n",
      "bs=5    1.82e+00  1.72e+00  1.81e+00  1.81e+00  0.00e+00  1.81e+00  1.77e+00  1.74e+00  1.79e-01\n",
      "bs=8    1.28e-01  2.20e-01  1.62e-01  1.62e-01  1.82e+00  0.00e+00  2.48e-01  1.99e-01  1.87e+00\n",
      "bs=9    2.65e-01  1.40e-01  2.02e-01  2.02e-01  1.81e+00  2.57e-01  0.00e+00  9.24e-02  1.85e+00\n",
      "bs=16   2.13e-01  1.12e-01  1.76e-01  1.76e-01  1.85e+00  1.99e-01  1.07e-01  0.00e+00  1.89e+00\n",
      "bs=17   1.90e+00  1.80e+00  1.88e+00  1.88e+00  1.79e-01  1.89e+00  1.84e+00  1.82e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_2\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  5.68e-01  1.73e+00  1.73e+00  3.78e-01  3.13e-01  2.98e-01  5.69e-01  1.74e+00\n",
      "bs=2    5.68e-01  0.00e+00  1.64e+00  1.64e+00  5.87e-01  5.52e-01  5.24e-01  3.20e-01  1.64e+00\n",
      "bs=3    1.73e+00  1.64e+00  0.00e+00  0.00e+00  1.73e+00  1.74e+00  1.74e+00  1.59e+00  2.96e-01\n",
      "bs=4    1.73e+00  1.64e+00  0.00e+00  0.00e+00  1.73e+00  1.74e+00  1.74e+00  1.59e+00  2.96e-01\n",
      "bs=5    3.78e-01  5.87e-01  1.73e+00  1.73e+00  0.00e+00  3.92e-01  3.30e-01  6.39e-01  1.73e+00\n",
      "bs=8    3.13e-01  5.52e-01  1.74e+00  1.74e+00  3.92e-01  0.00e+00  2.77e-01  5.54e-01  1.74e+00\n",
      "bs=9    2.98e-01  5.24e-01  1.74e+00  1.74e+00  3.30e-01  2.77e-01  0.00e+00  5.53e-01  1.74e+00\n",
      "bs=16   5.69e-01  3.20e-01  1.59e+00  1.59e+00  6.39e-01  5.54e-01  5.53e-01  0.00e+00  1.59e+00\n",
      "bs=17   1.74e+00  1.64e+00  2.96e-01  2.96e-01  1.73e+00  1.74e+00  1.74e+00  1.59e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  1.90e-01  2.51e-01  2.51e-01  2.08e-01  2.17e-01  1.66e-01  1.31e-01  2.53e-01\n",
      "bs=2    1.90e-01  0.00e+00  2.28e-01  2.28e-01  7.03e-02  6.45e-02  1.18e-01  1.41e-01  2.71e-01\n",
      "bs=3    2.51e-01  2.28e-01  0.00e+00  0.00e+00  2.45e-01  2.06e-01  2.54e-01  2.45e-01  7.48e-02\n",
      "bs=4    2.51e-01  2.28e-01  0.00e+00  0.00e+00  2.45e-01  2.06e-01  2.54e-01  2.45e-01  7.48e-02\n",
      "bs=5    2.08e-01  7.03e-02  2.45e-01  2.45e-01  0.00e+00  7.51e-02  1.25e-01  1.72e-01  2.90e-01\n",
      "bs=8    2.17e-01  6.45e-02  2.06e-01  2.06e-01  7.51e-02  0.00e+00  1.24e-01  1.71e-01  2.58e-01\n",
      "bs=9    1.66e-01  1.18e-01  2.54e-01  2.54e-01  1.25e-01  1.24e-01  0.00e+00  1.72e-01  2.71e-01\n",
      "bs=16   1.31e-01  1.41e-01  2.45e-01  2.45e-01  1.72e-01  1.71e-01  1.72e-01  0.00e+00  2.69e-01\n",
      "bs=17   2.53e-01  2.71e-01  7.48e-02  7.48e-02  2.90e-01  2.58e-01  2.71e-01  2.69e-01  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  6.09e-01  9.45e-01  9.45e-01  3.71e+00  5.08e-01  4.51e-01  5.80e-01  4.19e+00\n",
      "bs=2    6.09e-01  0.00e+00  8.91e-01  8.91e-01  3.74e+00  4.88e-01  5.54e-01  5.08e-01  4.13e+00\n",
      "bs=3    9.45e-01  8.91e-01  0.00e+00  0.00e+00  4.13e+00  9.53e-01  9.18e-01  8.62e-01  3.69e+00\n",
      "bs=4    9.45e-01  8.91e-01  0.00e+00  0.00e+00  4.13e+00  9.53e-01  9.18e-01  8.62e-01  3.69e+00\n",
      "bs=5    3.71e+00  3.74e+00  4.13e+00  4.13e+00  0.00e+00  3.70e+00  3.65e+00  3.78e+00  1.06e+00\n",
      "bs=8    5.08e-01  4.88e-01  9.53e-01  9.53e-01  3.70e+00  0.00e+00  5.17e-01  6.05e-01  4.18e+00\n",
      "bs=9    4.51e-01  5.54e-01  9.18e-01  9.18e-01  3.65e+00  5.17e-01  0.00e+00  5.36e-01  4.14e+00\n",
      "bs=16   5.80e-01  5.08e-01  8.62e-01  8.62e-01  3.78e+00  6.05e-01  5.36e-01  0.00e+00  4.12e+00\n",
      "bs=17   4.19e+00  4.13e+00  3.69e+00  3.69e+00  1.06e+00  4.18e+00  4.14e+00  4.12e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.21e-01  2.33e-01  2.33e-01  7.69e-01  1.73e-01  2.36e-01  2.16e-01  8.02e-01\n",
      "bs=2    2.21e-01  0.00e+00  2.13e-01  2.13e-01  7.20e-01  1.76e-01  1.80e-01  1.42e-01  7.90e-01\n",
      "bs=3    2.33e-01  2.13e-01  0.00e+00  0.00e+00  7.58e-01  1.82e-01  2.14e-01  2.19e-01  7.24e-01\n",
      "bs=4    2.33e-01  2.13e-01  0.00e+00  0.00e+00  7.58e-01  1.82e-01  2.14e-01  2.19e-01  7.24e-01\n",
      "bs=5    7.54e-01  6.77e-01  7.43e-01  7.43e-01  0.00e+00  6.80e-01  6.96e-01  7.31e-01  1.98e-01\n",
      "bs=8    1.73e-01  1.76e-01  1.82e-01  1.82e-01  6.84e-01  0.00e+00  1.72e-01  2.14e-01  7.66e-01\n",
      "bs=9    2.37e-01  1.82e-01  2.15e-01  2.15e-01  7.11e-01  1.75e-01  0.00e+00  1.74e-01  7.58e-01\n",
      "bs=16   2.16e-01  1.42e-01  2.19e-01  2.19e-01  7.65e-01  2.14e-01  1.78e-01  0.00e+00  7.97e-01\n",
      "bs=17   7.98e-01  7.58e-01  7.19e-01  7.19e-01  1.98e-01  7.72e-01  7.57e-01  7.74e-01  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Key vectors - Mean: 1.96e+00, Range: [0.00e+00, 4.19e+00]\n",
      "  Logprobs - Mean: 4.08e-01, Range: [0.00e+00, 8.02e-01]\n",
      "\n",
      "⚠ NOTE: 1/36 comparisons are equivalent (< 1e-09)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1]\n",
      "  Class 2: [2]\n",
      "  Class 3: [3, 4]\n",
      "  Class 4: [5]\n",
      "  Class 5: [8]\n",
      "  Class 6: [9]\n",
      "  Class 7: [16]\n",
      "  Class 8: [17]\n",
      "\n",
      "Equivalent pairs (will be excluded from cross-hardware signal):\n",
      "  (3, 4)\n",
      "\n",
      "✓ SANITY CHECK PASSED\n",
      "\n",
      "✓ Saved to: /workspace/experiments/decode_20251125_205131.json\n",
      "\n",
      "Next step: Copy to verifier machine, set TEACHER_FORCING=True\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Hardware Prefill vs Decode Detectability Experiment (Transformers)\n",
    "\n",
    "Tests whether batch size claims can be verified across different GPU architectures\n",
    "using floating-point forensics (key vectors and logprobs).\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A (e.g., A100) with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts prefill + decode signals, saves to JSON\n",
    "2. Copy JSON to Machine B (e.g., H100)\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, extracts signals, compares\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = False\n",
    "REFERENCE_FILE = \"/workspace/experiments/decode_reference.json\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "ATTN_IMPLEMENTATION = \"eager\"  # Options: \"eager\", \"sdpa\", \"flash_attention_2\"\n",
    "\n",
    "BATCH_SIZES = [1, 2, 3, 4, 5, 8, 9, 16, 17]\n",
    "LAYER_INDICES = [28]  # Last layer only\n",
    "MAX_NEW_TOKENS = 20\n",
    "TOKENS_PER_SLICE = 1000\n",
    "NUM_REFERENCES = 3\n",
    "\n",
    "# Threshold for considering two batch sizes \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Will be initialized from PDF in main()\n",
    "REFERENCE_SEQUENCES = None\n",
    "DUMMY_SETS = None\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    \"\"\"Setup logging to file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    \"\"\"Print to both console and log file.\"\"\"\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    \"\"\"Close log file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text content from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_sequences_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load all PDFs and split into equal-length slices.\n",
    "    Returns REFERENCE_SEQUENCES and DUMMY_SETS dictionaries.\n",
    "    \"\"\"\n",
    "    pdf_files = glob.glob(\"/workspace/*.pdf\")\n",
    "    if not pdf_files:\n",
    "        pdf_files = glob.glob(\"*.pdf\")\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "\n",
    "    all_tokens = []\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "        all_tokens.extend(tokens)\n",
    "        log_print(f\"    → {len(tokens)} tokens\")\n",
    "\n",
    "    log_print(f\"Total tokens: {len(all_tokens)}\")\n",
    "\n",
    "    max_batch_size = max(BATCH_SIZES)\n",
    "    slices_needed = num_references * max_batch_size\n",
    "    tokens_needed = slices_needed * TOKENS_PER_SLICE\n",
    "\n",
    "    if len(all_tokens) < tokens_needed:\n",
    "        raise ValueError(f\"Need {tokens_needed} tokens but only have {len(all_tokens)}\")\n",
    "\n",
    "    log_print(f\"Creating {slices_needed} slices of {TOKENS_PER_SLICE} tokens each\")\n",
    "\n",
    "    slices = []\n",
    "    for i in range(slices_needed):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        slice_tokens = all_tokens[start:end]\n",
    "        slice_text = tokenizer.decode(slice_tokens)\n",
    "        slices.append(slice_text)\n",
    "\n",
    "    reference_sequences = {}\n",
    "    dummy_sets = {}\n",
    "\n",
    "    for ref_idx in range(num_references):\n",
    "        ref_name = f\"ref_{ref_idx}\"\n",
    "        base_idx = ref_idx * max_batch_size\n",
    "        reference_sequences[ref_name] = slices[base_idx]\n",
    "        dummy_sets[ref_name] = slices[base_idx + 1 : base_idx + max_batch_size]\n",
    "\n",
    "    return reference_sequences, dummy_sets\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"attn_implementation\": ATTN_IMPLEMENTATION,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import flash_attn\n",
    "        info[\"flash_attn_version\"] = flash_attn.__version__\n",
    "    except ImportError:\n",
    "        info[\"flash_attn_version\"] = \"N/A\"\n",
    "        \n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    container_fields = ['python_version', 'cuda_version', 'cudnn_version']\n",
    "    pip_fields = ['torch_version', 'transformers_version', 'numpy_version', 'flash_attn_version']\n",
    "    config_fields = ['attn_implementation']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "    \n",
    "    container_mismatches = []\n",
    "    pip_mismatches = []\n",
    "    config_mismatches = []\n",
    "    \n",
    "    log_print(\"\\nScript configuration:\")\n",
    "    for field in config_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            config_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nContainer-level dependencies:\")\n",
    "    for field in container_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            container_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nPip-installable packages:\")\n",
    "    for field in pip_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        \n",
    "        if field == 'flash_attn_version':\n",
    "            ref_attn = reference_env.get('attn_implementation', '')\n",
    "            ver_attn = verifier_env.get('attn_implementation', '')\n",
    "            if ref_attn != 'flash_attention_2' and ver_attn != 'flash_attention_2':\n",
    "                log_print(f\"  - {field}: not using flash_attention_2 (skip)\")\n",
    "                continue\n",
    "            if ref_val == 'N/A' or ver_val == 'N/A':\n",
    "                log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "                pip_mismatches.append((field, ref_val, ver_val))\n",
    "                continue\n",
    "        \n",
    "        if ref_val == 'N/A' and ver_val == 'N/A':\n",
    "            log_print(f\"  - {field}: not installed (OK)\")\n",
    "            continue\n",
    "            \n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            pip_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not container_mismatches and not pip_mismatches and not config_mismatches:\n",
    "        log_print(\"\\n\" + \"-\"*60)\n",
    "        log_print(\"✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"✗ ENVIRONMENT MISMATCH - FIX REQUIRED\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# SIGNAL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_signals_from_output(outputs, layer_indices, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs from element 0 at specified position.\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    top_k = torch.topk(log_probs, k=20)\n",
    "\n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': top_k.indices.cpu().tolist(),\n",
    "        'log_probs': top_k.values.cpu().tolist()\n",
    "    }\n",
    "\n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract signals from multiple positions during prefill.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        prefill_signals[pos_label] = extract_signals_from_output(outputs, layer_indices, position=pos)\n",
    "    return prefill_signals\n",
    "\n",
    "def extract_signals_for_token_ids(outputs, layer_indices, token_ids, position=-1):\n",
    "    \"\"\"Extract signals for SPECIFIC token IDs (used in verification).\"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_ids_tensor = torch.tensor(token_ids, device=logits.device)\n",
    "    selected_logprobs = log_probs[token_ids_tensor]\n",
    "\n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': token_ids,\n",
    "        'log_probs': selected_logprobs.cpu().tolist()\n",
    "    }\n",
    "\n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals_for_token_ids(outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract prefill signals using reference token IDs.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        if pos_label in ref_prefill_signals:\n",
    "            ref_token_ids = ref_prefill_signals[pos_label]['logprobs']['token_ids']\n",
    "            prefill_signals[pos_label] = extract_signals_for_token_ids(\n",
    "                outputs, layer_indices, ref_token_ids, position=pos\n",
    "            )\n",
    "    return prefill_signals\n",
    "\n",
    "# ============================================================================\n",
    "# DECODE GENERATION (TEACHER_FORCING = False)\n",
    "# ============================================================================\n",
    "\n",
    "def compute_min_length_across_batches(ref_text, ref_name, tokenizer, batch_sizes):\n",
    "    \"\"\"Pre-compute minimum sequence length across all batch configurations.\"\"\"\n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    min_length = float('inf')\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        if batch_size == 1:\n",
    "            batch_texts = [ref_text]\n",
    "        else:\n",
    "            batch_texts = [ref_text] + ref_dummies[:batch_size-1]\n",
    "        \n",
    "        token_lengths = [len(tokenizer.encode(t, add_special_tokens=True)) for t in batch_texts]\n",
    "        min_length = min(min_length, min(token_lengths))\n",
    "        \n",
    "    return min_length\n",
    "\n",
    "def run_decode_with_extraction(model, tokenizer, ref_text, ref_name, batch_size, \n",
    "                               layer_indices, forced_length=None):\n",
    "    \"\"\"\n",
    "    Run decode generation and extract signals from last 3 generation steps.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    if batch_size == 1:\n",
    "        batch_texts = [ref_text]\n",
    "    else:\n",
    "        batch_texts = [ref_text] + ref_dummies[:batch_size-1]\n",
    "    \n",
    "    all_token_ids = [tokenizer.encode(t, add_special_tokens=True) for t in batch_texts]\n",
    "    \n",
    "    if forced_length is not None:\n",
    "        min_length = forced_length\n",
    "    else:\n",
    "        min_length = min(len(ids) for ids in all_token_ids)\n",
    "        \n",
    "    truncated_token_ids = [ids[:min_length] for ids in all_token_ids]\n",
    "    \n",
    "    input_ids = torch.tensor(truncated_token_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    log_print(f\"      Prompt: {prompt_length} tokens\", end=\"\")\n",
    "    \n",
    "    all_batch_generated_ids = [[] for _ in range(batch_size)]\n",
    "    generation_signals = []\n",
    "    \n",
    "    # FIRST STEP: Prefill\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "        \n",
    "    past_kv = outputs.past_key_values\n",
    "    prefill_signals = extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1])\n",
    "    \n",
    "    next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "    for batch_idx in range(batch_size):\n",
    "        all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "        \n",
    "    signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    \n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((inputs['attention_mask'].shape[0], 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS\n",
    "    for step in range(1, MAX_NEW_TOKENS):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "        for batch_idx in range(batch_size):\n",
    "            all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "            \n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((attention_mask.shape[0], 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "        if all_batch_generated_ids[0][-1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "    # Extract last 3 decode signals\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "        \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    final_length = prompt_length + num_generated\n",
    "    log_print(f\" → Final: {final_length} tokens ({num_generated} generated)\")\n",
    "    \n",
    "    return {\n",
    "        'generated_ids': all_batch_generated_ids[0],\n",
    "        'all_batch_generated_ids': all_batch_generated_ids,\n",
    "        'prompt_token_ids': truncated_token_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TEACHER-FORCED DECODE (TEACHER_FORCING = True)\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_decode(model, tokenizer, ref_name, reference_data, \n",
    "                              verify_batch_size, layer_indices, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced decode: feed reference tokens, extract signals.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ref_prompt_ids = reference_data['prompt_token_ids'][0]\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    ref_batch_size = len(reference_data['prompt_token_ids'])\n",
    "    \n",
    "    log_print(f\"      Prompt: {len(ref_prompt_ids)}, Gen: {len(ref_generated_ids)}\", end=\"\")\n",
    "    \n",
    "    if is_diagonal:\n",
    "        log_print(f\", exact neighbors (bs={ref_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = reference_data['prompt_token_ids']\n",
    "        batch_generated_ids = reference_data['all_batch_generated_ids']\n",
    "        actual_batch_size = ref_batch_size\n",
    "    else:\n",
    "        log_print(f\", arb neighbors (bs={verify_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = [ref_prompt_ids]\n",
    "        batch_generated_ids = [ref_generated_ids]\n",
    "        \n",
    "        ref_dummies = DUMMY_SETS[ref_name]\n",
    "        for i in range(verify_batch_size - 1):\n",
    "            dummy_ids = tokenizer.encode(ref_dummies[i], add_special_tokens=True)\n",
    "            dummy_ids = dummy_ids[:len(ref_prompt_ids)]\n",
    "            if len(dummy_ids) < len(ref_prompt_ids):\n",
    "                dummy_ids = dummy_ids + [tokenizer.pad_token_id or 0] * (len(ref_prompt_ids) - len(dummy_ids))\n",
    "            batch_prompt_ids.append(dummy_ids)\n",
    "            batch_generated_ids.append([])\n",
    "        \n",
    "        actual_batch_size = verify_batch_size\n",
    "        \n",
    "    input_ids = torch.tensor(batch_prompt_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    generation_signals = []\n",
    "    num_steps = len(ref_generated_ids)\n",
    "    \n",
    "    # FIRST STEP: Prefill\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "        \n",
    "    past_kv = outputs.past_key_values\n",
    "    ref_prefill_signals = reference_data['prefill_signals']\n",
    "    prefill_signals = extract_prefill_signals_for_token_ids(\n",
    "        outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]\n",
    "    )\n",
    "    \n",
    "    ref_decode_signals = reference_data['decode_signals']\n",
    "    ref_step_data = list(ref_decode_signals.values())[0] if ref_decode_signals else None\n",
    "    if ref_step_data:\n",
    "        ref_token_ids = ref_step_data['signals']['logprobs']['token_ids']\n",
    "        signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "    else:\n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        \n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    # Prepare next tokens\n",
    "    if is_diagonal:\n",
    "        next_tokens = torch.tensor(\n",
    "            [batch_generated_ids[i][0] for i in range(actual_batch_size)],\n",
    "            dtype=torch.long, device='cuda'\n",
    "        )\n",
    "    else:\n",
    "        next_tokens_list = [ref_generated_ids[0]]\n",
    "        argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)\n",
    "        for i in range(actual_batch_size - 1):\n",
    "            next_tokens_list.append(argmax_tokens[i].item())\n",
    "            batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "        next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "        \n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((actual_batch_size, 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS\n",
    "    for step in range(1, num_steps):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        ref_signals_list = list(ref_decode_signals.values())\n",
    "        if step < len(ref_signals_list):\n",
    "            ref_token_ids = ref_signals_list[step]['signals']['logprobs']['token_ids']\n",
    "            signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "        else:\n",
    "            signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "            \n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        if step < num_steps - 1:\n",
    "            if is_diagonal:\n",
    "                next_tokens = torch.tensor(\n",
    "                    [batch_generated_ids[i][step] for i in range(actual_batch_size)],\n",
    "                    dtype=torch.long, device='cuda'\n",
    "                )\n",
    "            else:\n",
    "                next_tokens_list = [ref_generated_ids[step]]\n",
    "                argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)\n",
    "                for i in range(actual_batch_size - 1):\n",
    "                    next_tokens_list.append(argmax_tokens[i].item())\n",
    "                    batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "                next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((actual_batch_size, 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "    # Extract last 3\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "        \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(f\" → {num_generated} steps\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(decode_measurements, tokenizer):\n",
    "    \"\"\"Verify element 0 generates identical tokens across all batch sizes.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    tokens_by_bs = {}\n",
    "    for bs, data in decode_measurements.items():\n",
    "        tokens_by_bs[bs] = data['generated_ids']\n",
    "        \n",
    "    bs_list = sorted(tokens_by_bs.keys())\n",
    "    reference_tokens = tokens_by_bs[bs_list[0]]\n",
    "    \n",
    "    all_same = True\n",
    "    log_print(\"\\nGenerated tokens by batch size:\")\n",
    "    for bs in bs_list:\n",
    "        tokens = tokens_by_bs[bs]\n",
    "        match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "        decoded_text = tokenizer.decode(tokens)\n",
    "        log_print(f\"  bs={bs}:\")\n",
    "        log_print(f\"    IDs:  {tokens}\")\n",
    "        log_print(f\"    Text: {repr(decoded_text)}\")\n",
    "        log_print(f\"    {match_str}\")\n",
    "        if tokens != reference_tokens:\n",
    "            all_same = False\n",
    "            \n",
    "    if all_same:\n",
    "        log_print(\"\\n✓ Element 0 generates IDENTICAL tokens across all batch sizes\")\n",
    "    else:\n",
    "        log_print(\"\\n⚠ Element 0 generates DIFFERENT tokens across batch sizes\")\n",
    "        \n",
    "    return all_same\n",
    "\n",
    "def compute_l2_distance(vec1, vec2):\n",
    "    \"\"\"Compute L2 distance between two vectors.\"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return float(np.linalg.norm(v1 - v2))\n",
    "\n",
    "def compute_logprob_distance(logprobs1, logprobs2):\n",
    "    \"\"\"\n",
    "    Compute L2 distance between logprob distributions.\n",
    "    Uses top 5 token IDs from first signal as canonical (stored top 20 as buffer).\n",
    "    \"\"\"\n",
    "    # Top 5 from first signal as canonical\n",
    "    canonical_ids = logprobs1['token_ids'][:5]\n",
    "    vec1 = logprobs1['log_probs'][:5]\n",
    "    \n",
    "    # Look up these token IDs in second signal\n",
    "    map2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "    \n",
    "    vec2 = []\n",
    "    for tid in canonical_ids:\n",
    "        if tid in map2:\n",
    "            vec2.append(map2[tid])\n",
    "        else:\n",
    "            return float('inf')  # token not in top-20 of other run\n",
    "    \n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2, layer_indices):\n",
    "    \"\"\"Compare two signal sets, return distances.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "    \n",
    "    all_key_dists = []\n",
    "    all_logprob_dists = []\n",
    "    \n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]['signals'] if 'signals' in signals1[pos_label] else signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]['signals'] if 'signals' in signals2[pos_label] else signals2[pos_label]\n",
    "        \n",
    "        # Key vectors\n",
    "        for layer_name in sig1['key_vectors'].keys():\n",
    "            dist = compute_l2_distance(\n",
    "                sig1['key_vectors'][layer_name],\n",
    "                sig2['key_vectors'][layer_name]\n",
    "            )\n",
    "            all_key_dists.append(dist)\n",
    "            \n",
    "        # Logprobs\n",
    "        dist = compute_logprob_distance(sig1['logprobs'], sig2['logprobs'])\n",
    "        all_logprob_dists.append(dist)\n",
    "        \n",
    "    return {\n",
    "        'key_vectors_max': max(all_key_dists) if all_key_dists else 0.0,\n",
    "        'key_vectors_mean': np.mean(all_key_dists) if all_key_dists else 0.0,\n",
    "        'logprobs_max': max(all_logprob_dists) if all_logprob_dists else 0.0,\n",
    "        'logprobs_mean': np.mean(all_logprob_dists) if all_logprob_dists else 0.0\n",
    "    }\n",
    "\n",
    "def find_equivalent_pairs(matrix, batch_sizes, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Find pairs of batch sizes that produce equivalent results (same kernel).\n",
    "    Returns list of (bs1, bs2) tuples where bs1 < bs2.\n",
    "    \"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n_bs = len(batch_sizes)\n",
    "    \n",
    "    for i in range(n_bs):\n",
    "        for j in range(i + 1, n_bs):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((batch_sizes[i], batch_sizes[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, batch_sizes):\n",
    "    \"\"\"\n",
    "    Group batch sizes into kernel equivalence classes.\n",
    "    \"\"\"\n",
    "    parent = {bs: bs for bs in batch_sizes}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        union(bs1, bs2)\n",
    "    \n",
    "    groups = {}\n",
    "    for bs in batch_sizes:\n",
    "        root = find(bs)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(bs)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "def analyze_within_hardware(measurements, batch_sizes, layer_indices, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware batch size effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE BATCH SIZE EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    by_ref = {}\n",
    "    for m in measurements:\n",
    "        ref = m['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "        by_ref[ref][m['batch_size']] = m[signals_key]\n",
    "\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix_key = np.zeros((n_bs, n_bs))\n",
    "        matrix_logprob = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, bs1 in enumerate(batch_sizes):\n",
    "            for j, bs2 in enumerate(batch_sizes):\n",
    "                if bs1 not in ref_data or bs2 not in ref_data:\n",
    "                    continue\n",
    "                if i == j:\n",
    "                    matrix_key[i, j] = 0.0\n",
    "                    matrix_logprob[i, j] = 0.0\n",
    "                else:\n",
    "                    distances = compare_signals(ref_data[bs1], ref_data[bs2], layer_indices)\n",
    "                    matrix_key[i, j] = distances['key_vectors_mean']\n",
    "                    matrix_logprob[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "        log_print(\"\\nKey Vectors (mean L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, bs in enumerate(batch_sizes):\n",
    "            row_str = f\"bs={bs:<3}\"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:6.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        log_print(\"\\nLogprobs (mean L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, bs in enumerate(batch_sizes):\n",
    "            row_str = f\"bs={bs:<3}\"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:6.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "    log_print(\"\\nKey Vectors (mean L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        row_str = f\"bs={bs:<3}\"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_key_matrix[i,j]:6.2e}\"\n",
    "        log_print(row_str)\n",
    "    \n",
    "    log_print(\"\\nLogprobs (mean L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        row_str = f\"bs={bs:<3}\"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_logprob_matrix[i,j]:6.2e}\"\n",
    "        log_print(row_str)\n",
    "    \n",
    "    # Use key vectors for equivalence detection (more sensitive)\n",
    "    off_diag_key = avg_key_matrix[np.triu_indices(n_bs, k=1)]\n",
    "    off_diag_logprob = avg_logprob_matrix[np.triu_indices(n_bs, k=1)]\n",
    "    \n",
    "    key_mean = np.mean(off_diag_key)\n",
    "    logprob_mean = np.mean(off_diag_logprob)\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    log_print(f\"  Key vectors - Mean: {key_mean:.2e}, Range: [{np.min(off_diag_key):.2e}, {np.max(off_diag_key):.2e}]\")\n",
    "    log_print(f\"  Logprobs - Mean: {logprob_mean:.2e}, Range: [{np.min(off_diag_logprob):.2e}, {np.max(off_diag_logprob):.2e}]\")\n",
    "    \n",
    "    # Find equivalent pairs using key vectors\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_key_matrix, batch_sizes)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, batch_sizes)\n",
    "    \n",
    "    # Check if all zeros\n",
    "    zero_count = np.sum(off_diag_key < EQUIVALENCE_THRESHOLD)\n",
    "    total_count = len(off_diag_key)\n",
    "    \n",
    "    if zero_count == total_count:\n",
    "        log_print(f\"\\n⚠ WARNING: {zero_count}/{total_count} comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All batch sizes produce identical results (single kernel class)\")\n",
    "    elif zero_count > 0:\n",
    "        log_print(f\"\\n⚠ NOTE: {zero_count}/{total_count} comparisons are equivalent (< {EQUIVALENCE_THRESHOLD})\")\n",
    "    \n",
    "    # Display kernel classes\n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs (will be excluded from cross-hardware signal):\")\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2})\")\n",
    "    \n",
    "    threshold = 1e-10\n",
    "    if key_mean > threshold and logprob_mean > threshold:\n",
    "        log_print(\"\\n✓ SANITY CHECK PASSED\")\n",
    "    else:\n",
    "        log_print(\"\\n✗ SANITY CHECK FAILED (No variation)\")\n",
    "        \n",
    "    return {\n",
    "        'key_matrix': avg_key_matrix.tolist(),\n",
    "        'logprob_matrix': avg_logprob_matrix.tolist(),\n",
    "        'per_reference_key_matrices': [m.tolist() for m in all_key_matrices],\n",
    "        'per_reference_logprob_matrices': [m.tolist() for m in all_logprob_matrices],\n",
    "        'key_vectors_mean': float(key_mean),\n",
    "        'logprobs_mean': float(logprob_mean),\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes]\n",
    "    }\n",
    "\n",
    "def analyze_cross_hardware_matrix(comparison_results, batch_sizes, layer_indices, \n",
    "                                   signal_source='decode', equivalent_pairs=None):\n",
    "    \"\"\"\n",
    "    Analyze the comparison matrix and determine detectability.\n",
    "    Excludes equivalent pairs from SNR signal calculation.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    # Convert to set of both orderings for easy lookup\n",
    "    equiv_set = set()\n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        equiv_set.add((bs1, bs2))\n",
    "        equiv_set.add((bs2, bs1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_batch_size'], result['verify_batch_size'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    # Per-reference matrices\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix_key = np.zeros((n_bs, n_bs))\n",
    "        matrix_logprob = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            for j, verify_bs in enumerate(batch_sizes):\n",
    "                key = (claimed_bs, verify_bs)\n",
    "                if key in ref_data:\n",
    "                    matrix_key[i, j] = ref_data[key][dist_key]['key_vectors_mean']\n",
    "                    matrix_logprob[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        # Display matrices\n",
    "        header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "        \n",
    "        log_print(f\"\\nKey Vectors (mean L2):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        log_print(f\"\\nLogprobs (mean L2):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "\n",
    "    # Aggregate matrices\n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors (mean L2):\")\n",
    "    log_print(header)\n",
    "    for i, claimed_bs in enumerate(batch_sizes):\n",
    "        row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_key_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "    \n",
    "    log_print(f\"\\nLogprobs (mean L2):\")\n",
    "    log_print(header)\n",
    "    for i, claimed_bs in enumerate(batch_sizes):\n",
    "        row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_logprob_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "    \n",
    "    # Compute statistics for both signal types\n",
    "    results = {}\n",
    "    \n",
    "    for signal_type, avg_matrix in [('key_vectors', avg_key_matrix), ('logprobs', avg_logprob_matrix)]:\n",
    "        diagonal = [avg_matrix[i, i] for i in range(n_bs)]\n",
    "        \n",
    "        off_diagonal_all = []\n",
    "        off_diagonal_meaningful = []\n",
    "        excluded_pairs = []\n",
    "        \n",
    "        for i, bs1 in enumerate(batch_sizes):\n",
    "            for j, bs2 in enumerate(batch_sizes):\n",
    "                if i != j:\n",
    "                    off_diagonal_all.append(avg_matrix[i, j])\n",
    "                    if (bs1, bs2) in equiv_set:\n",
    "                        excluded_pairs.append((bs1, bs2))\n",
    "                    else:\n",
    "                        off_diagonal_meaningful.append(avg_matrix[i, j])\n",
    "        \n",
    "        baseline_mean = np.mean(diagonal)\n",
    "        signal_all_mean = np.mean(off_diagonal_all) if off_diagonal_all else 0.0\n",
    "        signal_meaningful_mean = np.mean(off_diagonal_meaningful) if off_diagonal_meaningful else 0.0\n",
    "        \n",
    "        snr_all = signal_all_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "        snr_meaningful = signal_meaningful_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "        \n",
    "        results[signal_type] = {\n",
    "            'matrix': avg_matrix.tolist(),\n",
    "            'baseline_mean': float(baseline_mean),\n",
    "            'signal_all_mean': float(signal_all_mean),\n",
    "            'signal_meaningful_mean': float(signal_meaningful_mean),\n",
    "            'snr_all': float(snr_all),\n",
    "            'snr_meaningful': float(snr_meaningful),\n",
    "            'n_meaningful_pairs': len(off_diagonal_meaningful)\n",
    "        }\n",
    "    \n",
    "    # Print SNR summary\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same batch size):\")\n",
    "    log_print(f\"  Key vectors: {results['key_vectors']['baseline_mean']:.2e}\")\n",
    "    log_print(f\"  Logprobs: {results['logprobs']['baseline_mean']:.2e}\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Key vectors - Mean: {results['key_vectors']['signal_all_mean']:.2e}, SNR: {results['key_vectors']['snr_all']:.2f}×\")\n",
    "    log_print(f\"  Logprobs - Mean: {results['logprobs']['signal_all_mean']:.2e}, SNR: {results['logprobs']['snr_all']:.2f}×\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2}) and ({bs2}, {bs1})\")\n",
    "        log_print(f\"  Total excluded: {len(equivalent_pairs) * 2} cells\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "    log_print(f\"  Count: {results['key_vectors']['n_meaningful_pairs']}\")\n",
    "    if results['key_vectors']['n_meaningful_pairs'] > 0:\n",
    "        log_print(f\"  Key vectors - Mean: {results['key_vectors']['signal_meaningful_mean']:.2e}, SNR: {results['key_vectors']['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"  Logprobs - Mean: {results['logprobs']['signal_meaningful_mean']:.2e}, SNR: {results['logprobs']['snr_meaningful']:.2f}×\")\n",
    "    else:\n",
    "        log_print(\"  No meaningful pairs (all batch sizes are equivalent)\")\n",
    "    \n",
    "    return {\n",
    "        'per_reference_key_matrices': [m.tolist() for m in all_key_matrices],\n",
    "        'per_reference_logprob_matrices': [m.tolist() for m in all_logprob_matrices],\n",
    "        'key_vectors': results['key_vectors'],\n",
    "        'logprobs': results['logprobs'],\n",
    "        'excluded_pairs': equivalent_pairs\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    global REFERENCE_SEQUENCES, DUMMY_SETS\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    \n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION (reference)\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"Attention: {ATTN_IMPLEMENTATION}\")\n",
    "    log_print(f\"Layers: {LAYER_INDICES}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    REFERENCE_SEQUENCES, DUMMY_SETS = create_sequences_from_pdf(tokenizer)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=ATTN_IMPLEMENTATION\n",
    "    )\n",
    "    \n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        # VERIFICATION MODE\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            reference = json.load(f)\n",
    "            \n",
    "        ref_env = reference['metadata']['environment']\n",
    "        validate_environment_match(ref_env, system_info)\n",
    "        \n",
    "        # Load equivalent pairs from reference\n",
    "        prefill_sanity = reference.get('prefill_sanity_check', {})\n",
    "        decode_sanity = reference.get('decode_sanity_check', {})\n",
    "        \n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_sanity.get('equivalent_pairs', [])]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_sanity.get('equivalent_pairs', [])]\n",
    "        \n",
    "        log_print(f\"\\nLoaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "        \n",
    "        comparison_results = []\n",
    "        ref_by_key = {}\n",
    "        for m in reference['measurements']:\n",
    "            key = (m['ref_name'], m['batch_size'])\n",
    "            ref_by_key[key] = m\n",
    "            \n",
    "        for ref_name in sorted(REFERENCE_SEQUENCES.keys()):\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            for claimed_bs in BATCH_SIZES:\n",
    "                ref_key = (ref_name, claimed_bs)\n",
    "                if ref_key not in ref_by_key:\n",
    "                    log_print(f\"  ⚠ No reference data for {ref_name} bs={claimed_bs}\")\n",
    "                    continue\n",
    "                ref_data = ref_by_key[ref_key]\n",
    "                \n",
    "                log_print(f\"\\n  Claimed batch size: {claimed_bs}\")\n",
    "                \n",
    "                for verify_bs in BATCH_SIZES:\n",
    "                    is_diagonal = (claimed_bs == verify_bs)\n",
    "                    \n",
    "                    log_print(f\"    Verify bs={verify_bs} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "                    \n",
    "                    verify_result = run_teacher_forced_decode(\n",
    "                        model, tokenizer, ref_name, ref_data,\n",
    "                        verify_bs, LAYER_INDICES, is_diagonal\n",
    "                    )\n",
    "                    \n",
    "                    prefill_distances = compare_signals(\n",
    "                        ref_data['prefill_signals'], verify_result['prefill_signals'], LAYER_INDICES\n",
    "                    )\n",
    "                    decode_distances = compare_signals(\n",
    "                        ref_data['decode_signals'], verify_result['decode_signals'], LAYER_INDICES\n",
    "                    )\n",
    "                    \n",
    "                    log_print(f\"      Key: {decode_distances['key_vectors_mean']:.2e}, LP: {decode_distances['logprobs_mean']:.2e}\")\n",
    "                    \n",
    "                    comparison_results.append({\n",
    "                        'ref_name': ref_name,\n",
    "                        'claimed_batch_size': claimed_bs,\n",
    "                        'verify_batch_size': verify_bs,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances,\n",
    "                    })\n",
    "\n",
    "        prefill_analysis = analyze_cross_hardware_matrix(\n",
    "            comparison_results, BATCH_SIZES, LAYER_INDICES, \n",
    "            signal_source='prefill', equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware_matrix(\n",
    "            comparison_results, BATCH_SIZES, LAYER_INDICES,\n",
    "            signal_source='decode', equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "        \n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON (meaningful SNR)\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill - Key: {prefill_analysis['key_vectors']['snr_meaningful']:.2f}×, LP: {prefill_analysis['logprobs']['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode  - Key: {decode_analysis['key_vectors']['snr_meaningful']:.2f}×, LP: {decode_analysis['logprobs']['snr_meaningful']:.2f}×\")\n",
    "        \n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'layer_indices': LAYER_INDICES,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "        filepath = os.path.join(output_dir, f\"verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "            \n",
    "    else:\n",
    "        # GENERATION MODE\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'layer_indices': LAYER_INDICES\n",
    "            },\n",
    "            'measurements': []\n",
    "        }\n",
    "        \n",
    "        for ref_name, ref_text in REFERENCE_SEQUENCES.items():\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            min_prompt_length = compute_min_length_across_batches(\n",
    "                ref_text, ref_name, tokenizer, BATCH_SIZES\n",
    "            )\n",
    "            log_print(f\"\\nGlobal minimum prompt length: {min_prompt_length} tokens\\n\")\n",
    "            \n",
    "            for batch_size in BATCH_SIZES:\n",
    "                log_print(f\"  bs={batch_size}:\", end=\"\")\n",
    "                decode_data = run_decode_with_extraction(\n",
    "                    model, tokenizer, ref_text, ref_name, batch_size, LAYER_INDICES,\n",
    "                    forced_length=min_prompt_length\n",
    "                )\n",
    "                results['measurements'].append({\n",
    "                    'ref_name': ref_name,\n",
    "                    'batch_size': batch_size,\n",
    "                    'generated_ids': decode_data['generated_ids'],\n",
    "                    'prompt_token_ids': decode_data['prompt_token_ids'],\n",
    "                    'prefill_signals': decode_data['prefill_signals'],\n",
    "                    'decode_signals': decode_data['decode_signals'],\n",
    "                    'all_batch_generated_ids': decode_data['all_batch_generated_ids']\n",
    "                })\n",
    "        \n",
    "        # Token consistency check\n",
    "        for ref_name in REFERENCE_SEQUENCES.keys():\n",
    "            log_print(f\"\\n--- Token consistency for {ref_name} ---\")\n",
    "            ref_measurements = {m['batch_size']: m for m in results['measurements'] if m['ref_name'] == ref_name}\n",
    "            check_token_consistency(ref_measurements, tokenizer)\n",
    "                \n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, LAYER_INDICES, 'prefill'\n",
    "        )\n",
    "        decode_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, LAYER_INDICES, 'decode'\n",
    "        )\n",
    "        \n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"decode_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        log_print(f\"\\n✓ Saved to: {filepath}\")\n",
    "        log_print(f\"\\nNext step: Copy to verifier machine, set TEACHER_FORCING=True\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110663da-75c2-44e0-9e3e-ec21e8f034ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
