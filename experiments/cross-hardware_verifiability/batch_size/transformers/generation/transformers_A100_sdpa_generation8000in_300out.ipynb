{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fb9e08-8680-473f-b415-54312ed3069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-HARDWARE BATCH SIZE DETECTABILITY - GENERATION (reference)\n",
      "================================================================================\n",
      "\n",
      "System: c6c6242ae438\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "Attention: sdpa\n",
      "Layers: [28]\n",
      "Found 14 PDF(s)\n",
      "  Loading: /workspace/memos.pdf\n",
      "  Loading: /workspace/scaling_reasoning_pretraining.pdf\n",
      "  Loading: /workspace/verifying training data.pdf\n",
      "  Loading: /workspace/AI_deception.pdf\n",
      "  Loading: /workspace/BaGuaLu.pdf\n",
      "  Loading: /workspace/COCOmix.pdf\n",
      "  Loading: /workspace/Are_you_secretly_training_AI.pdf\n",
      "  Loading: /workspace/k2.pdf\n",
      "  Loading: /workspace/minimax.pdf\n",
      "  Loading: /workspace/Superintelligence_Strategy_Expert.pdf\n",
      "  Loading: /workspace/dragon hatchling.pdf\n",
      "  Loading: /workspace/recurrent.pdf\n",
      "  Loading: /workspace/Llama3.1.pdf\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (626347 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total source tokens: 626347\n",
      "Prompt structure: 33 prefix + 8000 snippet + 34 suffix = 8067 tokens\n",
      "All 51 prompts: 8067 tokens each\n",
      "\n",
      "Sample prompt ending:\n",
      "' must adopt a memory-centric design\"\\n\\nBased on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\\n<|im_start|>assistant\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "881bad2a450d411c984fa7af08f2162e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_0\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 8067 tokens\n",
      "\n",
      "  bs=1:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=2:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=3:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=4:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=5:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=8:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=9:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=16:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=17:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_1\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 8067 tokens\n",
      "\n",
      "  bs=1:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=2:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=3:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=4:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=5:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=8:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=9:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=16:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=17:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_2\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 8067 tokens\n",
      "\n",
      "  bs=1:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=2:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=3:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=4:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=5:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=8:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=9:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=16:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "  bs=17:      Prompt: 8067 tokens → Final: 8367 tokens (300 generated)\n",
      "\n",
      "--- Token consistency for ref_0 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 315, 264, 501, 4938, 6240, 1849, 369, 3460, 4128, 4119, 320, 4086, 21634, 8, 323, 20443, 4586, 11229, 320, 1890, 40, 8, 5942, 13, 576, 2197, 594, 3832, 4925, 374, 279, 2884, 323, 8129, 315, 264, 4938, 10350, 1849, 320, 18816, 3126, 8, 429, 21538, 311, 2621, 279, 9481, 315, 1482, 4938, 6240, 304, 444, 10994, 82, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 7524, 3990, 334, 510, 256, 481, 3070, 27428, 17862, 95518, 576, 2197, 5646, 11682, 27787, 315, 6350, 3412, 11, 22049, 8040, 11, 323, 264, 10981, 6291, 13, 1084, 1083, 5610, 10916, 3793, 323, 18940, 11, 892, 525, 14260, 315, 3412, 15689, 624, 256, 481, 3070, 62226, 8259, 95518, 576, 2197, 5707, 264, 15817, 23251, 315, 279, 3491, 11, 6350, 9904, 11, 323, 264, 10981, 6291, 11, 892, 374, 28583, 315, 10916, 6682, 382, 17, 13, 3070, 13019, 33364, 334, 510, 256, 481, 3070, 10642, 9551, 304, 444, 10994, 82, 95518, 576, 2197, 34334, 279, 9481, 315, 1482, 4938, 6240, 304, 444, 10994, 82, 11, 1741, 438, 279, 6853, 315, 1632, 38717, 4938, 6240, 5942, 11, 323, 279, 1184, 369, 264, 803, 11050, 323, 18776, 4938, 1849, 624, 256, 481, 3070, 9286, 16488, 3251, 21392, 320, 1890, 40, 32295, 25, 576, 2197, 33845, 15082, 40, 323, 1246, 4938, 6240, 374, 16587, 369, 15082, 40, 5942, 11, 18860, 429, 279, 3832, 4925, 374, 518, 279, 19246, 315, 15235, 323, 15082, 40, 624, 256, 481, 3070, 2008, 19586, 12478, 320, 18816, 3126, 32295, 25, 576, 2197, 38919, 13550, 3126, 11, 264, 4938, 10350, 1849, 6188, 311, 2621, 279, 9481, 315, 1482, 4938, 6240, 304, 444, 10994, 82, 13, 1084, 16555, 279, 6200, 19128, 11, 2884, 11, 323, 16928, 315, 13550, 3126, 382]\n",
      "    Text: \"This excerpt is from a research paper or technical report, likely focused on the development of a new memory management system for large language models (LLMs) and artificial general intelligence (AGI) systems. The document's subject matter is the design and implementation of a memory operating system (MemOS) that aims to address the limitations of current memory management in LLMs.\\n\\n### Reasoning:\\n\\n1. **Document Type**:\\n   - **Research Paper**: The document includes detailed descriptions of existing research, categorizations, and a proposed solution. It also contains technical terms and concepts, which are typical of research papers.\\n   - **Technical Report**: The document provides a comprehensive overview of the problem, existing solutions, and a proposed solution, which is characteristic of technical reports.\\n\\n2. **Subject Matter**:\\n   - **Memory Management in LLMs**: The document discusses the limitations of current memory management in LLMs, such as the lack of well-defined memory management systems, and the need for a more efficient and flexible memory system.\\n   - **Artificial General Intelligence (AGI)**: The document mentions AGI and how memory management is crucial for AGI systems, indicating that the subject matter is at the intersection of AI and AGI.\\n   - **Proposed Solution (MemOS)**: The document introduces MemOS, a memory operating system designed to address the limitations of current memory management in LLMs. It describes the core philosophy, design, and capabilities of MemOS.\\n\\n\"\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 3766, 315, 264, 501, 4938, 6240, 1849, 369, 3460, 4128, 4119, 320, 4086, 21634, 8, 323, 20443, 4586, 11229, 320, 1890, 40, 568, 576, 2197, 594, 3832, 4925, 374, 279, 2884, 323, 8129, 315, 264, 4938, 10350, 1849, 320, 18816, 3126, 8, 429, 21538, 311, 2621, 279, 9481, 315, 1482, 4938, 6240, 304, 444, 10994, 82, 323, 15082, 40, 5942, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 7524, 28596, 323, 15042, 334, 510, 256, 481, 576, 2197, 12033, 448, 11160, 320, 277, 55, 344, 12816, 11, 2265, 11, 12014, 11, 11938, 804, 8, 14260, 315, 14250, 15689, 624, 256, 481, 1084, 5646, 458, 8115, 11, 16800, 11, 323, 11682, 14158, 320, 68, 1302, 2572, 220, 17, 13, 16, 11, 220, 17, 13, 17, 11, 220, 17, 13, 18, 11, 220, 17, 13, 19, 8, 429, 525, 28583, 315, 32930, 10916, 6682, 476, 3412, 15689, 382, 17, 13, 3070, 2762, 323, 45945, 2449, 334, 510, 256, 481, 576, 1467, 34334, 18940, 1075, 330, 34253, 11434, 26874, 320, 4086, 21634, 35393, 330, 9286, 16488, 3251, 21392, 320, 1890, 40, 35393, 330, 17269, 6240, 1335, 323, 330, 3296, 1095, 1849, 320, 3126, 8, 2884, 16170, 1189, 4220, 525, 10916, 3793, 16626, 1483, 304, 279, 2070, 315, 20443, 11229, 323, 6366, 8038, 624, 256, 481, 576, 2197, 33845, 3151, 3412, 4278, 323, 4119, 320, 68, 1302, 2572, 479, 2828, 12, 17, 11, 479, 2828, 12, 18, 11, 52486, 11, 328, 9188, 11, 13550, 15, 8, 323, 34334, 862, 4938, 23783, 11, 18860, 264, 5538, 10916, 5244, 382, 18, 13, 3070, 74033, 323, 54193, 334, 510, 256, 481, 576, 16800, 323, 14158, 9355, 21101, 279, 1184, 369, 264, 501, 4938, 6240, 1849, 320, 18816, 3126, 8, 311]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and application of a new memory management system for large language models (LLMs) and artificial general intelligence (AGI). The document\\'s subject matter is the design and implementation of a memory operating system (MemOS) that aims to address the limitations of current memory management in LLMs and AGI systems.\\n\\n### Reasoning:\\n\\n1. **Document Structure and Format**:\\n   - The document begins with metadata (arXiv identifier, title, authors, affiliations) typical of academic papers.\\n   - It includes an abstract, introduction, and detailed sections (e.g., 2.1, 2.2, 2.3, 2.4) that are characteristic of structured technical reports or research papers.\\n\\n2. **Content and Terminology**:\\n   - The text discusses concepts like \"Large Language Models (LLMs),\" \"Artificial General Intelligence (AGI),\" \"memory management,\" and \"operating system (OS) design principles.\" These are technical terms commonly used in the field of artificial intelligence and computer science.\\n   - The document mentions specific research works and models (e.g., GPT-2, GPT-3, CTRL, SLayer, Mem0) and discusses their memory mechanisms, indicating a deep technical focus.\\n\\n3. **Purpose and Goals**:\\n   - The introduction and sections clearly outline the need for a new memory management system (MemOS) to'\n",
      "    ✗ DIFFERENT\n",
      "  bs=3:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 315, 264, 501, 4938, 6240, 1849, 369, 3460, 4128, 4119, 320, 4086, 21634, 8, 323, 20443, 4586, 11229, 320, 1890, 40, 8, 5942, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 12033, 448, 458, 796, 55, 344, 12816, 320, 277, 55, 344, 25, 17, 20, 15, 22, 13, 15, 18, 22, 17, 19, 85, 16, 701, 892, 374, 264, 855, 1350, 3538, 369, 12344, 15689, 13, 1096, 13230, 432, 374, 264, 3412, 5567, 476, 10916, 1895, 13, 576, 9362, 315, 458, 8115, 11, 16800, 11, 323, 11682, 14158, 320, 68, 1302, 2572, 330, 17, 13850, 304, 20286, 11434, 26874, 899, 4623, 11554, 419, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2197, 34334, 279, 9481, 315, 1482, 4938, 6240, 5942, 304, 444, 10994, 82, 323, 54625, 264, 501, 1849, 2598, 13550, 3126, 320, 10642, 35133, 739, 568, 1084, 1594, 2342, 1119, 279, 11513, 315, 18150, 4938, 304, 444, 10994, 82, 11, 1741, 438, 1293, 63633, 32711, 11, 76875, 4345, 2022, 11, 323, 6540, 28137, 13, 576, 2197, 1083, 49629, 279, 1184, 369, 264, 4938, 64424, 1849, 429, 646, 3705, 97782, 6540, 3941, 2155, 35915, 28405, 323, 8173, 382, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5707, 10916, 3565, 911, 279, 1482, 1584, 315, 4938, 6240, 304, 444, 10994, 82, 11, 2670, 5257, 4494, 315, 4938, 320, 68, 1302, 2572, 5733, 4938, 11, 15099, 4938, 11, 60847, 4938, 8, 323, 6350, 19827, 1075, 19470, 831, 61635, 26980, 23470, 320, 49, 1890, 568, 1084, 1083, 34334, 279, 1184, 369, 264, 803, 36438, 323, 14071, 4938, 6240, 1849, 11, 892, 374, 279, 6200, 315, 279, 10981, 13550, 3126, 382, 19, 13, 3070, 27428, 794, 1134, 95518, 576]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development of a new memory management system for large language models (LLMs) and artificial general intelligence (AGI) systems. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document begins with an arXiv identifier (arXiv:2507.03724v1), which is a preprint server for scientific papers. This suggests it is a research paper or technical report. The presence of an abstract, introduction, and detailed sections (e.g., \"2 Memory in Large Language Models\") further supports this.\\n\\n2. **Subject Matter**: The document discusses the limitations of current memory management systems in LLMs and proposes a new system called MemOS (Memory Operating System). It delves into the challenges of managing memory in LLMs, such as long-context reasoning, continual personalization, and knowledge consistency. The document also outlines the need for a memory-centric system that can handle heterogeneous knowledge across different temporal scales and sources.\\n\\n3. **Technical Details**: The document provides technical details about the current state of memory management in LLMs, including various types of memory (e.g., parameter memory, activation memory, plaintext memory) and existing approaches like Retrieval-Augmented Generation (RAG). It also discusses the need for a more systematic and controlled memory management system, which is the core of the proposed MemOS.\\n\\n4. **Research Stages**: The'\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 315, 264, 501, 4938, 6240, 1849, 369, 3460, 4128, 4119, 320, 4086, 21634, 8, 323, 20443, 4586, 11229, 320, 1890, 40, 8, 5942, 13, 576, 2197, 34334, 279, 1482, 9481, 315, 4938, 6240, 304, 444, 10994, 82, 323, 38919, 264, 501, 1849, 2598, 13550, 3126, 11, 892, 21538, 311, 2621, 1493, 9481, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 7524, 3990, 334, 510, 256, 481, 3070, 27428, 17862, 95518, 576, 2197, 5646, 11682, 10916, 27787, 11, 31787, 48025, 11, 323, 264, 2797, 3412, 16538, 13, 1084, 1083, 15057, 1008, 4278, 323, 5707, 264, 32930, 21101, 11, 892, 525, 14260, 4419, 315, 264, 3412, 5567, 624, 256, 481, 3070, 62226, 8259, 95518, 576, 2197, 374, 32930, 304, 264, 1616, 429, 432, 1410, 387, 264, 10916, 1895, 11, 44193, 279, 4401, 315, 264, 501, 1849, 323, 1181, 16928, 382, 17, 13, 3070, 13019, 33364, 334, 510, 256, 481, 3070, 10642, 9551, 304, 444, 10994, 82, 95518, 576, 2197, 23497, 389, 279, 4938, 6240, 11513, 16601, 553, 444, 10994, 82, 323, 54625, 264, 501, 1849, 11, 13550, 3126, 11, 311, 2621, 1493, 11513, 624, 256, 481, 3070, 9286, 16488, 3251, 21392, 320, 1890, 40, 32295, 25, 576, 2197, 33845, 15082, 40, 323, 34334, 1246, 279, 10981, 1849, 646, 1824, 279, 4401, 315, 15082, 40, 5942, 624, 256, 481, 3070, 34253, 11434, 26874, 320, 4086, 21634, 32295, 25, 576, 2197, 41717, 34334, 279, 1482, 1584, 323, 9481, 315, 444, 10994, 82, 11, 2670, 862, 4938, 6240, 4714, 382, 18, 13, 3070, 1592, 20725, 334, 510, 256, 481, 3070, 5405, 28008, 804, 95518, 576, 2197, 21314, 279, 9481, 315, 6350, 4938, 6240, 5942, 304, 444, 10994, 82, 11, 1741, 438, 6853, 315, 1632, 38717, 4938, 6240, 11, 2805]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development of a new memory management system for large language models (LLMs) and artificial general intelligence (AGI) systems. The document discusses the current limitations of memory management in LLMs and introduces a new system called MemOS, which aims to address these limitations.\\n\\n### Reasoning:\\n\\n1. **Document Type**:\\n   - **Research Paper**: The document includes detailed technical descriptions, theoretical frameworks, and a clear research objective. It also references other works and provides a structured outline, which are typical features of a research paper.\\n   - **Technical Report**: The document is structured in a way that it could be a technical report, detailing the development of a new system and its capabilities.\\n\\n2. **Subject Matter**:\\n   - **Memory Management in LLMs**: The document focuses on the memory management challenges faced by LLMs and proposes a new system, MemOS, to address these challenges.\\n   - **Artificial General Intelligence (AGI)**: The document mentions AGI and discusses how the proposed system can support the development of AGI systems.\\n   - **Large Language Models (LLMs)**: The document extensively discusses the current state and limitations of LLMs, including their memory management issues.\\n\\n3. **Key Points**:\\n   - **Current Limitations**: The document highlights the limitations of existing memory management systems in LLMs, such as lack of well-defined memory management, short'\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 3766, 315, 264, 501, 4938, 6240, 1849, 369, 3460, 4128, 4119, 320, 4086, 21634, 8, 323, 20443, 4586, 11229, 320, 1890, 40, 568, 576, 2197, 594, 3832, 4925, 374, 279, 2884, 323, 8129, 315, 264, 4938, 10350, 1849, 320, 18816, 3126, 8, 429, 21538, 311, 2621, 279, 9481, 315, 1482, 4938, 6240, 304, 444, 10994, 82, 323, 15082, 40, 5942, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 7524, 28596, 323, 15042, 334, 510, 256, 481, 576, 2197, 12033, 448, 11160, 320, 277, 55, 344, 12816, 11, 2265, 11, 12014, 11, 11938, 804, 8, 14260, 315, 14250, 15689, 624, 256, 481, 1084, 5646, 458, 8115, 11, 16800, 11, 323, 11682, 14158, 320, 68, 1302, 2572, 220, 17, 13, 16, 11, 220, 17, 13, 17, 11, 220, 17, 13, 18, 11, 220, 17, 13, 19, 8, 429, 525, 28583, 315, 32930, 10916, 6682, 476, 3412, 15689, 382, 17, 13, 3070, 2762, 323, 45945, 2449, 334, 510, 256, 481, 576, 1467, 34334, 18940, 1075, 330, 34253, 11434, 26874, 320, 4086, 21634, 35393, 330, 9286, 16488, 3251, 21392, 320, 1890, 40, 35393, 330, 17269, 6240, 1335, 323, 330, 3296, 1095, 1849, 320, 3126, 8, 2884, 16170, 1189, 4220, 525, 10916, 3793, 16626, 1483, 304, 279, 2070, 315, 20443, 11229, 323, 6366, 8038, 624, 256, 481, 576, 2197, 33845, 3151, 3412, 4278, 323, 4119, 320, 68, 1302, 2572, 479, 2828, 12, 17, 11, 479, 2828, 12, 18, 11, 52486, 11, 328, 9188, 11, 13550, 15, 8, 323, 34334, 862, 4938, 23783, 11, 18860, 264, 5538, 10916, 5244, 382, 18, 13, 3070, 74033, 323, 54193, 334, 510, 256, 481, 576, 16800, 323, 14158, 9355, 21101, 279, 1184, 369, 264, 501, 4938, 6240, 1849, 320, 18816, 3126, 8, 311]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and application of a new memory management system for large language models (LLMs) and artificial general intelligence (AGI). The document\\'s subject matter is the design and implementation of a memory operating system (MemOS) that aims to address the limitations of current memory management in LLMs and AGI systems.\\n\\n### Reasoning:\\n\\n1. **Document Structure and Format**:\\n   - The document begins with metadata (arXiv identifier, title, authors, affiliations) typical of academic papers.\\n   - It includes an abstract, introduction, and detailed sections (e.g., 2.1, 2.2, 2.3, 2.4) that are characteristic of structured technical reports or research papers.\\n\\n2. **Content and Terminology**:\\n   - The text discusses concepts like \"Large Language Models (LLMs),\" \"Artificial General Intelligence (AGI),\" \"memory management,\" and \"operating system (OS) design principles.\" These are technical terms commonly used in the field of artificial intelligence and computer science.\\n   - The document mentions specific research works and models (e.g., GPT-2, GPT-3, CTRL, SLayer, Mem0) and discusses their memory mechanisms, indicating a deep technical focus.\\n\\n3. **Purpose and Goals**:\\n   - The introduction and sections clearly outline the need for a new memory management system (MemOS) to'\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 315, 264, 501, 4938, 6240, 1849, 369, 3460, 4128, 4119, 320, 4086, 21634, 8, 323, 20443, 4586, 11229, 320, 1890, 40, 8, 5942, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 12033, 448, 458, 796, 55, 344, 12816, 320, 277, 55, 344, 25, 17, 20, 15, 22, 13, 15, 18, 22, 17, 19, 85, 16, 701, 892, 374, 264, 855, 1350, 3538, 369, 12344, 15689, 13, 1096, 13230, 432, 374, 264, 3412, 5567, 476, 10916, 1895, 13, 576, 9362, 315, 458, 8115, 11, 16800, 11, 323, 11682, 14158, 320, 68, 1302, 2572, 330, 17, 13850, 304, 20286, 11434, 26874, 899, 4623, 11554, 419, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2197, 34334, 279, 9481, 315, 1482, 4938, 6240, 304, 444, 10994, 82, 323, 54625, 264, 501, 1849, 2598, 13550, 3126, 320, 10642, 35133, 739, 8, 311, 2621, 1493, 9481, 13, 1084, 23497, 389, 1246, 4938, 6240, 646, 18379, 279, 16928, 315, 444, 10994, 82, 11, 3259, 1105, 803, 14452, 369, 15082, 40, 13, 5309, 3501, 2924, 510, 256, 481, 3070, 10642, 9551, 304, 444, 10994, 82, 95518, 576, 2197, 34334, 279, 1482, 1584, 315, 4938, 6240, 304, 444, 10994, 82, 11, 2670, 17995, 323, 11464, 4938, 4494, 11, 323, 1246, 1493, 7802, 279, 1614, 594, 5068, 323, 10515, 2897, 624, 256, 481, 3070, 1143, 42370, 95518, 1084, 49629, 3807, 11513, 304, 4938, 6240, 11, 1741, 438, 1293, 30508, 24036, 33479, 11, 69717, 311, 6540, 15379, 11, 4345, 2022, 11, 323, 5312, 54025, 4938, 11906, 624, 256, 481, 3070, 2008, 19586, 12478, 95518, 13550, 3126, 374, 11523, 438, 264, 6291, 311, 1493, 11513, 11, 26625, 4938, 438, 264, 1849, 11591, 5101, 429, 646, 387, 20975]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development of a new memory management system for large language models (LLMs) and artificial general intelligence (AGI) systems. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document begins with an arXiv identifier (arXiv:2507.03724v1), which is a preprint server for scientific papers. This suggests it is a research paper or technical report. The presence of an abstract, introduction, and detailed sections (e.g., \"2 Memory in Large Language Models\") further supports this.\\n\\n2. **Subject Matter**: The document discusses the limitations of current memory management in LLMs and proposes a new system called MemOS (Memory Operating System) to address these limitations. It focuses on how memory management can enhance the capabilities of LLMs, making them more suitable for AGI. Key points include:\\n   - **Memory Management in LLMs**: The document discusses the current state of memory management in LLMs, including implicit and explicit memory types, and how these affect the model\\'s performance and adaptability.\\n   - **Challenges**: It outlines several challenges in memory management, such as long-range dependency modeling, adapting to knowledge evolution, personalization, and cross-platform memory migration.\\n   - **Proposed Solution**: MemOS is introduced as a solution to these challenges, treating memory as a system-level resource that can be explicitly'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 315, 264, 501, 4938, 6240, 1849, 369, 3460, 4128, 4119, 320, 4086, 21634, 8, 323, 20443, 4586, 11229, 320, 1890, 40, 8, 5942, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 12033, 448, 458, 796, 55, 344, 12816, 320, 277, 55, 344, 25, 17, 20, 15, 22, 13, 15, 18, 22, 17, 19, 85, 16, 701, 892, 374, 264, 855, 1350, 3538, 369, 12344, 15689, 13, 1096, 13230, 432, 374, 264, 3412, 5567, 476, 10916, 1895, 13, 576, 9362, 315, 458, 8115, 11, 16800, 11, 323, 11682, 14158, 320, 68, 1302, 2572, 330, 17, 13850, 304, 20286, 11434, 26874, 899, 4623, 11554, 419, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2197, 34334, 279, 9481, 315, 1482, 4938, 6240, 304, 444, 10994, 82, 323, 54625, 264, 501, 1849, 2598, 13550, 3126, 320, 10642, 35133, 739, 8, 311, 2621, 1493, 9481, 13, 1084, 23497, 389, 1246, 4938, 6240, 646, 18379, 279, 16928, 315, 444, 10994, 82, 11, 3259, 1105, 803, 14452, 369, 15082, 40, 13, 5309, 3501, 2924, 510, 256, 481, 3070, 10642, 9551, 304, 444, 10994, 82, 95518, 576, 2197, 34334, 279, 1482, 1584, 315, 4938, 6240, 304, 444, 10994, 82, 11, 2670, 17995, 323, 11464, 4938, 4494, 11, 323, 1246, 1493, 7802, 279, 1614, 594, 5068, 323, 10515, 2897, 624, 256, 481, 3070, 1143, 42370, 95518, 1084, 49629, 3807, 11513, 304, 4938, 6240, 11, 1741, 438, 1293, 30508, 24036, 33479, 11, 69717, 311, 6540, 15379, 11, 4345, 2022, 11, 323, 5312, 54025, 4938, 11906, 624, 256, 481, 3070, 2008, 19586, 12478, 95518, 13550, 3126, 374, 11523, 438, 264, 6291, 311, 1493, 11513, 11, 26625, 4938, 438, 264, 1849, 11591, 5101, 429, 646, 387, 20975]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development of a new memory management system for large language models (LLMs) and artificial general intelligence (AGI) systems. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document begins with an arXiv identifier (arXiv:2507.03724v1), which is a preprint server for scientific papers. This suggests it is a research paper or technical report. The presence of an abstract, introduction, and detailed sections (e.g., \"2 Memory in Large Language Models\") further supports this.\\n\\n2. **Subject Matter**: The document discusses the limitations of current memory management in LLMs and proposes a new system called MemOS (Memory Operating System) to address these limitations. It focuses on how memory management can enhance the capabilities of LLMs, making them more suitable for AGI. Key points include:\\n   - **Memory Management in LLMs**: The document discusses the current state of memory management in LLMs, including implicit and explicit memory types, and how these affect the model\\'s performance and adaptability.\\n   - **Challenges**: It outlines several challenges in memory management, such as long-range dependency modeling, adapting to knowledge evolution, personalization, and cross-platform memory migration.\\n   - **Proposed Solution**: MemOS is introduced as a solution to these challenges, treating memory as a system-level resource that can be explicitly'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 3766, 315, 264, 501, 4938, 6240, 1849, 369, 3460, 4128, 4119, 320, 4086, 21634, 8, 323, 20443, 4586, 11229, 320, 1890, 40, 568, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 12033, 448, 458, 796, 55, 344, 5785, 11, 892, 374, 264, 855, 1350, 3538, 369, 12344, 15689, 13, 1096, 13230, 432, 374, 264, 3412, 5567, 476, 10916, 1895, 13, 576, 9362, 315, 458, 8115, 11, 16800, 11, 323, 11682, 14158, 389, 4938, 6240, 304, 444, 10994, 82, 4623, 11554, 419, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2197, 34334, 279, 9481, 315, 1482, 4938, 6240, 5942, 304, 444, 10994, 82, 323, 54625, 264, 501, 1849, 2598, 13550, 3126, 320, 10642, 35133, 739, 568, 1084, 1594, 2342, 1119, 279, 11513, 315, 18150, 4938, 304, 444, 10994, 82, 11, 1741, 438, 1293, 63633, 32711, 11, 76875, 4345, 2022, 11, 323, 6540, 28137, 13, 576, 2197, 1083, 49629, 279, 4419, 323, 7567, 315, 13550, 3126, 11, 2670, 1181, 5726, 311, 10091, 4938, 438, 264, 1849, 11591, 5101, 11, 8241, 683, 1100, 2897, 11, 12188, 487, 11, 323, 3637, 35315, 2897, 382, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5707, 10916, 3565, 911, 279, 1482, 1584, 315, 4938, 6240, 304, 444, 10994, 82, 11, 22049, 4849, 2155, 4494, 315, 4938, 320, 68, 1302, 2572, 17995, 323, 11464, 11, 2805, 9663, 323, 1293, 9663, 8, 323, 24392, 5257, 23783, 369, 18150, 1493, 18923, 13, 1084, 1083, 49629, 279, 17628, 315, 3412, 304, 419, 3082, 11, 504, 279, 2856, 7271, 323, 26403, 311, 279, 4401, 315, 3738, 12681, 4938, 323, 5392, 5980, 4938, 6240, 382, 19, 13, 3070, 24206, 30441, 95518, 576, 2197, 44437, 448, 264, 11129, 369]\n",
      "    Text: \"This excerpt is from a research paper or technical report, likely focused on the development and application of a new memory management system for large language models (LLMs) and artificial general intelligence (AGI). Here's the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document begins with an arXiv reference, which is a preprint server for scientific papers. This suggests it is a research paper or technical report. The presence of an abstract, introduction, and detailed sections on memory management in LLMs further supports this.\\n\\n2. **Subject Matter**: The document discusses the limitations of current memory management systems in LLMs and proposes a new system called MemOS (Memory Operating System). It delves into the challenges of managing memory in LLMs, such as long-context reasoning, continual personalization, and knowledge consistency. The document also outlines the features and benefits of MemOS, including its ability to manage memory as a system-level resource, providing controllability, plasticity, and evolvability.\\n\\n3. **Technical Details**: The document provides technical details about the current state of memory management in LLMs, categorizing different types of memory (e.g., implicit and explicit, short-term and long-term) and discussing various mechanisms for managing these memories. It also outlines the stages of research in this area, from the initial definition and exploration to the development of human-like memory and tool-based memory management.\\n\\n4. **Future Vision**: The document concludes with a vision for\"\n",
      "    ✗ DIFFERENT\n",
      "  bs=17:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 3766, 315, 264, 501, 4938, 6240, 1849, 369, 3460, 4128, 4119, 320, 4086, 21634, 8, 323, 20443, 4586, 11229, 320, 1890, 40, 568, 576, 2197, 594, 3832, 4925, 374, 279, 2884, 323, 8129, 315, 264, 4938, 10350, 1849, 320, 18816, 3126, 8, 429, 21538, 311, 2621, 279, 9481, 315, 1482, 4938, 6240, 304, 444, 10994, 82, 323, 15082, 40, 5942, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 7524, 28596, 323, 15042, 334, 510, 256, 481, 576, 2197, 12033, 448, 11160, 320, 277, 55, 344, 12816, 11, 2265, 11, 12014, 11, 11938, 804, 8, 14260, 315, 14250, 15689, 624, 256, 481, 1084, 5646, 458, 8115, 11, 16800, 11, 323, 11682, 14158, 320, 68, 1302, 2572, 220, 17, 13, 16, 11, 220, 17, 13, 17, 11, 220, 17, 13, 18, 11, 220, 17, 13, 19, 8, 429, 525, 28583, 315, 32930, 10916, 6682, 476, 3412, 15689, 382, 17, 13, 3070, 2762, 323, 45945, 2449, 334, 510, 256, 481, 576, 1467, 34334, 18940, 1075, 330, 34253, 11434, 26874, 320, 4086, 21634, 35393, 330, 9286, 16488, 3251, 21392, 320, 1890, 40, 35393, 330, 17269, 6240, 1335, 323, 330, 3296, 1095, 1849, 320, 3126, 8, 2884, 16170, 1189, 4220, 525, 10916, 3793, 16626, 1483, 304, 279, 2070, 315, 20443, 11229, 323, 6366, 8038, 624, 256, 481, 576, 2197, 33845, 3151, 3412, 4278, 323, 4119, 320, 68, 1302, 2572, 479, 2828, 12, 17, 11, 479, 2828, 12, 18, 11, 52486, 11, 328, 9188, 11, 13550, 15, 8, 323, 34334, 862, 4938, 23783, 11, 18860, 264, 5538, 10916, 5244, 382, 18, 13, 3070, 74033, 323, 54193, 334, 510, 256, 481, 576, 16800, 323, 14158, 9355, 21101, 279, 1184, 369, 264, 501, 4938, 6240, 1849, 320, 18816, 3126, 8, 311]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and application of a new memory management system for large language models (LLMs) and artificial general intelligence (AGI). The document\\'s subject matter is the design and implementation of a memory operating system (MemOS) that aims to address the limitations of current memory management in LLMs and AGI systems.\\n\\n### Reasoning:\\n\\n1. **Document Structure and Format**:\\n   - The document begins with metadata (arXiv identifier, title, authors, affiliations) typical of academic papers.\\n   - It includes an abstract, introduction, and detailed sections (e.g., 2.1, 2.2, 2.3, 2.4) that are characteristic of structured technical reports or research papers.\\n\\n2. **Content and Terminology**:\\n   - The text discusses concepts like \"Large Language Models (LLMs),\" \"Artificial General Intelligence (AGI),\" \"memory management,\" and \"operating system (OS) design principles.\" These are technical terms commonly used in the field of artificial intelligence and computer science.\\n   - The document mentions specific research works and models (e.g., GPT-2, GPT-3, CTRL, SLayer, Mem0) and discusses their memory mechanisms, indicating a deep technical focus.\\n\\n3. **Purpose and Goals**:\\n   - The introduction and sections clearly outline the need for a new memory management system (MemOS) to'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "--- Token consistency for ref_1 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 5567, 476, 10916, 1895, 10735, 389, 82687, 304, 20443, 11229, 11, 7945, 304, 279, 5671, 315, 4128, 4119, 323, 5662, 6832, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 5610, 264, 1140, 315, 12014, 11, 15311, 315, 15689, 11, 323, 9814, 27787, 315, 3412, 13347, 13, 1096, 3561, 374, 14260, 369, 264, 93698, 476, 5785, 3772, 315, 264, 3412, 5567, 382, 17, 13, 3070, 27428, 40894, 95518, 576, 15689, 10007, 3421, 264, 2088, 315, 13347, 2670, 510, 256, 481, 96378, 8314, 4128, 4119, 448, 10997, 6832, 198, 256, 481, 42915, 63530, 6832, 16928, 315, 4128, 4119, 198, 256, 481, 41164, 4686, 95757, 4586, 2022, 304, 5662, 6832, 198, 256, 481, 26759, 287, 323, 9115, 16928, 315, 15235, 4119, 198, 256, 481, 31925, 1582, 66084, 12538, 198, 256, 481, 71794, 3233, 954, 52498, 323, 862, 8357, 198, 256, 481, 20286, 4128, 4119, 323, 862, 32711, 16928, 198, 256, 481, 27604, 66084, 315, 6540, 504, 8131, 4119, 311, 9155, 6174, 198, 256, 481, 34355, 32189, 323, 33479, 304, 4128, 4119, 271, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5646, 10916, 3565, 1741, 438, 1614, 12282, 11, 4862, 29425, 11, 323, 3151, 4862, 5029, 11, 892, 525, 14260, 304, 3412, 15689, 382, 19, 13, 3070, 23877, 1216, 95518, 576, 9362, 315, 8737, 1216, 448, 22000, 3565, 323, 5107, 3059, 13230, 264, 15817, 3412, 2197, 382, 20, 13, 3070, 43842, 323, 2350, 26565, 95518, 7718, 3793, 323, 31747, 1075, 330, 11528, 4119, 1335, 330, 4701, 261, 1335, 330, 53103, 1335, 330, 89053, 1582, 66084, 1335, 330, 98223, 3233, 954, 52498, 1335, 323, 330, 68487, 32189, 1, 525, 13814, 1483, 11, 18860, 264, 5244, 389, 10847, 5662, 6832, 323, 15235]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research paper or technical report focused on advancements in artificial intelligence, particularly in the areas of language models and machine learning. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Content and Structure**: The document contains a list of authors, titles of papers, and brief descriptions of research topics. This format is typical for a bibliography or reference section of a research paper.\\n\\n2. **Research Topics**: The papers listed cover a range of topics including:\\n   - Decomposing language models with dictionary learning\\n   - Few-shot learning capabilities of language models\\n   - Weak-to-strong generalization in machine learning\\n   - Reasoning and planning capabilities of AI models\\n   - Knowledge distillation techniques\\n   - Sparse autoencoders and their applications\\n   - Large language models and their reasoning capabilities\\n   - Distillation of knowledge from larger models to smaller ones\\n   - Concept extraction and modeling in language models\\n\\n3. **Technical Details**: The document includes technical details such as model sizes, training datasets, and specific training parameters, which are typical in research papers.\\n\\n4. **Appendices**: The presence of appendices with experimental details and additional results suggests a comprehensive research document.\\n\\n5. **Keywords and Phrases**: Common terms and phrases like \"language models,\" \"transformer,\" \"attention,\" \"knowledge distillation,\" \"sparse autoencoders,\" and \"concept extraction\" are frequently used, indicating a focus on advanced machine learning and AI'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 5567, 476, 264, 10916, 1895, 10735, 389, 82687, 304, 20443, 11229, 11, 7945, 304, 279, 5671, 315, 4128, 4119, 323, 5662, 6832, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 5610, 264, 1140, 315, 12014, 11, 8110, 553, 15057, 311, 5257, 3412, 15689, 323, 10916, 6682, 13, 1096, 5944, 374, 14260, 315, 14250, 476, 10916, 9293, 11, 1741, 438, 3412, 15689, 476, 9816, 28307, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 15057, 3421, 264, 6884, 2088, 315, 13347, 5435, 311, 15235, 11, 2670, 4128, 4119, 11, 5662, 6832, 12538, 11, 323, 3151, 3412, 14613, 13, 1752, 3110, 11, 15057, 2924, 4278, 389, 4128, 1614, 4862, 11, 6540, 1582, 66084, 11, 33444, 3233, 954, 52498, 11, 323, 7286, 32189, 13, 1096, 14807, 429, 279, 2197, 374, 4363, 10735, 389, 279, 10916, 13566, 315, 15235, 3412, 323, 4401, 382, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5646, 11682, 10916, 1995, 11, 1741, 438, 22000, 3565, 11, 4862, 5029, 11, 323, 5068, 16734, 13, 1096, 2188, 315, 7716, 374, 28583, 315, 3412, 15689, 476, 10916, 6682, 382, 19, 13, 3070, 23877, 1216, 95518, 576, 9362, 315, 8737, 1216, 448, 22000, 3565, 323, 5107, 3059, 4623, 11554, 279, 4522, 429, 419, 374, 264, 10916, 2197, 11, 4363, 264, 3412, 5567, 476, 264, 10916, 1895, 382, 20, 13, 3070, 43842, 323, 75772, 95518, 576, 2197, 33845, 3151, 18940, 323, 12538, 1741, 438, 330, 1645, 960, 76, 8159, 487, 1335, 330, 68487, 32189, 1335, 330, 89053, 1582, 66084, 1335, 323, 330, 98223, 3233, 954, 52498, 1189, 4220, 525, 10916, 3793, 16626, 1483, 304, 15235, 323, 5662, 6832, 3412, 382, 21, 13, 3070, 23877, 941, 8883, 95518, 576, 8737, 1216, 3410]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research paper or a technical report focused on advancements in artificial intelligence, particularly in the areas of language models and machine learning. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Content and Structure**: The document contains a list of authors, followed by references to various research papers and technical reports. This structure is typical of academic or technical documents, such as research papers or conference proceedings.\\n\\n2. **Subject Matter**: The references cover a wide range of topics related to AI, including language models, machine learning techniques, and specific research findings. For example, references include works on language model training, knowledge distillation, sparse autoencoders, and concept extraction. This indicates that the document is likely focused on the technical aspects of AI research and development.\\n\\n3. **Technical Details**: The document includes detailed technical information, such as experimental details, training parameters, and performance metrics. This level of detail is characteristic of research papers or technical reports.\\n\\n4. **Appendices**: The presence of appendices with experimental details and additional results further supports the idea that this is a technical document, likely a research paper or a technical report.\\n\\n5. **Keywords and Concepts**: The document mentions specific concepts and techniques such as \"monosemanticity,\" \"concept extraction,\" \"knowledge distillation,\" and \"sparse autoencoders.\" These are technical terms commonly used in AI and machine learning research.\\n\\n6. **Appendix Content**: The appendices provide'\n",
      "    ✗ DIFFERENT\n",
      "  bs=3:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 5567, 476, 264, 10916, 1895, 10735, 389, 82687, 304, 20443, 11229, 11, 7945, 304, 279, 5671, 315, 4128, 4119, 323, 5662, 6832, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 5610, 264, 1140, 315, 12014, 11, 15311, 315, 15689, 11, 323, 9814, 27787, 315, 3412, 13347, 13, 1096, 3561, 374, 14260, 369, 14250, 15689, 476, 10916, 6682, 382, 17, 13, 3070, 27428, 40894, 95518, 576, 15689, 10007, 3421, 264, 2088, 315, 13347, 2670, 510, 256, 481, 96378, 8314, 4128, 4119, 448, 10997, 6832, 198, 256, 481, 42915, 63530, 6832, 16928, 315, 4128, 4119, 198, 256, 481, 41164, 4686, 95757, 4586, 2022, 304, 5662, 6832, 198, 256, 481, 26759, 287, 323, 9115, 16928, 315, 15235, 4119, 198, 256, 481, 31925, 1582, 66084, 12538, 198, 256, 481, 71794, 3233, 954, 52498, 323, 862, 8357, 198, 256, 481, 20286, 4128, 4119, 323, 862, 32711, 16928, 198, 256, 481, 27604, 66084, 315, 6540, 504, 8131, 4119, 311, 9155, 6174, 198, 256, 481, 34355, 32189, 323, 33479, 304, 4128, 4119, 271, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5646, 10916, 3565, 1741, 438, 1614, 77235, 11, 4862, 5029, 11, 323, 22000, 3059, 11, 892, 525, 4185, 304, 3412, 15689, 382, 19, 13, 3070, 23877, 1216, 95518, 576, 9362, 315, 8737, 1216, 448, 22000, 3565, 323, 5107, 3059, 13230, 264, 15817, 10916, 1895, 476, 3412, 5567, 382, 20, 13, 3070, 57890, 323, 87780, 95518, 576, 12014, 525, 36406, 448, 5257, 14336, 11, 2670, 23106, 323, 3412, 11104, 11, 892, 374, 14260, 369, 14250, 3412, 382, 21, 13, 3070, 43842, 323, 75772, 95518, 576, 2197, 33845, 18940, 1075, 330, 1645, 960, 76, 8159, 487, 1335, 330, 68487, 32189, 1335, 330, 89053, 1582]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research paper or a technical report focused on advancements in artificial intelligence, particularly in the areas of language models and machine learning. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Content and Structure**: The document contains a list of authors, titles of papers, and brief descriptions of research topics. This format is typical for academic papers or technical reports.\\n\\n2. **Research Topics**: The papers listed cover a range of topics including:\\n   - Decomposing language models with dictionary learning\\n   - Few-shot learning capabilities of language models\\n   - Weak-to-strong generalization in machine learning\\n   - Reasoning and planning capabilities of AI models\\n   - Knowledge distillation techniques\\n   - Sparse autoencoders and their applications\\n   - Large language models and their reasoning capabilities\\n   - Distillation of knowledge from larger models to smaller ones\\n   - Concept extraction and modeling in language models\\n\\n3. **Technical Details**: The document includes technical details such as model architectures, training parameters, and experimental results, which are common in research papers.\\n\\n4. **Appendices**: The presence of appendices with experimental details and additional results suggests a comprehensive technical report or research paper.\\n\\n5. **Authors and Institutions**: The authors are affiliated with various institutions, including universities and research organizations, which is typical for academic research.\\n\\n6. **Keywords and Concepts**: The document mentions concepts like \"monosemanticity,\" \"concept extraction,\" \"knowledge dist'\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 5567, 476, 264, 10916, 1895, 10735, 389, 82687, 304, 20443, 11229, 11, 7945, 304, 279, 5671, 315, 4128, 4119, 323, 5662, 6832, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 5610, 264, 1140, 315, 12014, 11, 15311, 315, 15689, 11, 323, 9814, 27787, 315, 3412, 13347, 13, 1096, 3561, 374, 14260, 369, 14250, 15689, 476, 10916, 6682, 382, 17, 13, 3070, 27428, 40894, 95518, 576, 15689, 10007, 3421, 264, 2088, 315, 13347, 5435, 311, 15235, 11, 2670, 4128, 4119, 11, 6540, 1582, 66084, 11, 33444, 3233, 954, 52498, 11, 323, 7286, 32189, 13, 4220, 525, 678, 4541, 5671, 315, 3412, 304, 279, 2070, 315, 5662, 6832, 323, 15235, 382, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5646, 10916, 3565, 1741, 438, 1614, 12282, 11, 4862, 29425, 11, 323, 3151, 4862, 5029, 13, 1096, 2188, 315, 7716, 374, 28583, 315, 3412, 15689, 1380, 12014, 3410, 15817, 1995, 911, 862, 5413, 323, 21204, 382, 19, 13, 3070, 840, 49632, 323, 18099, 95518, 576, 2197, 1083, 5646, 14158, 389, 22000, 3565, 323, 3059, 11, 892, 525, 4185, 304, 3412, 15689, 311, 9593, 279, 14613, 323, 19869, 279, 26279, 315, 279, 10981, 5413, 382, 20, 13, 3070, 44576, 928, 1374, 10393, 804, 95518, 2619, 525, 1083, 14158, 429, 3410, 43801, 40841, 11, 1741, 438, 279, 4008, 315, 279, 3539, 7339, 58083, 14762, 323, 1181, 5068, 7707, 311, 1008, 5413, 13, 1096, 374, 14260, 315, 3412, 15689, 429, 9213, 311, 10339, 501, 18940, 323, 862, 8357, 382, 21, 13, 3070, 23877, 1216, 95518, 576, 9362, 315, 8737, 1216, 448, 11682, 1995, 911, 17646, 323, 4862, 5029, 374, 4185, 304, 3412, 15689, 311, 3410, 264, 15817, 8660, 315, 279, 37052, 382]\n",
      "    Text: \"Based on the excerpt provided, this document appears to be from a research paper or a technical report focused on advancements in artificial intelligence, particularly in the areas of language models and machine learning. Here's the reasoning behind this conclusion:\\n\\n1. **Content and Structure**: The document contains a list of authors, titles of papers, and brief descriptions of research topics. This format is typical for academic papers or technical reports.\\n\\n2. **Research Topics**: The papers listed cover a range of topics related to AI, including language models, knowledge distillation, sparse autoencoders, and concept extraction. These are all active areas of research in the field of machine learning and AI.\\n\\n3. **Technical Details**: The document includes technical details such as model sizes, training datasets, and specific training parameters. This level of detail is characteristic of research papers where authors provide comprehensive information about their methods and experiments.\\n\\n4. **Experiments and Results**: The document also includes sections on experimental details and results, which are common in research papers to validate the findings and demonstrate the effectiveness of the proposed methods.\\n\\n5. **Conceptual Explanations**: There are also sections that provide conceptual explanations, such as the description of the CoCoMix technique and its performance compared to other methods. This is typical of research papers that aim to explain new concepts and their applications.\\n\\n6. **Appendices**: The presence of appendices with detailed information about architecture and training parameters is common in research papers to provide a comprehensive understanding of the methodology.\\n\\n\"\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 5567, 476, 264, 10916, 1895, 10735, 389, 82687, 304, 20443, 11229, 11, 7945, 304, 279, 5671, 315, 4128, 4119, 323, 3460, 4128, 4119, 320, 4086, 21634, 568, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 5610, 264, 1140, 315, 12014, 11, 15311, 11, 323, 15057, 311, 5257, 3412, 15689, 323, 10916, 6682, 13, 1096, 5944, 374, 14260, 315, 14250, 476, 10916, 9293, 11, 1741, 438, 3412, 15689, 476, 9816, 28307, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 15057, 3421, 264, 6884, 2088, 315, 13347, 5435, 311, 15235, 11, 2670, 510, 256, 481, 3070, 13806, 26874, 95518, 25694, 15057, 4263, 82687, 304, 4128, 4119, 11, 1741, 438, 330, 13806, 4119, 525, 2421, 63530, 52143, 1, 323, 330, 59684, 1615, 960, 76, 8159, 487, 25, 22826, 287, 17929, 2005, 4419, 504, 86704, 793, 220, 18, 4438, 4711, 10040, 256, 481, 3070, 98491, 8979, 954, 52498, 95518, 2619, 525, 5248, 15057, 311, 33444, 3233, 954, 52498, 11, 892, 525, 1483, 369, 4565, 32189, 323, 12871, 2719, 13951, 304, 5662, 6832, 624, 256, 481, 3070, 44576, 94506, 323, 27604, 66084, 95518, 44078, 1075, 330, 6217, 10352, 76, 25, 31925, 1582, 66084, 369, 855, 85370, 4128, 4119, 1, 323, 330, 23356, 9400, 279, 6540, 304, 264, 29728, 3922, 1, 13216, 279, 5244, 389, 73185, 323, 59408, 22414, 4419, 504, 3460, 4119, 624, 256, 481, 3070, 54615, 16595, 323, 26759, 287, 95518, 44078, 1741, 438, 330, 18837, 8668, 7563, 2450, 49645, 655, 50550, 32711, 304, 3460, 4128, 4119, 1, 323, 330, 38687, 1573, 498, 6468, 25, 16151, 4128, 4119, 448, 18087, 11211, 1, 4190, 279, 26403, 315, 501, 12538, 311, 7269, 279, 32711, 323, 14198, 2897, 315, 15235, 4119, 382, 18, 13]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research paper or a technical report focused on advancements in artificial intelligence, particularly in the areas of language models and large language models (LLMs). Here\\'s the reasoning behind this conclusion:\\n\\n1. **Content and Structure**: The document contains a list of authors, titles, and references to various research papers and technical reports. This structure is typical of academic or technical documents, such as research papers or conference proceedings.\\n\\n2. **Subject Matter**: The references cover a wide range of topics related to AI, including:\\n   - **Language Models**: Several references discuss advancements in language models, such as \"Language models are few-shot learners\" and \"Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet.\"\\n   - **Sparse Autoencoders**: There are multiple references to sparse autoencoders, which are used for feature extraction and dimensionality reduction in machine learning.\\n   - **Concept Extraction and Distillation**: References like \"Miniplm: Knowledge distillation for pre-training language models\" and \"Distilling the knowledge in a neural network\" indicate the focus on refining and extracting meaningful features from large models.\\n   - **Prompt Engineering and Reasoning**: References such as \"Chain-of-thought prompting elicits reasoning in large language models\" and \"Think before you speak: Training language models with pause tokens\" suggest the exploration of new techniques to improve the reasoning and interpretability of AI models.\\n\\n3.'\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 5567, 476, 10916, 1895, 10735, 389, 82687, 304, 20443, 11229, 11, 7945, 304, 279, 5671, 315, 4128, 4119, 323, 5662, 6832, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 5610, 264, 1140, 315, 12014, 11, 15311, 315, 15689, 11, 323, 9814, 27787, 315, 3412, 13347, 13, 1096, 3561, 374, 14260, 369, 14250, 15689, 476, 10916, 6682, 382, 17, 13, 3070, 27428, 40894, 95518, 576, 15689, 10007, 3421, 264, 2088, 315, 13347, 5435, 311, 15235, 11, 2670, 4128, 4119, 11, 6540, 1582, 66084, 11, 33444, 3233, 954, 52498, 11, 323, 7286, 32189, 13, 4220, 525, 678, 4541, 5671, 315, 3412, 304, 279, 2070, 315, 5662, 6832, 323, 5810, 4128, 8692, 382, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5646, 10916, 3565, 1741, 438, 1614, 12282, 11, 4862, 29425, 11, 323, 3151, 22000, 83723, 320, 68, 1302, 2572, 4862, 389, 220, 17, 15, 15, 33, 11211, 11, 1667, 5264, 5981, 1178, 438, 279, 4862, 10337, 568, 1096, 2188, 315, 7716, 374, 28583, 315, 3412, 15689, 382, 19, 13, 3070, 23877, 1216, 95518, 576, 9362, 315, 8737, 1216, 448, 22000, 3565, 323, 5107, 3059, 320, 68, 1302, 2572, 5068, 14849, 2337, 4862, 8, 374, 4185, 304, 3412, 15689, 311, 3410, 15817, 1995, 382, 20, 13, 3070, 43842, 323, 75772, 95518, 576, 2197, 33845, 3151, 18940, 323, 12538, 1741, 438, 330, 1645, 960, 76, 8159, 487, 1335, 330, 68487, 32189, 1335, 330, 89053, 1582, 66084, 1335, 323, 330, 98223, 3233, 954, 52498, 1189, 4220, 525, 10916, 3793, 1483, 304, 279, 2070, 315, 15235, 323, 5662, 6832, 382, 21, 13, 3070, 31712, 95518, 576, 2197, 5646, 15057, 311, 1008, 15689, 323, 10916, 6682, 11, 892, 374, 14260, 304, 14250, 323]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research paper or technical report focused on advancements in artificial intelligence, particularly in the areas of language models and machine learning. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Content and Structure**: The document contains a list of authors, titles of papers, and brief descriptions of research topics. This format is typical for academic papers or technical reports.\\n\\n2. **Research Topics**: The papers listed cover a range of topics related to AI, including language models, knowledge distillation, sparse autoencoders, and concept extraction. These are all active areas of research in the field of machine learning and natural language processing.\\n\\n3. **Technical Details**: The document includes technical details such as model sizes, training datasets, and specific experimental setups (e.g., training on 200B tokens, using OpenWebText as the training dataset). This level of detail is characteristic of research papers.\\n\\n4. **Appendices**: The presence of appendices with experimental details and additional results (e.g., performance tracking during training) is common in research papers to provide comprehensive information.\\n\\n5. **Keywords and Concepts**: The document mentions specific concepts and techniques such as \"monosemanticity,\" \"concept extraction,\" \"knowledge distillation,\" and \"sparse autoencoders.\" These are technical terms used in the field of AI and machine learning.\\n\\n6. **References**: The document includes references to other papers and technical reports, which is typical in academic and'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 5567, 476, 10916, 1895, 10735, 389, 82687, 304, 20443, 11229, 11, 7945, 304, 279, 5671, 315, 4128, 4119, 323, 5662, 6832, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 5610, 264, 1140, 315, 12014, 11, 15311, 315, 15689, 11, 323, 9814, 27787, 315, 3412, 13347, 13, 1096, 3561, 374, 14260, 369, 14250, 15689, 476, 10916, 6682, 382, 17, 13, 3070, 27428, 40894, 95518, 576, 15689, 10007, 3421, 264, 2088, 315, 13347, 5435, 311, 15235, 11, 2670, 4128, 4119, 11, 6540, 1582, 66084, 11, 33444, 3233, 954, 52498, 11, 323, 7286, 32189, 13, 4220, 525, 678, 4541, 5671, 315, 3412, 304, 279, 2070, 315, 5662, 6832, 323, 5810, 4128, 8692, 382, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5646, 10916, 3565, 1741, 438, 1614, 12282, 11, 4862, 29425, 11, 323, 3151, 22000, 83723, 320, 68, 1302, 2572, 4862, 389, 220, 17, 15, 15, 33, 11211, 11, 1667, 5264, 5981, 1178, 438, 279, 4862, 10337, 568, 1096, 2188, 315, 7716, 374, 28583, 315, 3412, 15689, 382, 19, 13, 3070, 23877, 1216, 95518, 576, 9362, 315, 8737, 1216, 448, 22000, 3565, 323, 5107, 3059, 320, 68, 1302, 2572, 5068, 14849, 2337, 4862, 8, 374, 4185, 304, 3412, 15689, 311, 3410, 15817, 1995, 382, 20, 13, 3070, 43842, 323, 75772, 95518, 576, 2197, 33845, 3151, 18940, 323, 12538, 1741, 438, 330, 1645, 960, 76, 8159, 487, 1335, 330, 68487, 32189, 1335, 330, 89053, 1582, 66084, 1335, 323, 330, 98223, 3233, 954, 52498, 1189, 4220, 525, 10916, 3793, 1483, 304, 279, 2070, 315, 15235, 323, 5662, 6832, 382, 21, 13, 3070, 31712, 95518, 576, 2197, 5646, 15057, 311, 1008, 15689, 323, 10916, 6682, 11, 892, 374, 14260, 304, 14250, 323]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research paper or technical report focused on advancements in artificial intelligence, particularly in the areas of language models and machine learning. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Content and Structure**: The document contains a list of authors, titles of papers, and brief descriptions of research topics. This format is typical for academic papers or technical reports.\\n\\n2. **Research Topics**: The papers listed cover a range of topics related to AI, including language models, knowledge distillation, sparse autoencoders, and concept extraction. These are all active areas of research in the field of machine learning and natural language processing.\\n\\n3. **Technical Details**: The document includes technical details such as model sizes, training datasets, and specific experimental setups (e.g., training on 200B tokens, using OpenWebText as the training dataset). This level of detail is characteristic of research papers.\\n\\n4. **Appendices**: The presence of appendices with experimental details and additional results (e.g., performance tracking during training) is common in research papers to provide comprehensive information.\\n\\n5. **Keywords and Concepts**: The document mentions specific concepts and techniques such as \"monosemanticity,\" \"concept extraction,\" \"knowledge distillation,\" and \"sparse autoencoders.\" These are technical terms used in the field of AI and machine learning.\\n\\n6. **References**: The document includes references to other papers and technical reports, which is typical in academic and'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 5567, 476, 264, 10916, 1895, 10735, 389, 82687, 304, 20443, 11229, 11, 7945, 304, 279, 5671, 315, 4128, 4119, 323, 5662, 6832, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 5610, 264, 1140, 315, 12014, 11, 8110, 553, 15057, 311, 5257, 3412, 15689, 323, 10916, 6682, 13, 1096, 5944, 374, 14260, 315, 14250, 15689, 476, 10916, 6682, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 15057, 3421, 264, 6884, 2088, 315, 13347, 5435, 311, 15235, 11, 2670, 4128, 4119, 11, 5662, 6832, 12538, 11, 323, 3151, 3412, 14613, 13, 1752, 3110, 11, 432, 15057, 4278, 389, 4128, 4119, 438, 2421, 63530, 52143, 11, 7469, 4686, 95757, 4586, 2022, 11, 323, 6540, 1582, 66084, 13, 4220, 525, 678, 4541, 5671, 315, 3412, 304, 15235, 382, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5646, 11682, 22000, 3059, 323, 5068, 16734, 11, 1741, 438, 10519, 73152, 487, 323, 13403, 389, 5257, 29425, 13, 1096, 2188, 315, 10916, 7716, 374, 28583, 315, 3412, 15689, 382, 19, 13, 3070, 44576, 82, 323, 45945, 2449, 95518, 576, 2197, 5711, 10916, 3793, 323, 18940, 3151, 311, 15235, 3412, 11, 1741, 438, 330, 1645, 960, 76, 8159, 487, 1335, 330, 68487, 32189, 1335, 330, 89053, 1582, 66084, 1335, 323, 330, 98223, 3233, 954, 52498, 1189, 4220, 3793, 525, 16626, 1483, 304, 15235, 323, 5662, 6832, 17206, 382, 20, 13, 3070, 23877, 1216, 95518, 576, 9362, 315, 8737, 1216, 448, 22000, 3565, 323, 5107, 3059, 374, 14260, 315, 3412, 15689, 11, 8241, 264, 15817, 1651, 315, 279, 37052, 323, 14613, 382, 21, 13, 3070, 57890, 323, 87780, 95518, 576, 1140, 315, 12014, 5646, 5036, 504, 5257, 14336, 11, 892, 374, 4185, 304, 39706, 3412]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research paper or a technical report focused on advancements in artificial intelligence, particularly in the areas of language models and machine learning. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Content and Structure**: The document contains a list of authors, followed by references to various research papers and technical reports. This structure is typical of academic papers or technical reports.\\n\\n2. **Subject Matter**: The references cover a wide range of topics related to AI, including language models, machine learning techniques, and specific research findings. For example, it references works on language models as few-shot learners, weak-to-strong generalization, and knowledge distillation. These are all active areas of research in AI.\\n\\n3. **Technical Details**: The document includes detailed experimental results and performance metrics, such as validation perplexity and accuracy on various datasets. This level of technical detail is characteristic of research papers.\\n\\n4. **Concepts and Terminology**: The document uses technical terms and concepts specific to AI research, such as \"monosemanticity,\" \"concept extraction,\" \"knowledge distillation,\" and \"sparse autoencoders.\" These terms are commonly used in AI and machine learning literature.\\n\\n5. **Appendices**: The presence of appendices with experimental details and additional results is typical of research papers, providing a comprehensive view of the methodology and findings.\\n\\n6. **Authors and Institutions**: The list of authors includes names from various institutions, which is common in collaborative research'\n",
      "    ✗ DIFFERENT\n",
      "  bs=17:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 5567, 476, 10916, 1895, 10735, 389, 82687, 304, 20443, 11229, 11, 7945, 304, 279, 5671, 315, 4128, 4119, 323, 5662, 6832, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 2762, 323, 28596, 95518, 576, 2197, 5610, 264, 1140, 315, 12014, 11, 15311, 315, 15689, 11, 323, 9814, 27787, 315, 3412, 13347, 13, 1096, 3561, 374, 14260, 369, 264, 93698, 476, 5785, 3772, 315, 264, 3412, 5567, 382, 17, 13, 3070, 27428, 40894, 95518, 576, 15689, 10007, 3421, 264, 2088, 315, 13347, 5435, 311, 15235, 11, 2670, 4128, 4119, 11, 6540, 1582, 66084, 11, 33444, 3233, 954, 52498, 11, 323, 7286, 32189, 13, 4220, 525, 678, 4541, 5671, 315, 3412, 304, 279, 2070, 315, 5662, 6832, 323, 5810, 4128, 8692, 382, 18, 13, 3070, 62226, 12309, 95518, 576, 2197, 5646, 10916, 3565, 1741, 438, 1614, 12282, 11, 4862, 29425, 11, 323, 3151, 22000, 83723, 320, 68, 1302, 2572, 4862, 389, 220, 17, 15, 15, 33, 11211, 11, 1667, 5264, 5981, 1178, 438, 279, 4862, 10337, 568, 1096, 2188, 315, 7716, 374, 28583, 315, 264, 10916, 1895, 476, 3412, 5567, 382, 19, 13, 3070, 23877, 1216, 95518, 576, 9362, 315, 8737, 1216, 448, 22000, 3565, 323, 5107, 3059, 320, 68, 1302, 2572, 5068, 14849, 2337, 4862, 8, 374, 4185, 304, 3412, 15689, 311, 3410, 15817, 1995, 382, 20, 13, 3070, 43842, 323, 75772, 95518, 576, 2197, 33845, 18940, 1075, 330, 1645, 960, 76, 8159, 487, 1335, 330, 68487, 32189, 1335, 323, 330, 89053, 1582, 66084, 1335, 892, 525, 3151, 311, 279, 2070, 315, 15235, 323, 5662, 6832, 382, 21, 13, 3070, 57890, 323, 87780, 95518, 576, 12014, 10007, 2924, 5036, 504, 5257, 14336, 323, 7079, 11, 1741, 438, 18183, 39350, 6691, 40, 11, 62379]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research paper or technical report focused on advancements in artificial intelligence, particularly in the areas of language models and machine learning. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Content and Structure**: The document contains a list of authors, titles of papers, and brief descriptions of research topics. This format is typical for a bibliography or reference section of a research paper.\\n\\n2. **Research Topics**: The papers listed cover a range of topics related to AI, including language models, knowledge distillation, sparse autoencoders, and concept extraction. These are all active areas of research in the field of machine learning and natural language processing.\\n\\n3. **Technical Details**: The document includes technical details such as model sizes, training datasets, and specific experimental setups (e.g., training on 200B tokens, using OpenWebText as the training dataset). This level of detail is characteristic of a technical report or research paper.\\n\\n4. **Appendices**: The presence of appendices with experimental details and additional results (e.g., performance tracking during training) is common in research papers to provide comprehensive information.\\n\\n5. **Keywords and Concepts**: The document mentions concepts like \"monosemanticity,\" \"concept extraction,\" and \"knowledge distillation,\" which are specific to the field of AI and machine learning.\\n\\n6. **Authors and Institutions**: The authors listed include names from various institutions and projects, such as DeepSeek-AI, Transformer'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "--- Token consistency for ref_2 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 18580, 6358, 476, 4842, 5567, 10735, 389, 279, 5313, 4763, 24154, 315, 10847, 20443, 11229, 320, 15469, 568, 576, 2197, 594, 3832, 4925, 374, 279, 4650, 15276, 323, 18580, 11513, 36160, 553, 15235, 11, 7945, 304, 279, 2266, 315, 5313, 4763, 323, 6489, 4300, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 3851, 323, 9608, 334, 510, 256, 481, 576, 2197, 34334, 279, 1184, 369, 264, 15817, 8282, 311, 2621, 279, 11513, 36160, 553, 15235, 11, 18860, 432, 374, 264, 18580, 6358, 624, 256, 481, 1084, 33845, 279, 12650, 315, 20102, 15235, 1526, 264, 5313, 4763, 18342, 11, 22561, 264, 4842, 35085, 2197, 382, 17, 13, 3070, 1001, 1222, 18320, 334, 510, 256, 481, 576, 1467, 49629, 3807, 17736, 36160, 553, 15235, 11, 2670, 510, 257, 481, 444, 769, 278, 38193, 9893, 198, 257, 481, 88472, 267, 41710, 38170, 990, 198, 257, 481, 74358, 1787, 12635, 362, 3872, 198, 257, 481, 15235, 40403, 3072, 21440, 198, 257, 481, 24704, 315, 2524, 916, 15235, 5942, 198, 256, 481, 4220, 17736, 525, 29139, 304, 279, 2266, 315, 5313, 4763, 11, 18860, 264, 5244, 389, 18580, 323, 4842, 24154, 382, 18, 13, 3070, 2580, 89367, 23752, 334, 510, 256, 481, 576, 2197, 54625, 264, 18580, 12626, 429, 5646, 4040, 15892, 11, 2477, 776, 75, 11437, 367, 11, 323, 76551, 624, 256, 481, 1084, 34334, 279, 7286, 315, 330, 51440, 928, 2688, 3073, 15235, 8394, 1688, 1, 320, 4835, 1791, 8, 438, 264, 1352, 315, 4040, 15892, 11, 4428, 311, 279, 7286, 315, 74042, 2688, 3073, 79747, 320, 44, 1808, 8, 304, 11249, 8282, 624, 256, 481, 1096, 13230, 264, 11682, 18580, 6358, 323, 4842, 13734, 382, 19, 13, 3070, 48983, 938, 323, 576, 90767, 9608, 334, 510, 256, 481, 576]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a strategic analysis or policy paper focused on the national security implications of advanced artificial intelligence (AI). The document\\'s subject matter is the potential risks and strategic challenges posed by AI, particularly in the context of national security and international relations.\\n\\n### Reasoning:\\n\\n1. **Title and Context**:\\n   - The document discusses the need for a comprehensive strategy to address the challenges posed by AI, indicating it is a strategic analysis.\\n   - It mentions the importance of viewing AI through a national security lens, suggesting a policy-oriented document.\\n\\n2. **Threat Analysis**:\\n   - The text outlines several threats posed by AI, including:\\n     - Lethal autonomous weapons\\n     - Catastrophic malicious use\\n     - Powerful open-weight AIs\\n     - AI-powered mass surveillance\\n     - Loss of control over AI systems\\n   - These threats are analyzed in the context of national security, indicating a focus on strategic and policy implications.\\n\\n3. **Strategic Framework**:\\n   - The document proposes a strategic framework that includes deterrence, nonproliferation, and competitiveness.\\n   - It discusses the concept of \"Mutual Assured AI Malfunction\" (MAIM) as a form of deterrence, similar to the concept of Mutual Assured Destruction (MAD) in nuclear strategy.\\n   - This suggests a detailed strategic analysis and policy proposal.\\n\\n4. **Historical and Theoretical Context**:\\n   - The'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 18580, 6358, 476, 4842, 5567, 10735, 389, 279, 5313, 4763, 24154, 315, 20443, 11229, 320, 15469, 568, 576, 2197, 594, 3832, 4925, 374, 279, 4650, 15276, 323, 11513, 36160, 553, 15235, 11, 7945, 304, 279, 2266, 315, 5313, 4763, 11, 323, 54625, 14830, 311, 2621, 1493, 4714, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 1972, 323, 34920, 334, 510, 256, 481, 576, 2197, 34334, 264, 6884, 2088, 315, 15235, 13904, 17736, 11, 2670, 43989, 38193, 9893, 11, 53577, 38170, 990, 11, 7988, 1787, 12635, 362, 3872, 11, 323, 15235, 40403, 3072, 21440, 624, 256, 481, 1084, 1083, 14230, 26829, 4714, 1741, 438, 20337, 264, 6094, 33377, 315, 8729, 11, 15235, 3188, 11, 323, 279, 5421, 315, 15235, 389, 14402, 323, 6955, 2355, 382, 17, 13, 3070, 30812, 8234, 23752, 334, 510, 256, 481, 576, 1467, 65059, 20102, 15235, 1526, 264, 5313, 4763, 18342, 11, 13330, 71374, 448, 13656, 18579, 24673, 14310, 1075, 11249, 9893, 624, 256, 481, 1084, 34334, 18580, 10707, 11, 23145, 11, 323, 279, 4709, 315, 2524, 916, 15235, 5942, 11, 678, 315, 892, 525, 9023, 5313, 4763, 10520, 382, 18, 13, 3070, 1001, 1222, 18320, 334, 510, 256, 481, 576, 2197, 35511, 3151, 17736, 1741, 438, 20762, 5302, 11, 53891, 19571, 11, 323, 650, 58607, 15235, 5942, 624, 256, 481, 1084, 1594, 2342, 1119, 279, 4650, 369, 15235, 311, 7283, 6277, 43492, 11, 1855, 6032, 23733, 25778, 11, 323, 6786, 20847, 80110, 389, 9023, 13737, 382, 19, 13, 3070, 2580, 89367, 3911, 85975, 334, 510, 256, 481, 576, 1467, 49629, 2326, 10981, 14830, 369, 18150, 15235, 15276, 25, 902, 16869, 11, 35979, 4313, 65744, 82, 11, 323, 264, 18580, 53201, 624, 256, 481, 1084, 1083, 38919, 264, 501, 8282, 2598, 330, 51440, 928]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a strategic analysis or policy paper focused on the national security implications of artificial intelligence (AI). The document\\'s subject matter is the potential risks and challenges posed by AI, particularly in the context of national security, and proposes strategies to address these issues.\\n\\n### Reasoning:\\n\\n1. **Context and Scope**:\\n   - The document discusses a wide range of AI-related threats, including lethal autonomous weapons, catastrophic malicious use, powerful open-weight AIs, and AI-powered mass surveillance.\\n   - It also addresses broader issues such as maintaining a shared grasp of reality, AI rights, and the impact of AI on employment and economic power.\\n\\n2. **National Security Framework**:\\n   - The text emphasizes viewing AI through a national security lens, drawing parallels with historical dual-use technologies like nuclear weapons.\\n   - It discusses strategic competition, terrorism, and the loss of control over AI systems, all of which are critical national security concerns.\\n\\n3. **Threat Analysis**:\\n   - The document identifies specific threats such as rival states, rogue actors, and uncontrolled AI systems.\\n   - It delves into the potential for AI to enable military dominance, create bioweapons, and conduct cyberattacks on critical infrastructure.\\n\\n4. **Strategic Proposals**:\\n   - The text outlines three proposed strategies for managing AI risks: no restrictions, voluntary moratoriums, and a strategic monopoly.\\n   - It also introduces a new strategy called \"Mutual'\n",
      "    ✗ DIFFERENT\n",
      "  bs=3:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 18580, 6358, 476, 4842, 5567, 10735, 389, 279, 5313, 4763, 24154, 315, 10847, 20443, 11229, 320, 15469, 568, 576, 2197, 594, 3832, 4925, 374, 279, 4650, 15276, 323, 18580, 11513, 36160, 553, 15235, 11, 7945, 304, 279, 2266, 315, 5313, 4763, 323, 6489, 4300, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 3851, 323, 9608, 334, 510, 256, 481, 576, 2197, 34334, 279, 1184, 369, 264, 15817, 8282, 311, 2621, 279, 11513, 36160, 553, 15235, 11, 18860, 432, 374, 264, 18580, 6358, 624, 256, 481, 1084, 33845, 279, 12650, 315, 20102, 15235, 1526, 264, 5313, 4763, 18342, 11, 22561, 264, 4842, 35085, 2197, 382, 17, 13, 3070, 1001, 1222, 18320, 334, 510, 256, 481, 576, 1467, 35511, 3807, 17736, 5435, 311, 15235, 11, 2670, 510, 257, 481, 444, 769, 278, 38193, 9893, 198, 257, 481, 88472, 267, 41710, 38170, 990, 198, 257, 481, 74358, 1787, 12635, 362, 3872, 198, 257, 481, 15235, 40403, 3072, 21440, 198, 256, 481, 1084, 1083, 34334, 279, 4650, 369, 15235, 311, 387, 1483, 304, 6032, 354, 841, 2142, 323, 20847, 80110, 389, 9023, 13737, 11, 38586, 279, 18579, 24673, 6993, 315, 15235, 382, 18, 13, 3070, 2580, 89367, 23752, 334, 510, 256, 481, 576, 2197, 54625, 264, 18580, 12626, 429, 5646, 4040, 15892, 11, 2477, 776, 75, 11437, 367, 11, 323, 76551, 624, 256, 481, 1084, 26643, 71374, 1948, 279, 23185, 5004, 12626, 315, 4040, 15892, 11, 2477, 776, 75, 11437, 367, 11, 323, 45211, 323, 13230, 69717, 1493, 16170, 311, 279, 4911, 11513, 36160, 553, 15235, 382, 19, 13, 3070, 28256, 5729, 323, 14356, 938, 21144, 804, 334, 510, 256, 481, 576, 1467, 34334, 279, 4650, 369, 15235, 311, 1855, 501, 12716, 311, 3072, 19258, 11, 1741, 438, 1526, 11229, 50262]\n",
      "    Text: \"Based on the excerpt provided, this document appears to be from a strategic analysis or policy paper focused on the national security implications of advanced artificial intelligence (AI). The document's subject matter is the potential risks and strategic challenges posed by AI, particularly in the context of national security and international relations.\\n\\n### Reasoning:\\n\\n1. **Title and Context**:\\n   - The document discusses the need for a comprehensive strategy to address the challenges posed by AI, indicating it is a strategic analysis.\\n   - It mentions the importance of viewing AI through a national security lens, suggesting a policy-oriented document.\\n\\n2. **Threat Analysis**:\\n   - The text identifies several threats related to AI, including:\\n     - Lethal autonomous weapons\\n     - Catastrophic malicious use\\n     - Powerful open-weight AIs\\n     - AI-powered mass surveillance\\n   - It also discusses the potential for AI to be used in bioterrorism and cyberattacks on critical infrastructure, highlighting the dual-use nature of AI.\\n\\n3. **Strategic Framework**:\\n   - The document proposes a strategic framework that includes deterrence, nonproliferation, and competitiveness.\\n   - It draws parallels between the Cold War framework of deterrence, nonproliferation, and containment and suggests adapting these principles to the unique challenges posed by AI.\\n\\n4. **Technological and Ethical Considerations**:\\n   - The text discusses the potential for AI to create new paths to mass destruction, such as through intelligence recursion\"\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 18580, 6358, 476, 4842, 5567, 10735, 389, 279, 5313, 4763, 24154, 315, 10847, 20443, 11229, 320, 15469, 568, 576, 2197, 594, 3832, 4925, 374, 279, 4650, 15276, 323, 18580, 11513, 36160, 553, 15235, 11, 7945, 304, 279, 2266, 315, 5313, 4763, 323, 6489, 4300, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 3851, 323, 9608, 334, 510, 256, 481, 576, 2197, 34334, 279, 1184, 369, 264, 15817, 8282, 311, 2621, 279, 11513, 36160, 553, 15235, 11, 18860, 432, 374, 264, 18580, 6358, 624, 256, 481, 1084, 33845, 279, 12650, 315, 20102, 15235, 1526, 264, 5313, 4763, 18342, 11, 22561, 264, 4842, 35085, 2197, 382, 17, 13, 3070, 1001, 1222, 18320, 334, 510, 256, 481, 576, 1467, 35511, 3807, 17736, 5435, 311, 15235, 11, 2670, 510, 257, 481, 444, 769, 278, 38193, 9893, 198, 257, 481, 88472, 267, 41710, 38170, 990, 198, 257, 481, 74358, 1787, 12635, 362, 3872, 198, 257, 481, 15235, 40403, 3072, 21440, 198, 256, 481, 1084, 1083, 34334, 279, 4650, 369, 15235, 311, 43809, 279, 8172, 315, 2355, 11, 2176, 46279, 323, 5327, 6613, 11, 323, 279, 15276, 315, 13188, 2524, 916, 15235, 5942, 382, 18, 13, 3070, 2580, 89367, 23752, 334, 510, 256, 481, 576, 2197, 49629, 264, 18580, 12626, 369, 27020, 15235, 11513, 11, 2670, 4040, 15892, 11, 2477, 776, 75, 11437, 367, 11, 323, 76551, 624, 256, 481, 1084, 15057, 13656, 11715, 2524, 14830, 11, 1741, 438, 279, 7286, 315, 330, 51440, 928, 2688, 3073, 79747, 1, 320, 44, 1808, 8, 304, 279, 11249, 4231, 11, 311, 4038, 71374, 448, 4650, 15235, 14830, 382, 19, 13, 3070, 62226, 323, 10974, 21144, 804, 334, 510, 256, 481, 576, 1467, 1594, 2342, 1119, 10916, 11513, 323, 4842, 10953, 11, 1741, 438, 510, 257]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a strategic analysis or policy paper focused on the national security implications of advanced artificial intelligence (AI). The document\\'s subject matter is the potential risks and strategic challenges posed by AI, particularly in the context of national security and international relations.\\n\\n### Reasoning:\\n\\n1. **Title and Context**:\\n   - The document discusses the need for a comprehensive strategy to address the challenges posed by AI, indicating it is a strategic analysis.\\n   - It mentions the importance of viewing AI through a national security lens, suggesting a policy-oriented document.\\n\\n2. **Threat Analysis**:\\n   - The text identifies several threats related to AI, including:\\n     - Lethal autonomous weapons\\n     - Catastrophic malicious use\\n     - Powerful open-weight AIs\\n     - AI-powered mass surveillance\\n   - It also discusses the potential for AI to reshape the balance of power, both economically and militarily, and the risks of losing control over AI systems.\\n\\n3. **Strategic Framework**:\\n   - The document outlines a strategic framework for addressing AI challenges, including deterrence, nonproliferation, and competitiveness.\\n   - It references historical arms control strategies, such as the concept of \"Mutual Assured Destruction\" (MAD) in the nuclear age, to draw parallels with potential AI strategies.\\n\\n4. **Technical and Policy Considerations**:\\n   - The text delves into technical challenges and policy measures, such as:\\n    '\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 18580, 6358, 476, 4842, 5567, 10735, 389, 279, 5313, 4763, 24154, 315, 20443, 11229, 320, 15469, 568, 576, 2197, 594, 3832, 4925, 374, 279, 4650, 15276, 323, 11513, 36160, 553, 15235, 11, 7945, 304, 279, 2266, 315, 5313, 4763, 11, 323, 54625, 14830, 311, 2621, 1493, 4714, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 1972, 323, 34920, 334, 510, 256, 481, 576, 2197, 34334, 264, 6884, 2088, 315, 15235, 13904, 17736, 11, 2670, 43989, 38193, 9893, 11, 53577, 38170, 990, 11, 7988, 1787, 12635, 362, 3872, 11, 323, 15235, 40403, 3072, 21440, 624, 256, 481, 1084, 1083, 14230, 26829, 4714, 1741, 438, 20337, 264, 6094, 33377, 315, 8729, 11, 15235, 3188, 11, 323, 279, 5421, 315, 15235, 389, 14402, 323, 6955, 2355, 382, 17, 13, 3070, 30812, 8234, 25806, 334, 510, 256, 481, 576, 1467, 65059, 279, 1184, 311, 1651, 15235, 11513, 1526, 264, 5313, 4763, 18342, 11, 13330, 71374, 448, 13656, 18579, 24673, 14310, 1075, 11249, 9893, 624, 256, 481, 1084, 34334, 279, 4650, 369, 15235, 311, 43809, 279, 8172, 315, 2355, 11, 2176, 46279, 323, 5327, 6613, 11, 323, 279, 15276, 315, 13188, 2524, 916, 15235, 5942, 382, 18, 13, 3070, 1001, 1222, 18320, 334, 510, 256, 481, 576, 2197, 35511, 3151, 17736, 1741, 438, 20762, 5302, 11, 53891, 19571, 11, 323, 650, 58607, 15235, 5942, 624, 256, 481, 1084, 3565, 279, 15276, 315, 6032, 354, 841, 2142, 11, 20847, 80110, 389, 9023, 13737, 11, 323, 279, 4650, 369, 15235, 311, 7283, 6277, 43492, 382, 19, 13, 3070, 2580, 89367, 3911, 85975, 334, 510, 256, 481, 576, 1467, 49629, 2326, 10981, 14830, 369, 18150, 15235, 15276, 25, 264, 6078, 12462, 5486, 11, 264, 35979, 4313, 65744, 11, 323, 264, 53201, 8282, 624, 256, 481]\n",
      "    Text: \"Based on the excerpt provided, this document appears to be from a strategic analysis or policy paper focused on the national security implications of artificial intelligence (AI). The document's subject matter is the potential risks and challenges posed by AI, particularly in the context of national security, and proposes strategies to address these issues.\\n\\n### Reasoning:\\n\\n1. **Context and Scope**:\\n   - The document discusses a wide range of AI-related threats, including lethal autonomous weapons, catastrophic malicious use, powerful open-weight AIs, and AI-powered mass surveillance.\\n   - It also addresses broader issues such as maintaining a shared grasp of reality, AI rights, and the impact of AI on employment and economic power.\\n\\n2. **National Security Focus**:\\n   - The text emphasizes the need to view AI challenges through a national security lens, drawing parallels with historical dual-use technologies like nuclear weapons.\\n   - It discusses the potential for AI to reshape the balance of power, both economically and militarily, and the risks of losing control over AI systems.\\n\\n3. **Threat Analysis**:\\n   - The document identifies specific threats such as rival states, rogue actors, and uncontrolled AI systems.\\n   - It details the risks of bioterrorism, cyberattacks on critical infrastructure, and the potential for AI to enable military dominance.\\n\\n4. **Strategic Proposals**:\\n   - The text outlines three proposed strategies for managing AI risks: a hands-off approach, a voluntary moratorium, and a monopoly strategy.\\n   -\"\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 18580, 6358, 476, 4842, 5567, 10735, 389, 279, 5313, 4763, 24154, 315, 10847, 20443, 11229, 320, 15469, 568, 576, 2197, 4363, 34334, 279, 4650, 15276, 323, 7567, 315, 15235, 11, 7945, 304, 279, 2266, 315, 5313, 4763, 11, 323, 54625, 14830, 311, 10091, 1493, 15276, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 3851, 323, 9608, 334, 510, 256, 481, 576, 2197, 34334, 330, 9522, 92275, 8282, 1, 323, 330, 41140, 4763, 1335, 18860, 264, 5244, 389, 279, 19246, 315, 15235, 323, 86898, 19753, 624, 256, 481, 1084, 33845, 330, 86, 18504, 5322, 1, 323, 330, 83, 373, 10916, 5322, 1335, 22561, 264, 32930, 5486, 311, 27020, 6351, 4714, 5435, 311, 15235, 382, 17, 13, 3070, 1001, 1222, 18320, 334, 510, 256, 481, 576, 1467, 35511, 3807, 17736, 311, 5313, 4763, 11, 2670, 510, 257, 481, 444, 769, 278, 38193, 9893, 198, 257, 481, 88472, 267, 41710, 38170, 990, 315, 15235, 198, 257, 481, 74358, 1787, 12635, 362, 3872, 198, 257, 481, 15235, 40403, 3072, 21440, 198, 257, 481, 24704, 315, 2524, 916, 15235, 5942, 198, 256, 481, 1084, 1083, 34334, 279, 4650, 369, 15235, 311, 387, 1483, 304, 6032, 354, 841, 2142, 323, 20847, 80110, 389, 9023, 13737, 382, 18, 13, 3070, 2580, 89367, 23752, 334, 510, 256, 481, 576, 2197, 54625, 264, 18580, 12626, 369, 27020, 15235, 11513, 11, 20102, 15235, 1526, 264, 330, 41140, 4763, 18342, 10040, 256, 481, 1084, 13230, 2326, 1887, 14830, 510, 257, 481, 42296, 12462, 3489, 9860, 17288, 323, 15623, 20093, 899, 8282, 198, 257, 481, 8446, 65744, 8282, 198, 257, 481, 3128, 39763, 8282, 198, 256, 481, 1084, 1083, 49629, 264, 330, 57251, 7417, 27745, 1, 429, 5646, 4040, 15892, 11, 2477, 776, 75, 11437, 367, 11, 323, 76551]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a strategic analysis or policy paper focused on the national security implications of advanced artificial intelligence (AI). The document likely discusses the potential risks and benefits of AI, particularly in the context of national security, and proposes strategies to manage these risks.\\n\\n### Reasoning:\\n\\n1. **Title and Context**:\\n   - The document discusses \"superintelligence strategy\" and \"national security,\" indicating a focus on the intersection of AI and geopolitical stability.\\n   - It mentions \"wicked problems\" and \"tame technical problems,\" suggesting a structured approach to addressing complex issues related to AI.\\n\\n2. **Threat Analysis**:\\n   - The text identifies several threats to national security, including:\\n     - Lethal autonomous weapons\\n     - Catastrophic malicious use of AI\\n     - Powerful open-weight AIs\\n     - AI-powered mass surveillance\\n     - Loss of control over AI systems\\n   - It also discusses the potential for AI to be used in bioterrorism and cyberattacks on critical infrastructure.\\n\\n3. **Strategic Framework**:\\n   - The document proposes a strategic framework for addressing AI challenges, viewing AI through a \"national security lens.\"\\n   - It suggests three main strategies:\\n     - Hands-off (\"Move Fast and Break Things\") strategy\\n     - Moratorium strategy\\n     - Monopoly strategy\\n   - It also outlines a \"Multipolar Strategy\" that includes deterrence, nonproliferation, and competitiveness'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 18580, 6358, 476, 4842, 5567, 10735, 389, 279, 5313, 4763, 24154, 315, 10847, 20443, 11229, 320, 15469, 568, 576, 2197, 4363, 34334, 279, 4650, 15276, 323, 7567, 315, 15235, 11, 7945, 304, 279, 2266, 315, 5313, 4763, 11, 323, 54625, 14830, 311, 10091, 1493, 15276, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 3851, 323, 9608, 334, 510, 256, 481, 576, 2197, 34334, 330, 9522, 92275, 8282, 1, 323, 330, 41140, 4763, 1335, 18860, 264, 5244, 389, 279, 19246, 315, 15235, 323, 86898, 19753, 624, 256, 481, 1084, 33845, 330, 86, 18504, 5322, 1, 323, 330, 83, 373, 10916, 5322, 1335, 22561, 264, 32930, 5486, 311, 27020, 6351, 4714, 5435, 311, 15235, 382, 17, 13, 3070, 1001, 1222, 18320, 334, 510, 256, 481, 576, 1467, 35511, 3807, 17736, 311, 5313, 4763, 11, 2670, 510, 257, 481, 444, 769, 278, 38193, 9893, 198, 257, 481, 88472, 267, 41710, 38170, 990, 315, 15235, 198, 257, 481, 74358, 1787, 12635, 362, 3872, 198, 257, 481, 15235, 40403, 3072, 21440, 198, 257, 481, 24704, 315, 2524, 916, 15235, 5942, 198, 256, 481, 1084, 1083, 34334, 279, 4650, 369, 15235, 311, 387, 1483, 304, 6032, 354, 841, 2142, 323, 20847, 80110, 389, 9023, 13737, 382, 18, 13, 3070, 2580, 89367, 23752, 334, 510, 256, 481, 576, 2197, 54625, 264, 18580, 12626, 369, 27020, 15235, 11513, 11, 20102, 15235, 1526, 264, 330, 41140, 4763, 18342, 10040, 256, 481, 1084, 13230, 2326, 1887, 14830, 510, 257, 481, 42296, 12462, 3489, 9860, 17288, 323, 15623, 20093, 899, 8282, 198, 257, 481, 8446, 65744, 8282, 198, 257, 481, 3128, 39763, 8282, 198, 256, 481, 1084, 1083, 49629, 264, 330, 57251, 7417, 27745, 1, 429, 5646, 4040, 15892, 11, 2477, 776, 75, 11437, 367, 11, 323, 76551]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a strategic analysis or policy paper focused on the national security implications of advanced artificial intelligence (AI). The document likely discusses the potential risks and benefits of AI, particularly in the context of national security, and proposes strategies to manage these risks.\\n\\n### Reasoning:\\n\\n1. **Title and Context**:\\n   - The document discusses \"superintelligence strategy\" and \"national security,\" indicating a focus on the intersection of AI and geopolitical stability.\\n   - It mentions \"wicked problems\" and \"tame technical problems,\" suggesting a structured approach to addressing complex issues related to AI.\\n\\n2. **Threat Analysis**:\\n   - The text identifies several threats to national security, including:\\n     - Lethal autonomous weapons\\n     - Catastrophic malicious use of AI\\n     - Powerful open-weight AIs\\n     - AI-powered mass surveillance\\n     - Loss of control over AI systems\\n   - It also discusses the potential for AI to be used in bioterrorism and cyberattacks on critical infrastructure.\\n\\n3. **Strategic Framework**:\\n   - The document proposes a strategic framework for addressing AI challenges, viewing AI through a \"national security lens.\"\\n   - It suggests three main strategies:\\n     - Hands-off (\"Move Fast and Break Things\") strategy\\n     - Moratorium strategy\\n     - Monopoly strategy\\n   - It also outlines a \"Multipolar Strategy\" that includes deterrence, nonproliferation, and competitiveness'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 18580, 6358, 476, 4842, 5567, 10735, 389, 279, 5313, 4763, 24154, 315, 20443, 11229, 320, 15469, 568, 576, 2197, 594, 3832, 4925, 374, 279, 4650, 15276, 323, 11513, 36160, 553, 15235, 11, 7945, 304, 279, 2266, 315, 5313, 4763, 11, 323, 54625, 14830, 311, 2621, 1493, 4714, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 1972, 323, 34920, 334, 510, 256, 481, 576, 2197, 34334, 264, 7205, 2088, 315, 15235, 13904, 17736, 11, 2670, 43989, 38193, 9893, 11, 53577, 38170, 990, 11, 7988, 1787, 12635, 362, 3872, 11, 323, 15235, 40403, 3072, 21440, 624, 256, 481, 1084, 1083, 14230, 279, 4650, 369, 15235, 311, 43809, 279, 8172, 315, 2355, 304, 6955, 323, 6277, 3793, 11, 438, 1632, 438, 279, 15276, 315, 13188, 2524, 916, 15235, 5942, 382, 17, 13, 3070, 1001, 1222, 18320, 334, 510, 256, 481, 576, 1467, 35511, 3151, 17736, 1741, 438, 20762, 5302, 11, 53891, 19571, 11, 323, 650, 58607, 15235, 5942, 624, 256, 481, 1084, 34334, 279, 4650, 369, 15235, 311, 7283, 501, 7586, 315, 6032, 354, 841, 2142, 323, 20847, 80110, 389, 9023, 13737, 11, 38586, 279, 18579, 24673, 6993, 315, 15235, 14310, 382, 18, 13, 3070, 2580, 89367, 23752, 334, 510, 256, 481, 576, 2197, 54625, 264, 18580, 12626, 369, 27020, 15235, 11513, 11, 20102, 15235, 1526, 264, 5313, 4763, 18342, 624, 256, 481, 1084, 13230, 264, 12557, 7417, 8282, 429, 5646, 4040, 15892, 11, 2477, 776, 75, 11437, 367, 11, 323, 76551, 11, 13330, 71374, 311, 279, 23185, 5004, 12626, 315, 4040, 15892, 11, 2477, 776, 75, 11437, 367, 11, 323, 45211, 382, 19, 13, 3070, 2008, 19586, 22676, 334, 510, 256, 481, 576, 1467, 49629, 3151, 14830, 1741, 438, 264, 330, 51440, 928, 2688, 3073, 15235, 8394, 1688, 1]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a strategic analysis or policy paper focused on the national security implications of artificial intelligence (AI). The document\\'s subject matter is the potential risks and challenges posed by AI, particularly in the context of national security, and proposes strategies to address these issues.\\n\\n### Reasoning:\\n\\n1. **Context and Scope**:\\n   - The document discusses a broad range of AI-related threats, including lethal autonomous weapons, catastrophic malicious use, powerful open-weight AIs, and AI-powered mass surveillance.\\n   - It also addresses the potential for AI to reshape the balance of power in economic and military terms, as well as the risks of losing control over AI systems.\\n\\n2. **Threat Analysis**:\\n   - The text identifies specific threats such as rival states, rogue actors, and uncontrolled AI systems.\\n   - It discusses the potential for AI to enable new forms of bioterrorism and cyberattacks on critical infrastructure, highlighting the dual-use nature of AI technologies.\\n\\n3. **Strategic Framework**:\\n   - The document proposes a strategic framework for addressing AI challenges, viewing AI through a national security lens.\\n   - It suggests a multipolar strategy that includes deterrence, nonproliferation, and competitiveness, drawing parallels to the Cold War framework of deterrence, nonproliferation, and containment.\\n\\n4. **Proposed Solutions**:\\n   - The text outlines specific strategies such as a \"Mutual Assured AI Malfunction\"'\n",
      "    ✗ DIFFERENT\n",
      "  bs=17:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 18580, 6358, 476, 4842, 5567, 10735, 389, 279, 5313, 4763, 24154, 315, 10847, 20443, 11229, 320, 15469, 568, 576, 2197, 4363, 34334, 279, 4650, 15276, 323, 7567, 315, 15235, 11, 7945, 304, 279, 2266, 315, 5313, 4763, 11, 323, 54625, 14830, 311, 10091, 1493, 15276, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 1972, 323, 34920, 334, 510, 256, 481, 576, 2197, 34334, 264, 6884, 2088, 315, 15235, 13904, 4714, 11, 2670, 43989, 38193, 9893, 11, 53577, 38170, 990, 11, 7988, 1787, 12635, 362, 3872, 11, 323, 15235, 40403, 3072, 21440, 13, 1096, 7205, 6891, 13230, 264, 15817, 6358, 315, 15235, 594, 5421, 389, 5313, 4763, 624, 5872, 17, 13, 3070, 1001, 1222, 18320, 334, 510, 256, 481, 576, 1467, 35511, 3807, 17736, 11, 1741, 438, 20762, 5302, 11, 53891, 19571, 11, 323, 279, 5214, 315, 13188, 2524, 916, 15235, 5942, 13, 1084, 1083, 34334, 279, 4650, 369, 15235, 311, 7283, 501, 7586, 315, 6032, 354, 841, 2142, 323, 20847, 80110, 389, 9023, 13737, 13, 1096, 14807, 264, 5244, 389, 279, 4763, 11513, 36160, 553, 15235, 382, 18, 13, 3070, 2580, 89367, 23752, 334, 510, 256, 481, 576, 2197, 54625, 264, 18580, 12626, 369, 27020, 15235, 11513, 11, 20102, 15235, 1526, 264, 5313, 4763, 18342, 13, 1084, 13230, 429, 15235, 594, 5421, 389, 1584, 2355, 11, 6955, 10707, 11, 323, 6277, 16928, 374, 5089, 323, 7460, 264, 21765, 2033, 382, 19, 13, 3070, 13825, 3911, 85975, 334, 510, 256, 481, 576, 1467, 49629, 2326, 1887, 14830, 369, 18150, 15235, 15276, 25, 32410, 678, 312, 7458, 11, 24931, 264, 35979, 4313, 65744, 11, 323, 33018, 264, 18580, 53201, 13, 1084, 1083, 34334, 279, 40759, 315, 1817, 5486, 323, 13230, 264, 330, 57251, 7417, 27745, 1, 429, 32411, 4040]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a strategic analysis or policy paper focused on the national security implications of advanced artificial intelligence (AI). The document likely discusses the potential risks and benefits of AI, particularly in the context of national security, and proposes strategies to manage these risks.\\n\\n### Reasoning:\\n\\n1. **Context and Scope**:\\n   - The document discusses a wide range of AI-related issues, including lethal autonomous weapons, catastrophic malicious use, powerful open-weight AIs, and AI-powered mass surveillance. This broad scope suggests a comprehensive analysis of AI\\'s impact on national security.\\n   \\n2. **Threat Analysis**:\\n   - The text identifies several threats, such as rival states, rogue actors, and the risk of losing control over AI systems. It also discusses the potential for AI to enable new forms of bioterrorism and cyberattacks on critical infrastructure. This indicates a focus on the security challenges posed by AI.\\n\\n3. **Strategic Framework**:\\n   - The document proposes a strategic framework for addressing AI challenges, viewing AI through a national security lens. It suggests that AI\\'s impact on state power, economic competition, and military capabilities is significant and requires a robust response.\\n\\n4. **Policy Proposals**:\\n   - The text outlines three main strategies for managing AI risks: lifting all restraints, implementing a voluntary moratorium, and pursuing a strategic monopoly. It also discusses the flaws of each approach and suggests a \"Multipolar Strategy\" that combines deter'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_0\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_1\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_2\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Key vectors - Mean: 0.00e+00, Range: [0.00e+00, 0.00e+00]\n",
      "  Logprobs - Mean: 0.00e+00, Range: [0.00e+00, 0.00e+00]\n",
      "\n",
      "⚠ WARNING: 36/36 comparisons are EXACTLY ZERO\n",
      "  All batch sizes produce identical results (single kernel class)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1, 2, 3, 4, 5, 8, 9, 16, 17]\n",
      "\n",
      "Equivalent pairs (will be excluded from cross-hardware signal):\n",
      "  (1, 2)\n",
      "  (1, 3)\n",
      "  (1, 4)\n",
      "  (1, 5)\n",
      "  (1, 8)\n",
      "  (1, 9)\n",
      "  (1, 16)\n",
      "  (1, 17)\n",
      "  (2, 3)\n",
      "  (2, 4)\n",
      "  (2, 5)\n",
      "  (2, 8)\n",
      "  (2, 9)\n",
      "  (2, 16)\n",
      "  (2, 17)\n",
      "  (3, 4)\n",
      "  (3, 5)\n",
      "  (3, 8)\n",
      "  (3, 9)\n",
      "  (3, 16)\n",
      "  (3, 17)\n",
      "  (4, 5)\n",
      "  (4, 8)\n",
      "  (4, 9)\n",
      "  (4, 16)\n",
      "  (4, 17)\n",
      "  (5, 8)\n",
      "  (5, 9)\n",
      "  (5, 16)\n",
      "  (5, 17)\n",
      "  (8, 9)\n",
      "  (8, 16)\n",
      "  (8, 17)\n",
      "  (9, 16)\n",
      "  (9, 17)\n",
      "  (16, 17)\n",
      "\n",
      "✗ SANITY CHECK FAILED (No variation)\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (DECODE)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_0\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.45e+01  3.12e+01  2.93e+01  2.46e+01  2.35e+01  2.35e+01  2.57e+01  2.45e+01\n",
      "bs=2    2.45e+01  0.00e+00  2.88e+01  2.50e+01  6.17e-01  2.55e+01  2.55e+01  3.03e+01  3.20e-01\n",
      "bs=3    3.12e+01  2.88e+01  0.00e+00  2.95e+01  2.88e+01  2.96e+01  2.96e+01  2.93e+01  2.88e+01\n",
      "bs=4    2.93e+01  2.50e+01  2.95e+01  0.00e+00  2.50e+01  2.28e+01  2.28e+01  3.14e+01  2.50e+01\n",
      "bs=5    2.46e+01  6.17e-01  2.88e+01  2.50e+01  0.00e+00  2.55e+01  2.55e+01  3.03e+01  6.08e-01\n",
      "bs=8    2.35e+01  2.55e+01  2.96e+01  2.28e+01  2.55e+01  0.00e+00  0.00e+00  2.36e+01  2.55e+01\n",
      "bs=9    2.35e+01  2.55e+01  2.96e+01  2.28e+01  2.55e+01  0.00e+00  0.00e+00  2.36e+01  2.55e+01\n",
      "bs=16   2.57e+01  3.03e+01  2.93e+01  3.14e+01  3.03e+01  2.36e+01  2.36e+01  0.00e+00  3.02e+01\n",
      "bs=17   2.45e+01  3.20e-01  2.88e+01  2.50e+01  6.08e-01  2.55e+01  2.55e+01  3.02e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00       inf       inf  1.88e-01       inf       inf       inf  1.41e-01\n",
      "bs=3         inf       inf  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00       inf       inf       inf       inf       inf\n",
      "bs=5         inf  1.88e-01       inf       inf  0.00e+00       inf       inf       inf  1.33e-01\n",
      "bs=8         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf  1.24e-01       inf       inf  1.16e-01       inf       inf       inf  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_1\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  3.25e+01  3.00e+01  2.66e+01  2.75e+01  2.68e+01  2.68e+01  2.57e+01  2.57e+01\n",
      "bs=2    3.25e+01  0.00e+00  3.22e+01  2.85e+01  3.23e+01  3.11e+01  3.11e+01  3.20e+01  3.42e+01\n",
      "bs=3    3.00e+01  3.22e+01  0.00e+00  2.42e+01  3.05e+01  2.91e+01  2.91e+01  2.84e+01  3.26e+01\n",
      "bs=4    2.66e+01  2.85e+01  2.42e+01  0.00e+00  2.88e+01  2.48e+01  2.48e+01  2.26e+01  3.19e+01\n",
      "bs=5    2.75e+01  3.23e+01  3.05e+01  2.88e+01  0.00e+00  2.95e+01  2.95e+01  2.90e+01  3.18e+01\n",
      "bs=8    2.68e+01  3.11e+01  2.91e+01  2.48e+01  2.95e+01  0.00e+00  0.00e+00  1.61e+01  3.20e+01\n",
      "bs=9    2.68e+01  3.11e+01  2.91e+01  2.48e+01  2.95e+01  0.00e+00  0.00e+00  1.61e+01  3.20e+01\n",
      "bs=16   2.57e+01  3.20e+01  2.84e+01  2.26e+01  2.90e+01  1.61e+01  1.61e+01  0.00e+00  3.02e+01\n",
      "bs=17   2.57e+01  3.42e+01  3.26e+01  3.19e+01  3.18e+01  3.20e+01  3.20e+01  3.02e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=3         inf       inf  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00       inf       inf       inf       inf       inf\n",
      "bs=5         inf       inf       inf       inf  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf       inf       inf       inf       inf       inf  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_2\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  3.05e+01  2.90e+01  2.37e+01  2.39e+01  2.64e+01  2.64e+01  3.39e+01  2.62e+01\n",
      "bs=2    3.05e+01  0.00e+00  2.56e+01  2.63e+01  2.76e+01  2.92e+01  2.92e+01  3.32e+01  2.30e+01\n",
      "bs=3    2.90e+01  2.56e+01  0.00e+00  2.59e+01  2.90e+01  2.79e+01  2.79e+01  3.17e+01  2.37e+01\n",
      "bs=4    2.37e+01  2.63e+01  2.59e+01  0.00e+00  2.38e+01  2.45e+01  2.45e+01  3.47e+01  2.40e+01\n",
      "bs=5    2.39e+01  2.76e+01  2.90e+01  2.38e+01  0.00e+00  2.20e+01  2.20e+01  3.23e+01  2.10e+01\n",
      "bs=8    2.64e+01  2.92e+01  2.79e+01  2.45e+01  2.20e+01  0.00e+00  0.00e+00  3.06e+01  2.36e+01\n",
      "bs=9    2.64e+01  2.92e+01  2.79e+01  2.45e+01  2.20e+01  0.00e+00  0.00e+00  3.06e+01  2.36e+01\n",
      "bs=16   3.39e+01  3.32e+01  3.17e+01  3.47e+01  3.23e+01  3.06e+01  3.06e+01  0.00e+00  3.17e+01\n",
      "bs=17   2.62e+01  2.30e+01  2.37e+01  2.40e+01  2.10e+01  2.36e+01  2.36e+01  3.17e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=3         inf       inf  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00       inf       inf       inf       inf       inf\n",
      "bs=5         inf       inf       inf       inf  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf       inf       inf       inf       inf       inf  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.92e+01  3.01e+01  2.65e+01  2.53e+01  2.56e+01  2.56e+01  2.84e+01  2.55e+01\n",
      "bs=2    2.92e+01  0.00e+00  2.89e+01  2.66e+01  2.02e+01  2.86e+01  2.86e+01  3.18e+01  1.92e+01\n",
      "bs=3    3.01e+01  2.89e+01  0.00e+00  2.65e+01  2.95e+01  2.88e+01  2.88e+01  2.98e+01  2.84e+01\n",
      "bs=4    2.65e+01  2.66e+01  2.65e+01  0.00e+00  2.59e+01  2.40e+01  2.40e+01  2.96e+01  2.70e+01\n",
      "bs=5    2.53e+01  2.02e+01  2.95e+01  2.59e+01  0.00e+00  2.57e+01  2.57e+01  3.05e+01  1.78e+01\n",
      "bs=8    2.56e+01  2.86e+01  2.88e+01  2.40e+01  2.57e+01  0.00e+00  0.00e+00  2.35e+01  2.71e+01\n",
      "bs=9    2.56e+01  2.86e+01  2.88e+01  2.40e+01  2.57e+01  0.00e+00  0.00e+00  2.35e+01  2.71e+01\n",
      "bs=16   2.84e+01  3.18e+01  2.98e+01  2.96e+01  3.05e+01  2.35e+01  2.35e+01  0.00e+00  3.07e+01\n",
      "bs=17   2.55e+01  1.92e+01  2.84e+01  2.70e+01  1.78e+01  2.71e+01  2.71e+01  3.07e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=3         inf       inf  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00       inf       inf       inf       inf       inf\n",
      "bs=5         inf       inf       inf       inf  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf       inf       inf       inf       inf       inf  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Key vectors - Mean: 2.59e+01, Range: [0.00e+00, 3.18e+01]\n",
      "  Logprobs - Mean: 0.00e+00, Range: [0.00e+00, inf]\n",
      "\n",
      "⚠ NOTE: 1/36 comparisons are equivalent (< 1e-09)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1]\n",
      "  Class 2: [2]\n",
      "  Class 3: [3]\n",
      "  Class 4: [4]\n",
      "  Class 5: [5]\n",
      "  Class 6: [8, 9]\n",
      "  Class 7: [16]\n",
      "  Class 8: [17]\n",
      "\n",
      "Equivalent pairs (will be excluded from cross-hardware signal):\n",
      "  (8, 9)\n",
      "\n",
      "✗ SANITY CHECK FAILED (No variation)\n",
      "\n",
      "✓ Saved to: /workspace/experiments/decode_20251126_170540.json\n",
      "\n",
      "Next step: Copy to verifier machine, set TEACHER_FORCING=True\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Hardware Prefill vs Decode Detectability Experiment (Transformers)\n",
    "\n",
    "Tests whether batch size claims can be verified across different GPU architectures\n",
    "using floating-point forensics (key vectors and logprobs).\n",
    "\n",
    "UPDATED: Uses chat-formatted prompts for more realistic/higher-entropy responses.\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A (e.g., A100) with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts prefill + decode signals, saves to JSON\n",
    "2. Copy JSON to Machine B (e.g., H100)\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, extracts signals, compares\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = False\n",
    "REFERENCE_FILE = \"/workspace/experiments/decode_reference.json\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "ATTN_IMPLEMENTATION = \"sdpa\"  # Options: \"eager\", \"sdpa\", \"flash_attention_2\"\n",
    "\n",
    "BATCH_SIZES = [1, 2, 3, 4, 5, 8, 9, 16, 17]\n",
    "LAYER_INDICES = [28]  # Last layer only\n",
    "MAX_NEW_TOKENS = 300\n",
    "TOKENS_PER_SLICE = 8000  \n",
    "NUM_REFERENCES = 3\n",
    "\n",
    "# Threshold for considering two batch sizes \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Will be initialized from PDF in main()\n",
    "REFERENCE_SEQUENCES = None  # Now contains token IDs, not text\n",
    "DUMMY_SETS = None           # Now contains token IDs, not text\n",
    "\n",
    "# ============================================================================\n",
    "# CHAT TEMPLATE CONFIGURATION (Qwen recommended)\n",
    "# ============================================================================\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Here is an excerpt from a document:\n",
    "\n",
    "\"{snippet}\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.\"\"\"\n",
    "\n",
    "\n",
    "def create_chat_messages(snippet_text):\n",
    "    \"\"\"Create chat messages list for the tokenizer.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(snippet=snippet_text.strip())}\n",
    "    ]\n",
    "\n",
    "\n",
    "def tokenize_chat_prompt(tokenizer, snippet_text):\n",
    "    \"\"\"\n",
    "    Tokenize a chat prompt using the tokenizer's apply_chat_template.\n",
    "    Returns token IDs ready for model input.\n",
    "    \"\"\"\n",
    "    messages = create_chat_messages(snippet_text)\n",
    "    token_ids = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    return token_ids\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    \"\"\"Setup logging to file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    \"\"\"Print to both console and log file.\"\"\"\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    \"\"\"Close log file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING - UPDATED TO RETURN TOKEN IDS WITH CHAT TEMPLATE\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text content from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def create_sequences_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load all PDFs, create chat-formatted prompts, and return token IDs.\n",
    "    All assembly done at token level - guaranteed identical lengths.\n",
    "    \"\"\"\n",
    "    pdf_files = glob.glob(\"/workspace/*.pdf\")\n",
    "    if not pdf_files:\n",
    "        pdf_files = glob.glob(\"*.pdf\")\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    # Tokenize PDF content\n",
    "    content_tokens = tokenizer.encode(all_text, add_special_tokens=False)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    max_batch_size = max(BATCH_SIZES)\n",
    "    slices_needed = num_references * max_batch_size\n",
    "\n",
    "    if len(content_tokens) < slices_needed * TOKENS_PER_SLICE:\n",
    "        raise ValueError(f\"Need {slices_needed * TOKENS_PER_SLICE} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    # Tokenize template parts (prefix before snippet, suffix after)\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "    \n",
    "    suffix = f\"\"\"\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "    \n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    # Assemble prompts at token level\n",
    "    all_prompts = []\n",
    "    for i in range(slices_needed):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        all_prompts.append(prompt)\n",
    "    \n",
    "    # Verify\n",
    "    lengths = set(len(p) for p in all_prompts)\n",
    "    assert len(lengths) == 1, f\"Length mismatch: {lengths}\"\n",
    "    log_print(f\"All {slices_needed} prompts: {total_len} tokens each\")\n",
    "    \n",
    "    # Show sample ending\n",
    "    log_print(f\"\\nSample prompt ending:\")\n",
    "    log_print(repr(tokenizer.decode(all_prompts[0][-40:])))\n",
    "\n",
    "    reference_sequences = {}\n",
    "    dummy_sets = {}\n",
    "\n",
    "    for ref_idx in range(num_references):\n",
    "        ref_name = f\"ref_{ref_idx}\"\n",
    "        base_idx = ref_idx * max_batch_size\n",
    "        reference_sequences[ref_name] = all_prompts[base_idx]\n",
    "        dummy_sets[ref_name] = all_prompts[base_idx + 1 : base_idx + max_batch_size]\n",
    "\n",
    "    return reference_sequences, dummy_sets\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"attn_implementation\": ATTN_IMPLEMENTATION,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import flash_attn\n",
    "        info[\"flash_attn_version\"] = flash_attn.__version__\n",
    "    except ImportError:\n",
    "        info[\"flash_attn_version\"] = \"N/A\"\n",
    "        \n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    container_fields = ['python_version', 'cuda_version', 'cudnn_version']\n",
    "    pip_fields = ['torch_version', 'transformers_version', 'numpy_version', 'flash_attn_version']\n",
    "    config_fields = ['attn_implementation']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "    \n",
    "    container_mismatches = []\n",
    "    pip_mismatches = []\n",
    "    config_mismatches = []\n",
    "    \n",
    "    log_print(\"\\nScript configuration:\")\n",
    "    for field in config_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            config_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nContainer-level dependencies:\")\n",
    "    for field in container_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            container_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nPip-installable packages:\")\n",
    "    for field in pip_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        \n",
    "        if field == 'flash_attn_version':\n",
    "            ref_attn = reference_env.get('attn_implementation', '')\n",
    "            ver_attn = verifier_env.get('attn_implementation', '')\n",
    "            if ref_attn != 'flash_attention_2' and ver_attn != 'flash_attention_2':\n",
    "                log_print(f\"  - {field}: not using flash_attention_2 (skip)\")\n",
    "                continue\n",
    "            if ref_val == 'N/A' or ver_val == 'N/A':\n",
    "                log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "                pip_mismatches.append((field, ref_val, ver_val))\n",
    "                continue\n",
    "        \n",
    "        if ref_val == 'N/A' and ver_val == 'N/A':\n",
    "            log_print(f\"  - {field}: not installed (OK)\")\n",
    "            continue\n",
    "            \n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            pip_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not container_mismatches and not pip_mismatches and not config_mismatches:\n",
    "        log_print(\"\\n\" + \"-\"*60)\n",
    "        log_print(\"✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"✗ ENVIRONMENT MISMATCH - FIX REQUIRED\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# SIGNAL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_signals_from_output(outputs, layer_indices, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs from element 0 at specified position.\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    top_k = torch.topk(log_probs, k=20)\n",
    "\n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': top_k.indices.cpu().tolist(),\n",
    "        'log_probs': top_k.values.cpu().tolist()\n",
    "    }\n",
    "\n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract signals from multiple positions during prefill.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        prefill_signals[pos_label] = extract_signals_from_output(outputs, layer_indices, position=pos)\n",
    "    return prefill_signals\n",
    "\n",
    "def extract_signals_for_token_ids(outputs, layer_indices, token_ids, position=-1):\n",
    "    \"\"\"Extract signals for SPECIFIC token IDs (used in verification).\"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_ids_tensor = torch.tensor(token_ids, device=logits.device)\n",
    "    selected_logprobs = log_probs[token_ids_tensor]\n",
    "\n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': token_ids,\n",
    "        'log_probs': selected_logprobs.cpu().tolist()\n",
    "    }\n",
    "\n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals_for_token_ids(outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract prefill signals using reference token IDs.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        if pos_label in ref_prefill_signals:\n",
    "            ref_token_ids = ref_prefill_signals[pos_label]['logprobs']['token_ids']\n",
    "            prefill_signals[pos_label] = extract_signals_for_token_ids(\n",
    "                outputs, layer_indices, ref_token_ids, position=pos\n",
    "            )\n",
    "    return prefill_signals\n",
    "\n",
    "# ============================================================================\n",
    "# DECODE GENERATION (TEACHER_FORCING = False) - UPDATED FOR TOKEN IDS\n",
    "# ============================================================================\n",
    "\n",
    "def run_decode_with_extraction(model, tokenizer, ref_token_ids, ref_name, batch_size, \n",
    "                               layer_indices):\n",
    "    \"\"\"\n",
    "    Run decode generation and extract signals from last 3 generation steps.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    if batch_size == 1:\n",
    "        batch_token_ids = [ref_token_ids]\n",
    "    else:\n",
    "        batch_token_ids = [ref_token_ids] + ref_dummies[:batch_size-1]\n",
    "    \n",
    "    input_ids = torch.tensor(batch_token_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    log_print(f\"      Prompt: {prompt_length} tokens\", end=\"\")\n",
    "    \n",
    "    all_batch_generated_ids = [[] for _ in range(batch_size)]\n",
    "    generation_signals = []\n",
    "    \n",
    "    # FIRST STEP: Prefill\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "        \n",
    "    past_kv = outputs.past_key_values\n",
    "    prefill_signals = extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1])\n",
    "    \n",
    "    next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "    for batch_idx in range(batch_size):\n",
    "        all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "        \n",
    "    signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    \n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((inputs['attention_mask'].shape[0], 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS\n",
    "    for step in range(1, MAX_NEW_TOKENS):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "        for batch_idx in range(batch_size):\n",
    "            all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "            \n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((attention_mask.shape[0], 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "        if all_batch_generated_ids[0][-1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "    # Extract last 3 decode signals\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "        \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    final_length = prompt_length + num_generated\n",
    "    log_print(f\" → Final: {final_length} tokens ({num_generated} generated)\")\n",
    "    \n",
    "    return {\n",
    "        'generated_ids': all_batch_generated_ids[0],\n",
    "        'all_batch_generated_ids': all_batch_generated_ids,\n",
    "        'prompt_token_ids': batch_token_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TEACHER-FORCED DECODE (TEACHER_FORCING = True) - UPDATED FOR TOKEN IDS\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_decode(model, tokenizer, ref_name, reference_data, \n",
    "                              verify_batch_size, layer_indices, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced decode: feed reference tokens, extract signals.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ref_prompt_ids = reference_data['prompt_token_ids'][0]\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    ref_batch_size = len(reference_data['prompt_token_ids'])\n",
    "    \n",
    "    log_print(f\"      Prompt: {len(ref_prompt_ids)}, Gen: {len(ref_generated_ids)}\", end=\"\")\n",
    "    \n",
    "    if is_diagonal:\n",
    "        log_print(f\", exact neighbors (bs={ref_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = reference_data['prompt_token_ids']\n",
    "        batch_generated_ids = reference_data['all_batch_generated_ids']\n",
    "        actual_batch_size = ref_batch_size\n",
    "    else:\n",
    "        log_print(f\", arb neighbors (bs={verify_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = [ref_prompt_ids]\n",
    "        batch_generated_ids = [ref_generated_ids]\n",
    "        \n",
    "        ref_dummies = DUMMY_SETS[ref_name]\n",
    "        for i in range(verify_batch_size - 1):\n",
    "            batch_prompt_ids.append(ref_dummies[i])\n",
    "            batch_generated_ids.append([])\n",
    "        \n",
    "        actual_batch_size = verify_batch_size\n",
    "        \n",
    "    input_ids = torch.tensor(batch_prompt_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    generation_signals = []\n",
    "    num_steps = len(ref_generated_ids)\n",
    "    \n",
    "    # FIRST STEP: Prefill\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "        \n",
    "    past_kv = outputs.past_key_values\n",
    "    ref_prefill_signals = reference_data['prefill_signals']\n",
    "    prefill_signals = extract_prefill_signals_for_token_ids(\n",
    "        outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]\n",
    "    )\n",
    "    \n",
    "    ref_decode_signals = reference_data['decode_signals']\n",
    "    ref_step_data = list(ref_decode_signals.values())[0] if ref_decode_signals else None\n",
    "    if ref_step_data:\n",
    "        ref_token_ids = ref_step_data['signals']['logprobs']['token_ids']\n",
    "        signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "    else:\n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        \n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    # Prepare next tokens\n",
    "    if is_diagonal:\n",
    "        next_tokens = torch.tensor(\n",
    "            [batch_generated_ids[i][0] for i in range(actual_batch_size)],\n",
    "            dtype=torch.long, device='cuda'\n",
    "        )\n",
    "    else:\n",
    "        next_tokens_list = [ref_generated_ids[0]]\n",
    "        argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)\n",
    "        for i in range(actual_batch_size - 1):\n",
    "            next_tokens_list.append(argmax_tokens[i].item())\n",
    "            batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "        next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "        \n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((actual_batch_size, 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS\n",
    "    for step in range(1, num_steps):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        ref_signals_list = list(ref_decode_signals.values())\n",
    "        if step < len(ref_signals_list):\n",
    "            ref_token_ids = ref_signals_list[step]['signals']['logprobs']['token_ids']\n",
    "            signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "        else:\n",
    "            signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "            \n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        if step < num_steps - 1:\n",
    "            if is_diagonal:\n",
    "                next_tokens = torch.tensor(\n",
    "                    [batch_generated_ids[i][step] for i in range(actual_batch_size)],\n",
    "                    dtype=torch.long, device='cuda'\n",
    "                )\n",
    "            else:\n",
    "                next_tokens_list = [ref_generated_ids[step]]\n",
    "                argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)\n",
    "                for i in range(actual_batch_size - 1):\n",
    "                    next_tokens_list.append(argmax_tokens[i].item())\n",
    "                    batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "                next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((actual_batch_size, 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "    # Extract last 3\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "        \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(f\" → {num_generated} steps\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(decode_measurements, tokenizer):\n",
    "    \"\"\"Verify element 0 generates identical tokens across all batch sizes.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    tokens_by_bs = {}\n",
    "    for bs, data in decode_measurements.items():\n",
    "        tokens_by_bs[bs] = data['generated_ids']\n",
    "        \n",
    "    bs_list = sorted(tokens_by_bs.keys())\n",
    "    reference_tokens = tokens_by_bs[bs_list[0]]\n",
    "    \n",
    "    all_same = True\n",
    "    log_print(\"\\nGenerated tokens by batch size:\")\n",
    "    for bs in bs_list:\n",
    "        tokens = tokens_by_bs[bs]\n",
    "        match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "        decoded_text = tokenizer.decode(tokens)\n",
    "        log_print(f\"  bs={bs}:\")\n",
    "        log_print(f\"    IDs:  {tokens}\")\n",
    "        log_print(f\"    Text: {repr(decoded_text)}\")\n",
    "        log_print(f\"    {match_str}\")\n",
    "        if tokens != reference_tokens:\n",
    "            all_same = False\n",
    "            \n",
    "    if all_same:\n",
    "        log_print(\"\\n✓ Element 0 generates IDENTICAL tokens across all batch sizes\")\n",
    "    else:\n",
    "        log_print(\"\\n⚠ Element 0 generates DIFFERENT tokens across batch sizes\")\n",
    "        \n",
    "    return all_same\n",
    "\n",
    "def compute_l2_distance(vec1, vec2):\n",
    "    \"\"\"Compute L2 distance between two vectors.\"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return float(np.linalg.norm(v1 - v2))\n",
    "\n",
    "def compute_logprob_distance(logprobs1, logprobs2):\n",
    "    \"\"\"\n",
    "    Compute L2 distance between logprob distributions.\n",
    "    Uses top 5 token IDs from first signal as canonical (stored top 20 as buffer).\n",
    "    \"\"\"\n",
    "    canonical_ids = logprobs1['token_ids'][:5]\n",
    "    vec1 = logprobs1['log_probs'][:5]\n",
    "    \n",
    "    map2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "    \n",
    "    vec2 = []\n",
    "    for tid in canonical_ids:\n",
    "        if tid in map2:\n",
    "            vec2.append(map2[tid])\n",
    "        else:\n",
    "            return float('inf')\n",
    "    \n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2, layer_indices):\n",
    "    \"\"\"Compare two signal sets, return distances.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "    \n",
    "    all_key_dists = []\n",
    "    all_logprob_dists = []\n",
    "    \n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]['signals'] if 'signals' in signals1[pos_label] else signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]['signals'] if 'signals' in signals2[pos_label] else signals2[pos_label]\n",
    "        \n",
    "        for layer_name in sig1['key_vectors'].keys():\n",
    "            dist = compute_l2_distance(\n",
    "                sig1['key_vectors'][layer_name],\n",
    "                sig2['key_vectors'][layer_name]\n",
    "            )\n",
    "            all_key_dists.append(dist)\n",
    "            \n",
    "        dist = compute_logprob_distance(sig1['logprobs'], sig2['logprobs'])\n",
    "        all_logprob_dists.append(dist)\n",
    "        \n",
    "    return {\n",
    "        'key_vectors_max': max(all_key_dists) if all_key_dists else 0.0,\n",
    "        'key_vectors_mean': np.mean(all_key_dists) if all_key_dists else 0.0,\n",
    "        'logprobs_max': max(all_logprob_dists) if all_logprob_dists else 0.0,\n",
    "        'logprobs_mean': np.mean(all_logprob_dists) if all_logprob_dists else 0.0\n",
    "    }\n",
    "\n",
    "def find_equivalent_pairs(matrix, batch_sizes, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"Find pairs of batch sizes that produce equivalent results (same kernel).\"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n_bs = len(batch_sizes)\n",
    "    \n",
    "    for i in range(n_bs):\n",
    "        for j in range(i + 1, n_bs):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((batch_sizes[i], batch_sizes[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, batch_sizes):\n",
    "    \"\"\"Group batch sizes into kernel equivalence classes.\"\"\"\n",
    "    parent = {bs: bs for bs in batch_sizes}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        union(bs1, bs2)\n",
    "    \n",
    "    groups = {}\n",
    "    for bs in batch_sizes:\n",
    "        root = find(bs)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(bs)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "def analyze_within_hardware(measurements, batch_sizes, layer_indices, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware batch size effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE BATCH SIZE EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    by_ref = {}\n",
    "    for m in measurements:\n",
    "        ref = m['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "        by_ref[ref][m['batch_size']] = m[signals_key]\n",
    "\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix_key = np.zeros((n_bs, n_bs))\n",
    "        matrix_logprob = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, bs1 in enumerate(batch_sizes):\n",
    "            for j, bs2 in enumerate(batch_sizes):\n",
    "                if bs1 in ref_data and bs2 in ref_data:\n",
    "                    distances = compare_signals(ref_data[bs1], ref_data[bs2], layer_indices)\n",
    "                    matrix_key[i, j] = distances['key_vectors_mean']\n",
    "                    matrix_logprob[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "        \n",
    "        log_print(f\"\\nKey Vectors (mean L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, bs in enumerate(batch_sizes):\n",
    "            row_str = f\"bs={bs:<3}\"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        log_print(f\"\\nLogprobs (mean L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, bs in enumerate(batch_sizes):\n",
    "            row_str = f\"bs={bs:<3}\"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors (mean L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        row_str = f\"bs={bs:<3}\"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_key_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    log_print(f\"\\nLogprobs (mean L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        row_str = f\"bs={bs:<3}\"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_logprob_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    off_diag_key = avg_key_matrix[np.triu_indices(n_bs, k=1)]\n",
    "    off_diag_logprob = avg_logprob_matrix[np.triu_indices(n_bs, k=1)]\n",
    "    \n",
    "    key_mean = np.mean(off_diag_key[np.isfinite(off_diag_key)]) if np.any(np.isfinite(off_diag_key)) else 0\n",
    "    logprob_mean = np.mean(off_diag_logprob[np.isfinite(off_diag_logprob)]) if np.any(np.isfinite(off_diag_logprob)) else 0\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    log_print(f\"  Key vectors - Mean: {key_mean:.2e}, Range: [{np.min(off_diag_key):.2e}, {np.max(off_diag_key):.2e}]\")\n",
    "    log_print(f\"  Logprobs - Mean: {logprob_mean:.2e}, Range: [{np.min(off_diag_logprob):.2e}, {np.max(off_diag_logprob):.2e}]\")\n",
    "\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_key_matrix, batch_sizes)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, batch_sizes)\n",
    "    \n",
    "    zero_count = np.sum(off_diag_key < EQUIVALENCE_THRESHOLD)\n",
    "    total_count = len(off_diag_key)\n",
    "    \n",
    "    if zero_count == total_count:\n",
    "        log_print(f\"\\n⚠ WARNING: {zero_count}/{total_count} comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All batch sizes produce identical results (single kernel class)\")\n",
    "    elif zero_count > 0:\n",
    "        log_print(f\"\\n⚠ NOTE: {zero_count}/{total_count} comparisons are equivalent (< {EQUIVALENCE_THRESHOLD})\")\n",
    "    \n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs (will be excluded from cross-hardware signal):\")\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2})\")\n",
    "    \n",
    "    threshold = 1e-10\n",
    "    if key_mean > threshold and logprob_mean > threshold:\n",
    "        log_print(\"\\n✓ SANITY CHECK PASSED\")\n",
    "    else:\n",
    "        log_print(\"\\n✗ SANITY CHECK FAILED (No variation)\")\n",
    "        \n",
    "    return {\n",
    "        'key_matrix': avg_key_matrix.tolist(),\n",
    "        'logprob_matrix': avg_logprob_matrix.tolist(),\n",
    "        'per_reference_key_matrices': [m.tolist() for m in all_key_matrices],\n",
    "        'per_reference_logprob_matrices': [m.tolist() for m in all_logprob_matrices],\n",
    "        'key_vectors_mean': float(key_mean),\n",
    "        'logprobs_mean': float(logprob_mean),\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes]\n",
    "    }\n",
    "\n",
    "def analyze_cross_hardware_matrix(comparison_results, batch_sizes, layer_indices, \n",
    "                                   signal_source='decode', equivalent_pairs=None):\n",
    "    \"\"\"\n",
    "    Analyze the comparison matrix and determine detectability.\n",
    "    Excludes equivalent pairs from SNR signal calculation.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    equiv_set = set()\n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        equiv_set.add((bs1, bs2))\n",
    "        equiv_set.add((bs2, bs1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_batch_size'], result['verify_batch_size'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix_key = np.zeros((n_bs, n_bs))\n",
    "        matrix_logprob = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            for j, verify_bs in enumerate(batch_sizes):\n",
    "                key = (claimed_bs, verify_bs)\n",
    "                if key in ref_data:\n",
    "                    matrix_key[i, j] = ref_data[key][dist_key]['key_vectors_mean']\n",
    "                    matrix_logprob[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "        \n",
    "        log_print(f\"\\nKey Vectors (mean L2):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        log_print(f\"\\nLogprobs (mean L2):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "\n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors (mean L2):\")\n",
    "    log_print(header)\n",
    "    for i, claimed_bs in enumerate(batch_sizes):\n",
    "        row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_key_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    log_print(f\"\\nLogprobs (mean L2):\")\n",
    "    log_print(header)\n",
    "    for i, claimed_bs in enumerate(batch_sizes):\n",
    "        row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_logprob_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    diag_key = np.diag(avg_key_matrix)\n",
    "    diag_logprob = np.diag(avg_logprob_matrix)\n",
    "    diag_key_mean = np.mean(diag_key)\n",
    "    diag_logprob_mean = np.mean(diag_logprob)\n",
    "\n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same batch size):\")\n",
    "    log_print(f\"  Key vectors: {diag_key_mean:.2e}\")\n",
    "    log_print(f\"  Logprobs: {diag_logprob_mean:.2e}\")\n",
    "\n",
    "    off_diag_key = []\n",
    "    off_diag_logprob = []\n",
    "    for i in range(n_bs):\n",
    "        for j in range(n_bs):\n",
    "            if i != j:\n",
    "                off_diag_key.append(avg_key_matrix[i, j])\n",
    "                off_diag_logprob.append(avg_logprob_matrix[i, j])\n",
    "    \n",
    "    off_diag_key = np.array(off_diag_key)\n",
    "    off_diag_logprob = np.array(off_diag_logprob)\n",
    "    \n",
    "    off_key_mean = np.mean(off_diag_key)\n",
    "    off_logprob_mean = np.mean(off_diag_logprob)\n",
    "    \n",
    "    snr_key = off_key_mean / diag_key_mean if diag_key_mean > 0 else float('inf')\n",
    "    snr_logprob = off_logprob_mean / diag_logprob_mean if diag_logprob_mean > 0 else float('inf')\n",
    "\n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Key vectors - Mean: {off_key_mean:.2e}, SNR: {snr_key:.2f}×\")\n",
    "    log_print(f\"  Logprobs - Mean: {off_logprob_mean:.2e}, SNR: {snr_logprob:.2f}×\")\n",
    "\n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        excluded_count = 0\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2}) and ({bs2}, {bs1})\")\n",
    "            excluded_count += 2\n",
    "        log_print(f\"  Total excluded: {excluded_count} cells\")\n",
    "        \n",
    "        meaningful_key = []\n",
    "        meaningful_logprob = []\n",
    "        for i in range(n_bs):\n",
    "            for j in range(n_bs):\n",
    "                if i != j:\n",
    "                    bs_i, bs_j = batch_sizes[i], batch_sizes[j]\n",
    "                    if (bs_i, bs_j) not in equiv_set:\n",
    "                        meaningful_key.append(avg_key_matrix[i, j])\n",
    "                        meaningful_logprob.append(avg_logprob_matrix[i, j])\n",
    "        \n",
    "        if meaningful_key:\n",
    "            meaningful_key = np.array(meaningful_key)\n",
    "            meaningful_logprob = np.array(meaningful_logprob)\n",
    "            meaningful_key_mean = np.mean(meaningful_key)\n",
    "            meaningful_logprob_mean = np.mean(meaningful_logprob)\n",
    "            \n",
    "            snr_key_meaningful = meaningful_key_mean / diag_key_mean if diag_key_mean > 0 else float('inf')\n",
    "            snr_logprob_meaningful = meaningful_logprob_mean / diag_logprob_mean if diag_logprob_mean > 0 else float('inf')\n",
    "            \n",
    "            log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "            log_print(f\"  Count: {len(meaningful_key)}\")\n",
    "            log_print(f\"  Key vectors - Mean: {meaningful_key_mean:.2e}, SNR: {snr_key_meaningful:.2f}×\")\n",
    "            log_print(f\"  Logprobs - Mean: {meaningful_logprob_mean:.2e}, SNR: {snr_logprob_meaningful:.2f}×\")\n",
    "        else:\n",
    "            snr_key_meaningful = snr_key\n",
    "            snr_logprob_meaningful = snr_logprob\n",
    "    else:\n",
    "        snr_key_meaningful = snr_key\n",
    "        snr_logprob_meaningful = snr_logprob\n",
    "\n",
    "    return {\n",
    "        'key_vectors': {\n",
    "            'diagonal_mean': float(diag_key_mean),\n",
    "            'off_diagonal_mean': float(off_key_mean),\n",
    "            'snr': float(snr_key),\n",
    "            'snr_meaningful': float(snr_key_meaningful)\n",
    "        },\n",
    "        'logprobs': {\n",
    "            'diagonal_mean': float(diag_logprob_mean),\n",
    "            'off_diagonal_mean': float(off_logprob_mean),\n",
    "            'snr': float(snr_logprob),\n",
    "            'snr_meaningful': float(snr_logprob_meaningful)\n",
    "        },\n",
    "        'matrices': {\n",
    "            'key_vectors': avg_key_matrix.tolist(),\n",
    "            'logprobs': avg_logprob_matrix.tolist()\n",
    "        },\n",
    "        'excluded_pairs': equivalent_pairs\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    global REFERENCE_SEQUENCES, DUMMY_SETS\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    \n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION (reference)\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"Attention: {ATTN_IMPLEMENTATION}\")\n",
    "    log_print(f\"Layers: {LAYER_INDICES}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    REFERENCE_SEQUENCES, DUMMY_SETS = create_sequences_from_pdf(tokenizer)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=ATTN_IMPLEMENTATION\n",
    "    )\n",
    "    \n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        # VERIFICATION MODE\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            reference = json.load(f)\n",
    "            \n",
    "        ref_env = reference['metadata']['environment']\n",
    "        validate_environment_match(ref_env, system_info)\n",
    "        \n",
    "        prefill_sanity = reference.get('prefill_sanity_check', {})\n",
    "        decode_sanity = reference.get('decode_sanity_check', {})\n",
    "        \n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_sanity.get('equivalent_pairs', [])]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_sanity.get('equivalent_pairs', [])]\n",
    "        \n",
    "        log_print(f\"\\nLoaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "        \n",
    "        comparison_results = []\n",
    "        ref_by_key = {}\n",
    "        for m in reference['measurements']:\n",
    "            key = (m['ref_name'], m['batch_size'])\n",
    "            ref_by_key[key] = m\n",
    "            \n",
    "        for ref_name in sorted(REFERENCE_SEQUENCES.keys()):\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            for claimed_bs in BATCH_SIZES:\n",
    "                ref_key = (ref_name, claimed_bs)\n",
    "                if ref_key not in ref_by_key:\n",
    "                    log_print(f\"  ⚠ No reference data for {ref_name} bs={claimed_bs}\")\n",
    "                    continue\n",
    "                ref_data = ref_by_key[ref_key]\n",
    "                \n",
    "                log_print(f\"\\n  Claimed batch size: {claimed_bs}\")\n",
    "                \n",
    "                for verify_bs in BATCH_SIZES:\n",
    "                    is_diagonal = (claimed_bs == verify_bs)\n",
    "                    \n",
    "                    log_print(f\"    Verify bs={verify_bs} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "                    \n",
    "                    verify_result = run_teacher_forced_decode(\n",
    "                        model, tokenizer, ref_name, ref_data,\n",
    "                        verify_bs, LAYER_INDICES, is_diagonal\n",
    "                    )\n",
    "                    \n",
    "                    prefill_distances = compare_signals(\n",
    "                        ref_data['prefill_signals'], verify_result['prefill_signals'], LAYER_INDICES\n",
    "                    )\n",
    "                    decode_distances = compare_signals(\n",
    "                        ref_data['decode_signals'], verify_result['decode_signals'], LAYER_INDICES\n",
    "                    )\n",
    "                    \n",
    "                    log_print(f\"      Key: {decode_distances['key_vectors_mean']:.2e}, LP: {decode_distances['logprobs_mean']:.2e}\")\n",
    "                    \n",
    "                    comparison_results.append({\n",
    "                        'ref_name': ref_name,\n",
    "                        'claimed_batch_size': claimed_bs,\n",
    "                        'verify_batch_size': verify_bs,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances,\n",
    "                    })\n",
    "\n",
    "        prefill_analysis = analyze_cross_hardware_matrix(\n",
    "            comparison_results, BATCH_SIZES, LAYER_INDICES, \n",
    "            signal_source='prefill', equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware_matrix(\n",
    "            comparison_results, BATCH_SIZES, LAYER_INDICES,\n",
    "            signal_source='decode', equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "        \n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON (meaningful SNR)\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill - Key: {prefill_analysis['key_vectors']['snr_meaningful']:.2f}×, LP: {prefill_analysis['logprobs']['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode  - Key: {decode_analysis['key_vectors']['snr_meaningful']:.2f}×, LP: {decode_analysis['logprobs']['snr_meaningful']:.2f}×\")\n",
    "        \n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'layer_indices': LAYER_INDICES,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "        filepath = os.path.join(output_dir, f\"verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "            \n",
    "    else:\n",
    "        # GENERATION MODE\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'layer_indices': LAYER_INDICES\n",
    "            },\n",
    "            'measurements': []\n",
    "        }\n",
    "        \n",
    "        for ref_name, ref_token_ids in REFERENCE_SEQUENCES.items():\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            log_print(f\"\\nPrompt length: {len(ref_token_ids)} tokens\\n\")\n",
    "            \n",
    "            for batch_size in BATCH_SIZES:\n",
    "                log_print(f\"  bs={batch_size}:\", end=\"\")\n",
    "                decode_data = run_decode_with_extraction(\n",
    "                    model, tokenizer, ref_token_ids, ref_name, batch_size, LAYER_INDICES\n",
    "                )\n",
    "                results['measurements'].append({\n",
    "                    'ref_name': ref_name,\n",
    "                    'batch_size': batch_size,\n",
    "                    'generated_ids': decode_data['generated_ids'],\n",
    "                    'prompt_token_ids': decode_data['prompt_token_ids'],\n",
    "                    'prefill_signals': decode_data['prefill_signals'],\n",
    "                    'decode_signals': decode_data['decode_signals'],\n",
    "                    'all_batch_generated_ids': decode_data['all_batch_generated_ids']\n",
    "                })\n",
    "        \n",
    "        # Token consistency check\n",
    "        for ref_name in REFERENCE_SEQUENCES.keys():\n",
    "            log_print(f\"\\n--- Token consistency for {ref_name} ---\")\n",
    "            ref_measurements = {m['batch_size']: m for m in results['measurements'] if m['ref_name'] == ref_name}\n",
    "            check_token_consistency(ref_measurements, tokenizer)\n",
    "                \n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, LAYER_INDICES, 'prefill'\n",
    "        )\n",
    "        decode_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, LAYER_INDICES, 'decode'\n",
    "        )\n",
    "        \n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"decode_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        log_print(f\"\\n✓ Saved to: {filepath}\")\n",
    "        log_print(f\"\\nNext step: Copy to verifier machine, set TEACHER_FORCING=True\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4face-f066-4ac2-b379-5d6c1a46e210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
