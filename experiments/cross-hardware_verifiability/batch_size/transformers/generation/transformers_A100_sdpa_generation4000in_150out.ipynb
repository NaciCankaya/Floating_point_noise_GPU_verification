{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98fb9e08-8680-473f-b415-54312ed3069b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CROSS-HARDWARE BATCH SIZE DETECTABILITY - GENERATION (reference)\n",
      "================================================================================\n",
      "\n",
      "System: c6c6242ae438\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "Attention: sdpa\n",
      "Layers: [28]\n",
      "Found 2 PDF(s)\n",
      "  Loading: /workspace/Llama3.1.pdf\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (219497 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total source tokens: 219497\n",
      "Prompt structure: 33 prefix + 4000 snippet + 34 suffix = 4067 tokens\n",
      "All 51 prompts: 4067 tokens each\n",
      "\n",
      "Sample prompt ending:\n",
      "' websites\\nare likely to contain\"\\n\\nBased on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\\n<|im_start|>assistant\\n'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4e72a19fcc42bbb41b01947c9e8683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_0\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 4067 tokens\n",
      "\n",
      "  bs=1:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=2:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=3:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=4:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=5:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=8:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=9:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=16:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=17:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_1\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 4067 tokens\n",
      "\n",
      "  bs=1:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=2:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=3:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=4:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=5:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=8:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=9:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=16:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=17:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_2\n",
      "================================================================================\n",
      "\n",
      "Prompt length: 4067 tokens\n",
      "\n",
      "  bs=1:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=2:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=3:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=4:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=5:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=8:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=9:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=16:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "  bs=17:      Prompt: 4067 tokens → Final: 4217 tokens (150 generated)\n",
      "\n",
      "--- Token consistency for ref_0 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 16460, 315, 264, 501, 738, 315, 16266, 4119, 2598, 444, 80001, 220, 18, 13, 576, 2197, 594, 3832, 4925, 374, 30188, 2163, 20443, 11229, 11, 11689, 3460, 4128, 4119, 323, 862, 16928, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 374, 32930, 304, 264, 1616, 429, 374, 14260, 315, 3412, 15689, 476, 10916, 6682, 13, 1084, 5646, 14158, 1075, 330, 37155, 1335, 330, 4703, 12, 36930, 1335, 330, 4133, 12, 36930, 1335, 323, 330, 81330, 39288, 1335, 892, 525, 4185, 304, 14250, 476, 10916, 4378, 13, 576, 9362, 315, 12632, 11, 12396, 11, 323, 11682, 27787, 315, 80798, 323, 3059, 4623, 11554, 419, 23850, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 6028, 5244, 374, 389, 444, 80001, 220, 18, 11, 264, 501, 738]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and evaluation of a new set of foundation models called Llama 3. The document\\'s subject matter is centered around artificial intelligence, specifically large language models and their capabilities. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document is structured in a way that is typical of research papers or technical reports. It includes sections like \"Introduction,\" \"Pre-Training,\" \"Post-Training,\" and \"Experimental Evaluation,\" which are common in academic or technical writing. The presence of tables, figures, and detailed descriptions of methodologies and results further supports this classification.\\n\\n2. **Subject Matter**: The primary focus is on Llama 3, a new set'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 16460, 315, 264, 501, 738, 315, 16266, 4119, 2598, 444, 80001, 220, 18, 13, 576, 2197, 594, 3832, 4925, 374, 30188, 2163, 20443, 11229, 11, 11689, 3460, 4128, 4119, 323, 862, 16928, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 12033, 448, 264, 11682, 16800, 323, 23251, 315, 279, 444, 80001, 220, 18, 4119, 11, 2670, 862, 17646, 11, 4862, 1882, 11, 323, 16460, 13, 1084, 1083, 5646, 12632, 323, 12396, 11, 892, 525, 14260, 304, 3412, 15689, 311, 3042, 821, 323, 3059, 13, 576, 9362, 315, 264, 11682, 25305, 1140, 304, 279, 87577, 4623, 11554, 279, 4522, 429, 419, 374, 264, 15908, 3412, 2197, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2197, 34334, 279, 4401, 315, 444, 80001, 220, 18, 11, 264]\n",
      "    Text: \"This excerpt is from a research paper or technical report, likely focused on the development and evaluation of a new set of foundation models called Llama 3. The document's subject matter is centered around artificial intelligence, specifically large language models and their capabilities. Here's the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document begins with a detailed introduction and overview of the Llama 3 models, including their architecture, training process, and evaluation. It also includes tables and figures, which are typical in research papers to present data and results. The presence of a detailed contributor list in the appendix further supports the idea that this is a formal research document.\\n\\n2. **Subject Matter**: The document discusses the development of Llama 3, a\"\n",
      "    ✗ DIFFERENT\n",
      "  bs=3:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 16460, 315, 264, 501, 738, 315, 16266, 4119, 2598, 444, 80001, 220, 18, 13, 576, 2197, 594, 3832, 4925, 374, 30188, 2163, 20443, 11229, 11, 11689, 3460, 4128, 4119, 323, 862, 16928, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 12033, 448, 264, 11682, 16800, 323, 23251, 315, 279, 444, 80001, 220, 18, 4119, 11, 2670, 862, 17646, 11, 4862, 1882, 11, 323, 16460, 13, 1084, 1083, 5646, 12632, 323, 12396, 11, 892, 525, 14260, 304, 3412, 15689, 311, 3042, 821, 323, 3059, 13, 576, 9362, 315, 264, 11682, 25305, 1140, 304, 279, 87577, 4623, 11554, 279, 4522, 429, 419, 374, 264, 15908, 3412, 2197, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 2197, 34334, 279, 4401, 315, 444, 80001, 220, 18, 11, 264]\n",
      "    Text: \"This excerpt is from a research paper or technical report, likely focused on the development and evaluation of a new set of foundation models called Llama 3. The document's subject matter is centered around artificial intelligence, specifically large language models and their capabilities. Here's the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document begins with a detailed introduction and overview of the Llama 3 models, including their architecture, training process, and evaluation. It also includes tables and figures, which are typical in research papers to present data and results. The presence of a detailed contributor list in the appendix further supports the idea that this is a formal research document.\\n\\n2. **Subject Matter**: The document discusses the development of Llama 3, a\"\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 16460, 315, 264, 501, 738, 315, 16266, 4119, 2598, 444, 80001, 220, 18, 13, 576, 2197, 594, 3832, 4925, 374, 30188, 2163, 20443, 11229, 11, 11689, 3460, 4128, 4119, 323, 862, 16928, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 374, 32930, 304, 264, 1616, 429, 374, 14260, 315, 3412, 15689, 476, 10916, 6682, 13, 1084, 5646, 14158, 1075, 330, 37155, 1335, 330, 4703, 12, 36930, 1335, 330, 4133, 12, 36930, 1335, 323, 330, 40404, 318, 57597, 55795, 1335, 892, 525, 4185, 304, 14250, 476, 10916, 4378, 13, 576, 9362, 315, 12632, 11, 12396, 11, 323, 11682, 27787, 315, 80798, 323, 3059, 4623, 11554, 419, 23850, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 6028, 5244, 374, 389, 444, 80001, 220, 18, 11, 264]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and evaluation of a new set of foundation models called Llama 3. The document\\'s subject matter is centered around artificial intelligence, specifically large language models and their capabilities. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document is structured in a way that is typical of research papers or technical reports. It includes sections like \"Introduction,\" \"Pre-Training,\" \"Post-Training,\" and \"Multimodal Extensions,\" which are common in academic or technical writing. The presence of tables, figures, and detailed descriptions of methodologies and results further supports this classification.\\n\\n2. **Subject Matter**: The primary focus is on Llama 3, a'\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 16460, 315, 264, 501, 738, 315, 16266, 4119, 2598, 444, 80001, 220, 18, 13, 576, 2197, 594, 3832, 4925, 374, 30188, 2163, 20443, 11229, 11, 11689, 3460, 4128, 4119, 323, 862, 16928, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 374, 32930, 304, 264, 1616, 429, 374, 14260, 315, 3412, 15689, 476, 10916, 6682, 13, 1084, 5646, 14158, 1075, 330, 37155, 1335, 330, 4703, 12, 36930, 1335, 330, 4133, 12, 36930, 1335, 323, 330, 81330, 39288, 1335, 892, 525, 4185, 304, 14250, 476, 10916, 4378, 13, 576, 9362, 315, 12632, 11, 12396, 11, 323, 11682, 27787, 315, 80798, 323, 3059, 4623, 11554, 419, 23850, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 6028, 5244, 374, 389, 444, 80001, 220, 18, 11, 264, 501, 738]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and evaluation of a new set of foundation models called Llama 3. The document\\'s subject matter is centered around artificial intelligence, specifically large language models and their capabilities. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document is structured in a way that is typical of research papers or technical reports. It includes sections like \"Introduction,\" \"Pre-Training,\" \"Post-Training,\" and \"Experimental Evaluation,\" which are common in academic or technical writing. The presence of tables, figures, and detailed descriptions of methodologies and results further supports this classification.\\n\\n2. **Subject Matter**: The primary focus is on Llama 3, a new set'\n",
      "    ✓\n",
      "  bs=8:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 16460, 315, 264, 501, 738, 315, 16266, 4119, 2598, 444, 80001, 220, 18, 13, 576, 2197, 594, 3832, 4925, 374, 30188, 2163, 20443, 11229, 11, 11689, 3460, 4128, 4119, 323, 862, 16928, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 374, 32930, 304, 264, 1616, 429, 374, 14260, 315, 3412, 15689, 476, 10916, 6682, 13, 1084, 5646, 14158, 1075, 330, 37155, 1335, 330, 4703, 12, 36930, 1335, 330, 4133, 12, 36930, 1335, 323, 330, 40404, 318, 57597, 55795, 1335, 892, 525, 4185, 304, 14250, 476, 10916, 4378, 13, 576, 9362, 315, 12632, 11, 12396, 11, 323, 11682, 27787, 315, 80798, 323, 3059, 4623, 11554, 419, 23850, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 6028, 5244, 374, 389, 444, 80001, 220, 18, 11, 264]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and evaluation of a new set of foundation models called Llama 3. The document\\'s subject matter is centered around artificial intelligence, specifically large language models and their capabilities. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document is structured in a way that is typical of research papers or technical reports. It includes sections like \"Introduction,\" \"Pre-Training,\" \"Post-Training,\" and \"Multimodal Extensions,\" which are common in academic or technical writing. The presence of tables, figures, and detailed descriptions of methodologies and results further supports this classification.\\n\\n2. **Subject Matter**: The primary focus is on Llama 3, a'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 16460, 315, 264, 501, 738, 315, 16266, 4119, 2598, 444, 80001, 220, 18, 13, 576, 2197, 594, 3832, 4925, 374, 30188, 2163, 20443, 11229, 11, 11689, 3460, 4128, 4119, 323, 862, 16928, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 374, 32930, 304, 264, 1616, 429, 374, 14260, 315, 3412, 15689, 476, 10916, 6682, 13, 1084, 5646, 14158, 1075, 330, 37155, 1335, 330, 4703, 12, 36930, 1335, 330, 4133, 12, 36930, 1335, 323, 330, 40404, 318, 57597, 55795, 1335, 892, 525, 4185, 304, 14250, 476, 10916, 4378, 13, 576, 9362, 315, 12632, 11, 12396, 11, 323, 11682, 27787, 315, 80798, 323, 3059, 4623, 11554, 419, 23850, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 6028, 5244, 374, 389, 444, 80001, 220, 18, 11, 264]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and evaluation of a new set of foundation models called Llama 3. The document\\'s subject matter is centered around artificial intelligence, specifically large language models and their capabilities. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document is structured in a way that is typical of research papers or technical reports. It includes sections like \"Introduction,\" \"Pre-Training,\" \"Post-Training,\" and \"Multimodal Extensions,\" which are common in academic or technical writing. The presence of tables, figures, and detailed descriptions of methodologies and results further supports this classification.\\n\\n2. **Subject Matter**: The primary focus is on Llama 3, a'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 16460, 315, 264, 501, 738, 315, 16266, 4119, 2598, 444, 80001, 220, 18, 13, 576, 2197, 594, 3832, 4925, 374, 30188, 2163, 20443, 11229, 11, 11689, 3460, 4128, 4119, 323, 862, 16928, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 3851, 323, 28338, 334, 510, 256, 481, 576, 2197, 12033, 448, 264, 2265, 429, 33845, 330, 43, 80001, 220, 18, 6252, 67, 315, 26874, 1, 323, 374, 66113, 553, 279, 444, 80001, 7909, 518, 15819, 13, 1096, 14807, 432, 374, 264, 10916, 2197, 5435, 311, 15235, 3412, 624, 256, 481, 576, 16800, 34334, 279, 4401, 315, 16266, 4119, 11, 892, 525, 4586, 58238, 15235, 4119, 6188, 311, 1824, 264, 6884, 2088, 315, 9079, 382, 17, 13, 3070, 62226, 12309, 334, 510, 256, 481, 576, 2197, 5707, 11682, 10916, 1995, 911, 279, 444, 80001]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and evaluation of a new set of foundation models called Llama 3. The document\\'s subject matter is centered around artificial intelligence, specifically large language models and their capabilities.\\n\\n### Reasoning:\\n\\n1. **Title and Introduction**:\\n   - The document begins with a title that mentions \"Llama 3 Herd of Models\" and is authored by the Llama Team at Meta. This indicates it is a technical document related to AI research.\\n   - The introduction discusses the development of foundation models, which are general-purpose AI models designed to support a wide range of tasks.\\n\\n2. **Technical Details**:\\n   - The document provides detailed technical information about the Llama'\n",
      "    ✗ DIFFERENT\n",
      "  bs=17:\n",
      "    IDs:  [1986, 49465, 374, 504, 264, 3412, 5567, 476, 10916, 1895, 11, 4363, 10735, 389, 279, 4401, 323, 16460, 315, 264, 501, 738, 315, 16266, 4119, 2598, 444, 80001, 220, 18, 13, 576, 2197, 594, 3832, 4925, 374, 30188, 2163, 20443, 11229, 11, 11689, 3460, 4128, 4119, 323, 862, 16928, 13, 5692, 594, 279, 32711, 4815, 419, 16688, 1447, 16, 13, 3070, 7524, 3990, 95518, 576, 2197, 374, 32930, 304, 264, 1616, 429, 374, 14260, 315, 3412, 15689, 476, 10916, 6682, 13, 1084, 5646, 14158, 1075, 330, 37155, 1335, 330, 4703, 12, 36930, 1335, 330, 4133, 12, 36930, 1335, 323, 330, 40404, 318, 57597, 55795, 1335, 892, 525, 4185, 304, 14250, 476, 10916, 4378, 13, 576, 9362, 315, 12632, 11, 12396, 11, 323, 11682, 27787, 315, 80798, 323, 3059, 4623, 11554, 419, 23850, 382, 17, 13, 3070, 13019, 33364, 95518, 576, 6028, 5244, 374, 389, 444, 80001, 220, 18, 11, 264]\n",
      "    Text: 'This excerpt is from a research paper or technical report, likely focused on the development and evaluation of a new set of foundation models called Llama 3. The document\\'s subject matter is centered around artificial intelligence, specifically large language models and their capabilities. Here\\'s the reasoning behind this conclusion:\\n\\n1. **Document Type**: The document is structured in a way that is typical of research papers or technical reports. It includes sections like \"Introduction,\" \"Pre-Training,\" \"Post-Training,\" and \"Multimodal Extensions,\" which are common in academic or technical writing. The presence of tables, figures, and detailed descriptions of methodologies and results further supports this classification.\\n\\n2. **Subject Matter**: The primary focus is on Llama 3, a'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "--- Token consistency for ref_1 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 476, 10916, 1895, 11, 4363, 5435, 311, 264, 3460, 4128, 1614, 476, 264, 4013, 315, 4128, 4119, 13, 576, 2197, 11469, 264, 3460, 1372, 315, 20343, 323, 54930, 862, 1824, 11, 892, 374, 14260, 369, 3412, 15689, 476, 10916, 6682, 13, 22406, 11, 279, 2197, 5646, 15057, 311, 10916, 6682, 323, 3412, 15689, 11, 4623, 12613, 419, 16688, 382, 785, 3832, 4925, 315, 279, 2197, 374, 4363, 5435, 311, 279, 4401, 323, 15673, 315, 264, 4128, 1614, 11, 10767, 6941, 330, 43, 80001, 220, 18, 1, 3118, 389, 279, 15823, 26139, 3772, 13, 1096, 374, 68357, 553, 279, 11682, 1140, 315, 20343, 323, 279, 6286, 315, 1824, 369, 330, 43, 80001, 220, 18, 1, 304, 279, 15823, 26139, 13, 576, 2197, 1083, 5646, 15057, 311, 1008, 4128, 4119, 323, 3412, 15689, 11, 892, 13230, 429, 432]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research or technical report, likely related to a large language model or a series of language models. The document lists a large number of contributors and acknowledges their support, which is typical for research papers or technical reports. Additionally, the document includes references to technical reports and research papers, further supporting this conclusion.\\n\\nThe subject matter of the document is likely related to the development and improvement of a language model, possibly named \"Llama 3\" based on the acknowledgments section. This is evidenced by the detailed list of contributors and the mention of support for \"Llama 3\" in the acknowledgments. The document also includes references to other language models and research papers, which suggests that it'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 476, 10916, 1895, 5435, 311, 264, 3460, 4128, 1614, 11, 4363, 6941, 444, 80001, 220, 18, 13, 576, 2197, 11469, 264, 3460, 1372, 315, 20343, 323, 15823, 26139, 11, 892, 374, 14260, 369, 264, 3412, 2390, 476, 3162, 4401, 5041, 13, 576, 2213, 1083, 5646, 15057, 311, 10916, 6682, 323, 14250, 15689, 11, 18860, 429, 279, 2197, 374, 4363, 264, 11682, 1895, 476, 9705, 315, 279, 4401, 323, 3412, 4815, 279, 4128, 1614, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 52984, 4831, 1759, 95518, 576, 16376, 1140, 315, 20343, 11, 2670, 5036, 315, 7775, 323, 862, 12783, 11, 13230, 264, 39706, 5041, 11, 892, 374, 4185, 304, 3460, 12934, 3412, 7079, 476, 3162, 4401, 27172, 382, 17, 13, 3070, 90236, 83027, 95518, 576, 2197, 5646, 264, 3772, 12235, 311, 60608, 7775, 323, 11104, 429, 3897]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research or technical report related to a large language model, likely named Llama 3. The document lists a large number of contributors and acknowledgments, which is typical for a research project or software development effort. The content also includes references to technical reports and academic papers, indicating that the document is likely a detailed report or documentation of the development and research behind the language model.\\n\\n### Reasoning:\\n\\n1. **Contributor List**: The extensive list of contributors, including names of individuals and their roles, suggests a collaborative effort, which is common in large-scale research projects or software development initiatives.\\n\\n2. **Acknowledgements**: The document includes a section dedicated to acknowledging individuals and organizations that provided'\n",
      "    ✗ DIFFERENT\n",
      "  bs=3:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 476, 10916, 1895, 5435, 311, 264, 3460, 4128, 1614, 11, 4363, 6941, 444, 80001, 220, 18, 13, 576, 2197, 11469, 264, 3460, 1372, 315, 20343, 323, 15823, 26139, 11, 892, 374, 14260, 369, 264, 3412, 2390, 476, 3162, 4401, 5041, 13, 576, 2213, 1083, 5646, 15057, 311, 10916, 6682, 323, 14250, 15689, 11, 18860, 429, 279, 2197, 374, 4363, 264, 11682, 1895, 476, 9705, 315, 279, 4401, 323, 3412, 4815, 279, 4128, 1614, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 52984, 4831, 1759, 95518, 576, 16376, 1140, 315, 20343, 11, 2670, 5036, 315, 7775, 323, 862, 12783, 11, 13230, 264, 39706, 5041, 11, 892, 374, 4185, 304, 3460, 12934, 3412, 7079, 476, 3162, 4401, 27172, 382, 17, 13, 3070, 90236, 83027, 95518, 576, 2197, 5646, 264, 3772, 12235, 311, 60608, 7775, 323, 11104, 429, 3897]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research or technical report related to a large language model, likely named Llama 3. The document lists a large number of contributors and acknowledgments, which is typical for a research project or software development effort. The content also includes references to technical reports and academic papers, indicating that the document is likely a detailed report or documentation of the development and research behind the language model.\\n\\n### Reasoning:\\n\\n1. **Contributor List**: The extensive list of contributors, including names of individuals and their roles, suggests a collaborative effort, which is common in large-scale research projects or software development initiatives.\\n\\n2. **Acknowledgements**: The document includes a section dedicated to acknowledging individuals and organizations that provided'\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 476, 10916, 1895, 11, 4363, 5435, 311, 264, 3460, 4128, 1614, 476, 264, 4013, 315, 4128, 4119, 13, 576, 2197, 11469, 264, 3460, 1372, 315, 20343, 323, 54930, 862, 1824, 11, 892, 374, 14260, 369, 3412, 15689, 476, 10916, 6682, 304, 279, 2070, 315, 20443, 11229, 11, 7945, 304, 5810, 4128, 8692, 320, 45, 12567, 3593, 14374, 26759, 287, 1447, 16, 13, 3070, 52984, 4831, 1759, 95518, 576, 2197, 8471, 448, 264, 1293, 1140, 315, 20343, 11, 892, 374, 4185, 304, 3412, 15689, 11, 5310, 1846, 15860, 3460, 39706, 8869, 13, 1096, 13230, 264, 5089, 2390, 448, 5248, 20343, 11, 4363, 264, 3460, 4128, 1614, 476, 264, 4013, 315, 4119, 382, 17, 13, 3070, 90236, 83027, 95518, 576, 2197, 5646, 458, 330, 90236, 83027, 1, 3772, 11, 1380, 432, 9339, 5257, 7775, 323, 11104, 369, 862]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research or technical report, likely related to a large language model or a series of language models. The document lists a large number of contributors and acknowledges their support, which is typical for research papers or technical reports in the field of artificial intelligence, particularly in natural language processing (NLP).\\n\\n### Reasoning:\\n\\n1. **Contributor List**: The document starts with a long list of contributors, which is common in research papers, especially those involving large collaborative efforts. This suggests a significant project with multiple contributors, likely a large language model or a series of models.\\n\\n2. **Acknowledgements**: The document includes an \"Acknowledgements\" section, where it thanks various individuals and organizations for their'\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 476, 10916, 1895, 5435, 311, 264, 3460, 4128, 1614, 11, 4363, 6941, 444, 80001, 220, 18, 13, 576, 2197, 11469, 264, 3460, 1372, 315, 20343, 323, 15823, 26139, 11, 892, 374, 14260, 369, 264, 3412, 2390, 476, 3162, 4401, 5041, 13, 576, 3832, 4925, 374, 30188, 2163, 264, 4128, 1614, 11, 438, 68357, 553, 279, 5036, 315, 20343, 323, 279, 15057, 311, 10916, 6682, 323, 3412, 15689, 429, 5244, 389, 4128, 4119, 323, 862, 8357, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 52984, 4831, 1759, 95518, 576, 16376, 1140, 315, 20343, 11, 2670, 5036, 1075, 330, 32, 329, 487, 64, 26894, 1335, 330, 43, 96119, 8394, 78, 1335, 323, 330, 43, 3101, 300, 38185, 9034, 1335, 13230, 264, 39706, 5041, 11, 10767, 264, 3412, 2390, 476, 264, 3162, 4401, 20162, 13, 576, 5036, 525, 16807]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research or technical report related to a large language model, likely named Llama 3. The document lists a large number of contributors and acknowledgments, which is typical for a research project or software development effort. The subject matter is centered around a language model, as evidenced by the names of contributors and the references to technical reports and research papers that focus on language models and their applications.\\n\\n### Reasoning:\\n\\n1. **Contributor List**: The extensive list of contributors, including names like \"Aaditya Singh,\" \"Lubo Malo,\" and \"Lukas Blecher,\" suggests a collaborative effort, possibly a research project or a software development initiative. The names are diverse'\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 476, 10916, 1895, 11, 4363, 5435, 311, 264, 3460, 4128, 1614, 476, 264, 4013, 315, 4128, 4119, 13, 576, 2197, 11469, 264, 3460, 1372, 315, 20343, 323, 54930, 862, 1824, 11, 892, 374, 14260, 369, 3412, 15689, 476, 10916, 6682, 304, 279, 2070, 315, 20443, 11229, 11, 7945, 304, 5810, 4128, 8692, 320, 45, 12567, 3593, 14374, 26759, 287, 1447, 16, 13, 3070, 52984, 4831, 1759, 95518, 576, 2197, 12033, 448, 264, 1293, 1140, 315, 20343, 11, 892, 374, 4185, 304, 3412, 15689, 11, 5310, 1846, 15860, 3460, 39706, 8869, 13, 1096, 13230, 264, 5089, 2390, 448, 5248, 20343, 11, 4363, 264, 3460, 4128, 1614, 476, 264, 4013, 315, 4119, 382, 17, 13, 3070, 90236, 83027, 95518, 576, 2197, 5646, 458, 330, 90236, 83027, 1, 3772, 11, 1380, 432, 9339, 3151, 7775, 323, 11104, 369, 862]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research or technical report, likely related to a large language model or a series of language models. The document lists a large number of contributors and acknowledges their support, which is typical for research papers or technical reports in the field of artificial intelligence, particularly in natural language processing (NLP).\\n\\n### Reasoning:\\n\\n1. **Contributor List**: The document begins with a long list of contributors, which is common in research papers, especially those involving large collaborative efforts. This suggests a significant project with multiple contributors, likely a large language model or a series of models.\\n\\n2. **Acknowledgements**: The document includes an \"Acknowledgements\" section, where it thanks specific individuals and organizations for their'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 476, 10916, 1895, 11, 4363, 5435, 311, 264, 3460, 4128, 1614, 476, 264, 4013, 315, 4128, 4119, 13, 576, 2197, 11469, 264, 3460, 1372, 315, 20343, 323, 54930, 862, 1824, 11, 892, 374, 14260, 369, 3412, 15689, 476, 10916, 6682, 304, 279, 2070, 315, 20443, 11229, 11, 7945, 304, 5810, 4128, 8692, 320, 45, 12567, 3593, 14374, 26759, 287, 1447, 16, 13, 3070, 52984, 4831, 1759, 95518, 576, 2197, 12033, 448, 264, 1293, 1140, 315, 20343, 11, 892, 374, 4185, 304, 3412, 15689, 11, 5310, 1846, 15860, 3460, 39706, 8869, 13, 1096, 13230, 264, 5089, 2390, 448, 5248, 20343, 11, 4363, 264, 3460, 4128, 1614, 476, 264, 4013, 315, 4119, 382, 17, 13, 3070, 90236, 83027, 95518, 576, 2197, 5646, 458, 330, 90236, 83027, 1, 3772, 11, 1380, 432, 9339, 3151, 7775, 323, 11104, 369, 862]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research or technical report, likely related to a large language model or a series of language models. The document lists a large number of contributors and acknowledges their support, which is typical for research papers or technical reports in the field of artificial intelligence, particularly in natural language processing (NLP).\\n\\n### Reasoning:\\n\\n1. **Contributor List**: The document begins with a long list of contributors, which is common in research papers, especially those involving large collaborative efforts. This suggests a significant project with multiple contributors, likely a large language model or a series of models.\\n\\n2. **Acknowledgements**: The document includes an \"Acknowledgements\" section, where it thanks specific individuals and organizations for their'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 476, 10916, 1895, 5435, 311, 264, 3460, 4128, 1614, 11, 4363, 6941, 444, 80001, 220, 18, 13, 576, 2197, 11469, 264, 3460, 1372, 315, 20343, 323, 15823, 26139, 11, 892, 374, 14260, 369, 264, 3412, 2390, 476, 3162, 4401, 5041, 13, 576, 3832, 4925, 374, 30188, 2163, 264, 4128, 1614, 11, 438, 68357, 553, 279, 5036, 315, 20343, 323, 279, 15057, 311, 10916, 6682, 323, 3412, 15689, 5435, 311, 4128, 4119, 382, 14374, 26759, 287, 1447, 16, 13, 3070, 52984, 4831, 1759, 25, 1019, 256, 481, 576, 2197, 12033, 448, 264, 1293, 1140, 315, 20343, 11, 892, 374, 4185, 304, 14250, 15689, 11, 3412, 7079, 11, 476, 3162, 4401, 8869, 13, 576, 5036, 2924, 11811, 11, 24198, 11, 323, 1008, 15387, 6398, 304, 279, 4401, 323, 7497, 315, 264, 4128, 1614, 382, 17, 13, 3070, 90236]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research or technical report related to a large language model, likely named Llama 3. The document lists a large number of contributors and acknowledgments, which is typical for a research project or software development effort. The subject matter is centered around a language model, as evidenced by the names of contributors and the references to technical reports and research papers related to language models.\\n\\n### Reasoning:\\n\\n1. **Contributor List:**\\n   - The document begins with a long list of contributors, which is common in academic papers, research projects, or software development efforts. The names include researchers, engineers, and other professionals involved in the development and testing of a language model.\\n\\n2. **Acknowled'\n",
      "    ✗ DIFFERENT\n",
      "  bs=17:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 3412, 476, 10916, 1895, 11, 4363, 5435, 311, 264, 3460, 4128, 1614, 476, 264, 4013, 315, 4128, 4119, 13, 576, 2197, 11469, 264, 3460, 1372, 315, 20343, 323, 54930, 862, 1824, 11, 892, 374, 4185, 304, 3412, 15689, 476, 10916, 6682, 13, 22406, 11, 279, 2197, 5646, 264, 3772, 315, 15057, 11, 892, 4623, 11554, 419, 16688, 382, 785, 3832, 4925, 315, 279, 2197, 374, 4363, 5435, 311, 82687, 304, 5810, 4128, 8692, 320, 45, 12567, 8, 323, 5662, 6832, 11, 11689, 21080, 389, 3460, 4128, 4119, 13, 1096, 374, 29476, 504, 279, 5036, 315, 20343, 323, 279, 15057, 11, 892, 2924, 4278, 389, 5257, 13566, 315, 451, 12567, 11, 1741, 438, 41733, 7681, 454, 1693, 11, 7299, 65489, 42578, 4119, 11, 9124, 4128, 4119, 369, 2421, 63530, 6832, 11, 323, 5666, 5980, 15908, 13664, 369, 6888]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a research or technical report, likely related to a large language model or a series of language models. The document lists a large number of contributors and acknowledges their support, which is common in research papers or technical reports. Additionally, the document includes a section of references, which further supports this conclusion.\\n\\nThe subject matter of the document is likely related to advancements in natural language processing (NLP) and machine learning, specifically focusing on large language models. This is evident from the names of contributors and the references, which include works on various aspects of NLP, such as semantic deduplication, multi-query transformer models, visual language models for few-shot learning, and operation-based formalisms for math'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "--- Token consistency for ref_2 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 10735, 389, 279, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 5942, 11, 7945, 304, 279, 2266, 315, 6489, 19473, 323, 4763, 10520, 13, 5692, 525, 279, 1376, 3501, 429, 1824, 419, 16688, 1447, 16, 13, 3070, 62226, 25806, 95518, 576, 2197, 34334, 5257, 10916, 23783, 323, 18940, 1741, 438, 26193, 24231, 11, 11773, 27406, 11, 3728, 22901, 11, 44321, 25463, 11, 28030, 11773, 11, 323, 15235, 54192, 7611, 13, 4220, 13347, 525, 11136, 9761, 304, 10916, 6682, 476, 4158, 47293, 382, 17, 13, 3070, 77606, 681, 323, 55473, 95518, 362, 5089, 13348, 315, 279, 1467, 374, 12235, 311, 24392, 1246, 311, 2569, 323, 10146, 15235, 5942, 11, 2670, 279, 990, 315, 89129, 12538, 11, 11773, 23783, 11, 323, 821, 25463, 13, 1096, 5285, 82, 448, 279, 3832, 4925, 315, 264, 1895]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report focused on the governance and verification of artificial intelligence (AI) systems, particularly in the context of international agreements and security concerns. Here are the key points that support this conclusion:\\n\\n1. **Technical Focus**: The document discusses various technical mechanisms and concepts such as confidential computing, hardware licensing, location verification, enforced encryption, networking hardware, and AI-enabled devices. These topics are typically covered in technical reports or whitepapers.\\n\\n2. **Governance and Verification**: A significant portion of the text is dedicated to discussing how to govern and verify AI systems, including the use of cryptographic techniques, hardware mechanisms, and data encryption. This aligns with the subject matter of a report'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 10735, 389, 279, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 5942, 11, 7945, 304, 279, 2266, 315, 6489, 19473, 323, 4763, 10520, 13, 5692, 525, 279, 1376, 3501, 429, 1824, 419, 16688, 1447, 16, 13, 3070, 62226, 25806, 95518, 576, 2197, 34334, 5257, 10916, 23783, 323, 18940, 1741, 438, 26193, 24231, 11, 11773, 27406, 11, 3728, 22901, 11, 44321, 25463, 11, 28030, 11773, 11, 323, 15235, 54192, 7611, 13, 4220, 525, 678, 10916, 5671, 9760, 311, 279, 4401, 323, 23172, 315, 15235, 5942, 382, 17, 13, 3070, 77606, 681, 323, 55473, 95518, 362, 5089, 13348, 315, 279, 1467, 374, 12235, 311, 24392, 1246, 311, 2569, 323, 10146, 15235, 5942, 11, 2670, 279, 990, 315, 89129, 12538, 11, 11773, 23783, 11, 323, 821, 25463, 13, 1096, 14807, 429, 279, 2197, 374, 11658]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report focused on the governance and verification of artificial intelligence (AI) systems, particularly in the context of international agreements and security concerns. Here are the key points that support this conclusion:\\n\\n1. **Technical Focus**: The document discusses various technical mechanisms and concepts such as confidential computing, hardware licensing, location verification, enforced encryption, networking hardware, and AI-enabled devices. These are all technical areas relevant to the development and deployment of AI systems.\\n\\n2. **Governance and Verification**: A significant portion of the text is dedicated to discussing how to govern and verify AI systems, including the use of cryptographic techniques, hardware mechanisms, and data encryption. This indicates that the document is concerned'\n",
      "    ✗ DIFFERENT\n",
      "  bs=3:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 10735, 389, 279, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 5942, 11, 7945, 304, 279, 2266, 315, 6489, 19473, 323, 4763, 10520, 13, 5692, 525, 279, 1376, 3501, 429, 1824, 419, 16688, 1447, 16, 13, 3070, 62226, 25806, 95518, 576, 2197, 34334, 5257, 10916, 23783, 323, 18940, 1741, 438, 26193, 24231, 11, 11773, 27406, 11, 3728, 22901, 11, 44321, 25463, 11, 28030, 11773, 11, 323, 15235, 54192, 7611, 13, 4220, 525, 678, 10916, 5671, 9760, 311, 279, 4401, 323, 23172, 315, 15235, 5942, 382, 17, 13, 3070, 77606, 681, 323, 55473, 95518, 362, 5089, 13348, 315, 279, 1467, 374, 12235, 311, 24392, 1246, 311, 2569, 323, 10146, 15235, 5942, 11, 2670, 279, 990, 315, 89129, 12538, 11, 11773, 23783, 11, 323, 821, 25463, 13, 1096, 14807, 429, 279, 2197, 374, 11658]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report focused on the governance and verification of artificial intelligence (AI) systems, particularly in the context of international agreements and security concerns. Here are the key points that support this conclusion:\\n\\n1. **Technical Focus**: The document discusses various technical mechanisms and concepts such as confidential computing, hardware licensing, location verification, enforced encryption, networking hardware, and AI-enabled devices. These are all technical areas relevant to the development and deployment of AI systems.\\n\\n2. **Governance and Verification**: A significant portion of the text is dedicated to discussing how to govern and verify AI systems, including the use of cryptographic techniques, hardware mechanisms, and data encryption. This indicates that the document is concerned'\n",
      "    ✗ DIFFERENT\n",
      "  bs=4:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 10735, 389, 279, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 5942, 11, 7945, 304, 279, 2266, 315, 6489, 19473, 323, 4763, 10520, 13, 5692, 525, 279, 1376, 3501, 429, 1824, 419, 16688, 1447, 16, 13, 3070, 62226, 25806, 95518, 576, 2197, 34334, 5257, 10916, 23783, 323, 18940, 1741, 438, 26193, 24231, 11, 11773, 27406, 11, 3728, 22901, 11, 44321, 25463, 11, 28030, 11773, 11, 323, 19734, 13737, 13, 4220, 525, 678, 10916, 5671, 9760, 311, 279, 5666, 323, 4763, 315, 15235, 5942, 382, 17, 13, 3070, 77606, 681, 323, 55473, 95518, 362, 5089, 13348, 315, 279, 1467, 374, 12235, 311, 24392, 1246, 311, 2569, 323, 10146, 15235, 5942, 11, 2670, 279, 990, 315, 89129, 12538, 11, 11773, 23783, 11, 323, 821, 25463, 13, 1096, 14807, 429, 279, 2197, 374, 11658, 448]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report focused on the governance and verification of artificial intelligence (AI) systems, particularly in the context of international agreements and security concerns. Here are the key points that support this conclusion:\\n\\n1. **Technical Focus**: The document discusses various technical mechanisms and concepts such as confidential computing, hardware licensing, location verification, enforced encryption, networking hardware, and electrical infrastructure. These are all technical areas relevant to the operation and security of AI systems.\\n\\n2. **Governance and Verification**: A significant portion of the text is dedicated to discussing how to govern and verify AI systems, including the use of cryptographic techniques, hardware mechanisms, and data encryption. This indicates that the document is concerned with'\n",
      "    ✗ DIFFERENT\n",
      "  bs=5:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 10735, 389, 279, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 5942, 11, 7945, 304, 279, 2266, 315, 6489, 19473, 323, 4763, 10520, 13, 5692, 525, 279, 1376, 3501, 429, 1824, 419, 16688, 1447, 16, 13, 3070, 62226, 25806, 95518, 576, 2197, 34334, 5257, 10916, 23783, 323, 18940, 1741, 438, 26193, 24231, 11, 11773, 27406, 11, 3728, 22901, 11, 44321, 25463, 11, 28030, 11773, 11, 323, 15235, 54192, 7611, 13, 4220, 525, 678, 10916, 5671, 9760, 311, 279, 4401, 323, 23172, 315, 15235, 5942, 382, 17, 13, 3070, 77606, 681, 323, 55473, 95518, 362, 5089, 13348, 315, 279, 1467, 374, 12235, 311, 24392, 1246, 311, 2569, 323, 10146, 15235, 5942, 11, 2670, 279, 990, 315, 89129, 12538, 11, 11773, 23783, 11, 323, 821, 25463, 13, 1096, 14807, 429, 279, 2197, 374, 11658]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report focused on the governance and verification of artificial intelligence (AI) systems, particularly in the context of international agreements and security concerns. Here are the key points that support this conclusion:\\n\\n1. **Technical Focus**: The document discusses various technical mechanisms and concepts such as confidential computing, hardware licensing, location verification, enforced encryption, networking hardware, and AI-enabled devices. These are all technical areas relevant to the development and deployment of AI systems.\\n\\n2. **Governance and Verification**: A significant portion of the text is dedicated to discussing how to govern and verify AI systems, including the use of cryptographic techniques, hardware mechanisms, and data encryption. This indicates that the document is concerned'\n",
      "    ✗ DIFFERENT\n",
      "  bs=8:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 10735, 389, 279, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 5942, 11, 7945, 304, 279, 2266, 315, 6489, 19473, 323, 4763, 10520, 13, 5692, 525, 279, 1376, 3501, 429, 1824, 419, 16688, 1447, 16, 13, 3070, 62226, 25806, 95518, 576, 2197, 34334, 5257, 10916, 23783, 323, 18940, 1741, 438, 26193, 24231, 11, 11773, 27406, 11, 3728, 22901, 11, 44321, 25463, 11, 28030, 11773, 11, 323, 15235, 54192, 7611, 13, 4220, 525, 678, 10916, 5671, 9760, 311, 279, 4401, 11, 23172, 11, 323, 34086, 315, 15235, 5942, 382, 17, 13, 3070, 77606, 681, 323, 55473, 95518, 362, 5089, 13348, 315, 279, 1467, 374, 12235, 311, 24392, 1246, 311, 5978, 429, 15235, 5942, 525, 48698, 448, 6489, 19473, 323, 1246, 311, 10146, 862, 8733, 13, 1096, 5646, 13347, 1075, 27406, 30856, 11, 3728]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report focused on the governance and verification of artificial intelligence (AI) systems, particularly in the context of international agreements and security concerns. Here are the key points that support this conclusion:\\n\\n1. **Technical Focus**: The document discusses various technical mechanisms and concepts such as confidential computing, hardware licensing, location verification, enforced encryption, networking hardware, and AI-enabled devices. These are all technical areas relevant to the development, deployment, and governance of AI systems.\\n\\n2. **Governance and Verification**: A significant portion of the text is dedicated to discussing how to ensure that AI systems are compliant with international agreements and how to verify their compliance. This includes topics like licensing schemes, location'\n",
      "    ✗ DIFFERENT\n",
      "  bs=9:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 10735, 389, 279, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 5942, 11, 7945, 304, 279, 2266, 315, 6489, 19473, 323, 4763, 10520, 13, 5692, 525, 279, 1376, 3501, 429, 1824, 419, 16688, 1447, 16, 13, 3070, 62226, 25806, 95518, 576, 2197, 34334, 5257, 10916, 23783, 323, 18940, 1741, 438, 26193, 24231, 11, 11773, 27406, 11, 3728, 22901, 11, 44321, 25463, 11, 28030, 11773, 11, 323, 15235, 54192, 7611, 13, 4220, 525, 678, 10916, 5671, 9760, 311, 279, 4401, 11, 23172, 11, 323, 34086, 315, 15235, 5942, 382, 17, 13, 3070, 77606, 681, 323, 55473, 95518, 362, 5089, 13348, 315, 279, 1467, 374, 12235, 311, 24392, 1246, 311, 5978, 429, 15235, 5942, 525, 48698, 448, 6489, 19473, 323, 1246, 311, 10146, 862, 8733, 13, 1096, 5646, 13347, 1075, 27406, 30856, 11, 3728]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report focused on the governance and verification of artificial intelligence (AI) systems, particularly in the context of international agreements and security concerns. Here are the key points that support this conclusion:\\n\\n1. **Technical Focus**: The document discusses various technical mechanisms and concepts such as confidential computing, hardware licensing, location verification, enforced encryption, networking hardware, and AI-enabled devices. These are all technical areas relevant to the development, deployment, and governance of AI systems.\\n\\n2. **Governance and Verification**: A significant portion of the text is dedicated to discussing how to ensure that AI systems are compliant with international agreements and how to verify their compliance. This includes topics like licensing schemes, location'\n",
      "    ✗ DIFFERENT\n",
      "  bs=16:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 10735, 389, 279, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 5942, 11, 7945, 304, 279, 2266, 315, 6489, 19473, 323, 4763, 10520, 13, 5692, 525, 279, 1376, 3501, 429, 1824, 419, 16688, 1447, 16, 13, 3070, 62226, 25806, 95518, 576, 2197, 34334, 5257, 10916, 23783, 323, 18940, 1741, 438, 26193, 24231, 11, 11773, 27406, 11, 3728, 22901, 11, 44321, 25463, 11, 28030, 11773, 11, 323, 15235, 54192, 7611, 13, 4220, 525, 678, 10916, 5671, 9760, 311, 279, 4401, 11, 23172, 11, 323, 34086, 315, 15235, 5942, 382, 17, 13, 3070, 77606, 681, 323, 55473, 95518, 362, 5089, 13348, 315, 279, 1467, 374, 12235, 311, 24392, 1246, 311, 2569, 323, 10146, 15235, 5942, 11, 2670, 279, 990, 315, 89129, 12538, 11, 11773, 23783, 11, 323, 821, 25463, 13, 1096, 14807, 429, 279]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report focused on the governance and verification of artificial intelligence (AI) systems, particularly in the context of international agreements and security concerns. Here are the key points that support this conclusion:\\n\\n1. **Technical Focus**: The document discusses various technical mechanisms and concepts such as confidential computing, hardware licensing, location verification, enforced encryption, networking hardware, and AI-enabled devices. These are all technical areas relevant to the development, deployment, and governance of AI systems.\\n\\n2. **Governance and Verification**: A significant portion of the text is dedicated to discussing how to govern and verify AI systems, including the use of cryptographic techniques, hardware mechanisms, and data encryption. This indicates that the'\n",
      "    ✗ DIFFERENT\n",
      "  bs=17:\n",
      "    IDs:  [28715, 389, 279, 49465, 3897, 11, 419, 2197, 7952, 311, 387, 504, 264, 10916, 476, 14250, 1895, 10735, 389, 279, 34086, 323, 22901, 315, 20443, 11229, 320, 15469, 8, 5942, 11, 7945, 304, 279, 2266, 315, 6489, 19473, 323, 4763, 10520, 13, 5692, 525, 279, 1376, 3501, 429, 1824, 419, 16688, 1447, 16, 13, 3070, 62226, 25806, 95518, 576, 2197, 34334, 5257, 10916, 23783, 323, 18940, 1741, 438, 26193, 24231, 11, 11773, 27406, 11, 3728, 22901, 11, 44321, 25463, 11, 28030, 11773, 11, 323, 19734, 13737, 13, 4220, 525, 678, 10916, 5671, 9760, 311, 279, 5666, 323, 4763, 315, 15235, 5942, 382, 17, 13, 3070, 77606, 681, 323, 55473, 95518, 362, 5089, 13348, 315, 279, 1467, 374, 12235, 311, 24392, 1246, 311, 2569, 323, 10146, 15235, 5942, 11, 2670, 279, 990, 315, 89129, 12538, 11, 11773, 23783, 11, 323, 821, 25463, 13, 1096, 14807, 429, 279, 2197, 374, 11658, 448]\n",
      "    Text: 'Based on the excerpt provided, this document appears to be from a technical or academic report focused on the governance and verification of artificial intelligence (AI) systems, particularly in the context of international agreements and security concerns. Here are the key points that support this conclusion:\\n\\n1. **Technical Focus**: The document discusses various technical mechanisms and concepts such as confidential computing, hardware licensing, location verification, enforced encryption, networking hardware, and electrical infrastructure. These are all technical areas relevant to the operation and security of AI systems.\\n\\n2. **Governance and Verification**: A significant portion of the text is dedicated to discussing how to govern and verify AI systems, including the use of cryptographic techniques, hardware mechanisms, and data encryption. This indicates that the document is concerned with'\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "⚠ Element 0 generates DIFFERENT tokens across batch sizes\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_0\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_1\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_2\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Key vectors - Mean: 0.00e+00, Range: [0.00e+00, 0.00e+00]\n",
      "  Logprobs - Mean: 0.00e+00, Range: [0.00e+00, 0.00e+00]\n",
      "\n",
      "⚠ WARNING: 36/36 comparisons are EXACTLY ZERO\n",
      "  All batch sizes produce identical results (single kernel class)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1, 2, 3, 4, 5, 8, 9, 16, 17]\n",
      "\n",
      "Equivalent pairs (will be excluded from cross-hardware signal):\n",
      "  (1, 2)\n",
      "  (1, 3)\n",
      "  (1, 4)\n",
      "  (1, 5)\n",
      "  (1, 8)\n",
      "  (1, 9)\n",
      "  (1, 16)\n",
      "  (1, 17)\n",
      "  (2, 3)\n",
      "  (2, 4)\n",
      "  (2, 5)\n",
      "  (2, 8)\n",
      "  (2, 9)\n",
      "  (2, 16)\n",
      "  (2, 17)\n",
      "  (3, 4)\n",
      "  (3, 5)\n",
      "  (3, 8)\n",
      "  (3, 9)\n",
      "  (3, 16)\n",
      "  (3, 17)\n",
      "  (4, 5)\n",
      "  (4, 8)\n",
      "  (4, 9)\n",
      "  (4, 16)\n",
      "  (4, 17)\n",
      "  (5, 8)\n",
      "  (5, 9)\n",
      "  (5, 16)\n",
      "  (5, 17)\n",
      "  (8, 9)\n",
      "  (8, 16)\n",
      "  (8, 17)\n",
      "  (9, 16)\n",
      "  (9, 17)\n",
      "  (16, 17)\n",
      "\n",
      "✗ SANITY CHECK FAILED (No variation)\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (DECODE)\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "REF_0\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.16e+01  2.16e+01  2.14e+01  4.86e-01  2.13e+01  2.13e+01  1.81e+01  2.14e+01\n",
      "bs=2    2.16e+01  0.00e+00  0.00e+00  5.69e+00  2.16e+01  5.68e+00  5.68e+00  2.56e+01  5.66e+00\n",
      "bs=3    2.16e+01  0.00e+00  0.00e+00  5.69e+00  2.16e+01  5.68e+00  5.68e+00  2.56e+01  5.66e+00\n",
      "bs=4    2.14e+01  5.69e+00  5.69e+00  0.00e+00  2.14e+01  3.21e-01  3.21e-01  2.55e+01  3.17e-01\n",
      "bs=5    4.86e-01  2.16e+01  2.16e+01  2.14e+01  0.00e+00  2.13e+01  2.13e+01  1.81e+01  2.14e+01\n",
      "bs=8    2.13e+01  5.68e+00  5.68e+00  3.21e-01  2.13e+01  0.00e+00  0.00e+00  2.55e+01  3.47e-01\n",
      "bs=9    2.13e+01  5.68e+00  5.68e+00  3.21e-01  2.13e+01  0.00e+00  0.00e+00  2.55e+01  3.47e-01\n",
      "bs=16   1.81e+01  2.56e+01  2.56e+01  2.55e+01  1.81e+01  2.55e+01  2.55e+01  0.00e+00  2.55e+01\n",
      "bs=17   2.14e+01  5.66e+00  5.66e+00  3.17e-01  2.14e+01  3.47e-01  3.47e-01  2.55e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf  2.49e-01       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=3         inf  0.00e+00  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00       inf  1.31e-01  1.31e-01       inf  2.49e-01\n",
      "bs=5    2.49e-01       inf       inf       inf  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf       inf       inf  1.31e-01       inf  0.00e+00  0.00e+00       inf  2.81e-01\n",
      "bs=9         inf       inf       inf  1.31e-01       inf  0.00e+00  0.00e+00       inf  2.81e-01\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf  2.49e-01       inf  2.81e-01  2.81e-01       inf  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_1\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.22e+01  2.22e+01  2.21e+01  2.10e+01  2.19e+01  2.19e+01  2.65e+01  2.70e+01\n",
      "bs=2    2.22e+01  0.00e+00  0.00e+00  6.51e+00  1.87e+01  6.60e+00  6.60e+00  2.63e+01  2.68e+01\n",
      "bs=3    2.22e+01  0.00e+00  0.00e+00  6.51e+00  1.87e+01  6.60e+00  6.60e+00  2.63e+01  2.68e+01\n",
      "bs=4    2.21e+01  6.51e+00  6.51e+00  0.00e+00  1.96e+01  2.00e+00  2.00e+00  2.58e+01  2.65e+01\n",
      "bs=5    2.10e+01  1.87e+01  1.87e+01  1.96e+01  0.00e+00  1.95e+01  1.95e+01  2.36e+01  2.86e+01\n",
      "bs=8    2.19e+01  6.60e+00  6.60e+00  2.00e+00  1.95e+01  0.00e+00  0.00e+00  2.57e+01  2.63e+01\n",
      "bs=9    2.19e+01  6.60e+00  6.60e+00  2.00e+00  1.95e+01  0.00e+00  0.00e+00  2.57e+01  2.63e+01\n",
      "bs=16   2.65e+01  2.63e+01  2.63e+01  2.58e+01  2.36e+01  2.57e+01  2.57e+01  0.00e+00  3.28e+01\n",
      "bs=17   2.70e+01  2.68e+01  2.68e+01  2.65e+01  2.86e+01  2.63e+01  2.63e+01  3.28e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=3         inf  0.00e+00  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00       inf  1.13e+00  1.13e+00       inf       inf\n",
      "bs=5         inf       inf       inf       inf  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf       inf       inf  1.18e+00       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf       inf       inf  1.18e+00       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf       inf       inf       inf       inf       inf  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "REF_2\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.49e+01  2.49e+01  2.34e+01  2.48e+01  2.74e+01  2.74e+01  2.40e+01  2.34e+01\n",
      "bs=2    2.49e+01  0.00e+00  0.00e+00  2.33e+01  2.73e-01  2.69e+01  2.69e+01  1.93e+01  2.33e+01\n",
      "bs=3    2.49e+01  0.00e+00  0.00e+00  2.33e+01  2.73e-01  2.69e+01  2.69e+01  1.93e+01  2.33e+01\n",
      "bs=4    2.34e+01  2.33e+01  2.33e+01  0.00e+00  2.33e+01  3.20e+01  3.20e+01  2.55e+01  2.95e-01\n",
      "bs=5    2.48e+01  2.73e-01  2.73e-01  2.33e+01  0.00e+00  2.69e+01  2.69e+01  1.93e+01  2.33e+01\n",
      "bs=8    2.74e+01  2.69e+01  2.69e+01  3.20e+01  2.69e+01  0.00e+00  0.00e+00  2.45e+01  3.20e+01\n",
      "bs=9    2.74e+01  2.69e+01  2.69e+01  3.20e+01  2.69e+01  0.00e+00  0.00e+00  2.45e+01  3.20e+01\n",
      "bs=16   2.40e+01  1.93e+01  1.93e+01  2.55e+01  1.93e+01  2.45e+01  2.45e+01  0.00e+00  2.56e+01\n",
      "bs=17   2.34e+01  2.33e+01  2.33e+01  2.95e-01  2.33e+01  3.20e+01  3.20e+01  2.56e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00  0.00e+00       inf  1.75e-01       inf       inf       inf       inf\n",
      "bs=3         inf  0.00e+00  0.00e+00       inf  1.75e-01       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00       inf       inf       inf       inf  2.30e-01\n",
      "bs=5         inf  1.75e-01  1.75e-01       inf  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf  1.99e-01       inf       inf       inf       inf  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "\n",
      "Key Vectors (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00  2.29e+01  2.29e+01  2.23e+01  1.54e+01  2.35e+01  2.35e+01  2.29e+01  2.39e+01\n",
      "bs=2    2.29e+01  0.00e+00  0.00e+00  1.18e+01  1.35e+01  1.31e+01  1.31e+01  2.37e+01  1.86e+01\n",
      "bs=3    2.29e+01  0.00e+00  0.00e+00  1.18e+01  1.35e+01  1.31e+01  1.31e+01  2.37e+01  1.86e+01\n",
      "bs=4    2.23e+01  1.18e+01  1.18e+01  0.00e+00  2.14e+01  1.14e+01  1.14e+01  2.56e+01  9.04e+00\n",
      "bs=5    1.54e+01  1.35e+01  1.35e+01  2.14e+01  0.00e+00  2.26e+01  2.26e+01  2.03e+01  2.44e+01\n",
      "bs=8    2.35e+01  1.31e+01  1.31e+01  1.14e+01  2.26e+01  0.00e+00  0.00e+00  2.52e+01  1.96e+01\n",
      "bs=9    2.35e+01  1.31e+01  1.31e+01  1.14e+01  2.26e+01  0.00e+00  0.00e+00  2.52e+01  1.96e+01\n",
      "bs=16   2.29e+01  2.37e+01  2.37e+01  2.56e+01  2.03e+01  2.52e+01  2.52e+01  0.00e+00  2.79e+01\n",
      "bs=17   2.39e+01  1.86e+01  1.86e+01  9.04e+00  2.44e+01  1.96e+01  1.96e+01  2.79e+01  0.00e+00\n",
      "\n",
      "Logprobs (mean L2 distance):\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 \n",
      "bs=1    0.00e+00       inf       inf       inf       inf       inf       inf       inf       inf\n",
      "bs=2         inf  0.00e+00  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=3         inf  0.00e+00  0.00e+00       inf       inf       inf       inf       inf       inf\n",
      "bs=4         inf       inf       inf  0.00e+00       inf       inf       inf       inf       inf\n",
      "bs=5         inf       inf       inf       inf  0.00e+00       inf       inf       inf       inf\n",
      "bs=8         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=9         inf       inf       inf       inf       inf  0.00e+00  0.00e+00       inf       inf\n",
      "bs=16        inf       inf       inf       inf       inf       inf       inf  0.00e+00       inf\n",
      "bs=17        inf       inf       inf       inf       inf       inf       inf       inf  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Key vectors - Mean: 1.81e+01, Range: [0.00e+00, 2.79e+01]\n",
      "  Logprobs - Mean: 0.00e+00, Range: [0.00e+00, inf]\n",
      "\n",
      "⚠ NOTE: 2/36 comparisons are equivalent (< 1e-09)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1]\n",
      "  Class 2: [2, 3]\n",
      "  Class 3: [4]\n",
      "  Class 4: [5]\n",
      "  Class 5: [8, 9]\n",
      "  Class 6: [16]\n",
      "  Class 7: [17]\n",
      "\n",
      "Equivalent pairs (will be excluded from cross-hardware signal):\n",
      "  (2, 3)\n",
      "  (8, 9)\n",
      "\n",
      "✗ SANITY CHECK FAILED (No variation)\n",
      "\n",
      "✓ Saved to: /workspace/experiments/decode_20251126_164754.json\n",
      "\n",
      "Next step: Copy to verifier machine, set TEACHER_FORCING=True\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-Hardware Prefill vs Decode Detectability Experiment (Transformers)\n",
    "\n",
    "Tests whether batch size claims can be verified across different GPU architectures\n",
    "using floating-point forensics (key vectors and logprobs).\n",
    "\n",
    "UPDATED: Uses chat-formatted prompts for more realistic/higher-entropy responses.\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A (e.g., A100) with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts prefill + decode signals, saves to JSON\n",
    "2. Copy JSON to Machine B (e.g., H100)\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, extracts signals, compares\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = False\n",
    "REFERENCE_FILE = \"/workspace/experiments/decode_reference.json\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "ATTN_IMPLEMENTATION = \"sdpa\"  # Options: \"eager\", \"sdpa\", \"flash_attention_2\"\n",
    "\n",
    "BATCH_SIZES = [1, 2, 3, 4, 5, 8, 9, 16, 17]\n",
    "LAYER_INDICES = [28]  # Last layer only\n",
    "MAX_NEW_TOKENS = 150\n",
    "TOKENS_PER_SLICE = 4000  \n",
    "NUM_REFERENCES = 3\n",
    "\n",
    "# Threshold for considering two batch sizes \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Will be initialized from PDF in main()\n",
    "REFERENCE_SEQUENCES = None  # Now contains token IDs, not text\n",
    "DUMMY_SETS = None           # Now contains token IDs, not text\n",
    "\n",
    "# ============================================================================\n",
    "# CHAT TEMPLATE CONFIGURATION (Qwen recommended)\n",
    "# ============================================================================\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "USER_TEMPLATE = \"\"\"Here is an excerpt from a document:\n",
    "\n",
    "\"{snippet}\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.\"\"\"\n",
    "\n",
    "\n",
    "def create_chat_messages(snippet_text):\n",
    "    \"\"\"Create chat messages list for the tokenizer.\"\"\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_TEMPLATE.format(snippet=snippet_text.strip())}\n",
    "    ]\n",
    "\n",
    "\n",
    "def tokenize_chat_prompt(tokenizer, snippet_text):\n",
    "    \"\"\"\n",
    "    Tokenize a chat prompt using the tokenizer's apply_chat_template.\n",
    "    Returns token IDs ready for model input.\n",
    "    \"\"\"\n",
    "    messages = create_chat_messages(snippet_text)\n",
    "    token_ids = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    return token_ids\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    \"\"\"Setup logging to file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    \"\"\"Print to both console and log file.\"\"\"\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    \"\"\"Close log file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING - UPDATED TO RETURN TOKEN IDS WITH CHAT TEMPLATE\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text content from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def create_sequences_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load all PDFs, create chat-formatted prompts, and return token IDs.\n",
    "    All assembly done at token level - guaranteed identical lengths.\n",
    "    \"\"\"\n",
    "    pdf_files = glob.glob(\"/workspace/*.pdf\")\n",
    "    if not pdf_files:\n",
    "        pdf_files = glob.glob(\"*.pdf\")\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    # Tokenize PDF content\n",
    "    content_tokens = tokenizer.encode(all_text, add_special_tokens=False)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    max_batch_size = max(BATCH_SIZES)\n",
    "    slices_needed = num_references * max_batch_size\n",
    "\n",
    "    if len(content_tokens) < slices_needed * TOKENS_PER_SLICE:\n",
    "        raise ValueError(f\"Need {slices_needed * TOKENS_PER_SLICE} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    # Tokenize template parts (prefix before snippet, suffix after)\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "    \n",
    "    suffix = f\"\"\"\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "    \n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    # Assemble prompts at token level\n",
    "    all_prompts = []\n",
    "    for i in range(slices_needed):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        all_prompts.append(prompt)\n",
    "    \n",
    "    # Verify\n",
    "    lengths = set(len(p) for p in all_prompts)\n",
    "    assert len(lengths) == 1, f\"Length mismatch: {lengths}\"\n",
    "    log_print(f\"All {slices_needed} prompts: {total_len} tokens each\")\n",
    "    \n",
    "    # Show sample ending\n",
    "    log_print(f\"\\nSample prompt ending:\")\n",
    "    log_print(repr(tokenizer.decode(all_prompts[0][-40:])))\n",
    "\n",
    "    reference_sequences = {}\n",
    "    dummy_sets = {}\n",
    "\n",
    "    for ref_idx in range(num_references):\n",
    "        ref_name = f\"ref_{ref_idx}\"\n",
    "        base_idx = ref_idx * max_batch_size\n",
    "        reference_sequences[ref_name] = all_prompts[base_idx]\n",
    "        dummy_sets[ref_name] = all_prompts[base_idx + 1 : base_idx + max_batch_size]\n",
    "\n",
    "    return reference_sequences, dummy_sets\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"attn_implementation\": ATTN_IMPLEMENTATION,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import flash_attn\n",
    "        info[\"flash_attn_version\"] = flash_attn.__version__\n",
    "    except ImportError:\n",
    "        info[\"flash_attn_version\"] = \"N/A\"\n",
    "        \n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    container_fields = ['python_version', 'cuda_version', 'cudnn_version']\n",
    "    pip_fields = ['torch_version', 'transformers_version', 'numpy_version', 'flash_attn_version']\n",
    "    config_fields = ['attn_implementation']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "    \n",
    "    container_mismatches = []\n",
    "    pip_mismatches = []\n",
    "    config_mismatches = []\n",
    "    \n",
    "    log_print(\"\\nScript configuration:\")\n",
    "    for field in config_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            config_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nContainer-level dependencies:\")\n",
    "    for field in container_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            container_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nPip-installable packages:\")\n",
    "    for field in pip_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        \n",
    "        if field == 'flash_attn_version':\n",
    "            ref_attn = reference_env.get('attn_implementation', '')\n",
    "            ver_attn = verifier_env.get('attn_implementation', '')\n",
    "            if ref_attn != 'flash_attention_2' and ver_attn != 'flash_attention_2':\n",
    "                log_print(f\"  - {field}: not using flash_attention_2 (skip)\")\n",
    "                continue\n",
    "            if ref_val == 'N/A' or ver_val == 'N/A':\n",
    "                log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "                pip_mismatches.append((field, ref_val, ver_val))\n",
    "                continue\n",
    "        \n",
    "        if ref_val == 'N/A' and ver_val == 'N/A':\n",
    "            log_print(f\"  - {field}: not installed (OK)\")\n",
    "            continue\n",
    "            \n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            pip_mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not container_mismatches and not pip_mismatches and not config_mismatches:\n",
    "        log_print(\"\\n\" + \"-\"*60)\n",
    "        log_print(\"✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"✗ ENVIRONMENT MISMATCH - FIX REQUIRED\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# SIGNAL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_signals_from_output(outputs, layer_indices, position=-1):\n",
    "    \"\"\"\n",
    "    Extract key vectors and logprobs from element 0 at specified position.\n",
    "    \"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    top_k = torch.topk(log_probs, k=20)\n",
    "\n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': top_k.indices.cpu().tolist(),\n",
    "        'log_probs': top_k.values.cpu().tolist()\n",
    "    }\n",
    "\n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract signals from multiple positions during prefill.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        prefill_signals[pos_label] = extract_signals_from_output(outputs, layer_indices, position=pos)\n",
    "    return prefill_signals\n",
    "\n",
    "def extract_signals_for_token_ids(outputs, layer_indices, token_ids, position=-1):\n",
    "    \"\"\"Extract signals for SPECIFIC token IDs (used in verification).\"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        layer_keys = outputs.past_key_values[layer_idx - 1][0]\n",
    "        token_keys = layer_keys[0, :, position, :]\n",
    "        key_dim = token_keys.shape[0] * token_keys.shape[1]\n",
    "        key_vector = token_keys.reshape(key_dim).cpu().clone()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_vector.float().numpy().tolist()\n",
    "\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_ids_tensor = torch.tensor(token_ids, device=logits.device)\n",
    "    selected_logprobs = log_probs[token_ids_tensor]\n",
    "\n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': token_ids,\n",
    "        'log_probs': selected_logprobs.cpu().tolist()\n",
    "    }\n",
    "\n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals_for_token_ids(outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract prefill signals using reference token IDs.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        if pos_label in ref_prefill_signals:\n",
    "            ref_token_ids = ref_prefill_signals[pos_label]['logprobs']['token_ids']\n",
    "            prefill_signals[pos_label] = extract_signals_for_token_ids(\n",
    "                outputs, layer_indices, ref_token_ids, position=pos\n",
    "            )\n",
    "    return prefill_signals\n",
    "\n",
    "# ============================================================================\n",
    "# DECODE GENERATION (TEACHER_FORCING = False) - UPDATED FOR TOKEN IDS\n",
    "# ============================================================================\n",
    "\n",
    "def run_decode_with_extraction(model, tokenizer, ref_token_ids, ref_name, batch_size, \n",
    "                               layer_indices):\n",
    "    \"\"\"\n",
    "    Run decode generation and extract signals from last 3 generation steps.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    if batch_size == 1:\n",
    "        batch_token_ids = [ref_token_ids]\n",
    "    else:\n",
    "        batch_token_ids = [ref_token_ids] + ref_dummies[:batch_size-1]\n",
    "    \n",
    "    input_ids = torch.tensor(batch_token_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    log_print(f\"      Prompt: {prompt_length} tokens\", end=\"\")\n",
    "    \n",
    "    all_batch_generated_ids = [[] for _ in range(batch_size)]\n",
    "    generation_signals = []\n",
    "    \n",
    "    # FIRST STEP: Prefill\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "        \n",
    "    past_kv = outputs.past_key_values\n",
    "    prefill_signals = extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1])\n",
    "    \n",
    "    next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "    for batch_idx in range(batch_size):\n",
    "        all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "        \n",
    "    signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    \n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((inputs['attention_mask'].shape[0], 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS\n",
    "    for step in range(1, MAX_NEW_TOKENS):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        next_tokens = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "        for batch_idx in range(batch_size):\n",
    "            all_batch_generated_ids[batch_idx].append(next_tokens[batch_idx].item())\n",
    "            \n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((attention_mask.shape[0], 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "        if all_batch_generated_ids[0][-1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "            \n",
    "    # Extract last 3 decode signals\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "        \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    final_length = prompt_length + num_generated\n",
    "    log_print(f\" → Final: {final_length} tokens ({num_generated} generated)\")\n",
    "    \n",
    "    return {\n",
    "        'generated_ids': all_batch_generated_ids[0],\n",
    "        'all_batch_generated_ids': all_batch_generated_ids,\n",
    "        'prompt_token_ids': batch_token_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TEACHER-FORCED DECODE (TEACHER_FORCING = True) - UPDATED FOR TOKEN IDS\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_decode(model, tokenizer, ref_name, reference_data, \n",
    "                              verify_batch_size, layer_indices, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced decode: feed reference tokens, extract signals.\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    ref_prompt_ids = reference_data['prompt_token_ids'][0]\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    ref_batch_size = len(reference_data['prompt_token_ids'])\n",
    "    \n",
    "    log_print(f\"      Prompt: {len(ref_prompt_ids)}, Gen: {len(ref_generated_ids)}\", end=\"\")\n",
    "    \n",
    "    if is_diagonal:\n",
    "        log_print(f\", exact neighbors (bs={ref_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = reference_data['prompt_token_ids']\n",
    "        batch_generated_ids = reference_data['all_batch_generated_ids']\n",
    "        actual_batch_size = ref_batch_size\n",
    "    else:\n",
    "        log_print(f\", arb neighbors (bs={verify_batch_size})\", end=\"\")\n",
    "        batch_prompt_ids = [ref_prompt_ids]\n",
    "        batch_generated_ids = [ref_generated_ids]\n",
    "        \n",
    "        ref_dummies = DUMMY_SETS[ref_name]\n",
    "        for i in range(verify_batch_size - 1):\n",
    "            batch_prompt_ids.append(ref_dummies[i])\n",
    "            batch_generated_ids.append([])\n",
    "        \n",
    "        actual_batch_size = verify_batch_size\n",
    "        \n",
    "    input_ids = torch.tensor(batch_prompt_ids, dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    generation_signals = []\n",
    "    num_steps = len(ref_generated_ids)\n",
    "    \n",
    "    # FIRST STEP: Prefill\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "        \n",
    "    past_kv = outputs.past_key_values\n",
    "    ref_prefill_signals = reference_data['prefill_signals']\n",
    "    prefill_signals = extract_prefill_signals_for_token_ids(\n",
    "        outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]\n",
    "    )\n",
    "    \n",
    "    ref_decode_signals = reference_data['decode_signals']\n",
    "    ref_step_data = list(ref_decode_signals.values())[0] if ref_decode_signals else None\n",
    "    if ref_step_data:\n",
    "        ref_token_ids = ref_step_data['signals']['logprobs']['token_ids']\n",
    "        signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "    else:\n",
    "        signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "        \n",
    "    absolute_position_index = inputs['input_ids'].shape[1] - 1\n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': absolute_position_index,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    # Prepare next tokens\n",
    "    if is_diagonal:\n",
    "        next_tokens = torch.tensor(\n",
    "            [batch_generated_ids[i][0] for i in range(actual_batch_size)],\n",
    "            dtype=torch.long, device='cuda'\n",
    "        )\n",
    "    else:\n",
    "        next_tokens_list = [ref_generated_ids[0]]\n",
    "        argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)\n",
    "        for i in range(actual_batch_size - 1):\n",
    "            next_tokens_list.append(argmax_tokens[i].item())\n",
    "            batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "        next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "        \n",
    "    attention_mask = torch.cat([\n",
    "        inputs['attention_mask'], \n",
    "        torch.ones((actual_batch_size, 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # SUBSEQUENT STEPS\n",
    "    for step in range(1, num_steps):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_tokens.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        ref_signals_list = list(ref_decode_signals.values())\n",
    "        if step < len(ref_signals_list):\n",
    "            ref_token_ids = ref_signals_list[step]['signals']['logprobs']['token_ids']\n",
    "            signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "        else:\n",
    "            signals = extract_signals_from_output(outputs, layer_indices, position=-1)\n",
    "            \n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        absolute_position_index = current_cache_length - 1\n",
    "        \n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': absolute_position_index,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        if step < num_steps - 1:\n",
    "            if is_diagonal:\n",
    "                next_tokens = torch.tensor(\n",
    "                    [batch_generated_ids[i][step] for i in range(actual_batch_size)],\n",
    "                    dtype=torch.long, device='cuda'\n",
    "                )\n",
    "            else:\n",
    "                next_tokens_list = [ref_generated_ids[step]]\n",
    "                argmax_tokens = outputs.logits[1:, -1, :].argmax(dim=-1)\n",
    "                for i in range(actual_batch_size - 1):\n",
    "                    next_tokens_list.append(argmax_tokens[i].item())\n",
    "                    batch_generated_ids[i + 1].append(argmax_tokens[i].item())\n",
    "                next_tokens = torch.tensor(next_tokens_list, dtype=torch.long, device='cuda')\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask, \n",
    "            torch.ones((actual_batch_size, 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "    # Extract last 3\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "        \n",
    "    del outputs, inputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(f\" → {num_generated} steps\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(decode_measurements, tokenizer):\n",
    "    \"\"\"Verify element 0 generates identical tokens across all batch sizes.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    tokens_by_bs = {}\n",
    "    for bs, data in decode_measurements.items():\n",
    "        tokens_by_bs[bs] = data['generated_ids']\n",
    "        \n",
    "    bs_list = sorted(tokens_by_bs.keys())\n",
    "    reference_tokens = tokens_by_bs[bs_list[0]]\n",
    "    \n",
    "    all_same = True\n",
    "    log_print(\"\\nGenerated tokens by batch size:\")\n",
    "    for bs in bs_list:\n",
    "        tokens = tokens_by_bs[bs]\n",
    "        match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "        decoded_text = tokenizer.decode(tokens)\n",
    "        log_print(f\"  bs={bs}:\")\n",
    "        log_print(f\"    IDs:  {tokens}\")\n",
    "        log_print(f\"    Text: {repr(decoded_text)}\")\n",
    "        log_print(f\"    {match_str}\")\n",
    "        if tokens != reference_tokens:\n",
    "            all_same = False\n",
    "            \n",
    "    if all_same:\n",
    "        log_print(\"\\n✓ Element 0 generates IDENTICAL tokens across all batch sizes\")\n",
    "    else:\n",
    "        log_print(\"\\n⚠ Element 0 generates DIFFERENT tokens across batch sizes\")\n",
    "        \n",
    "    return all_same\n",
    "\n",
    "def compute_l2_distance(vec1, vec2):\n",
    "    \"\"\"Compute L2 distance between two vectors.\"\"\"\n",
    "    v1 = np.array(vec1)\n",
    "    v2 = np.array(vec2)\n",
    "    return float(np.linalg.norm(v1 - v2))\n",
    "\n",
    "def compute_logprob_distance(logprobs1, logprobs2):\n",
    "    \"\"\"\n",
    "    Compute L2 distance between logprob distributions.\n",
    "    Uses top 5 token IDs from first signal as canonical (stored top 20 as buffer).\n",
    "    \"\"\"\n",
    "    canonical_ids = logprobs1['token_ids'][:5]\n",
    "    vec1 = logprobs1['log_probs'][:5]\n",
    "    \n",
    "    map2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "    \n",
    "    vec2 = []\n",
    "    for tid in canonical_ids:\n",
    "        if tid in map2:\n",
    "            vec2.append(map2[tid])\n",
    "        else:\n",
    "            return float('inf')\n",
    "    \n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2, layer_indices):\n",
    "    \"\"\"Compare two signal sets, return distances.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "    \n",
    "    all_key_dists = []\n",
    "    all_logprob_dists = []\n",
    "    \n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]['signals'] if 'signals' in signals1[pos_label] else signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]['signals'] if 'signals' in signals2[pos_label] else signals2[pos_label]\n",
    "        \n",
    "        for layer_name in sig1['key_vectors'].keys():\n",
    "            dist = compute_l2_distance(\n",
    "                sig1['key_vectors'][layer_name],\n",
    "                sig2['key_vectors'][layer_name]\n",
    "            )\n",
    "            all_key_dists.append(dist)\n",
    "            \n",
    "        dist = compute_logprob_distance(sig1['logprobs'], sig2['logprobs'])\n",
    "        all_logprob_dists.append(dist)\n",
    "        \n",
    "    return {\n",
    "        'key_vectors_max': max(all_key_dists) if all_key_dists else 0.0,\n",
    "        'key_vectors_mean': np.mean(all_key_dists) if all_key_dists else 0.0,\n",
    "        'logprobs_max': max(all_logprob_dists) if all_logprob_dists else 0.0,\n",
    "        'logprobs_mean': np.mean(all_logprob_dists) if all_logprob_dists else 0.0\n",
    "    }\n",
    "\n",
    "def find_equivalent_pairs(matrix, batch_sizes, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"Find pairs of batch sizes that produce equivalent results (same kernel).\"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n_bs = len(batch_sizes)\n",
    "    \n",
    "    for i in range(n_bs):\n",
    "        for j in range(i + 1, n_bs):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((batch_sizes[i], batch_sizes[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, batch_sizes):\n",
    "    \"\"\"Group batch sizes into kernel equivalence classes.\"\"\"\n",
    "    parent = {bs: bs for bs in batch_sizes}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        union(bs1, bs2)\n",
    "    \n",
    "    groups = {}\n",
    "    for bs in batch_sizes:\n",
    "        root = find(bs)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(bs)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "def analyze_within_hardware(measurements, batch_sizes, layer_indices, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware batch size effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE BATCH SIZE EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    by_ref = {}\n",
    "    for m in measurements:\n",
    "        ref = m['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "        by_ref[ref][m['batch_size']] = m[signals_key]\n",
    "\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix_key = np.zeros((n_bs, n_bs))\n",
    "        matrix_logprob = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, bs1 in enumerate(batch_sizes):\n",
    "            for j, bs2 in enumerate(batch_sizes):\n",
    "                if bs1 in ref_data and bs2 in ref_data:\n",
    "                    distances = compare_signals(ref_data[bs1], ref_data[bs2], layer_indices)\n",
    "                    matrix_key[i, j] = distances['key_vectors_mean']\n",
    "                    matrix_logprob[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "        \n",
    "        log_print(f\"\\nKey Vectors (mean L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, bs in enumerate(batch_sizes):\n",
    "            row_str = f\"bs={bs:<3}\"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        log_print(f\"\\nLogprobs (mean L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, bs in enumerate(batch_sizes):\n",
    "            row_str = f\"bs={bs:<3}\"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors (mean L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        row_str = f\"bs={bs:<3}\"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_key_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    log_print(f\"\\nLogprobs (mean L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        row_str = f\"bs={bs:<3}\"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_logprob_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    off_diag_key = avg_key_matrix[np.triu_indices(n_bs, k=1)]\n",
    "    off_diag_logprob = avg_logprob_matrix[np.triu_indices(n_bs, k=1)]\n",
    "    \n",
    "    key_mean = np.mean(off_diag_key[np.isfinite(off_diag_key)]) if np.any(np.isfinite(off_diag_key)) else 0\n",
    "    logprob_mean = np.mean(off_diag_logprob[np.isfinite(off_diag_logprob)]) if np.any(np.isfinite(off_diag_logprob)) else 0\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    log_print(f\"  Key vectors - Mean: {key_mean:.2e}, Range: [{np.min(off_diag_key):.2e}, {np.max(off_diag_key):.2e}]\")\n",
    "    log_print(f\"  Logprobs - Mean: {logprob_mean:.2e}, Range: [{np.min(off_diag_logprob):.2e}, {np.max(off_diag_logprob):.2e}]\")\n",
    "\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_key_matrix, batch_sizes)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, batch_sizes)\n",
    "    \n",
    "    zero_count = np.sum(off_diag_key < EQUIVALENCE_THRESHOLD)\n",
    "    total_count = len(off_diag_key)\n",
    "    \n",
    "    if zero_count == total_count:\n",
    "        log_print(f\"\\n⚠ WARNING: {zero_count}/{total_count} comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All batch sizes produce identical results (single kernel class)\")\n",
    "    elif zero_count > 0:\n",
    "        log_print(f\"\\n⚠ NOTE: {zero_count}/{total_count} comparisons are equivalent (< {EQUIVALENCE_THRESHOLD})\")\n",
    "    \n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs (will be excluded from cross-hardware signal):\")\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2})\")\n",
    "    \n",
    "    threshold = 1e-10\n",
    "    if key_mean > threshold and logprob_mean > threshold:\n",
    "        log_print(\"\\n✓ SANITY CHECK PASSED\")\n",
    "    else:\n",
    "        log_print(\"\\n✗ SANITY CHECK FAILED (No variation)\")\n",
    "        \n",
    "    return {\n",
    "        'key_matrix': avg_key_matrix.tolist(),\n",
    "        'logprob_matrix': avg_logprob_matrix.tolist(),\n",
    "        'per_reference_key_matrices': [m.tolist() for m in all_key_matrices],\n",
    "        'per_reference_logprob_matrices': [m.tolist() for m in all_logprob_matrices],\n",
    "        'key_vectors_mean': float(key_mean),\n",
    "        'logprobs_mean': float(logprob_mean),\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes]\n",
    "    }\n",
    "\n",
    "def analyze_cross_hardware_matrix(comparison_results, batch_sizes, layer_indices, \n",
    "                                   signal_source='decode', equivalent_pairs=None):\n",
    "    \"\"\"\n",
    "    Analyze the comparison matrix and determine detectability.\n",
    "    Excludes equivalent pairs from SNR signal calculation.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    equiv_set = set()\n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        equiv_set.add((bs1, bs2))\n",
    "        equiv_set.add((bs2, bs1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_batch_size'], result['verify_batch_size'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"{ref_name.upper()}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix_key = np.zeros((n_bs, n_bs))\n",
    "        matrix_logprob = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            for j, verify_bs in enumerate(batch_sizes):\n",
    "                key = (claimed_bs, verify_bs)\n",
    "                if key in ref_data:\n",
    "                    matrix_key[i, j] = ref_data[key][dist_key]['key_vectors_mean']\n",
    "                    matrix_logprob[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "        \n",
    "        log_print(f\"\\nKey Vectors (mean L2):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_key[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        log_print(f\"\\nLogprobs (mean L2):\")\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix_logprob[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "            \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "\n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors (mean L2):\")\n",
    "    log_print(header)\n",
    "    for i, claimed_bs in enumerate(batch_sizes):\n",
    "        row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_key_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    log_print(f\"\\nLogprobs (mean L2):\")\n",
    "    log_print(header)\n",
    "    for i, claimed_bs in enumerate(batch_sizes):\n",
    "        row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_logprob_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    diag_key = np.diag(avg_key_matrix)\n",
    "    diag_logprob = np.diag(avg_logprob_matrix)\n",
    "    diag_key_mean = np.mean(diag_key)\n",
    "    diag_logprob_mean = np.mean(diag_logprob)\n",
    "\n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same batch size):\")\n",
    "    log_print(f\"  Key vectors: {diag_key_mean:.2e}\")\n",
    "    log_print(f\"  Logprobs: {diag_logprob_mean:.2e}\")\n",
    "\n",
    "    off_diag_key = []\n",
    "    off_diag_logprob = []\n",
    "    for i in range(n_bs):\n",
    "        for j in range(n_bs):\n",
    "            if i != j:\n",
    "                off_diag_key.append(avg_key_matrix[i, j])\n",
    "                off_diag_logprob.append(avg_logprob_matrix[i, j])\n",
    "    \n",
    "    off_diag_key = np.array(off_diag_key)\n",
    "    off_diag_logprob = np.array(off_diag_logprob)\n",
    "    \n",
    "    off_key_mean = np.mean(off_diag_key)\n",
    "    off_logprob_mean = np.mean(off_diag_logprob)\n",
    "    \n",
    "    snr_key = off_key_mean / diag_key_mean if diag_key_mean > 0 else float('inf')\n",
    "    snr_logprob = off_logprob_mean / diag_logprob_mean if diag_logprob_mean > 0 else float('inf')\n",
    "\n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Key vectors - Mean: {off_key_mean:.2e}, SNR: {snr_key:.2f}×\")\n",
    "    log_print(f\"  Logprobs - Mean: {off_logprob_mean:.2e}, SNR: {snr_logprob:.2f}×\")\n",
    "\n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        excluded_count = 0\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2}) and ({bs2}, {bs1})\")\n",
    "            excluded_count += 2\n",
    "        log_print(f\"  Total excluded: {excluded_count} cells\")\n",
    "        \n",
    "        meaningful_key = []\n",
    "        meaningful_logprob = []\n",
    "        for i in range(n_bs):\n",
    "            for j in range(n_bs):\n",
    "                if i != j:\n",
    "                    bs_i, bs_j = batch_sizes[i], batch_sizes[j]\n",
    "                    if (bs_i, bs_j) not in equiv_set:\n",
    "                        meaningful_key.append(avg_key_matrix[i, j])\n",
    "                        meaningful_logprob.append(avg_logprob_matrix[i, j])\n",
    "        \n",
    "        if meaningful_key:\n",
    "            meaningful_key = np.array(meaningful_key)\n",
    "            meaningful_logprob = np.array(meaningful_logprob)\n",
    "            meaningful_key_mean = np.mean(meaningful_key)\n",
    "            meaningful_logprob_mean = np.mean(meaningful_logprob)\n",
    "            \n",
    "            snr_key_meaningful = meaningful_key_mean / diag_key_mean if diag_key_mean > 0 else float('inf')\n",
    "            snr_logprob_meaningful = meaningful_logprob_mean / diag_logprob_mean if diag_logprob_mean > 0 else float('inf')\n",
    "            \n",
    "            log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "            log_print(f\"  Count: {len(meaningful_key)}\")\n",
    "            log_print(f\"  Key vectors - Mean: {meaningful_key_mean:.2e}, SNR: {snr_key_meaningful:.2f}×\")\n",
    "            log_print(f\"  Logprobs - Mean: {meaningful_logprob_mean:.2e}, SNR: {snr_logprob_meaningful:.2f}×\")\n",
    "        else:\n",
    "            snr_key_meaningful = snr_key\n",
    "            snr_logprob_meaningful = snr_logprob\n",
    "    else:\n",
    "        snr_key_meaningful = snr_key\n",
    "        snr_logprob_meaningful = snr_logprob\n",
    "\n",
    "    return {\n",
    "        'key_vectors': {\n",
    "            'diagonal_mean': float(diag_key_mean),\n",
    "            'off_diagonal_mean': float(off_key_mean),\n",
    "            'snr': float(snr_key),\n",
    "            'snr_meaningful': float(snr_key_meaningful)\n",
    "        },\n",
    "        'logprobs': {\n",
    "            'diagonal_mean': float(diag_logprob_mean),\n",
    "            'off_diagonal_mean': float(off_logprob_mean),\n",
    "            'snr': float(snr_logprob),\n",
    "            'snr_meaningful': float(snr_logprob_meaningful)\n",
    "        },\n",
    "        'matrices': {\n",
    "            'key_vectors': avg_key_matrix.tolist(),\n",
    "            'logprobs': avg_logprob_matrix.tolist()\n",
    "        },\n",
    "        'excluded_pairs': equivalent_pairs\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    global REFERENCE_SEQUENCES, DUMMY_SETS\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    \n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION (reference)\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"Attention: {ATTN_IMPLEMENTATION}\")\n",
    "    log_print(f\"Layers: {LAYER_INDICES}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    REFERENCE_SEQUENCES, DUMMY_SETS = create_sequences_from_pdf(tokenizer)\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=ATTN_IMPLEMENTATION\n",
    "    )\n",
    "    \n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        # VERIFICATION MODE\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            reference = json.load(f)\n",
    "            \n",
    "        ref_env = reference['metadata']['environment']\n",
    "        validate_environment_match(ref_env, system_info)\n",
    "        \n",
    "        prefill_sanity = reference.get('prefill_sanity_check', {})\n",
    "        decode_sanity = reference.get('decode_sanity_check', {})\n",
    "        \n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_sanity.get('equivalent_pairs', [])]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_sanity.get('equivalent_pairs', [])]\n",
    "        \n",
    "        log_print(f\"\\nLoaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "        \n",
    "        comparison_results = []\n",
    "        ref_by_key = {}\n",
    "        for m in reference['measurements']:\n",
    "            key = (m['ref_name'], m['batch_size'])\n",
    "            ref_by_key[key] = m\n",
    "            \n",
    "        for ref_name in sorted(REFERENCE_SEQUENCES.keys()):\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            for claimed_bs in BATCH_SIZES:\n",
    "                ref_key = (ref_name, claimed_bs)\n",
    "                if ref_key not in ref_by_key:\n",
    "                    log_print(f\"  ⚠ No reference data for {ref_name} bs={claimed_bs}\")\n",
    "                    continue\n",
    "                ref_data = ref_by_key[ref_key]\n",
    "                \n",
    "                log_print(f\"\\n  Claimed batch size: {claimed_bs}\")\n",
    "                \n",
    "                for verify_bs in BATCH_SIZES:\n",
    "                    is_diagonal = (claimed_bs == verify_bs)\n",
    "                    \n",
    "                    log_print(f\"    Verify bs={verify_bs} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "                    \n",
    "                    verify_result = run_teacher_forced_decode(\n",
    "                        model, tokenizer, ref_name, ref_data,\n",
    "                        verify_bs, LAYER_INDICES, is_diagonal\n",
    "                    )\n",
    "                    \n",
    "                    prefill_distances = compare_signals(\n",
    "                        ref_data['prefill_signals'], verify_result['prefill_signals'], LAYER_INDICES\n",
    "                    )\n",
    "                    decode_distances = compare_signals(\n",
    "                        ref_data['decode_signals'], verify_result['decode_signals'], LAYER_INDICES\n",
    "                    )\n",
    "                    \n",
    "                    log_print(f\"      Key: {decode_distances['key_vectors_mean']:.2e}, LP: {decode_distances['logprobs_mean']:.2e}\")\n",
    "                    \n",
    "                    comparison_results.append({\n",
    "                        'ref_name': ref_name,\n",
    "                        'claimed_batch_size': claimed_bs,\n",
    "                        'verify_batch_size': verify_bs,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances,\n",
    "                    })\n",
    "\n",
    "        prefill_analysis = analyze_cross_hardware_matrix(\n",
    "            comparison_results, BATCH_SIZES, LAYER_INDICES, \n",
    "            signal_source='prefill', equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware_matrix(\n",
    "            comparison_results, BATCH_SIZES, LAYER_INDICES,\n",
    "            signal_source='decode', equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "        \n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON (meaningful SNR)\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill - Key: {prefill_analysis['key_vectors']['snr_meaningful']:.2f}×, LP: {prefill_analysis['logprobs']['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode  - Key: {decode_analysis['key_vectors']['snr_meaningful']:.2f}×, LP: {decode_analysis['logprobs']['snr_meaningful']:.2f}×\")\n",
    "        \n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'layer_indices': LAYER_INDICES,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "        filepath = os.path.join(output_dir, f\"verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "            \n",
    "    else:\n",
    "        # GENERATION MODE\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'layer_indices': LAYER_INDICES\n",
    "            },\n",
    "            'measurements': []\n",
    "        }\n",
    "        \n",
    "        for ref_name, ref_token_ids in REFERENCE_SEQUENCES.items():\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "            \n",
    "            log_print(f\"\\nPrompt length: {len(ref_token_ids)} tokens\\n\")\n",
    "            \n",
    "            for batch_size in BATCH_SIZES:\n",
    "                log_print(f\"  bs={batch_size}:\", end=\"\")\n",
    "                decode_data = run_decode_with_extraction(\n",
    "                    model, tokenizer, ref_token_ids, ref_name, batch_size, LAYER_INDICES\n",
    "                )\n",
    "                results['measurements'].append({\n",
    "                    'ref_name': ref_name,\n",
    "                    'batch_size': batch_size,\n",
    "                    'generated_ids': decode_data['generated_ids'],\n",
    "                    'prompt_token_ids': decode_data['prompt_token_ids'],\n",
    "                    'prefill_signals': decode_data['prefill_signals'],\n",
    "                    'decode_signals': decode_data['decode_signals'],\n",
    "                    'all_batch_generated_ids': decode_data['all_batch_generated_ids']\n",
    "                })\n",
    "        \n",
    "        # Token consistency check\n",
    "        for ref_name in REFERENCE_SEQUENCES.keys():\n",
    "            log_print(f\"\\n--- Token consistency for {ref_name} ---\")\n",
    "            ref_measurements = {m['batch_size']: m for m in results['measurements'] if m['ref_name'] == ref_name}\n",
    "            check_token_consistency(ref_measurements, tokenizer)\n",
    "                \n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, LAYER_INDICES, 'prefill'\n",
    "        )\n",
    "        decode_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, LAYER_INDICES, 'decode'\n",
    "        )\n",
    "        \n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"decode_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "            \n",
    "        log_print(f\"\\n✓ Saved to: {filepath}\")\n",
    "        log_print(f\"\\nNext step: Copy to verifier machine, set TEACHER_FORCING=True\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b4face-f066-4ac2-b379-5d6c1a46e210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
