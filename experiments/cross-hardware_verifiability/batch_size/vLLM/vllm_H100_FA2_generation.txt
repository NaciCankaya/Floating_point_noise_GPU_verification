================================================================================
vLLM CROSS-HARDWARE BATCH SIZE DETECTABILITY - GENERATION (decode)
================================================================================

System: c6ba2aea0d91
GPU: NVIDIA H100 80GB HBM3
vLLM: 0.11.2
PyTorch: 2.9.0+cu128
CUDA: 12.8

Configuration:
  Model: Qwen/Qwen2.5-7B-Instruct
  Batch sizes: [1, 2, 3, 4, 5, 8, 9, 16, 17]
  Max tokens: 20
  Top-k logprobs: 20

Loading vLLM model...
✓ Model loaded

Found 1 PDF(s)
  Loading: /workspace/Verification-for-International-AI-Governance.pdf
    → 120214 tokens
Total tokens: 120214
Creating 51 slices of 1000 tokens each
Created 3 reference sequences


================================================================================
REFERENCE: ref_0
================================================================================

Global minimum prompt length: 1000 tokens

  bs=1:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=2:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=3:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=4:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=5:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=8:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=9:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=16:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=17:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)

================================================================================
REFERENCE: ref_1
================================================================================

Global minimum prompt length: 1000 tokens

  bs=1:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=2:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=3:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=4:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=5:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=8:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=9:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=16:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=17:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)

================================================================================
REFERENCE: ref_2
================================================================================

Global minimum prompt length: 1000 tokens

  bs=1:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=2:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=3:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=4:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=5:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=8:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=9:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=16:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)
  bs=17:       Prompt: 1000 tokens → Final: 1020 tokens (20 generated)

✓ Generation results saved to: /workspace/experiments/vllm_decode_20251126_000455.json

--- Token consistency for ref_0 ---

================================================================================
TOKEN GENERATION CONSISTENCY CHECK
================================================================================

Generated tokens by batch size:
  bs=1:
    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
    Text: ' . . . . . . . . . . . . . . . . . . . .'
    ✓
  bs=2:
    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
    Text: ' . . . . . . . . . . . . . . . . . . . .'
    ✓
  bs=3:
    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
    Text: ' . . . . . . . . . . . . . . . . . . . .'
    ✓
  bs=4:
    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
    Text: ' . . . . . . . . . . . . . . . . . . . .'
    ✓
  bs=5:
    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
    Text: ' . . . . . . . . . . . . . . . . . . . .'
    ✓
  bs=8:
    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
    Text: ' . . . . . . . . . . . . . . . . . . . .'
    ✓
  bs=9:
    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
    Text: ' . . . . . . . . . . . . . . . . . . . .'
    ✓
  bs=16:
    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
    Text: ' . . . . . . . . . . . . . . . . . . . .'
    ✓
  bs=17:
    IDs:  [659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]
    Text: ' . . . . . . . . . . . . . . . . . . . .'
    ✓

✓ Element 0 generates IDENTICAL tokens across all batch sizes

--- Token consistency for ref_1 ---

================================================================================
TOKEN GENERATION CONSISTENCY CHECK
================================================================================

Generated tokens by batch size:
  bs=1:
    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 17, 15, 23, 16, 23, 18]
    Text: '://doi.org/10.1017/S00208183'
    ✓
  bs=2:
    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 17, 15, 23, 16, 23, 18]
    Text: '://doi.org/10.1017/S00208183'
    ✓
  bs=3:
    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 17, 15, 23, 16, 23, 18]
    Text: '://doi.org/10.1017/S00208183'
    ✓
  bs=4:
    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 17, 15, 23, 16, 23, 18]
    Text: '://doi.org/10.1017/S00208183'
    ✓
  bs=5:
    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 17, 15, 23, 16, 23, 18]
    Text: '://doi.org/10.1017/S00208183'
    ✓
  bs=8:
    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 17, 15, 23, 16, 23, 18]
    Text: '://doi.org/10.1017/S00208183'
    ✓
  bs=9:
    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 17, 15, 23, 16, 23, 18]
    Text: '://doi.org/10.1017/S00208183'
    ✓
  bs=16:
    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 17, 15, 23, 16, 23, 18]
    Text: '://doi.org/10.1017/S00208183'
    ✓
  bs=17:
    IDs:  [1110, 47786, 2659, 14, 16, 15, 13, 16, 15, 16, 22, 11374, 15, 15, 17, 15, 23, 16, 23, 18]
    Text: '://doi.org/10.1017/S00208183'
    ✓

✓ Element 0 generates IDENTICAL tokens across all batch sizes

--- Token consistency for ref_2 ---

================================================================================
TOKEN GENERATION CONSISTENCY CHECK
================================================================================

Generated tokens by batch size:
  bs=1:
    IDs:  [311, 387, 82004, 624, 16, 20, 17, 1986, 374, 264, 3491, 369, 894, 3093, 315, 22901, 11, 537, 1101, 51352]
    Text: ' to be reconstructed.\n152This is a problem for any kind of verification, not just downstream'
    ✓
  bs=2:
    IDs:  [311, 387, 82004, 624, 16, 20, 17, 1986, 374, 264, 3491, 369, 894, 3093, 315, 22901, 11, 537, 1101, 51352]
    Text: ' to be reconstructed.\n152This is a problem for any kind of verification, not just downstream'
    ✓
  bs=3:
    IDs:  [311, 387, 82004, 624, 16, 20, 17, 1986, 374, 264, 3491, 369, 894, 3093, 315, 22901, 11, 537, 1101, 51352]
    Text: ' to be reconstructed.\n152This is a problem for any kind of verification, not just downstream'
    ✓
  bs=4:
    IDs:  [311, 387, 82004, 624, 16, 20, 17, 1986, 374, 264, 3491, 369, 894, 3093, 315, 22901, 11, 537, 1101, 51352]
    Text: ' to be reconstructed.\n152This is a problem for any kind of verification, not just downstream'
    ✓
  bs=5:
    IDs:  [311, 387, 82004, 624, 16, 20, 17, 1986, 374, 264, 3491, 369, 894, 3093, 315, 22901, 11, 537, 1101, 51352]
    Text: ' to be reconstructed.\n152This is a problem for any kind of verification, not just downstream'
    ✓
  bs=8:
    IDs:  [311, 387, 82004, 624, 16, 20, 17, 1986, 374, 264, 3491, 369, 894, 3093, 315, 22901, 11, 537, 1101, 369]
    Text: ' to be reconstructed.\n152This is a problem for any kind of verification, not just for'
    ✗ DIFFERENT
  bs=9:
    IDs:  [311, 387, 82004, 624, 16, 20, 17, 1986, 374, 264, 3491, 369, 894, 3093, 315, 22901, 11, 537, 1101, 51352]
    Text: ' to be reconstructed.\n152This is a problem for any kind of verification, not just downstream'
    ✓
  bs=16:
    IDs:  [311, 387, 82004, 624, 16, 20, 17, 1986, 374, 264, 3491, 369, 894, 3093, 315, 22901, 11, 537, 1101, 51352]
    Text: ' to be reconstructed.\n152This is a problem for any kind of verification, not just downstream'
    ✓
  bs=17:
    IDs:  [311, 387, 82004, 624, 16, 20, 17, 1986, 374, 264, 3491, 369, 894, 3093, 315, 22901, 11, 537, 1101, 51352]
    Text: ' to be reconstructed.\n152This is a problem for any kind of verification, not just downstream'
    ✓

⚠ Element 0 generates DIFFERENT tokens across batch sizes

================================================================================
WITHIN-HARDWARE BATCH SIZE EFFECTS (PREFILL)
================================================================================

ref_0:
       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 
bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00

ref_1:
       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 
bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00

ref_2:
       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 
bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00

================================================================================
AGGREGATE (average across references):
================================================================================
       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 
bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=16   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00
bs=17   0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00

Off-diagonal stats:
  Mean: 0.00e+00
  Range: [0.00e+00, 0.00e+00]

⚠ WARNING: 36/36 comparisons are EXACTLY ZERO
  All batch sizes produce identical results (single kernel class)

Kernel equivalence classes:
  Class 1: [1, 2, 3, 4, 5, 8, 9, 16, 17]

Equivalent pairs (will be excluded from cross-hardware signal):
  (1, 2)
  (1, 3)
  (1, 4)
  (1, 5)
  (1, 8)
  (1, 9)
  (1, 16)
  (1, 17)
  (2, 3)
  (2, 4)
  (2, 5)
  (2, 8)
  (2, 9)
  (2, 16)
  (2, 17)
  (3, 4)
  (3, 5)
  (3, 8)
  (3, 9)
  (3, 16)
  (3, 17)
  (4, 5)
  (4, 8)
  (4, 9)
  (4, 16)
  (4, 17)
  (5, 8)
  (5, 9)
  (5, 16)
  (5, 17)
  (8, 9)
  (8, 16)
  (8, 17)
  (9, 16)
  (9, 17)
  (16, 17)

================================================================================
WITHIN-HARDWARE BATCH SIZE EFFECTS (DECODE)
================================================================================

ref_0:
       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 
bs=1    0.00e+00  1.42e-01  1.42e-01  2.01e-01  2.01e-01  1.86e-01  1.31e-01  1.18e-01  1.84e-01
bs=2    1.42e-01  0.00e+00  0.00e+00  5.89e-02  5.89e-02  1.55e-01  1.55e-01  1.42e-01  4.17e-02
bs=3    1.42e-01  0.00e+00  0.00e+00  5.89e-02  5.89e-02  1.55e-01  1.55e-01  1.42e-01  4.17e-02
bs=4    2.01e-01  5.89e-02  5.89e-02  0.00e+00  0.00e+00  1.55e-01  1.25e-01  1.42e-01  4.17e-02
bs=5    2.01e-01  5.89e-02  5.89e-02  0.00e+00  0.00e+00  1.55e-01  1.25e-01  1.42e-01  4.17e-02
bs=8    1.86e-01  1.55e-01  1.55e-01  1.55e-01  1.55e-01  0.00e+00  1.73e-01  2.24e-01  1.67e-01
bs=9    1.31e-01  1.55e-01  1.55e-01  1.25e-01  1.25e-01  1.73e-01  0.00e+00  7.22e-02  1.42e-01
bs=16   1.18e-01  1.42e-01  1.42e-01  1.42e-01  1.42e-01  2.24e-01  7.22e-02  0.00e+00  1.25e-01
bs=17   1.84e-01  4.17e-02  4.17e-02  4.17e-02  4.17e-02  1.67e-01  1.42e-01  1.25e-01  0.00e+00

ref_1:
       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 
bs=1    0.00e+00  1.56e-01  1.56e-01  1.92e-01  1.92e-01  2.10e-01  1.68e-01  1.39e-01  1.18e-01
bs=2    1.56e-01  0.00e+00  0.00e+00  1.30e-01  1.30e-01  1.72e-01  1.62e-01  7.17e-02  1.17e-01
bs=3    1.56e-01  0.00e+00  0.00e+00  1.30e-01  1.30e-01  1.72e-01  1.62e-01  7.17e-02  1.17e-01
bs=4    1.92e-01  1.30e-01  1.30e-01  0.00e+00  0.00e+00  4.18e-02  1.18e-01  7.18e-02  1.29e-01
bs=5    1.92e-01  1.30e-01  1.30e-01  0.00e+00  0.00e+00  4.18e-02  1.18e-01  7.18e-02  1.29e-01
bs=8    2.10e-01  1.72e-01  1.72e-01  4.18e-02  4.18e-02  0.00e+00  1.37e-01  1.14e-01  1.70e-01
bs=9    1.68e-01  1.62e-01  1.62e-01  1.18e-01  1.18e-01  1.37e-01  0.00e+00  1.08e-01  1.50e-01
bs=16   1.39e-01  7.17e-02  7.17e-02  7.18e-02  7.18e-02  1.14e-01  1.08e-01  0.00e+00  9.28e-02
bs=17   1.18e-01  1.17e-01  1.17e-01  1.29e-01  1.29e-01  1.70e-01  1.50e-01  9.28e-02  0.00e+00

ref_2:
       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 
bs=1    0.00e+00  7.70e-02  7.70e-02  1.44e-01  1.44e-01  1.12e-01  1.54e-01  1.07e-01  1.18e-01
bs=2    7.70e-02  0.00e+00  0.00e+00  9.15e-02  9.15e-02  1.51e-01  1.19e-01  1.13e-01  1.58e-01
bs=3    7.70e-02  0.00e+00  0.00e+00  9.15e-02  9.15e-02  1.51e-01  1.19e-01  1.13e-01  1.58e-01
bs=4    1.44e-01  9.15e-02  9.15e-02  0.00e+00  0.00e+00  7.77e-02  6.87e-02  4.14e-02  9.18e-02
bs=5    1.44e-01  9.15e-02  9.15e-02  0.00e+00  0.00e+00  7.77e-02  6.87e-02  4.14e-02  9.18e-02
bs=8    1.12e-01  1.51e-01  1.51e-01  7.77e-02  7.77e-02  0.00e+00  9.11e-02  7.73e-02  8.55e-02
bs=9    1.54e-01  1.19e-01  1.19e-01  6.87e-02  6.87e-02  9.11e-02  0.00e+00  9.07e-02  4.36e-02
bs=16   1.07e-01  1.13e-01  1.13e-01  4.14e-02  4.14e-02  7.73e-02  9.07e-02  0.00e+00  5.36e-02
bs=17   1.18e-01  1.58e-01  1.58e-01  9.18e-02  9.18e-02  8.55e-02  4.36e-02  5.36e-02  0.00e+00

================================================================================
AGGREGATE (average across references):
================================================================================
       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 bs= 16 bs= 17 
bs=1    0.00e+00  1.25e-01  1.25e-01  1.79e-01  1.79e-01  1.69e-01  1.51e-01  1.21e-01  1.40e-01
bs=2    1.25e-01  0.00e+00  0.00e+00  9.34e-02  9.34e-02  1.59e-01  1.45e-01  1.09e-01  1.05e-01
bs=3    1.25e-01  0.00e+00  0.00e+00  9.34e-02  9.34e-02  1.59e-01  1.45e-01  1.09e-01  1.05e-01
bs=4    1.79e-01  9.34e-02  9.34e-02  0.00e+00  0.00e+00  9.16e-02  1.04e-01  8.52e-02  8.73e-02
bs=5    1.79e-01  9.34e-02  9.34e-02  0.00e+00  0.00e+00  9.16e-02  1.04e-01  8.52e-02  8.73e-02
bs=8    1.69e-01  1.59e-01  1.59e-01  9.16e-02  9.16e-02  0.00e+00  1.34e-01  1.38e-01  1.41e-01
bs=9    1.51e-01  1.45e-01  1.45e-01  1.04e-01  1.04e-01  1.34e-01  0.00e+00  9.02e-02  1.12e-01
bs=16   1.21e-01  1.09e-01  1.09e-01  8.52e-02  8.52e-02  1.38e-01  9.02e-02  0.00e+00  9.05e-02
bs=17   1.40e-01  1.05e-01  1.05e-01  8.73e-02  8.73e-02  1.41e-01  1.12e-01  9.05e-02  0.00e+00

Off-diagonal stats:
  Mean: 1.12e-01
  Range: [0.00e+00, 1.79e-01]

⚠ NOTE: 2/36 comparisons are equivalent (< 1e-09)

Kernel equivalence classes:
  Class 1: [1]
  Class 2: [2, 3]
  Class 3: [4, 5]
  Class 4: [8]
  Class 5: [9]
  Class 6: [16]
  Class 7: [17]

Equivalent pairs (will be excluded from cross-hardware signal):
  (2, 3)
  (4, 5)

Next step: Copy /workspace/experiments/vllm_decode_20251126_000455.json to verifier machine
Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'
File size: 3.2 MB

================================================================================
EXPERIMENT COMPLETE
================================================================================

