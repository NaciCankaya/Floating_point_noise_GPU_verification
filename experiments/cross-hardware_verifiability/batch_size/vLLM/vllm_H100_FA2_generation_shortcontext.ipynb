{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb7d50dc-adb1-40d8-a790-5bc73c73ce77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "vLLM CROSS-HARDWARE BATCH SIZE DETECTABILITY - GENERATION (decode)\n",
      "================================================================================\n",
      "\n",
      "System: c6ba2aea0d91\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "vLLM: 0.11.2\n",
      "PyTorch: 2.9.0+cu128\n",
      "CUDA: 12.8\n",
      "\n",
      "Configuration:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  Batch sizes: [1, 2, 3, 4, 5, 8, 9]\n",
      "  Max tokens: 20\n",
      "  Top-k logprobs: 20\n",
      "\n",
      "Loading vLLM model...\n",
      "INFO 11-26 00:19:06 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-7B-Instruct'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 00:19:07 [model.py:631] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 11-26 00:19:07 [model.py:1745] Using max model len 32768\n",
      "INFO 11-26 00:19:07 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:11 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:12 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:46177 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:12 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:12 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen2.5-7B-Instruct...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:12 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:12 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  1.57it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.51it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.55it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.55it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:15 [default_loader.py:314] Loading weights took 2.63 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:16 [gpu_model_runner.py:3338] Model loading took 14.2488 GiB memory and 3.159590 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:19 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/f97525d4f8/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:19 [backends.py:647] Dynamo bytecode transform time: 3.32 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:22 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.451 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:23 [monitor.py:34] torch.compile takes 5.77 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:23 [gpu_worker.py:359] Available KV cache memory: 35.38 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:23 [kv_cache_utils.py:1229] GPU KV cache size: 662,384 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:23 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 20.21x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m 2025-11-26 00:19:23,964 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m 2025-11-26 00:19:23,975 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 26.39it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 44.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:27 [gpu_model_runner.py:4244] Graph capturing finished in 3 secs, took -2.16 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=4081)\u001b[0;0m INFO 11-26 00:19:27 [core.py:250] init engine (profile, create kv cache, warmup model) took 11.33 seconds\n",
      "INFO 11-26 00:19:28 [llm.py:352] Supported tasks: ['generate']\n",
      "✓ Model loaded\n",
      "\n",
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n",
      "    → 120214 tokens\n",
      "Total tokens: 120214\n",
      "Creating 27 slices of 200 tokens each\n",
      "Created 3 reference sequences\n",
      "\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_0\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 200 tokens\n",
      "\n",
      "  bs=1:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434057785dea4e4cbeb770fa762dd784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79dafb74b8d446ead8dbd272194676c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=2:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d032c37705484fabf14dd86d0e38e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48fc89bf0f8947faadfa236d5c413ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=3:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4523f13095df4647b4c05a0e8f27990b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6a1a4c99414a5f8ad50682ca5ed7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=4:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "989f22c073244d0e9698af9adb911da5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4d65ccd0b745c8a541dbca7fc3b05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=5:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46a5ab321264a649e298876faf2257a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb076e67586d4ce4a28925ca270f4203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=8:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f4e00571e8a44969596f7c84eafdd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a52bea7942c54b3c951190f9ad2e338c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=9:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88bb0dd6a6224e41a4ffa1b8f43620c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185233faa0194f6894de2176b975b69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_1\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 200 tokens\n",
      "\n",
      "  bs=1:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6625c9454cdb442992cd7a27ee263c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c22b61341724671be19fc21f6c146cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=2:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cb2799743647009d2766e1941dc69a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d096fcf8e8654f899f185abfb46daf96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=3:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7966a2f609a543348a47de4ff0820f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3906f0cd8468444985a7738b547caf44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=4:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e79d406627dc44e598523a073a7387b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4692c65f98e4ea3b9b45e6e30e910f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=5:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ab08567a314a6b9ebfc81af8acbf10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "552675208b854d7bb0298ae84cb91a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=8:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "509295461b8e43c8811d85157e25267d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f64e17ad9604635b11e308e6bb28908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=9:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29db54af5dc4aa6af83eda54a24f0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caae0f6d51f45b78b9029cf09a90e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "\n",
      "================================================================================\n",
      "REFERENCE: ref_2\n",
      "================================================================================\n",
      "\n",
      "Global minimum prompt length: 200 tokens\n",
      "\n",
      "  bs=1:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b73e8ed7114840b8af5e8a23f5622243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d05d6cf5fff448f395cc252c9ff3b28a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=2:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650f73b4db034967b03bf96771949e88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c89acf8f5344611bbd5e8a5120d8787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/2 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=3:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984199f4bb804fd684ef77eccea0d28b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf51143e94e147cdaace6df54c25e306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/3 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=4:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d864c58321e426ebfa1ee16bf159c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "511c84f0521743c1b1614378f4cc0f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/4 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=5:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b459797f036428ca59dcef73d8cdbf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ed0a0cc5e24facbc41fa27c550e7c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=8:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285d19100ed242b8b6953475ba6de1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c1faeb466940b3b6b4d3490908e792",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/8 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "  bs=9:       Prompt: 200 tokens"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b09d6741eac42d9b751af9dc42fac37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c1a0d6ad5b4c73978c5b9b695d2ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/9 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Final: 220 tokens (20 generated)\n",
      "\n",
      "✓ Generation results saved to: /workspace/experiments/vllm_decode_20251126_001933.json\n",
      "\n",
      "--- Token consistency for ref_0 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [2571, 8199, 42, 20175, 440, 17, 19, 11, 2016, 344, 5559, 50, 76367, 559, 2907, 17, 20, 11, 17, 21]\n",
      "    Text: 'lexanderKatzke24,ShivaniSrivastava25,26'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [2571, 8199, 42, 20175, 440, 17, 19, 11, 2016, 344, 5559, 50, 76367, 559, 2907, 17, 20, 11, 17, 21]\n",
      "    Text: 'lexanderKatzke24,ShivaniSrivastava25,26'\n",
      "    ✓\n",
      "  bs=3:\n",
      "    IDs:  [2571, 8199, 42, 20175, 440, 17, 19, 11, 2016, 344, 5559, 50, 76367, 559, 2907, 17, 20, 11, 17, 21]\n",
      "    Text: 'lexanderKatzke24,ShivaniSrivastava25,26'\n",
      "    ✓\n",
      "  bs=4:\n",
      "    IDs:  [2571, 8199, 42, 20175, 440, 17, 19, 11, 2016, 344, 5559, 50, 76367, 559, 2907, 17, 20, 11, 17, 21]\n",
      "    Text: 'lexanderKatzke24,ShivaniSrivastava25,26'\n",
      "    ✓\n",
      "  bs=5:\n",
      "    IDs:  [2571, 8199, 42, 20175, 440, 17, 19, 11, 2016, 344, 5559, 50, 76367, 559, 2907, 17, 20, 11, 17, 21]\n",
      "    Text: 'lexanderKatzke24,ShivaniSrivastava25,26'\n",
      "    ✓\n",
      "  bs=8:\n",
      "    IDs:  [2571, 8199, 42, 20175, 440, 17, 19, 11, 2016, 344, 5559, 50, 76367, 559, 2907, 17, 20, 11, 17, 21]\n",
      "    Text: 'lexanderKatzke24,ShivaniSrivastava25,26'\n",
      "    ✓\n",
      "  bs=9:\n",
      "    IDs:  [2571, 8199, 42, 20175, 440, 17, 19, 11, 2016, 344, 5559, 50, 76367, 559, 2907, 17, 20, 11, 17, 21]\n",
      "    Text: 'lexanderKatzke24,ShivaniSrivastava25,26'\n",
      "    ✓\n",
      "\n",
      "✓ Element 0 generates IDENTICAL tokens across all batch sizes\n",
      "\n",
      "--- Token consistency for ref_1 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [659, 659, 659, 659, 659, 220, 20, 20, 198, 17, 13, 18, 13, 16, 7420, 9471, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . 55\\n2.3.1 Power generation . . . .'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [659, 659, 659, 659, 659, 220, 20, 20, 198, 17, 13, 18, 13, 16, 7420, 9471, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . 55\\n2.3.1 Power generation . . . .'\n",
      "    ✓\n",
      "  bs=3:\n",
      "    IDs:  [659, 659, 659, 659, 659, 220, 20, 20, 198, 17, 13, 18, 13, 16, 7420, 9471, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . 55\\n2.3.1 Power generation . . . .'\n",
      "    ✓\n",
      "  bs=4:\n",
      "    IDs:  [659, 659, 659, 659, 659, 220, 20, 20, 198, 17, 13, 18, 13, 16, 7420, 9471, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . 55\\n2.3.1 Power generation . . . .'\n",
      "    ✓\n",
      "  bs=5:\n",
      "    IDs:  [659, 659, 659, 659, 659, 220, 20, 20, 198, 17, 13, 18, 13, 16, 7420, 9471, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . 55\\n2.3.1 Power generation . . . .'\n",
      "    ✓\n",
      "  bs=8:\n",
      "    IDs:  [659, 659, 659, 659, 659, 220, 20, 20, 198, 17, 13, 18, 13, 16, 7420, 9471, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . 55\\n2.3.1 Power generation . . . .'\n",
      "    ✓\n",
      "  bs=9:\n",
      "    IDs:  [659, 659, 659, 659, 659, 220, 20, 20, 198, 17, 13, 18, 13, 16, 7420, 9471, 659, 659, 659, 659]\n",
      "    Text: ' . . . . . 55\\n2.3.1 Power generation . . . .'\n",
      "    ✓\n",
      "\n",
      "✓ Element 0 generates IDENTICAL tokens across all batch sizes\n",
      "\n",
      "--- Token consistency for ref_2 ---\n",
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "Generated tokens by batch size:\n",
      "  bs=1:\n",
      "    IDs:  [22901, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' verification . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=2:\n",
      "    IDs:  [22901, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' verification . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=3:\n",
      "    IDs:  [22901, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' verification . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=4:\n",
      "    IDs:  [22901, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' verification . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=5:\n",
      "    IDs:  [22901, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' verification . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=8:\n",
      "    IDs:  [22901, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' verification . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "  bs=9:\n",
      "    IDs:  [22901, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659, 659]\n",
      "    Text: ' verification . . . . . . . . . . . . . . . . . . .'\n",
      "    ✓\n",
      "\n",
      "✓ Element 0 generates IDENTICAL tokens across all batch sizes\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "ref_0:\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=5    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00  0.00e+00\n",
      "\n",
      "ref_1:\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 \n",
      "bs=1    0.00e+00  0.00e+00  0.00e+00  0.00e+00  1.55e-01  0.00e+00  1.55e-01\n",
      "bs=2    0.00e+00  0.00e+00  0.00e+00  0.00e+00  1.55e-01  0.00e+00  1.55e-01\n",
      "bs=3    0.00e+00  0.00e+00  0.00e+00  0.00e+00  1.55e-01  0.00e+00  1.55e-01\n",
      "bs=4    0.00e+00  0.00e+00  0.00e+00  0.00e+00  1.55e-01  0.00e+00  1.55e-01\n",
      "bs=5    1.55e-01  1.55e-01  1.55e-01  1.55e-01  0.00e+00  1.55e-01  0.00e+00\n",
      "bs=8    0.00e+00  0.00e+00  0.00e+00  0.00e+00  1.55e-01  0.00e+00  1.55e-01\n",
      "bs=9    1.55e-01  1.55e-01  1.55e-01  1.55e-01  0.00e+00  1.55e-01  0.00e+00\n",
      "\n",
      "ref_2:\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 \n",
      "bs=1    0.00e+00  0.00e+00  1.31e-01  1.31e-01  1.31e-01  0.00e+00  0.00e+00\n",
      "bs=2    0.00e+00  0.00e+00  1.31e-01  1.31e-01  1.31e-01  0.00e+00  0.00e+00\n",
      "bs=3    1.31e-01  1.31e-01  0.00e+00  0.00e+00  0.00e+00  1.31e-01  1.31e-01\n",
      "bs=4    1.31e-01  1.31e-01  0.00e+00  0.00e+00  0.00e+00  1.31e-01  1.31e-01\n",
      "bs=5    1.31e-01  1.31e-01  0.00e+00  0.00e+00  0.00e+00  1.31e-01  1.31e-01\n",
      "bs=8    0.00e+00  0.00e+00  1.31e-01  1.31e-01  1.31e-01  0.00e+00  0.00e+00\n",
      "bs=9    0.00e+00  0.00e+00  1.31e-01  1.31e-01  1.31e-01  0.00e+00  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 \n",
      "bs=1    0.00e+00  0.00e+00  4.36e-02  4.36e-02  9.54e-02  0.00e+00  5.18e-02\n",
      "bs=2    0.00e+00  0.00e+00  4.36e-02  4.36e-02  9.54e-02  0.00e+00  5.18e-02\n",
      "bs=3    4.36e-02  4.36e-02  0.00e+00  0.00e+00  5.18e-02  4.36e-02  9.54e-02\n",
      "bs=4    4.36e-02  4.36e-02  0.00e+00  0.00e+00  5.18e-02  4.36e-02  9.54e-02\n",
      "bs=5    9.54e-02  9.54e-02  5.18e-02  5.18e-02  0.00e+00  9.54e-02  4.36e-02\n",
      "bs=8    0.00e+00  0.00e+00  4.36e-02  4.36e-02  9.54e-02  0.00e+00  5.18e-02\n",
      "bs=9    5.18e-02  5.18e-02  9.54e-02  9.54e-02  4.36e-02  5.18e-02  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Mean: 4.96e-02\n",
      "  Range: [0.00e+00, 9.54e-02]\n",
      "\n",
      "⚠ NOTE: 4/21 comparisons are equivalent (< 1e-09)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1, 2, 8]\n",
      "  Class 2: [3, 4]\n",
      "  Class 3: [5]\n",
      "  Class 4: [9]\n",
      "\n",
      "Equivalent pairs (will be excluded from cross-hardware signal):\n",
      "  (1, 2)\n",
      "  (1, 8)\n",
      "  (2, 8)\n",
      "  (3, 4)\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE BATCH SIZE EFFECTS (DECODE)\n",
      "================================================================================\n",
      "\n",
      "ref_0:\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 \n",
      "bs=1    0.00e+00  1.08e-01  9.34e-02  9.34e-02  9.34e-02  9.34e-02  3.81e-02\n",
      "bs=2    1.08e-01  0.00e+00  6.88e-02  6.88e-02  6.88e-02  6.88e-02  1.26e-01\n",
      "bs=3    9.34e-02  6.88e-02  0.00e+00  0.00e+00  0.00e+00  0.00e+00  1.06e-01\n",
      "bs=4    9.34e-02  6.88e-02  0.00e+00  0.00e+00  0.00e+00  0.00e+00  1.06e-01\n",
      "bs=5    9.34e-02  6.88e-02  0.00e+00  0.00e+00  0.00e+00  0.00e+00  1.06e-01\n",
      "bs=8    9.34e-02  6.88e-02  0.00e+00  0.00e+00  0.00e+00  0.00e+00  1.06e-01\n",
      "bs=9    3.81e-02  1.26e-01  1.06e-01  1.06e-01  1.06e-01  1.06e-01  0.00e+00\n",
      "\n",
      "ref_1:\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 \n",
      "bs=1    0.00e+00  1.74e-01  1.05e-01  1.05e-01  7.98e-02  1.05e-01  1.18e-01\n",
      "bs=2    1.74e-01  0.00e+00  1.60e-01  1.60e-01  1.79e-01  1.60e-01  1.42e-01\n",
      "bs=3    1.05e-01  1.60e-01  0.00e+00  0.00e+00  1.58e-01  0.00e+00  1.75e-01\n",
      "bs=4    1.05e-01  1.60e-01  0.00e+00  0.00e+00  1.58e-01  0.00e+00  1.75e-01\n",
      "bs=5    7.98e-02  1.79e-01  1.58e-01  1.58e-01  0.00e+00  1.58e-01  1.19e-01\n",
      "bs=8    1.05e-01  1.60e-01  0.00e+00  0.00e+00  1.58e-01  0.00e+00  1.75e-01\n",
      "bs=9    1.18e-01  1.42e-01  1.75e-01  1.75e-01  1.19e-01  1.75e-01  0.00e+00\n",
      "\n",
      "ref_2:\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 \n",
      "bs=1    0.00e+00  1.14e-01  1.14e-01  9.27e-02  9.27e-02  9.76e-02  1.51e-01\n",
      "bs=2    1.14e-01  0.00e+00  8.83e-02  2.95e-02  2.95e-02  7.60e-02  6.74e-02\n",
      "bs=3    1.14e-01  8.83e-02  0.00e+00  6.25e-02  6.25e-02  9.27e-02  1.01e-01\n",
      "bs=4    9.27e-02  2.95e-02  6.25e-02  0.00e+00  0.00e+00  7.60e-02  6.74e-02\n",
      "bs=5    9.27e-02  2.95e-02  6.25e-02  0.00e+00  0.00e+00  7.60e-02  6.74e-02\n",
      "bs=8    9.76e-02  7.60e-02  9.27e-02  7.60e-02  7.60e-02  0.00e+00  1.18e-01\n",
      "bs=9    1.51e-01  6.74e-02  1.01e-01  6.74e-02  6.74e-02  1.18e-01  0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "       bs=  1 bs=  2 bs=  3 bs=  4 bs=  5 bs=  8 bs=  9 \n",
      "bs=1    0.00e+00  1.32e-01  1.04e-01  9.69e-02  8.86e-02  9.85e-02  1.02e-01\n",
      "bs=2    1.32e-01  0.00e+00  1.06e-01  8.60e-02  9.25e-02  1.02e-01  1.12e-01\n",
      "bs=3    1.04e-01  1.06e-01  0.00e+00  2.08e-02  7.35e-02  3.09e-02  1.28e-01\n",
      "bs=4    9.69e-02  8.60e-02  2.08e-02  0.00e+00  5.27e-02  2.53e-02  1.16e-01\n",
      "bs=5    8.86e-02  9.25e-02  7.35e-02  5.27e-02  0.00e+00  7.80e-02  9.77e-02\n",
      "bs=8    9.85e-02  1.02e-01  3.09e-02  2.53e-02  7.80e-02  0.00e+00  1.33e-01\n",
      "bs=9    1.02e-01  1.12e-01  1.28e-01  1.16e-01  9.77e-02  1.33e-01  0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Mean: 8.93e-02\n",
      "  Range: [2.08e-02, 1.33e-01]\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: [1]\n",
      "  Class 2: [2]\n",
      "  Class 3: [3]\n",
      "  Class 4: [4]\n",
      "  Class 5: [5]\n",
      "  Class 6: [8]\n",
      "  Class 7: [9]\n",
      "\n",
      "Next step: Copy /workspace/experiments/vllm_decode_20251126_001933.json to verifier machine\n",
      "Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\n",
      "File size: 0.5 MB\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1126 00:19:45.892832121 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "vLLM Cross-Hardware Batch Size Detectability Experiment\n",
    "\n",
    "Tests whether batch size claims can be verified across different GPU architectures\n",
    "using logprob forensics with vLLM inference engine.\n",
    "\n",
    "Unlike Transformers, vLLM doesn't expose key vectors, so we rely solely on logprobs.\n",
    "vLLM may have finer-grained kernel selection than Transformers' 4 classes.\n",
    "\n",
    "Workflow:\n",
    "\n",
    "Run on Machine A (e.g., A100) with TEACHER_FORCING = False\n",
    "→ Generates tokens, extracts logprobs, saves to JSON\n",
    "\n",
    "Copy JSON to Machine B (e.g., H100)\n",
    "\n",
    "Run on Machine B with TEACHER_FORCING = True\n",
    "→ Teacher-forces A's tokens via prompt_logprobs, compares\n",
    "\n",
    "Usage:\n",
    "\n",
    "Machine A: python vllm_batch_detect.py\n",
    "Machine B: Edit TEACHER_FORCING=True, REFERENCE_FILE, then run\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = False\n",
    "REFERENCE_FILE = \"/workspace/experiments/vllm_reference.json\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "BATCH_SIZES = [1, 2, 3, 4, 5, 8, 9]\n",
    "MAX_NEW_TOKENS = 20\n",
    "TOKENS_PER_SLICE = 200\n",
    "NUM_REFERENCES = 3\n",
    "TOP_K_LOGPROBS = 20  # Store top-20 to ensure overlap for comparison\n",
    "\n",
    "# Threshold for considering two batch sizes \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Will be initialized from PDF in main()\n",
    "REFERENCE_SEQUENCES = None\n",
    "DUMMY_SETS = None\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    \"\"\"Setup logging to file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"vllm_experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    \"\"\"Print to both console and log file.\"\"\"\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    \"\"\"Close log file.\"\"\"\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    \"\"\"Load text content from a PDF file.\"\"\"\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_sequences_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load all PDFs and split into equal-length slices.\n",
    "    Returns REFERENCE_SEQUENCES and DUMMY_SETS dictionaries.\n",
    "    \"\"\"\n",
    "    pdf_files = glob.glob(\"/workspace/*.pdf\")\n",
    "    if not pdf_files:\n",
    "        pdf_files = glob.glob(\"*.pdf\")\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "\n",
    "    all_tokens = []\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        tokens = tokenizer.encode(text)\n",
    "        all_tokens.extend(tokens)\n",
    "        log_print(f\"    → {len(tokens)} tokens\")\n",
    "\n",
    "    log_print(f\"Total tokens: {len(all_tokens)}\")\n",
    "\n",
    "    max_batch_size = max(BATCH_SIZES)\n",
    "    slices_needed = num_references * max_batch_size\n",
    "    tokens_needed = slices_needed * TOKENS_PER_SLICE\n",
    "\n",
    "    if len(all_tokens) < tokens_needed:\n",
    "        raise ValueError(f\"Need {tokens_needed} tokens but only have {len(all_tokens)}\")\n",
    "\n",
    "    log_print(f\"Creating {slices_needed} slices of {TOKENS_PER_SLICE} tokens each\")\n",
    "\n",
    "    slices = []\n",
    "    for i in range(slices_needed):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        slice_tokens = all_tokens[start:end]\n",
    "        slice_text = tokenizer.decode(slice_tokens)\n",
    "        slices.append(slice_text)\n",
    "\n",
    "    reference_sequences = {}\n",
    "    dummy_sets = {}\n",
    "\n",
    "    for ref_idx in range(num_references):\n",
    "        ref_name = f\"ref_{ref_idx}\"\n",
    "        base_idx = ref_idx * max_batch_size\n",
    "        reference_sequences[ref_name] = slices[base_idx]\n",
    "        dummy_sets[ref_name] = slices[base_idx + 1 : base_idx + max_batch_size]\n",
    "\n",
    "    return reference_sequences, dummy_sets\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import torch\n",
    "    import transformers\n",
    "\n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        import vllm\n",
    "        info[\"vllm_version\"] = vllm.__version__\n",
    "    except (ImportError, AttributeError):\n",
    "        info[\"vllm_version\"] = \"unknown\"\n",
    "\n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    critical_fields = ['vllm_version', 'torch_version', 'cuda_version']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "\n",
    "    mismatches = []\n",
    "\n",
    "    log_print(\"\\nCritical dependencies:\")\n",
    "    for field in critical_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not mismatches:\n",
    "        log_print(\"\\n✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    else:\n",
    "        log_print(\"\\n⚠ ENVIRONMENT MISMATCHES DETECTED\")\n",
    "        log_print(\"  Results may be affected by software differences, not just hardware.\")\n",
    "        return {'valid': False, 'mismatches': mismatches}\n",
    "\n",
    "# ============================================================================\n",
    "# LOGPROB EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_logprobs_from_output(output, positions=[-3, -2, -1]):\n",
    "    \"\"\"\n",
    "    Extract logprobs from vLLM output at specified positions.\n",
    "    \"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    logprobs_list = output.outputs[0].logprobs\n",
    "    \n",
    "    if logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    num_generated = len(logprobs_list)\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else num_generated + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= num_generated:\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = logprobs_list[actual_idx]\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1]):\n",
    "    \"\"\"\n",
    "    Extract logprobs from prompt positions (for prefill analysis).\n",
    "    \"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "        \n",
    "        if token_logprobs is None:\n",
    "            continue\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION MODE\n",
    "# ============================================================================\n",
    "\n",
    "def compute_min_length_across_batches(ref_text, ref_name, tokenizer, batch_sizes):\n",
    "    \"\"\"Pre-compute minimum sequence length across all batch configurations.\"\"\"\n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    min_length = float('inf')\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        if batch_size == 1:\n",
    "            batch_texts = [ref_text]\n",
    "        else:\n",
    "            batch_texts = [ref_text] + ref_dummies[:batch_size-1]\n",
    "\n",
    "        token_lengths = [len(tokenizer.encode(t)) for t in batch_texts]\n",
    "        min_length = min(min_length, min(token_lengths))\n",
    "\n",
    "    return min_length\n",
    "\n",
    "def run_generation(llm, tokenizer, ref_text, ref_name, batch_size, forced_length=None):\n",
    "    \"\"\"\n",
    "    Run generation with specified batch size and extract signals.\n",
    "    \"\"\"\n",
    "    ref_dummies = DUMMY_SETS[ref_name]\n",
    "    \n",
    "    if batch_size == 1:\n",
    "        batch_texts = [ref_text]\n",
    "    else:\n",
    "        batch_texts = [ref_text] + ref_dummies[:batch_size-1]\n",
    "    \n",
    "    all_token_ids = [tokenizer.encode(t) for t in batch_texts]\n",
    "    \n",
    "    if forced_length is not None:\n",
    "        min_length = forced_length\n",
    "    else:\n",
    "        min_length = min(len(ids) for ids in all_token_ids)\n",
    "    \n",
    "    truncated_token_ids = [ids[:min_length] for ids in all_token_ids]\n",
    "    truncated_texts = [tokenizer.decode(ids) for ids in truncated_token_ids]\n",
    "    \n",
    "    prompt_length = len(truncated_token_ids[0])\n",
    "    log_print(f\"      Prompt: {prompt_length} tokens\", end=\"\")\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=0.0,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate(truncated_texts, sampling_params)\n",
    "    \n",
    "    output_0 = outputs[0]\n",
    "    generated_ids = list(output_0.outputs[0].token_ids)\n",
    "    num_generated = len(generated_ids)\n",
    "    \n",
    "    prefill_signals = extract_prompt_logprobs(output_0, prompt_length, positions=[-3, -2, -1])\n",
    "    decode_signals = extract_logprobs_from_output(output_0, positions=[-3, -2, -1])\n",
    "    \n",
    "    all_batch_generated_ids = [list(out.outputs[0].token_ids) for out in outputs]\n",
    "    \n",
    "    log_print(f\" → Final: {prompt_length + num_generated} tokens ({num_generated} generated)\")\n",
    "    \n",
    "    return {\n",
    "        'generated_ids': generated_ids,\n",
    "        'all_batch_generated_ids': all_batch_generated_ids,\n",
    "        'prompt_token_ids': truncated_token_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION MODE (TEACHER FORCING)\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_verification(llm, tokenizer, ref_name, reference_data,\n",
    "                                     verify_batch_size, is_diagonal):\n",
    "    ref_prompt_ids = reference_data['prompt_token_ids'][0]\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    ref_batch_size = len(reference_data['prompt_token_ids'])\n",
    "    \n",
    "    # Only pass PROMPT, not full sequence\n",
    "    prompt_text = tokenizer.decode(ref_prompt_ids)\n",
    "    prompt_length = len(ref_prompt_ids)\n",
    "    num_generated = len(ref_generated_ids)\n",
    "    \n",
    "    log_print(f\"      Prompt: {prompt_length}, Gen: {num_generated}\", end=\"\")\n",
    "    \n",
    "    if is_diagonal:\n",
    "        log_print(f\", exact neighbors (bs={ref_batch_size})\", end=\"\")\n",
    "        batch_texts = []\n",
    "        for i in range(ref_batch_size):\n",
    "            batch_texts.append(tokenizer.decode(reference_data['prompt_token_ids'][i]))\n",
    "        actual_batch_size = ref_batch_size\n",
    "    else:\n",
    "        log_print(f\", arb neighbors (bs={verify_batch_size})\", end=\"\")\n",
    "        batch_texts = [prompt_text]\n",
    "        \n",
    "        ref_dummies = DUMMY_SETS[ref_name]\n",
    "        for i in range(verify_batch_size - 1):\n",
    "            dummy_ids = tokenizer.encode(ref_dummies[i])[:prompt_length]\n",
    "            batch_texts.append(tokenizer.decode(dummy_ids))\n",
    "        \n",
    "        actual_batch_size = verify_batch_size\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=num_generated,  # Generate same number of tokens\n",
    "        temperature=0.0,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,  # Prefill signals\n",
    "        logprobs=TOP_K_LOGPROBS,          # Decode signals  <-- ADD THIS\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate(batch_texts, sampling_params)\n",
    "    output_0 = outputs[0]\n",
    "    \n",
    "    # Prefill signals (unchanged)\n",
    "    prefill_signals = extract_prompt_logprobs(output_0, prompt_length, positions=[-3, -2, -1])\n",
    "    \n",
    "    # Decode signals - from GENERATED tokens, not prompt\n",
    "    decode_logprobs = output_0.outputs[0].logprobs  # List[Dict[token_id, Logprob]]\n",
    "    \n",
    "    # Check token match\n",
    "    generated_ids = [list(lp.keys())[list(lp.values()).index(max(lp.values(), key=lambda x: x.logprob))] \n",
    "                     for lp in decode_logprobs]\n",
    "    # Simpler: just get the sampled token\n",
    "    generated_ids = [output_0.outputs[0].token_ids[i] for i in range(len(decode_logprobs))]\n",
    "    \n",
    "    if generated_ids != ref_generated_ids:\n",
    "        log_print(f\" WARN: token mismatch!\", end=\"\")\n",
    "    \n",
    "    # Extract last 3 decode positions\n",
    "    decode_signals = {}\n",
    "    if len(decode_logprobs) >= 3:\n",
    "        for i, pos in enumerate([-3, -2, -1]):\n",
    "            lp_dict = decode_logprobs[pos]\n",
    "            token_ids = list(lp_dict.keys())\n",
    "            log_probs = [lp_dict[tid].logprob for tid in token_ids]\n",
    "            decode_signals[f'pos_{pos}'] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    log_print(f\" → decoded\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_logprob_distance_canonical(logprobs1, logprobs2, canonical_ids):\n",
    "    \"\"\"\n",
    "    Compute L2 distance between logprobs for a canonical set of token IDs.\n",
    "    \"\"\"\n",
    "    lp1 = dict(zip(logprobs1['token_ids'], logprobs1['log_probs']))\n",
    "    lp2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "\n",
    "    for tid in canonical_ids:\n",
    "        if tid in lp1 and tid in lp2:\n",
    "            vec1.append(lp1[tid])\n",
    "            vec2.append(lp2[tid])\n",
    "\n",
    "    if len(vec1) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2):\n",
    "    \"\"\"Compare two signal sets using top 5 token IDs from first signal as canonical.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "\n",
    "    all_dists = []\n",
    "\n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]\n",
    "\n",
    "        # Use top 5 for comparison (stored top 20 as buffer)\n",
    "        canonical_ids = sig1['logprobs']['token_ids'][:5]\n",
    "        dist = compute_logprob_distance_canonical(\n",
    "            sig1['logprobs'], sig2['logprobs'], canonical_ids\n",
    "        )\n",
    "        all_dists.append(dist)\n",
    "\n",
    "    return {\n",
    "        'logprobs_max': max(all_dists) if all_dists else 0.0,\n",
    "        'logprobs_mean': np.mean(all_dists) if all_dists else 0.0\n",
    "    }\n",
    "\n",
    "def compare_signals_with_canonical_ids(signals1, signals2, canonical_token_ids):\n",
    "    \"\"\"Compare using pre-specified canonical token IDs (top 5).\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "\n",
    "    all_dists = []\n",
    "\n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]\n",
    "\n",
    "        # Use top 5 for comparison\n",
    "        canonical_ids = canonical_token_ids.get(pos_label, sig1['logprobs']['token_ids'][:5])[:5]\n",
    "        dist = compute_logprob_distance_canonical(\n",
    "            sig1['logprobs'], sig2['logprobs'], canonical_ids\n",
    "        )\n",
    "        all_dists.append(dist)\n",
    "\n",
    "    return {\n",
    "        'logprobs_max': max(all_dists) if all_dists else 0.0,\n",
    "        'logprobs_mean': np.mean(all_dists) if all_dists else 0.0\n",
    "    }\n",
    "\n",
    "def check_token_consistency(measurements, tokenizer):\n",
    "    \"\"\"Verify element 0 generates identical tokens across all batch sizes.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    tokens_by_bs = {}\n",
    "    for bs, data in measurements.items():\n",
    "        tokens_by_bs[bs] = data['generated_ids']\n",
    "\n",
    "    bs_list = sorted(tokens_by_bs.keys())\n",
    "    reference_tokens = tokens_by_bs[bs_list[0]]\n",
    "\n",
    "    all_same = True\n",
    "    log_print(\"\\nGenerated tokens by batch size:\")\n",
    "    for bs in bs_list:\n",
    "        tokens = tokens_by_bs[bs]\n",
    "        match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "        decoded_text = tokenizer.decode(tokens)\n",
    "        log_print(f\"  bs={bs}:\")\n",
    "        log_print(f\"    IDs:  {tokens}\")\n",
    "        log_print(f\"    Text: {repr(decoded_text)}\")\n",
    "        log_print(f\"    {match_str}\")\n",
    "        if tokens != reference_tokens:\n",
    "            all_same = False\n",
    "\n",
    "    if all_same:\n",
    "        log_print(\"\\n✓ Element 0 generates IDENTICAL tokens across all batch sizes\")\n",
    "    else:\n",
    "        log_print(\"\\n⚠ Element 0 generates DIFFERENT tokens across batch sizes\")\n",
    "\n",
    "    return all_same\n",
    "\n",
    "def find_equivalent_pairs(matrix, batch_sizes, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"\n",
    "    Find pairs of batch sizes that produce equivalent results (same kernel).\n",
    "    Returns list of (bs1, bs2) tuples where bs1 < bs2.\n",
    "    \"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n_bs = len(batch_sizes)\n",
    "    \n",
    "    for i in range(n_bs):\n",
    "        for j in range(i + 1, n_bs):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((batch_sizes[i], batch_sizes[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, batch_sizes):\n",
    "    \"\"\"\n",
    "    Group batch sizes into kernel equivalence classes.\n",
    "    Returns list of sets, each set containing batch sizes using the same kernel.\n",
    "    \"\"\"\n",
    "    # Union-find style grouping\n",
    "    parent = {bs: bs for bs in batch_sizes}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        union(bs1, bs2)\n",
    "    \n",
    "    # Group by root\n",
    "    groups = {}\n",
    "    for bs in batch_sizes:\n",
    "        root = find(bs)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(bs)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "def analyze_within_hardware(measurements, batch_sizes, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware batch size effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE BATCH SIZE EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    by_ref = {}\n",
    "    for m in measurements:\n",
    "        ref = m['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "        by_ref[ref][m['batch_size']] = m[signals_key]\n",
    "\n",
    "    all_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{ref_name}:\")\n",
    "\n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix = np.zeros((n_bs, n_bs))\n",
    "\n",
    "        # Use first batch size as canonical\n",
    "        canonical_bs = batch_sizes[0]\n",
    "        canonical_signals = ref_data.get(canonical_bs)\n",
    "\n",
    "        canonical_token_ids = {}\n",
    "        if canonical_signals:\n",
    "            for pos_label, pos_data in canonical_signals.items():\n",
    "                canonical_token_ids[pos_label] = pos_data['logprobs']['token_ids']\n",
    "\n",
    "        for i, bs1 in enumerate(batch_sizes):\n",
    "            for j, bs2 in enumerate(batch_sizes):\n",
    "                if bs1 not in ref_data or bs2 not in ref_data:\n",
    "                    continue\n",
    "\n",
    "                if i == j:\n",
    "                    matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    distances = compare_signals_with_canonical_ids(\n",
    "                        ref_data[bs1], ref_data[bs2], canonical_token_ids\n",
    "                    )\n",
    "                    matrix[i, j] = distances['logprobs_mean']\n",
    "\n",
    "        # Display matrix\n",
    "        header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "        log_print(header)\n",
    "        for i, bs in enumerate(batch_sizes):\n",
    "            row_str = f\"bs={bs:<3}\"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix[i,j]:6.2e}\"\n",
    "            log_print(row_str)\n",
    "\n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    header = \"       \" + \"\".join([f\"bs={bs:>3} \" for bs in batch_sizes])\n",
    "    log_print(header)\n",
    "    for i, bs in enumerate(batch_sizes):\n",
    "        row_str = f\"bs={bs:<3}\"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_matrix[i,j]:6.2e}\"\n",
    "        log_print(row_str)\n",
    "\n",
    "    # Statistics\n",
    "    off_diag = avg_matrix[np.triu_indices(n_bs, k=1)]\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    log_print(f\"  Mean: {np.mean(off_diag):.2e}\")\n",
    "    log_print(f\"  Range: [{np.min(off_diag):.2e}, {np.max(off_diag):.2e}]\")\n",
    "    \n",
    "    # Find equivalent pairs\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_matrix, batch_sizes)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, batch_sizes)\n",
    "    \n",
    "    # Check if all zeros\n",
    "    zero_count = np.sum(off_diag < EQUIVALENCE_THRESHOLD)\n",
    "    total_count = len(off_diag)\n",
    "    \n",
    "    if zero_count == total_count:\n",
    "        log_print(f\"\\n⚠ WARNING: {zero_count}/{total_count} comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All batch sizes produce identical results (single kernel class)\")\n",
    "    elif zero_count > 0:\n",
    "        log_print(f\"\\n⚠ NOTE: {zero_count}/{total_count} comparisons are equivalent (< {EQUIVALENCE_THRESHOLD})\")\n",
    "    \n",
    "    # Display kernel classes\n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs (will be excluded from cross-hardware signal):\")\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2})\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'off_diagonal_mean': float(np.mean(off_diag)),\n",
    "        'off_diagonal_range': [float(np.min(off_diag)), float(np.max(off_diag))],\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes]\n",
    "    }\n",
    "\n",
    "def analyze_cross_hardware(comparison_results, batch_sizes, signal_source='decode', \n",
    "                           equivalent_pairs=None):\n",
    "    \"\"\"\n",
    "    Analyze the comparison matrix and determine detectability.\n",
    "    Excludes equivalent pairs from SNR signal calculation.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE BATCH SIZE DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    # Convert to set of both orderings for easy lookup\n",
    "    equiv_set = set()\n",
    "    for bs1, bs2 in equivalent_pairs:\n",
    "        equiv_set.add((bs1, bs2))\n",
    "        equiv_set.add((bs2, bs1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_name']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_batch_size'], result['verify_batch_size'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_matrices = []\n",
    "    n_bs = len(batch_sizes)\n",
    "\n",
    "    # Per-reference matrices\n",
    "    for ref_name in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n{ref_name}:\")\n",
    "        \n",
    "        ref_data = by_ref[ref_name]\n",
    "        matrix = np.zeros((n_bs, n_bs))\n",
    "        \n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            for j, verify_bs in enumerate(batch_sizes):\n",
    "                key = (claimed_bs, verify_bs)\n",
    "                if key in ref_data:\n",
    "                    matrix[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        # Display matrix\n",
    "        header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "        log_print(header)\n",
    "        for i, claimed_bs in enumerate(batch_sizes):\n",
    "            row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "            for j in range(n_bs):\n",
    "                row_str += f\"  {matrix[i,j]:8.2e}\"\n",
    "            log_print(row_str)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate matrix\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \"\".join([f\"v_bs={bs:>3} \" for bs in batch_sizes])\n",
    "    log_print(header)\n",
    "    for i, claimed_bs in enumerate(batch_sizes):\n",
    "        row_str = f\"c_bs={claimed_bs:<3} \"\n",
    "        for j in range(n_bs):\n",
    "            row_str += f\"  {avg_matrix[i,j]:8.2e}\"\n",
    "        log_print(row_str)\n",
    "    \n",
    "    # Compute statistics\n",
    "    diagonal = [avg_matrix[i, i] for i in range(n_bs)]\n",
    "    \n",
    "    # Off-diagonal: exclude equivalent pairs\n",
    "    off_diagonal_all = []\n",
    "    off_diagonal_meaningful = []\n",
    "    excluded_pairs = []\n",
    "    \n",
    "    for i, bs1 in enumerate(batch_sizes):\n",
    "        for j, bs2 in enumerate(batch_sizes):\n",
    "            if i != j:\n",
    "                off_diagonal_all.append(avg_matrix[i, j])\n",
    "                if (bs1, bs2) in equiv_set:\n",
    "                    excluded_pairs.append((bs1, bs2))\n",
    "                else:\n",
    "                    off_diagonal_meaningful.append(avg_matrix[i, j])\n",
    "    \n",
    "    baseline_mean = np.mean(diagonal)\n",
    "    signal_all_mean = np.mean(off_diagonal_all) if off_diagonal_all else 0.0\n",
    "    signal_meaningful_mean = np.mean(off_diagonal_meaningful) if off_diagonal_meaningful else 0.0\n",
    "    \n",
    "    snr_all = signal_all_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    snr_meaningful = signal_meaningful_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same batch size):\")\n",
    "    log_print(f\"  Mean: {baseline_mean:.2e}\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_all)}\")\n",
    "    log_print(f\"  Mean: {signal_all_mean:.2e}\")\n",
    "    log_print(f\"  SNR (all): {snr_all:.2f}×\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        for bs1, bs2 in equivalent_pairs:\n",
    "            log_print(f\"  ({bs1}, {bs2}) and ({bs2}, {bs1})\")\n",
    "        log_print(f\"  Total excluded: {len(excluded_pairs)} cells\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_meaningful)}\")\n",
    "    if off_diagonal_meaningful:\n",
    "        log_print(f\"  Mean: {signal_meaningful_mean:.2e}\")\n",
    "        log_print(f\"  SNR (meaningful): {snr_meaningful:.2f}×\")\n",
    "    else:\n",
    "        log_print(\"  No meaningful pairs (all batch sizes are equivalent)\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'baseline_mean': float(baseline_mean),\n",
    "        'signal_all_mean': float(signal_all_mean),\n",
    "        'signal_meaningful_mean': float(signal_meaningful_mean),\n",
    "        'snr_all': float(snr_all),\n",
    "        'snr_meaningful': float(snr_meaningful),\n",
    "        'excluded_pairs': equivalent_pairs,\n",
    "        'n_excluded_cells': len(excluded_pairs),\n",
    "        'n_meaningful_pairs': len(off_diagonal_meaningful)\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    global REFERENCE_SEQUENCES, DUMMY_SETS\n",
    "\n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "\n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION (decode)\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"vLLM CROSS-HARDWARE BATCH SIZE DETECTABILITY - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"vLLM: {system_info['vllm_version']}\")\n",
    "    log_print(f\"PyTorch: {system_info['torch_version']}\")\n",
    "    log_print(f\"CUDA: {system_info['cuda_version']}\")\n",
    "\n",
    "    log_print(f\"\\nConfiguration:\")\n",
    "    log_print(f\"  Model: {MODEL_NAME}\")\n",
    "    log_print(f\"  Batch sizes: {BATCH_SIZES}\")\n",
    "    log_print(f\"  Max tokens: {MAX_NEW_TOKENS}\")\n",
    "    log_print(f\"  Top-k logprobs: {TOP_K_LOGPROBS}\")\n",
    "    if TEACHER_FORCING:\n",
    "        log_print(f\"  Reference file: {REFERENCE_FILE}\")\n",
    "    log_print()\n",
    "\n",
    "    # Initialize vLLM\n",
    "    log_print(\"Loading vLLM model...\")\n",
    "    llm = LLM(\n",
    "        model=MODEL_NAME,\n",
    "        download_dir=CACHE_DIR,\n",
    "        dtype=\"bfloat16\",\n",
    "        trust_remote_code=True,\n",
    "        gpu_memory_utilization=0.7,\n",
    "    )\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    log_print(\"✓ Model loaded\\n\")\n",
    "\n",
    "    # Initialize sequences from PDF\n",
    "    REFERENCE_SEQUENCES, DUMMY_SETS = create_sequences_from_pdf(tokenizer)\n",
    "    log_print(f\"Created {len(REFERENCE_SEQUENCES)} reference sequences\\n\")\n",
    "\n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if TEACHER_FORCING:\n",
    "        # ================================================================\n",
    "        # VERIFICATION MODE\n",
    "        # ================================================================\n",
    "        log_print(\"Loading reference file...\")\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            reference = json.load(f)\n",
    "\n",
    "        ref_env = reference['metadata']['environment']\n",
    "        ref_gpu = ref_env['gpu_name']\n",
    "        log_print(f\"Reference GPU: {ref_gpu}\")\n",
    "        log_print(f\"Verifier GPU:  {system_info['gpu_name']}\")\n",
    "\n",
    "        env_validation = validate_environment_match(ref_env, system_info)\n",
    "\n",
    "        ref_batch_sizes = reference['metadata']['batch_sizes']\n",
    "        if ref_batch_sizes != BATCH_SIZES:\n",
    "            log_print(f\"\\n✗ BATCH SIZE MISMATCH\")\n",
    "            sys.exit(1)\n",
    "\n",
    "        log_print(\"\\n✓ Configuration matches\\n\")\n",
    "        \n",
    "        # Load equivalent pairs from reference\n",
    "        prefill_equiv_pairs = reference.get('prefill_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        decode_equiv_pairs = reference.get('decode_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        \n",
    "        # Convert to tuples if stored as lists\n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_equiv_pairs]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_equiv_pairs]\n",
    "        \n",
    "        log_print(f\"Loaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "\n",
    "        comparison_results = []\n",
    "\n",
    "        ref_by_key = {}\n",
    "        for m in reference['measurements']:\n",
    "            key = (m['ref_name'], m['batch_size'])\n",
    "            ref_by_key[key] = m\n",
    "\n",
    "        for ref_name in sorted(REFERENCE_SEQUENCES.keys()):\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "\n",
    "            for claimed_bs in BATCH_SIZES:\n",
    "                ref_key = (ref_name, claimed_bs)\n",
    "                if ref_key not in ref_by_key:\n",
    "                    log_print(f\"  ⚠ No reference data for {ref_name} bs={claimed_bs}\")\n",
    "                    continue\n",
    "\n",
    "                ref_data = ref_by_key[ref_key]\n",
    "                log_print(f\"\\n  Claimed batch size: {claimed_bs}\")\n",
    "\n",
    "                for verify_bs in BATCH_SIZES:\n",
    "                    is_diagonal = (claimed_bs == verify_bs)\n",
    "\n",
    "                    log_print(f\"    Verify bs={verify_bs} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "\n",
    "                    verify_result = run_teacher_forced_verification(\n",
    "                        llm, tokenizer, ref_name, ref_data,\n",
    "                        verify_bs, is_diagonal\n",
    "                    )\n",
    "\n",
    "                    prefill_distances = compare_signals(\n",
    "                        ref_data['prefill_signals'],\n",
    "                        verify_result['prefill_signals']\n",
    "                    )\n",
    "\n",
    "                    decode_distances = compare_signals(\n",
    "                        ref_data['decode_signals'],\n",
    "                        verify_result['decode_signals']\n",
    "                    )\n",
    "\n",
    "                    log_print(f\"      Prefill: {prefill_distances['logprobs_mean']:.2e}, Decode: {decode_distances['logprobs_mean']:.2e}\")\n",
    "\n",
    "                    comparison_results.append({\n",
    "                        'ref_name': ref_name,\n",
    "                        'claimed_batch_size': claimed_bs,\n",
    "                        'verify_batch_size': verify_bs,\n",
    "                        'is_diagonal': is_diagonal,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances,\n",
    "                        'verify_prefill_signals': verify_result['prefill_signals'],\n",
    "                        'verify_decode_signals': verify_result['decode_signals']\n",
    "                    })\n",
    "\n",
    "        # Analyze with equivalent pair exclusion\n",
    "        prefill_analysis = analyze_cross_hardware(\n",
    "            comparison_results, BATCH_SIZES, signal_source='prefill',\n",
    "            equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware(\n",
    "            comparison_results, BATCH_SIZES, signal_source='decode',\n",
    "            equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill SNR (meaningful): {prefill_analysis['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode SNR (meaningful):  {decode_analysis['snr_meaningful']:.2f}×\")\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_gpu': ref_gpu,\n",
    "                'verifier_gpu': system_info['gpu_name'],\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'environment_validation': env_validation,\n",
    "                'model': MODEL_NAME,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'timestamp': timestamp,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "\n",
    "        filepath = os.path.join(output_dir, f\"vllm_verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "\n",
    "    else:\n",
    "        # ================================================================\n",
    "        # GENERATION MODE\n",
    "        # ================================================================\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'model': MODEL_NAME,\n",
    "                'batch_sizes': BATCH_SIZES,\n",
    "                'max_new_tokens': MAX_NEW_TOKENS,\n",
    "                'top_k_logprobs': TOP_K_LOGPROBS,\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'measurements': []\n",
    "        }\n",
    "\n",
    "        for ref_name, ref_text in REFERENCE_SEQUENCES.items():\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"REFERENCE: {ref_name}\")\n",
    "            log_print(\"=\"*80)\n",
    "\n",
    "            min_prompt_length = compute_min_length_across_batches(\n",
    "                ref_text, ref_name, tokenizer, BATCH_SIZES\n",
    "            )\n",
    "            log_print(f\"\\nGlobal minimum prompt length: {min_prompt_length} tokens\\n\")\n",
    "\n",
    "            for batch_size in BATCH_SIZES:\n",
    "                log_print(f\"  bs={batch_size}:\", end=\" \")\n",
    "\n",
    "                gen_data = run_generation(\n",
    "                    llm, tokenizer, ref_text, ref_name, batch_size,\n",
    "                    forced_length=min_prompt_length\n",
    "                )\n",
    "\n",
    "                results['measurements'].append({\n",
    "                    'ref_name': ref_name,\n",
    "                    'batch_size': batch_size,\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'all_batch_generated_ids': gen_data['all_batch_generated_ids'],\n",
    "                    'prompt_token_ids': gen_data['prompt_token_ids'],\n",
    "                    'prompt_length': gen_data['prompt_length'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals'],\n",
    "                    'num_generated': gen_data['num_generated']\n",
    "                })\n",
    "\n",
    "        filepath = os.path.join(output_dir, f\"vllm_decode_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Generation results saved to: {filepath}\")\n",
    "\n",
    "        # Token consistency check\n",
    "        for ref_name in REFERENCE_SEQUENCES.keys():\n",
    "            log_print(f\"\\n--- Token consistency for {ref_name} ---\")\n",
    "            ref_measurements = {m['batch_size']: m for m in results['measurements'] if m['ref_name'] == ref_name}\n",
    "            check_token_consistency(ref_measurements, tokenizer)\n",
    "\n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, signal_source='prefill'\n",
    "        )\n",
    "        decode_sanity = analyze_within_hardware(\n",
    "            results['measurements'], BATCH_SIZES, signal_source='decode'\n",
    "        )\n",
    "\n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\nNext step: Copy {filepath} to verifier machine\")\n",
    "        log_print(f\"Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\")\n",
    "\n",
    "        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        log_print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"EXPERIMENT COMPLETE\")\n",
    "    log_print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c767398-ecb2-4774-8242-c8a2cd04fd71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
