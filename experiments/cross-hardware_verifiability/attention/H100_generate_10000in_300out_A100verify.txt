================================================================================
ATTENTION IMPLEMENTATION CROSS-HARDWARE EXPERIMENT
================================================================================
GPU: NVIDIA A100-SXM4-80GB
Hostname: 2b0c722dfad5
Found 1 PDF(s)
  Loading: /workspace/Verification-for-International-AI-Governance.pdf
Total source tokens: 120215
Prompt structure: 33 prefix + 10000 snippet + 34 suffix = 10067 tokens

Created 6 prompts

================================================================================
VERIFICATION MODE
================================================================================
Loaded reference: H100_generate_10000in_300out.json

Environment comparison:
  Generator GPU: NVIDIA H100 NVL
  Verifier GPU:  NVIDIA A100-SXM4-80GB
  torch_version: 2.8.0+cu128 vs 2.8.0+cu128 ✓
  transformers_version: 4.57.3 vs 4.57.3 ✓
  cuda_version: 12.8 vs 12.8 ✓

--- Verifying with: eager ---
Loading model with attention: eager
  Claimed eager:
    ref_0: done
    ref_1: done
    ref_2: done
    ref_3: done
    ref_4: done
    ref_5: done
  Claimed sdpa:
    ref_0: done
    ref_1: done
    ref_2: done
    ref_3: done
    ref_4: done
    ref_5: done
  Claimed flash_attention_2:
    ref_0: done
    ref_1: done
    ref_2: done
    ref_3: done
    ref_4: done
    ref_5: done

--- Verifying with: sdpa ---
Loading model with attention: sdpa
  Claimed eager:
    ref_0: done
    ref_1: done
    ref_2: done
    ref_3: done
    ref_4: done
    ref_5: done
  Claimed sdpa:
    ref_0: done
    ref_1: done
    ref_2: done
    ref_3: done
    ref_4: done
    ref_5: done
  Claimed flash_attention_2:
    ref_0: done
    ref_1: done
    ref_2: done
    ref_3: done
    ref_4: done
    ref_5: done

--- Verifying with: flash_attention_2 ---
Loading model with attention: flash_attention_2
  Claimed eager:
    ref_0: done
    ref_1: done
    ref_2: done
    ref_3: done
    ref_4: done
    ref_5: done
  Claimed sdpa:
    ref_0: done
    ref_1: done
    ref_2: done
    ref_3: done
    ref_4: done
    ref_5: done
  Claimed flash_attention_2:
    ref_0: done
    ref_1: done
    ref_2: done
    ref_3: done
    ref_4: done
    ref_5: done

================================================================================
CROSS-HARDWARE VERIFICATION (PREFILL)
================================================================================

AGGREGATE (average across references):
  Rows = claimed attention, Cols = verified attention

Key Vectors (L2 distance):
                eager       sdpa flash_attention_2
     eager   8.51e-01   1.29e+01   1.30e+01
      sdpa   1.27e+01   7.23e-01   5.94e-01
flash_attention_2   1.27e+01   6.87e-01   8.34e-01

Logprobs (L2 distance):
                eager       sdpa flash_attention_2
     eager   7.48e-01   3.52e+00   3.58e+00
      sdpa   8.38e+00   2.46e-01   2.26e-01
flash_attention_2   8.37e+00   1.99e-01   1.79e-01

================================================================================
SNR ANALYSIS
================================================================================

Diagonal (baseline = cross-hardware, same attention):
  Key vectors: 8.03e-01
  Logprobs: 3.91e-01

Off-diagonal (different attention):
  Key vectors - Mean: 8.78e+00, SNR: 10.94×
  Logprobs - Mean: 4.05e+00, SNR: 10.34×

================================================================================
CROSS-HARDWARE VERIFICATION (DECODE)
================================================================================

AGGREGATE (average across references):
  Rows = claimed attention, Cols = verified attention

Key Vectors (L2 distance):
                eager       sdpa flash_attention_2
     eager   7.78e-01   4.48e+00   4.52e+00
      sdpa   2.37e+00   4.53e-01   4.64e-01
flash_attention_2   2.67e+00   4.93e-01   4.60e-01

Logprobs (L2 distance):
                eager       sdpa flash_attention_2
     eager   2.47e-01   1.47e+00   1.47e+00
      sdpa   9.69e-01   1.10e-01   1.11e-01
flash_attention_2   1.26e+00   1.19e-01   1.12e-01

================================================================================
SNR ANALYSIS
================================================================================

Diagonal (baseline = cross-hardware, same attention):
  Key vectors: 5.64e-01
  Logprobs: 1.56e-01

Off-diagonal (different attention):
  Key vectors - Mean: 2.50e+00, SNR: 4.43×
  Logprobs - Mean: 9.00e-01, SNR: 5.76×

Saved to: /workspace/experiments/attn_verify_20251126_200139.json
