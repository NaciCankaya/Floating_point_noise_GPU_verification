{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea310c-d17d-4c07-b23f-5f47130fdfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ATTENTION IMPLEMENTATION CROSS-HARDWARE EXPERIMENT\n",
      "================================================================================\n",
      "GPU: NVIDIA H100 NVL\n",
      "Hostname: d013d4f0b0a1\n",
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total source tokens: 120215\n",
      "Prompt structure: 33 prefix + 10000 snippet + 34 suffix = 10067 tokens\n",
      "\n",
      "Created 6 prompts\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION MODE\n",
      "================================================================================\n",
      "Loaded reference: A100_generate_10000in_300out.json\n",
      "\n",
      "Environment comparison:\n",
      "  Generator GPU: NVIDIA A100-SXM4-80GB\n",
      "  Verifier GPU:  NVIDIA H100 NVL\n",
      "  torch_version: 2.8.0+cu128 vs 2.8.0+cu128 ✓\n",
      "  transformers_version: 4.57.3 vs 4.57.3 ✓\n",
      "  cuda_version: 12.8 vs 12.8 ✓\n",
      "\n",
      "--- Verifying with: eager ---\n",
      "Loading model with attention: eager\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ca470d29804bf99559d6c19ae5c900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Claimed eager:\n",
      "    ref_0: done\n",
      "    ref_1: done\n",
      "    ref_2: done\n",
      "    ref_3: done\n",
      "    ref_4: done\n",
      "    ref_5: done\n",
      "  Claimed sdpa:\n",
      "    ref_0: done\n",
      "    ref_1: done\n",
      "    ref_2: done\n",
      "    ref_3: done\n",
      "    ref_4: done\n",
      "    ref_5: done\n",
      "  Claimed flash_attention_2:\n",
      "    ref_0: done\n",
      "    ref_1: done\n",
      "    ref_2: done\n",
      "    ref_3: done\n",
      "    ref_4: done\n",
      "    ref_5: done\n",
      "\n",
      "--- Verifying with: sdpa ---\n",
      "Loading model with attention: sdpa\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce2d5fb3661a49f199e09ac0f900329c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Claimed eager:\n",
      "    ref_0: done\n",
      "    ref_1: done\n",
      "    ref_2: done\n",
      "    ref_3: done\n",
      "    ref_4: done\n",
      "    ref_5: done\n",
      "  Claimed sdpa:\n",
      "    ref_0: done\n",
      "    ref_1: done\n",
      "    ref_2: done\n",
      "    ref_3: done\n",
      "    ref_4: done\n",
      "    ref_5: done\n",
      "  Claimed flash_attention_2:\n",
      "    ref_0: done\n",
      "    ref_1: done\n",
      "    ref_2: done\n",
      "    ref_3: done\n",
      "    ref_4: done\n",
      "    ref_5: done\n",
      "\n",
      "--- Verifying with: flash_attention_2 ---\n",
      "Loading model with attention: flash_attention_2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13af9e20889c4110a21730c1f6adbd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Claimed eager:\n",
      "    ref_0: done\n",
      "    ref_1: done\n",
      "    ref_2: done\n",
      "    ref_3: done\n",
      "    ref_4: done\n",
      "    ref_5: done\n",
      "  Claimed sdpa:\n",
      "    ref_0: done\n",
      "    ref_1: done\n",
      "    ref_2: done\n",
      "    ref_3: done\n",
      "    ref_4: done\n",
      "    ref_5: done\n",
      "  Claimed flash_attention_2:\n",
      "    ref_0: done\n",
      "    ref_1: done\n",
      "    ref_2: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cross-hardware attention implementation detectability experiment.\n",
    "Compares eager, sdpa, and flash_attention_2.\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import socket\n",
    "import platform\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = True\n",
    "REFERENCE_FILE = \"A100_generate_10000in_300out.json\"\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "ATTN_IMPLEMENTATIONS = ['eager', 'sdpa', 'flash_attention_2']\n",
    "\n",
    "LAYER_INDICES = [-1]\n",
    "TOKENS_PER_SLICE = 10000\n",
    "MAX_NEW_TOKENS = 300\n",
    "NUM_REFERENCES = 6\n",
    "\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"attn_experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_prompts(tokenizer, num_references=NUM_REFERENCES):\n",
    "    pdf_files = sorted(glob.glob(\"/workspace/*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        pdf_files = sorted(glob.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    content_tokens = tokenizer.encode(all_text, add_special_tokens=False)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    if len(content_tokens) < num_references * TOKENS_PER_SLICE:\n",
    "        raise ValueError(f\"Need {num_references * TOKENS_PER_SLICE} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "    \n",
    "    suffix = f\"\"\"\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    prefix_tokens = tokenizer.encode(prefix, add_special_tokens=False)\n",
    "    suffix_tokens = tokenizer.encode(suffix, add_special_tokens=False)\n",
    "    \n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    prompts = []\n",
    "    for i in range(num_references):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        prompts.append(prompt)\n",
    "    \n",
    "    return prompts\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info(attn_impl):\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"attn_implementation\": attn_impl,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import flash_attn\n",
    "        info[\"flash_attn_version\"] = flash_attn.__version__\n",
    "    except ImportError:\n",
    "        info[\"flash_attn_version\"] = \"N/A\"\n",
    "        \n",
    "    return info\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_model(attn_impl):\n",
    "    log_print(f\"Loading model with attention: {attn_impl}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"cuda\",\n",
    "        attn_implementation=attn_impl,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    model.eval()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# SIGNAL EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_signals(outputs, layer_indices, position=-1):\n",
    "    \"\"\"Extract key vectors and top-k logprobs at a position.\"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        key_states = outputs.past_key_values[layer_idx][0]\n",
    "        key_at_pos = key_states[0, :, position, :].detach().float().cpu().numpy().flatten()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_at_pos.tolist()\n",
    "    \n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    top_values, top_indices = torch.topk(log_probs, k=100)\n",
    "    \n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': top_indices.cpu().tolist(),\n",
    "        'log_probs': top_values.cpu().tolist()\n",
    "    }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract signals from multiple positions during prefill.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        prefill_signals[pos_label] = extract_signals(outputs, layer_indices, position=pos)\n",
    "    return prefill_signals\n",
    "\n",
    "def extract_signals_for_token_ids(outputs, layer_indices, token_ids, position=-1):\n",
    "    \"\"\"Extract signals for SPECIFIC token IDs (used in verification).\"\"\"\n",
    "    signals = {\n",
    "        'key_vectors': {},\n",
    "        'logprobs': {}\n",
    "    }\n",
    "\n",
    "    for layer_idx in layer_indices:\n",
    "        key_states = outputs.past_key_values[layer_idx][0]\n",
    "        key_at_pos = key_states[0, :, position, :].detach().float().cpu().numpy().flatten()\n",
    "        signals['key_vectors'][f'layer_{layer_idx}'] = key_at_pos.tolist()\n",
    "\n",
    "    logits = outputs.logits[0, position, :]\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    token_ids_tensor = torch.tensor(token_ids, device=logits.device)\n",
    "    selected_logprobs = log_probs[token_ids_tensor]\n",
    "\n",
    "    signals['logprobs'] = {\n",
    "        'token_ids': token_ids,\n",
    "        'log_probs': selected_logprobs.cpu().tolist()\n",
    "    }\n",
    "\n",
    "    return signals\n",
    "\n",
    "def extract_prefill_signals_for_token_ids(outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract prefill signals using reference token IDs.\"\"\"\n",
    "    prefill_signals = {}\n",
    "    for pos in positions:\n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        if pos_label in ref_prefill_signals:\n",
    "            ref_token_ids = ref_prefill_signals[pos_label]['logprobs']['token_ids']\n",
    "            prefill_signals[pos_label] = extract_signals_for_token_ids(\n",
    "                outputs, layer_indices, ref_token_ids, position=pos\n",
    "            )\n",
    "    return prefill_signals\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION MODE\n",
    "# ============================================================================\n",
    "\n",
    "def run_generation(model, tokenizer, prompt_ids, layer_indices):\n",
    "    \"\"\"Run generation and extract prefill + decode signals.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    input_ids = torch.tensor([prompt_ids], dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    prompt_length = input_ids.shape[1]\n",
    "    generated_ids = []\n",
    "    generation_signals = []\n",
    "    \n",
    "    # PREFILL\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "    \n",
    "    past_kv = outputs.past_key_values\n",
    "    prefill_signals = extract_prefill_signals(outputs, layer_indices, positions=[-3, -2, -1])\n",
    "    \n",
    "    # First token\n",
    "    next_token = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "    generated_ids.append(next_token[0].item())\n",
    "    \n",
    "    signals = extract_signals(outputs, layer_indices, position=-1)\n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': input_ids.shape[1] - 1,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    attention_mask = torch.cat([\n",
    "        attention_mask,\n",
    "        torch.ones((1, 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    # DECODE\n",
    "    for step in range(1, MAX_NEW_TOKENS):\n",
    "        new_inputs = {\n",
    "            'input_ids': next_token.unsqueeze(1),\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        next_token = outputs.logits[:, -1, :].argmax(dim=-1)\n",
    "        generated_ids.append(next_token[0].item())\n",
    "        \n",
    "        signals = extract_signals(outputs, layer_indices, position=-1)\n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': current_cache_length - 1,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask,\n",
    "            torch.ones((1, 1), device='cuda')\n",
    "        ], dim=1)\n",
    "        \n",
    "        if generated_ids[-1] == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    # Extract last 3 decode signals\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "    \n",
    "    del outputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'generated_ids': generated_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION MODE (TEACHER FORCING)\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced(model, tokenizer, reference_data, layer_indices):\n",
    "    \"\"\"Teacher-forced decode: feed reference tokens, extract signals.\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    prompt_ids = reference_data['prompt_ids']\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    \n",
    "    input_ids = torch.tensor([prompt_ids], dtype=torch.long, device='cuda')\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    inputs = {'input_ids': input_ids, 'attention_mask': attention_mask}\n",
    "    \n",
    "    generation_signals = []\n",
    "    num_steps = len(ref_generated_ids)\n",
    "    \n",
    "    # PREFILL\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True)\n",
    "    \n",
    "    past_kv = outputs.past_key_values\n",
    "    ref_prefill_signals = reference_data['prefill_signals']\n",
    "    prefill_signals = extract_prefill_signals_for_token_ids(\n",
    "        outputs, layer_indices, ref_prefill_signals, positions=[-3, -2, -1]\n",
    "    )\n",
    "    \n",
    "    # First step signal\n",
    "    ref_decode_signals = reference_data['decode_signals']\n",
    "    ref_step_data = list(ref_decode_signals.values())[0] if ref_decode_signals else None\n",
    "    if ref_step_data:\n",
    "        ref_token_ids = ref_step_data['signals']['logprobs']['token_ids']\n",
    "        signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "    else:\n",
    "        signals = extract_signals(outputs, layer_indices, position=-1)\n",
    "    \n",
    "    generation_signals.append({\n",
    "        'step': 0,\n",
    "        'absolute_position': input_ids.shape[1] - 1,\n",
    "        'signals': signals\n",
    "    })\n",
    "    \n",
    "    # Teacher-forced decode\n",
    "    attention_mask = torch.cat([\n",
    "        attention_mask,\n",
    "        torch.ones((1, 1), device='cuda')\n",
    "    ], dim=1)\n",
    "    \n",
    "    for step in range(1, num_steps):\n",
    "        next_token = torch.tensor([[ref_generated_ids[step - 1]]], dtype=torch.long, device='cuda')\n",
    "        \n",
    "        new_inputs = {\n",
    "            'input_ids': next_token,\n",
    "            'attention_mask': attention_mask,\n",
    "            'past_key_values': past_kv,\n",
    "            'use_cache': True\n",
    "        }\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**new_inputs)\n",
    "        \n",
    "        past_kv = outputs.past_key_values\n",
    "        \n",
    "        # Get reference token IDs for this step if available\n",
    "        step_key = f'pos_-{num_steps - step}' if step >= num_steps - 3 else None\n",
    "        if step_key and step_key in ref_decode_signals:\n",
    "            ref_token_ids = ref_decode_signals[step_key]['signals']['logprobs']['token_ids']\n",
    "            signals = extract_signals_for_token_ids(outputs, layer_indices, ref_token_ids, position=-1)\n",
    "        else:\n",
    "            signals = extract_signals(outputs, layer_indices, position=-1)\n",
    "        \n",
    "        current_cache_length = past_kv[0][0].shape[2]\n",
    "        generation_signals.append({\n",
    "            'step': step,\n",
    "            'absolute_position': current_cache_length - 1,\n",
    "            'signals': signals\n",
    "        })\n",
    "        \n",
    "        attention_mask = torch.cat([\n",
    "            attention_mask,\n",
    "            torch.ones((1, 1), device='cuda')\n",
    "        ], dim=1)\n",
    "    \n",
    "    # Extract last 3 decode signals\n",
    "    num_generated = len(generation_signals)\n",
    "    if num_generated >= 3:\n",
    "        last_3_signals = {\n",
    "            'pos_-3': generation_signals[-3],\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 2:\n",
    "        last_3_signals = {\n",
    "            'pos_-2': generation_signals[-2],\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    elif num_generated == 1:\n",
    "        last_3_signals = {\n",
    "            'pos_-1': generation_signals[-1]\n",
    "        }\n",
    "    else:\n",
    "        last_3_signals = {}\n",
    "    \n",
    "    del outputs, past_kv\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': last_3_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_key_distance(signals1, signals2, layer_indices):\n",
    "    distances = []\n",
    "    for layer_idx in layer_indices:\n",
    "        key = f'layer_{layer_idx}'\n",
    "        vec1 = np.array(signals1['key_vectors'][key])\n",
    "        vec2 = np.array(signals2['key_vectors'][key])\n",
    "        distances.append(np.linalg.norm(vec1 - vec2))\n",
    "    return np.mean(distances)\n",
    "\n",
    "def compute_logprob_distance(signals1, signals2):\n",
    "    idx1 = signals1['logprobs']['token_ids']\n",
    "    val1 = signals1['logprobs']['log_probs']\n",
    "    idx2 = signals2['logprobs']['token_ids']\n",
    "    val2 = signals2['logprobs']['log_probs']\n",
    "    \n",
    "    map1 = dict(zip(idx1, val1))\n",
    "    map2 = dict(zip(idx2, val2))\n",
    "    \n",
    "    common = set(idx1) & set(idx2)\n",
    "    if not common:\n",
    "        return float('inf')\n",
    "    \n",
    "    diffs = [(map1[i] - map2[i])**2 for i in common]\n",
    "    return np.sqrt(np.mean(diffs))\n",
    "\n",
    "def compare_signals(signals1, signals2, layer_indices):\n",
    "    \"\"\"Compare two sets of prefill or decode signals.\"\"\"\n",
    "    key_distances = []\n",
    "    logprob_distances = []\n",
    "    \n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "    \n",
    "    for pos in common_positions:\n",
    "        s1 = signals1[pos]['signals'] if 'signals' in signals1[pos] else signals1[pos]\n",
    "        s2 = signals2[pos]['signals'] if 'signals' in signals2[pos] else signals2[pos]\n",
    "        \n",
    "        key_distances.append(compute_key_distance(s1, s2, layer_indices))\n",
    "        logprob_distances.append(compute_logprob_distance(s1, s2))\n",
    "    \n",
    "    finite_logprobs = [d for d in logprob_distances if d != float('inf')]\n",
    "    \n",
    "    return {\n",
    "        'key_vectors_mean': np.mean(key_distances) if key_distances else 0,\n",
    "        'logprobs_mean': np.mean(finite_logprobs) if finite_logprobs else float('inf')\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# WITHIN-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_within_hardware(measurements, attn_types, layer_indices, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware attention implementation effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE ATTENTION EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    n = len(attn_types)\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    \n",
    "    header = \"           \" + \" \".join(f\"{a:>10}\" for a in attn_types)\n",
    "    \n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        matrix_key = np.zeros((n, n))\n",
    "        matrix_logprob = np.zeros((n, n))\n",
    "        \n",
    "        for i, attn_i in enumerate(attn_types):\n",
    "            for j, attn_j in enumerate(attn_types):\n",
    "                signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "                sig_i = measurements[attn_i][ref_idx][signals_key]\n",
    "                sig_j = measurements[attn_j][ref_idx][signals_key]\n",
    "                \n",
    "                distances = compare_signals(sig_i, sig_j, layer_indices)\n",
    "                matrix_key[i, j] = distances['key_vectors_mean']\n",
    "                matrix_logprob[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        log_print(f\"\\nKey Vectors (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, attn in enumerate(attn_types):\n",
    "            row = f\"{attn:>10} \" + \" \".join(f\"{matrix_key[i,j]:10.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "        \n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, attn in enumerate(attn_types):\n",
    "            row = f\"{attn:>10} \" + \" \".join(f\"{matrix_logprob[i,j]:10.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "        \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "    \n",
    "    # Aggregate\n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "    \n",
    "    log_print(f\"\\nAGGREGATE (average across references):\")\n",
    "    \n",
    "    header = \"           \" + \" \".join(f\"{a:>10}\" for a in attn_types)\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors (L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, attn in enumerate(attn_types):\n",
    "        row = f\"{attn:>10} \" + \" \".join(f\"{avg_key_matrix[i,j]:10.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "    \n",
    "    log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, attn in enumerate(attn_types):\n",
    "        row = f\"{attn:>10} \" + \" \".join(f\"{avg_logprob_matrix[i,j]:10.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "    \n",
    "    # Off-diagonal stats\n",
    "    off_diag_key = []\n",
    "    off_diag_logprob = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag_key.append(avg_key_matrix[i, j])\n",
    "                off_diag_logprob.append(avg_logprob_matrix[i, j])\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    log_print(f\"  Key vectors - Mean: {np.mean(off_diag_key):.2e}\")\n",
    "    log_print(f\"  Logprobs - Mean: {np.mean([d for d in off_diag_logprob if d != float('inf')]):.2e}\")\n",
    "    \n",
    "    # Check equivalences\n",
    "    equiv_pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if avg_key_matrix[i, j] < EQUIVALENCE_THRESHOLD:\n",
    "                equiv_pairs.append((attn_types[i], attn_types[j]))\n",
    "    \n",
    "    if equiv_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs:\")\n",
    "        for p in equiv_pairs:\n",
    "            log_print(f\"  {p}\")\n",
    "    \n",
    "    return {\n",
    "        'key_matrix': avg_key_matrix.tolist(),\n",
    "        'logprob_matrix': avg_logprob_matrix.tolist(),\n",
    "        'equivalent_pairs': equiv_pairs\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_cross_hardware(gen_measurements, ver_measurements, attn_types, layer_indices, signal_source='decode'):\n",
    "    \"\"\"Analyze cross-hardware verification results.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE VERIFICATION ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    n = len(attn_types)\n",
    "    all_key_matrices = []\n",
    "    all_logprob_matrices = []\n",
    "    \n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        matrix_key = np.zeros((n, n))\n",
    "        matrix_logprob = np.zeros((n, n))\n",
    "        \n",
    "        for i, claimed in enumerate(attn_types):\n",
    "            for j, verified in enumerate(attn_types):\n",
    "                signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "                \n",
    "                gen_sig = gen_measurements[claimed][ref_idx][signals_key]\n",
    "                ver_sig = ver_measurements[(claimed, verified)][ref_idx][signals_key]\n",
    "                \n",
    "                distances = compare_signals(gen_sig, ver_sig, layer_indices)\n",
    "                matrix_key[i, j] = distances['key_vectors_mean']\n",
    "                matrix_logprob[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        all_key_matrices.append(matrix_key)\n",
    "        all_logprob_matrices.append(matrix_logprob)\n",
    "    \n",
    "    avg_key_matrix = np.mean(all_key_matrices, axis=0)\n",
    "    avg_logprob_matrix = np.mean(all_logprob_matrices, axis=0)\n",
    "    \n",
    "    log_print(f\"\\nAGGREGATE (average across references):\")\n",
    "    log_print(\"  Rows = claimed attention, Cols = verified attention\")\n",
    "    \n",
    "    header = \"           \" + \" \".join(f\"{a:>10}\" for a in attn_types)\n",
    "    \n",
    "    log_print(f\"\\nKey Vectors (L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, attn in enumerate(attn_types):\n",
    "        row = f\"{attn:>10} \" + \" \".join(f\"{avg_key_matrix[i,j]:10.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "    \n",
    "    log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, attn in enumerate(attn_types):\n",
    "        row = f\"{attn:>10} \" + \" \".join(f\"{avg_logprob_matrix[i,j]:10.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "    \n",
    "    # SNR calculation\n",
    "    diagonal_key = np.mean([avg_key_matrix[i, i] for i in range(n)])\n",
    "    diagonal_logprob = np.mean([avg_logprob_matrix[i, i] for i in range(n) if avg_logprob_matrix[i, i] != float('inf')])\n",
    "    \n",
    "    off_diag_key = []\n",
    "    off_diag_logprob = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag_key.append(avg_key_matrix[i, j])\n",
    "                if avg_logprob_matrix[i, j] != float('inf'):\n",
    "                    off_diag_logprob.append(avg_logprob_matrix[i, j])\n",
    "    \n",
    "    off_diagonal_key = np.mean(off_diag_key)\n",
    "    off_diagonal_logprob = np.mean(off_diag_logprob) if off_diag_logprob else float('inf')\n",
    "    \n",
    "    key_snr = off_diagonal_key / diagonal_key if diagonal_key > 0 else float('inf')\n",
    "    logprob_snr = off_diagonal_logprob / diagonal_logprob if diagonal_logprob > 0 else float('inf')\n",
    "    \n",
    "    log_print(f\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same attention):\")\n",
    "    log_print(f\"  Key vectors: {diagonal_key:.2e}\")\n",
    "    log_print(f\"  Logprobs: {diagonal_logprob:.2e}\")\n",
    "    log_print(f\"\\nOff-diagonal (different attention):\")\n",
    "    log_print(f\"  Key vectors - Mean: {off_diagonal_key:.2e}, SNR: {key_snr:.2f}×\")\n",
    "    log_print(f\"  Logprobs - Mean: {off_diagonal_logprob:.2e}, SNR: {logprob_snr:.2f}×\")\n",
    "    \n",
    "    return {\n",
    "        'key_matrix': avg_key_matrix.tolist(),\n",
    "        'logprob_matrix': avg_logprob_matrix.tolist(),\n",
    "        'diagonal_key': diagonal_key,\n",
    "        'diagonal_logprob': diagonal_logprob,\n",
    "        'off_diagonal_key': off_diagonal_key,\n",
    "        'off_diagonal_logprob': off_diagonal_logprob,\n",
    "        'key_snr': key_snr,\n",
    "        'logprob_snr': logprob_snr\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    log_path = setup_logging()\n",
    "    log_print(\"=\" * 80)\n",
    "    log_print(\"ATTENTION IMPLEMENTATION CROSS-HARDWARE EXPERIMENT\")\n",
    "    log_print(\"=\" * 80)\n",
    "    \n",
    "    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\"\n",
    "    log_print(f\"GPU: {gpu_name}\")\n",
    "    log_print(f\"Hostname: {socket.gethostname()}\")\n",
    "    \n",
    "    tokenizer = load_tokenizer()\n",
    "    prompts = create_prompts(tokenizer, NUM_REFERENCES)\n",
    "    \n",
    "    log_print(f\"\\nCreated {len(prompts)} prompts\")\n",
    "    \n",
    "    if not TEACHER_FORCING:\n",
    "        # ================================================================\n",
    "        # GENERATION MODE\n",
    "        # ================================================================\n",
    "        log_print(\"\\n\" + \"=\" * 80)\n",
    "        log_print(\"GENERATION MODE\")\n",
    "        log_print(\"=\" * 80)\n",
    "        \n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'model': MODEL_NAME,\n",
    "                'layer_indices': LAYER_INDICES,\n",
    "                'tokens_per_slice': TOKENS_PER_SLICE,\n",
    "                'max_new_tokens': MAX_NEW_TOKENS,\n",
    "                'num_references': NUM_REFERENCES,\n",
    "                'environments': {}\n",
    "            },\n",
    "            'generations': {}\n",
    "        }\n",
    "        \n",
    "        measurements = {}\n",
    "        \n",
    "        for attn_impl in ATTN_IMPLEMENTATIONS:\n",
    "            log_print(f\"\\n--- Attention: {attn_impl} ---\")\n",
    "            \n",
    "            model = load_model(attn_impl)\n",
    "            results['metadata']['environments'][attn_impl] = collect_system_info(attn_impl)\n",
    "            \n",
    "            results['generations'][attn_impl] = []\n",
    "            measurements[attn_impl] = []\n",
    "            \n",
    "            for ref_idx, prompt_ids in enumerate(prompts):\n",
    "                log_print(f\"  ref_{ref_idx}: \", end=\"\")\n",
    "                gen_data = run_generation(model, tokenizer, prompt_ids, LAYER_INDICES)\n",
    "                \n",
    "                results['generations'][attn_impl].append({\n",
    "                    'ref_idx': ref_idx,\n",
    "                    'prompt_ids': gen_data['prompt_ids'],\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prompt_length': gen_data['prompt_length'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals'],\n",
    "                    'num_generated': gen_data['num_generated']\n",
    "                })\n",
    "                \n",
    "                measurements[attn_impl].append({\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals']\n",
    "                })\n",
    "                \n",
    "                log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "                log_print(f\"    -> {tokenizer.decode(gen_data['generated_ids'][:20])}...\")\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Within-hardware analysis\n",
    "        prefill_analysis = analyze_within_hardware(measurements, ATTN_IMPLEMENTATIONS, LAYER_INDICES, 'prefill')\n",
    "        decode_analysis = analyze_within_hardware(measurements, ATTN_IMPLEMENTATIONS, LAYER_INDICES, 'decode')\n",
    "        \n",
    "        results['within_hardware'] = {\n",
    "            'prefill': prefill_analysis,\n",
    "            'decode': decode_analysis\n",
    "        }\n",
    "        \n",
    "        # Save\n",
    "        output_dir = '/workspace/experiments'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filepath = os.path.join(output_dir, f\"attn_generate_{timestamp}.json\")\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        log_print(f\"\\nSaved to: {filepath}\")\n",
    "        log_print(f\"Next: Copy to verifier, set TEACHER_FORCING=True, set REFERENCE_FILE\")\n",
    "        \n",
    "    else:\n",
    "        # ================================================================\n",
    "        # VERIFICATION MODE\n",
    "        # ================================================================\n",
    "        log_print(\"\\n\" + \"=\" * 80)\n",
    "        log_print(\"VERIFICATION MODE\")\n",
    "        log_print(\"=\" * 80)\n",
    "        \n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "            content = content.replace('Infinity', '1e309')\n",
    "            reference = json.loads(content)\n",
    "        \n",
    "        log_print(f\"Loaded reference: {REFERENCE_FILE}\")\n",
    "        \n",
    "        # Environment validation\n",
    "        ref_env = reference['metadata']['environments'][ATTN_IMPLEMENTATIONS[0]]\n",
    "        ver_env = collect_system_info(ATTN_IMPLEMENTATIONS[0])\n",
    "        \n",
    "        log_print(\"\\nEnvironment comparison:\")\n",
    "        log_print(f\"  Generator GPU: {ref_env['gpu_name']}\")\n",
    "        log_print(f\"  Verifier GPU:  {ver_env['gpu_name']}\")\n",
    "        \n",
    "        check_fields = ['torch_version', 'transformers_version', 'cuda_version']\n",
    "        for field in check_fields:\n",
    "            ref_val = ref_env.get(field, 'N/A')\n",
    "            ver_val = ver_env.get(field, 'N/A')\n",
    "            match = \"✓\" if ref_val == ver_val else \"✗\"\n",
    "            log_print(f\"  {field}: {ref_val} vs {ver_val} {match}\")\n",
    "        \n",
    "        # Store generation signals\n",
    "        gen_measurements = {}\n",
    "        for attn_impl in ATTN_IMPLEMENTATIONS:\n",
    "            gen_measurements[attn_impl] = []\n",
    "            for gen_data in reference['generations'][attn_impl]:\n",
    "                gen_measurements[attn_impl].append({\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals']\n",
    "                })\n",
    "        \n",
    "        # Run verification for each combination\n",
    "        ver_measurements = {}\n",
    "        \n",
    "        for verify_attn in ATTN_IMPLEMENTATIONS:\n",
    "            log_print(f\"\\n--- Verifying with: {verify_attn} ---\")\n",
    "            \n",
    "            model = load_model(verify_attn)\n",
    "            \n",
    "            for claimed_attn in ATTN_IMPLEMENTATIONS:\n",
    "                log_print(f\"  Claimed {claimed_attn}:\")\n",
    "                \n",
    "                ver_measurements[(claimed_attn, verify_attn)] = []\n",
    "                \n",
    "                for ref_idx, gen_data in enumerate(reference['generations'][claimed_attn]):\n",
    "                    log_print(f\"    ref_{ref_idx}: \", end=\"\")\n",
    "                    \n",
    "                    ver_data = run_teacher_forced(model, tokenizer, gen_data, LAYER_INDICES)\n",
    "                    \n",
    "                    ver_measurements[(claimed_attn, verify_attn)].append({\n",
    "                        'prefill_signals': ver_data['prefill_signals'],\n",
    "                        'decode_signals': ver_data['decode_signals']\n",
    "                    })\n",
    "                    log_print(\"done\")\n",
    "            \n",
    "            del model\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Cross-hardware analysis\n",
    "        prefill_analysis = analyze_cross_hardware(\n",
    "            gen_measurements, ver_measurements, ATTN_IMPLEMENTATIONS, LAYER_INDICES, 'prefill'\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware(\n",
    "            gen_measurements, ver_measurements, ATTN_IMPLEMENTATIONS, LAYER_INDICES, 'decode'\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        output_dir = '/workspace/experiments'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filepath = os.path.join(output_dir, f\"attn_verify_{timestamp}.json\")\n",
    "        \n",
    "        verify_results = {\n",
    "            'metadata': {\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reference_environments': reference['metadata']['environments'],\n",
    "                'verifier_environments': {\n",
    "                    attn: collect_system_info(attn) for attn in ATTN_IMPLEMENTATIONS\n",
    "                }\n",
    "            },\n",
    "            'cross_hardware': {\n",
    "                'prefill': prefill_analysis,\n",
    "                'decode': decode_analysis\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(verify_results, f, indent=2)\n",
    "        \n",
    "        log_print(f\"\\nSaved to: {filepath}\")\n",
    "    \n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dcf6c2-436f-4480-91cd-afa70a523079",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
