{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccd1798-0be6-4d26-b542-00a4b71f1848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUANTIZATION FORMAT DETECTION EXPERIMENT - GENERATION\n",
      "================================================================================\n",
      "\n",
      "System: 0ce493a31677\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "vLLM: 0.11.2\n",
      "PyTorch: 2.9.0+cu128\n",
      "CUDA: 12.8\n",
      "\n",
      "Configurations:\n",
      "  awq: Qwen/Qwen3-8B-AWQ (quant=awq)\n",
      "  awq_marlin: Qwen/Qwen3-8B-AWQ (quant=awq_marlin)\n",
      "  gptq_marlin: AlphaGaO/Qwen3-8B-GPTQ (quant=gptq_marlin)\n",
      "\n",
      "Loading first model to create prompts...\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:25:00 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:25:01 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:25:01 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:25:01 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 09:25:01 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64fc7d5d28e42bd9722da94f564d40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-30 09:25:03 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:07 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:08 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:57113 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:08 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:08 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:09 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:09 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.09s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.24s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:12 [default_loader.py:314] Loading weights took 2.53 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:12 [gpu_model_runner.py:3338] Model loading took 5.7071 GiB memory and 3.260729 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:18 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/b0d34aaec5/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:18 [backends.py:647] Dynamo bytecode transform time: 5.42 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:24 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.485 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:25 [monitor.py:34] torch.compile takes 10.90 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:26 [gpu_worker.py:359] Available KV cache memory: 43.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:26 [kv_cache_utils.py:1229] GPU KV cache size: 319,728 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:26 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.81x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m 2025-11-30 09:25:26,530 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m 2025-11-30 09:25:26,543 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:03<00:00, 15.63it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 17.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:33 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took -0.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5477)\u001b[0;0m INFO 11-30 09:25:33 [core.py:250] init engine (profile, create kv cache, warmup model) took 20.76 seconds\n",
      "INFO 11-30 09:25:34 [llm.py:352] Supported tasks: ['generate']\n",
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n",
      "Total source tokens: 120215\n",
      "Prompt structure: 33 prefix + 8000 snippet + 34 suffix = 8067 tokens\n",
      "Created 4 prompts\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:25:40.873719098 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REPRODUCIBILITY CHECK (NOISE FLOOR MEASUREMENT)\n",
      "================================================================================\n",
      "Running 3 identical inference passes per config\n",
      "Measures within-format variance from atomics/non-deterministic kernels.\n",
      "\n",
      "\n",
      "--- Checking: awq ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:25:40 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:25:41 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:25:41 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:25:41 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 09:25:41 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed32d47e50764fb287bb10931dd821e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:46 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:47 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:56209 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:47 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:47 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:48 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:48 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.10s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.06s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.21s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:51 [default_loader.py:314] Loading weights took 2.47 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:51 [gpu_model_runner.py:3338] Model loading took 5.7071 GiB memory and 3.261414 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:57 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/b0d34aaec5/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:25:57 [backends.py:647] Dynamo bytecode transform time: 5.40 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:26:03 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.442 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:26:04 [monitor.py:34] torch.compile takes 10.85 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:26:05 [gpu_worker.py:359] Available KV cache memory: 43.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:26:05 [kv_cache_utils.py:1229] GPU KV cache size: 319,728 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:26:05 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.81x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m 2025-11-30 09:26:05,407 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m 2025-11-30 09:26:05,420 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:03<00:00, 15.11it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:03<00:00, 13.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:26:13 [gpu_model_runner.py:4244] Graph capturing finished in 8 secs, took -0.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5827)\u001b[0;0m INFO 11-30 09:26:13 [core.py:250] init engine (profile, create kv cache, warmup model) took 21.80 seconds\n",
      "INFO 11-30 09:26:14 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec6cdd016f1f489fb0e0ce7d6e99c20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdd85a55a47e4b8d806d139bc12ef9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfadb549083b4469bed1043e05da3bfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d53ad2c6d7104eedb7eb4274aa96826e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b2ea6b63fc47a0a267678d651f9d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f0d2b5130ca4ece98484de1a6c056fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "  awq noise floor: DETERMINISTIC (decode=0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:26:30.310816128 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking: awq_marlin ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:26:31 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq_marlin', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:26:31 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:26:31 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:26:31 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-30 09:26:31 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f15d3ed89f4d35a59baf47136d6cbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:37 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:38 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:54623 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:38 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:38 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:38 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:38 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.22s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.15s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.31s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:42 [default_loader.py:314] Loading weights took 2.66 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:42 [gpu_model_runner.py:3338] Model loading took 5.7073 GiB memory and 3.570837 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:48 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/d85c3e3665/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:48 [backends.py:647] Dynamo bytecode transform time: 5.78 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:53 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.384 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:54 [monitor.py:34] torch.compile takes 10.17 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:56 [gpu_worker.py:359] Available KV cache memory: 43.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:56 [kv_cache_utils.py:1229] GPU KV cache size: 319,728 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:26:56 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.81x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m 2025-11-30 09:26:56,446 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m 2025-11-30 09:26:56,668 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 25.29it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 27.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:27:01 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took -0.75 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6346)\u001b[0;0m INFO 11-30 09:27:01 [core.py:250] init engine (profile, create kv cache, warmup model) took 18.49 seconds\n",
      "INFO 11-30 09:27:02 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a3c2d143abc4aac81a1c4bcce396b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ad2e05e36704a79a8ffbf8ba9d8cce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4941d16547f4da38aaa71eed36cd9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c648b570004249893c0f973836256d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553af61bdfdd4adeaf2354b4dcc31a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c995c8831f4657ae28640d093d10c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "  awq_marlin noise floor: DETERMINISTIC (decode=0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:27:09.375314782 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking: gptq_marlin ---\n",
      "Loading model: AlphaGaO/Qwen3-8B-GPTQ\n",
      "  Quantization: gptq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:27:10 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq_marlin', 'model': 'AlphaGaO/Qwen3-8B-GPTQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5dfd46ecdec405492446c3dd88e9de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:27:11 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 09:27:11 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 09:27:11 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:27:11 [gptq_marlin.py:228] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 11-30 09:27:11 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07eb83acb5f249fe91d8124d9c2bca8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8194e72950124dc7bee3fde9b6fe2dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a05d28696734f4ab738f44cbde401fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43f45325b73f471ab98c1cdb9ccbea59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40040d4ebfd847e197e068336749a426",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1767ea54c974acab730fb45199e81c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bab12adca680470585464f17dfb2046e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e5e052af7243f399e680e4a199f61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354c65efb013486aa8fba6997d4e8e43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='AlphaGaO/Qwen3-8B-GPTQ', speculative_config=None, tokenizer='AlphaGaO/Qwen3-8B-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=AlphaGaO/Qwen3-8B-GPTQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:20 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:37087 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:20 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:21 [gpu_model_runner.py:3259] Starting to load model AlphaGaO/Qwen3-8B-GPTQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:21 [gptq_marlin.py:359] Using MacheteLinearKernel for GPTQMarlinLinearMethod\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:21 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:21 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:35 [weight_utils.py:441] Time spent downloading weights for AlphaGaO/Qwen3-8B-GPTQ: 14.027738 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.78it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  2.01it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.97it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:36 [default_loader.py:314] Loading weights took 1.09 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:37 [gpu_model_runner.py:3338] Model loading took 5.7137 GiB memory and 16.328707 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:44 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/c543182331/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:44 [backends.py:647] Dynamo bytecode transform time: 6.03 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:45 [backends.py:251] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:53 [backends.py:282] Compiling a graph for dynamic shape takes 9.43 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:56 [monitor.py:34] torch.compile takes 15.46 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:57 [gpu_worker.py:359] Available KV cache memory: 43.90 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:57 [kv_cache_utils.py:1229] GPU KV cache size: 319,664 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:27:57 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.80x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m 2025-11-30 09:27:57,718 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m 2025-11-30 09:27:57,731 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 33.38it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 38.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:28:01 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took -1.03 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6900)\u001b[0;0m INFO 11-30 09:28:01 [core.py:250] init engine (profile, create kv cache, warmup model) took 23.50 seconds\n",
      "INFO 11-30 09:28:02 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e45328957d4385a6046a140b9a3d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e01705b7c331449ca8ef92e83f3235e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03643fe2311940d0961b613c6fca0073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4096e5ec6837491e9f897d10283e56da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52970bc704d74b078b4c4e8f195e2689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cdaf693cb82486e9240dfedd786c50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "  gptq_marlin noise floor: DETERMINISTIC (decode=0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:28:09.814127070 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "NOISE FLOOR SUMMARY\n",
      "----------------------------------------\n",
      "  awq: decode=0.00e+00, prefill=0.00e+00\n",
      "  awq_marlin: decode=0.00e+00, prefill=0.00e+00\n",
      "  gptq_marlin: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "Cross-format signal must exceed this noise floor to be detectable.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GENERATION MODE\n",
      "================================================================================\n",
      "\n",
      "--- Config: awq ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:28:10 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:28:10 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:28:10 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:28:10 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 09:28:10 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c25acb5df5744448a101e0a3857045c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:16 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:17 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:35067 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:17 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:17 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:18 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:18 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.09s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.25s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:21 [default_loader.py:314] Loading weights took 2.54 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:21 [gpu_model_runner.py:3338] Model loading took 5.7071 GiB memory and 3.274051 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:27 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/b0d34aaec5/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:27 [backends.py:647] Dynamo bytecode transform time: 5.42 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:33 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.494 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:34 [monitor.py:34] torch.compile takes 10.92 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:35 [gpu_worker.py:359] Available KV cache memory: 43.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:35 [kv_cache_utils.py:1229] GPU KV cache size: 319,728 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:35 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.81x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m 2025-11-30 09:28:35,439 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m 2025-11-30 09:28:35,453 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:03<00:00, 15.14it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 18.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:42 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took -0.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7515)\u001b[0;0m INFO 11-30 09:28:42 [core.py:250] init engine (profile, create kv cache, warmup model) took 20.81 seconds\n",
      "INFO 11-30 09:28:43 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb2a4eff3de41889084ea9308eac3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9bf39cd90cf4034b669cd53dac91fd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a04d058bcd4fd6b9e5d8ecac307309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d66761c99e747e5abc0eff9b1b419f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eef218917dd411bb750145bc234bc29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a86af5acea148ada7cd8e178a186140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7db420222344ebea53a2d13b1c88144",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf7be9f848f4c9c8e4ebe04d5ceb480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's tackle this query. The user provided a lengthy excerpt from a document and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:29:03.149016838 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Config: awq_marlin ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:29:03 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq_marlin', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:29:04 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:29:04 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:29:04 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-30 09:29:04 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21ceb5b7ce29424fb9e39a98227cf472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:10 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:38303 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:10 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:11 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:11 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:11 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.05s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.08s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.23s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:14 [default_loader.py:314] Loading weights took 2.50 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:14 [gpu_model_runner.py:3338] Model loading took 5.7073 GiB memory and 3.401665 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:20 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/d85c3e3665/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:20 [backends.py:647] Dynamo bytecode transform time: 5.76 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:25 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.395 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:27 [monitor.py:34] torch.compile takes 10.15 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:28 [gpu_worker.py:359] Available KV cache memory: 43.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:28 [kv_cache_utils.py:1229] GPU KV cache size: 319,728 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:28 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.81x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m 2025-11-30 09:29:28,809 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m 2025-11-30 09:29:29,034 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 26.16it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 30.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:33 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took -0.75 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8037)\u001b[0;0m INFO 11-30 09:29:33 [core.py:250] init engine (profile, create kv cache, warmup model) took 18.35 seconds\n",
      "INFO 11-30 09:29:34 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abca8e528d684796976f67884991349f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3abde66e9eea4e1786ed9f91749ef250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93b540d8a9a4ae4a7a9bfd7d4d9ef77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1e028f483354977b2e3c04c7b2f2d62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's tackle this query. The user provided a lengthy excerpt from a document and...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b87f71af443a4a768b3f4df463ae1502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f265d89713ad4ab5bcde3539b4959ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdeb17fe39074f5fa8aef64fb4a7e586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f5d74fcfb4a42c99b506ea376d1da0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's tackle this query. The user provided a lengthy excerpt from a document and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:29:42.589355123 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Config: gptq_marlin ---\n",
      "Loading model: AlphaGaO/Qwen3-8B-GPTQ\n",
      "  Quantization: gptq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:29:43 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq_marlin', 'model': 'AlphaGaO/Qwen3-8B-GPTQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:29:43 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 09:29:43 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 09:29:43 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:29:43 [gptq_marlin.py:228] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 11-30 09:29:43 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33ae5096bd1649518be194128689a0bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:49 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='AlphaGaO/Qwen3-8B-GPTQ', speculative_config=None, tokenizer='AlphaGaO/Qwen3-8B-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=AlphaGaO/Qwen3-8B-GPTQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:50 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:37147 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:50 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:50 [gpu_model_runner.py:3259] Starting to load model AlphaGaO/Qwen3-8B-GPTQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:50 [gptq_marlin.py:359] Using MacheteLinearKernel for GPTQMarlinLinearMethod\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:50 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:50 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.79it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.65it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.67it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:52 [default_loader.py:314] Loading weights took 1.36 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:53 [gpu_model_runner.py:3338] Model loading took 5.7137 GiB memory and 2.637580 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:59 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/c543182331/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:29:59 [backends.py:647] Dynamo bytecode transform time: 5.96 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:30:05 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.473 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:30:07 [monitor.py:34] torch.compile takes 11.43 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:30:08 [gpu_worker.py:359] Available KV cache memory: 43.90 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:30:08 [kv_cache_utils.py:1229] GPU KV cache size: 319,680 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:30:08 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.80x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m 2025-11-30 09:30:08,809 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m 2025-11-30 09:30:08,821 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 29.90it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 39.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:30:12 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took -0.78 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=8561)\u001b[0;0m INFO 11-30 09:30:12 [core.py:250] init engine (profile, create kv cache, warmup model) took 18.70 seconds\n",
      "INFO 11-30 09:30:13 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084098bcb131435aaadae16c0aa66a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab2e4fffc704df78a2496760357427d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's see. The user provided an excerpt from a document and wants to know...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd9e34501914c7f9148e65ea36b8e76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c720d3fa54f34999819991c17152e881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's see. The user provided an excerpt from a document and wants to know...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1501c373b95543cf9bd5d0d9f924df33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eaaefa830934d639caaa3b02b10bb7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's see. The user provided an excerpt from a document and wants to know...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd0d77c0ec149788c6965c370262d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ebf876407b74037a0296da625a6e26e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's see. The user provided an excerpt from a document and wants to know...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:30:21.749824290 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's see. The user provided an excerpt from a document and wants to know what type of document it is and its subject matter\"...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "--- ref_1 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's tackle this query. The user provided a lengthy excerpt from a document and is asking about the type of document and its likely\"...\n",
      "    ✗ DIFFERENT\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's see. The user provided an excerpt from a document and wants to know what type of document it is and its subject matter\"...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "--- ref_2 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's see. The user provided an excerpt from a document and wants to know what type of document it is and its subject matter\"...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "--- ref_3 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's tackle this query. The user provided a lengthy excerpt from a document and is asking about the type of document and its subject\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's tackle this query. The user provided a lengthy excerpt from a document and is asking about the type of document and its subject\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's see. The user provided an excerpt from a document and wants to know what type of document it is and its subject matter\"...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE QUANTIZATION EFFECTS (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     3.19e-02     4.71e+00\n",
      "  awq_marlin     3.19e-02     0.00e+00     4.70e+00\n",
      " gptq_marlin     4.71e+00     4.70e+00     0.00e+00\n",
      "\n",
      "--- ref_1 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     3.67e-02     4.00e+00\n",
      "  awq_marlin     3.67e-02     0.00e+00     3.99e+00\n",
      " gptq_marlin     3.84e+00     3.83e+00     0.00e+00\n",
      "\n",
      "--- ref_2 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     5.74e-02     3.98e+00\n",
      "  awq_marlin     5.74e-02     0.00e+00     4.03e+00\n",
      " gptq_marlin     3.85e+00     3.90e+00     0.00e+00\n",
      "\n",
      "--- ref_3 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     2.91e-02     4.25e+00\n",
      "  awq_marlin     2.91e-02     0.00e+00     4.25e+00\n",
      " gptq_marlin     4.25e+00     4.25e+00     0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     3.88e-02     4.23e+00\n",
      "  awq_marlin     3.88e-02     0.00e+00     4.24e+00\n",
      " gptq_marlin     4.16e+00     4.17e+00     0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Mean: 2.81e+00\n",
      "  Range: [3.88e-02, 4.24e+00]\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: ['awq']\n",
      "  Class 2: ['awq_marlin']\n",
      "  Class 3: ['gptq_marlin']\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE QUANTIZATION EFFECTS (DECODE)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     5.19e-02          inf\n",
      "  awq_marlin     5.19e-02     0.00e+00          inf\n",
      " gptq_marlin          inf          inf     0.00e+00\n",
      "\n",
      "--- ref_1 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00          inf     5.24e+00\n",
      "  awq_marlin          inf     0.00e+00          inf\n",
      " gptq_marlin     5.24e+00          inf     0.00e+00\n",
      "\n",
      "--- ref_2 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     3.56e-02          inf\n",
      "  awq_marlin     3.56e-02     0.00e+00          inf\n",
      " gptq_marlin          inf          inf     0.00e+00\n",
      "\n",
      "--- ref_3 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     2.07e-02     6.18e+00\n",
      "  awq_marlin     2.07e-02     0.00e+00     6.18e+00\n",
      " gptq_marlin     6.18e+00     6.18e+00     0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00          inf          inf\n",
      "  awq_marlin          inf     0.00e+00          inf\n",
      " gptq_marlin          inf          inf     0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "\n",
      "⚠ WARNING: All comparisons are EXACTLY ZERO\n",
      "  All configs produce identical results (single kernel class)\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: ['awq']\n",
      "  Class 2: ['awq_marlin']\n",
      "  Class 3: ['gptq_marlin']\n",
      "\n",
      "✓ Generation results saved to: /workspace/experiments/quant_generate_20251130_092500.json\n",
      "\n",
      "Next step: Copy /workspace/experiments/quant_generate_20251130_092500.json to verifier machine\n",
      "Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\n",
      "File size: 1.5 MB\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quantization Format Detection Experiment using vLLM.\n",
    "Compares INT4 quantization methods: AWQ vs GPTQ, with and without Marlin kernels.\n",
    "\n",
    "Tests whether quantization format claims can be verified across different GPU architectures\n",
    "using logprob forensics with vLLM inference engine.\n",
    "\n",
    "Signal: logprobs only (vLLM limitation - no key vector access)\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts logprobs, saves to JSON\n",
    "\n",
    "2. Copy JSON to Machine B\n",
    "\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, compares logprobs\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = False\n",
    "REFERENCE_FILE = \"/workspace/experiments/quant_reference.json\"\n",
    "\n",
    "# Model configurations - all INT4, different quantization methods + kernels\n",
    "# \n",
    "# Available Qwen3-8B INT4 models for vLLM:\n",
    "#   - Qwen/Qwen3-8B-AWQ: AWQ quantization, supports awq/awq_marlin kernels\n",
    "#   - AlphaGaO/Qwen3-8B-GPTQ: GPTQ quantization, pre-converted to Marlin format\n",
    "#   - pytorch/Qwen3-8B-INT4: TorchAO HQQ (requires nightly vllm/torchao)\n",
    "#\n",
    "# Note: OpenVINO/Qwen3-8B-int4-ov is NOT vLLM compatible (OpenVINO backend only)\n",
    "#\n",
    "MODEL_CONFIGS = {\n",
    "    'awq': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'awq_marlin': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'gptq_marlin': {\n",
    "        'model_name': 'AlphaGaO/Qwen3-8B-GPTQ',\n",
    "        'quantization': 'gptq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    # Uncomment if you have nightly vllm + torchao installed:\n",
    "    # 'torchao_hqq': {\n",
    "    #     'model_name': 'pytorch/Qwen3-8B-INT4',\n",
    "    #     'quantization': 'torchao',\n",
    "    #     'dtype': 'bfloat16',\n",
    "    # },\n",
    "}\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "MAX_NEW_TOKENS = 100\n",
    "TOKENS_PER_SLICE = 8000\n",
    "NUM_REFERENCES = 4\n",
    "TOP_K_LOGPROBS = 5\n",
    "\n",
    "# Threshold for considering two configs \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Reproducibility check settings\n",
    "REPRODUCIBILITY_CHECK = True\n",
    "REPRODUCIBILITY_RUNS = 3\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"quant_experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_prompts_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load PDFs and create prompts with different content slices.\n",
    "    Returns list of prompt token ID lists.\n",
    "    \"\"\"\n",
    "    pdf_files = sorted(glob.glob(\"/workspace/*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        pdf_files = sorted(glob.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    content_tokens = tokenizer.encode(all_text)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    tokens_needed = num_references * TOKENS_PER_SLICE\n",
    "    if len(content_tokens) < tokens_needed:\n",
    "        raise ValueError(f\"Need {tokens_needed} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    # Build chat-formatted prompts\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "\n",
    "    suffix = f\"\"\"\\\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    suffix_tokens = tokenizer.encode(suffix)\n",
    "\n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    prompts = []\n",
    "    for i in range(num_references):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt_ids = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        prompts.append(prompt_ids)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import torch\n",
    "    import transformers\n",
    "\n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        import vllm\n",
    "        info[\"vllm_version\"] = vllm.__version__\n",
    "    except (ImportError, AttributeError):\n",
    "        info[\"vllm_version\"] = \"unknown\"\n",
    "\n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    critical_fields = ['vllm_version', 'torch_version', 'cuda_version']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "\n",
    "    mismatches = []\n",
    "\n",
    "    log_print(\"\\nCritical dependencies:\")\n",
    "    for field in critical_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not mismatches:\n",
    "        log_print(\"\\n✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    else:\n",
    "        log_print(\"\\n⚠ ENVIRONMENT MISMATCHES DETECTED\")\n",
    "        log_print(\"  Results may be affected by software differences, not just hardware.\")\n",
    "        return {'valid': False, 'mismatches': mismatches}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_vllm_model(config_name):\n",
    "    \"\"\"Load vLLM model with specified configuration.\"\"\"\n",
    "    config = MODEL_CONFIGS[config_name]\n",
    "    \n",
    "    log_print(f\"Loading model: {config['model_name']}\")\n",
    "    log_print(f\"  Quantization: {config['quantization']}\")\n",
    "    log_print(f\"  Dtype: {config['dtype']}\")\n",
    "    \n",
    "    kwargs = {\n",
    "        'model': config['model_name'],\n",
    "        'download_dir': CACHE_DIR,\n",
    "        'dtype': config['dtype'],\n",
    "        'trust_remote_code': True,\n",
    "        'gpu_memory_utilization': 0.7,\n",
    "        'quantization': config['quantization'],\n",
    "    }\n",
    "    \n",
    "    llm = LLM(**kwargs)\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    \n",
    "    return llm, tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# LOGPROB EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_logprobs_from_output(output, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from vLLM output at specified positions.\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    logprobs_list = output.outputs[0].logprobs\n",
    "    \n",
    "    if logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    num_generated = len(logprobs_list)\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else num_generated + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= num_generated:\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = logprobs_list[actual_idx]\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from prompt positions (for prefill analysis).\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "        \n",
    "        if token_logprobs is None:\n",
    "            continue\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION MODE\n",
    "# ============================================================================\n",
    "\n",
    "def run_generation(llm, tokenizer, prompt_ids):\n",
    "    \"\"\"Run generation and extract prefill + decode signals.\"\"\"\n",
    "    prompt_text = tokenizer.decode(prompt_ids)\n",
    "    prompt_length = len(prompt_ids)\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=0.0,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([prompt_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    generated_ids = list(output.outputs[0].token_ids)\n",
    "    num_generated = len(generated_ids)\n",
    "    \n",
    "    prefill_signals = extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1])\n",
    "    decode_signals = extract_logprobs_from_output(output, positions=[-3, -2, -1])\n",
    "    \n",
    "    return {\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'generated_ids': generated_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION MODE (TEACHER FORCING)\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_verification(llm, tokenizer, reference_data, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced verification.\n",
    "    For diagonal (same config): use exact reference\n",
    "    For off-diagonal: verify with different config\n",
    "    \n",
    "    vLLM doesn't support true teacher forcing, so we prefill the full sequence\n",
    "    (prompt + generated tokens) and extract logprobs.\n",
    "    \"\"\"\n",
    "    ref_prompt_ids = reference_data['prompt_ids']\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    \n",
    "    prompt_length = len(ref_prompt_ids)\n",
    "    num_generated = len(ref_generated_ids)\n",
    "    \n",
    "    # Full sequence = prompt + generated\n",
    "    full_ids = ref_prompt_ids + ref_generated_ids\n",
    "    full_text = tokenizer.decode(full_ids)\n",
    "    \n",
    "    log_print(f\"      Prompt: {prompt_length}, Gen: {num_generated}\", end=\"\")\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=1,  # Minimal generation, we just want logprobs\n",
    "        temperature=0.0,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([full_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract prefill signals from prompt portion\n",
    "    prefill_signals = {}\n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "            \n",
    "            if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            prefill_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    # Extract decode signals from generated portion\n",
    "    decode_signals = {}\n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            # Position in full sequence (prompt + generated)\n",
    "            full_pos = pos if pos >= 0 else (prompt_length + num_generated) + pos\n",
    "            \n",
    "            if full_pos < 0 or full_pos >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[full_pos]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            decode_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    log_print(f\" → verified\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_logprob_distance_canonical(logprobs1, logprobs2, canonical_ids):\n",
    "    \"\"\"Compute L2 distance between logprobs for a canonical set of token IDs.\"\"\"\n",
    "    lp1 = dict(zip(logprobs1['token_ids'], logprobs1['log_probs']))\n",
    "    lp2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "\n",
    "    for tid in canonical_ids:\n",
    "        if tid in lp1 and tid in lp2:\n",
    "            vec1.append(lp1[tid])\n",
    "            vec2.append(lp2[tid])\n",
    "\n",
    "    if len(vec1) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2):\n",
    "    \"\"\"Compare two signal sets using top 5 token IDs from first signal as canonical.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "\n",
    "    all_dists = []\n",
    "\n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]\n",
    "\n",
    "        # Use top 5 for comparison\n",
    "        canonical_ids = sig1['logprobs']['token_ids'][:5]\n",
    "        dist = compute_logprob_distance_canonical(\n",
    "            sig1['logprobs'], sig2['logprobs'], canonical_ids\n",
    "        )\n",
    "        all_dists.append(dist)\n",
    "\n",
    "    finite_dists = [d for d in all_dists if d != float('inf')]\n",
    "    \n",
    "    return {\n",
    "        'logprobs_mean': np.mean(finite_dists) if finite_dists else float('inf'),\n",
    "        'logprobs_max': max(finite_dists) if finite_dists else float('inf')\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# EQUIVALENCE DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def find_equivalent_pairs(matrix, config_names, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"Find pairs of configs that produce equivalent results (same kernel).\"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n = len(config_names)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((config_names[i], config_names[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, config_names):\n",
    "    \"\"\"Group configs into kernel equivalence classes.\"\"\"\n",
    "    parent = {cfg: cfg for cfg in config_names}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        union(cfg1, cfg2)\n",
    "    \n",
    "    groups = {}\n",
    "    for cfg in config_names:\n",
    "        root = find(cfg)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(cfg)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "# ============================================================================\n",
    "# WITHIN-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_within_hardware(measurements, config_names, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware quantization effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE QUANTIZATION EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    n = len(config_names)\n",
    "    all_matrices = []\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "\n",
    "        matrix = np.zeros((n, n))\n",
    "\n",
    "        for i, cfg_i in enumerate(config_names):\n",
    "            for j, cfg_j in enumerate(config_names):\n",
    "                signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "                sig_i = measurements[cfg_i][ref_idx][signals_key]\n",
    "                sig_j = measurements[cfg_j][ref_idx][signals_key]\n",
    "\n",
    "                if i == j:\n",
    "                    matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    distances = compare_signals(sig_i, sig_j)\n",
    "                    matrix[i, j] = distances['logprobs_mean']\n",
    "\n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, cfg in enumerate(config_names):\n",
    "            row = f\"{cfg:>12} \" + \" \".join(f\"{matrix[i,j]:12.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "\n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, cfg in enumerate(config_names):\n",
    "        row = f\"{cfg:>12} \" + \" \".join(f\"{avg_matrix[i,j]:12.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "\n",
    "    # Find equivalent pairs\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_matrix, config_names)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, config_names)\n",
    "\n",
    "    # Off-diagonal stats\n",
    "    off_diag = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag.append(avg_matrix[i, j])\n",
    "\n",
    "    finite_off_diag = [d for d in off_diag if d != float('inf')]\n",
    "\n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    if finite_off_diag:\n",
    "        log_print(f\"  Mean: {np.mean(finite_off_diag):.2e}\")\n",
    "        log_print(f\"  Range: [{np.min(finite_off_diag):.2e}, {np.max(finite_off_diag):.2e}]\")\n",
    "    \n",
    "    # Check if all zeros\n",
    "    zero_count = sum(1 for d in finite_off_diag if d < EQUIVALENCE_THRESHOLD)\n",
    "    if zero_count == len(finite_off_diag):\n",
    "        log_print(f\"\\n⚠ WARNING: All comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All configs produce identical results (single kernel class)\")\n",
    "\n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "\n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs:\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "\n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'off_diagonal_mean': float(np.mean(finite_off_diag)) if finite_off_diag else 0,\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes]\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_cross_hardware(comparison_results, config_names, signal_source='decode',\n",
    "                           equivalent_pairs=None):\n",
    "    \"\"\"Analyze the comparison matrix and determine detectability.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE QUANTIZATION DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    # Convert to set of both orderings for easy lookup\n",
    "    equiv_set = set()\n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        equiv_set.add((cfg1, cfg2))\n",
    "        equiv_set.add((cfg2, cfg1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_idx']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_config'], result['verify_config'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_matrices = []\n",
    "    n = len(config_names)\n",
    "\n",
    "    # Per-reference matrices\n",
    "    for ref_idx in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        ref_data = by_ref[ref_idx]\n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            for j, verify_cfg in enumerate(config_names):\n",
    "                key = (claimed_cfg, verify_cfg)\n",
    "                if key in ref_data:\n",
    "                    matrix[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(header)\n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            row = f\"{claimed_cfg:>12} \"\n",
    "            for j in range(n):\n",
    "                row += f\"{matrix[i,j]:12.2e} \"\n",
    "            log_print(row)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate matrix\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"  Rows = claimed config, Cols = verified config\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, claimed_cfg in enumerate(config_names):\n",
    "        row = f\"{claimed_cfg:>12} \"\n",
    "        for j in range(n):\n",
    "            row += f\"{avg_matrix[i,j]:12.2e} \"\n",
    "        log_print(row)\n",
    "    \n",
    "    # Compute statistics\n",
    "    diagonal = [avg_matrix[i, i] for i in range(n)]\n",
    "    \n",
    "    # Off-diagonal: exclude equivalent pairs\n",
    "    off_diagonal_all = []\n",
    "    off_diagonal_meaningful = []\n",
    "    excluded_pairs = []\n",
    "    \n",
    "    for i, cfg1 in enumerate(config_names):\n",
    "        for j, cfg2 in enumerate(config_names):\n",
    "            if i != j:\n",
    "                off_diagonal_all.append(avg_matrix[i, j])\n",
    "                if (cfg1, cfg2) in equiv_set:\n",
    "                    excluded_pairs.append((cfg1, cfg2))\n",
    "                else:\n",
    "                    off_diagonal_meaningful.append(avg_matrix[i, j])\n",
    "    \n",
    "    baseline_mean = np.mean(diagonal)\n",
    "    signal_all_mean = np.mean(off_diagonal_all) if off_diagonal_all else 0.0\n",
    "    signal_meaningful_mean = np.mean(off_diagonal_meaningful) if off_diagonal_meaningful else 0.0\n",
    "    \n",
    "    snr_all = signal_all_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    snr_meaningful = signal_meaningful_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same config):\")\n",
    "    log_print(f\"  Mean: {baseline_mean:.2e}\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_all)}\")\n",
    "    log_print(f\"  Mean: {signal_all_mean:.2e}\")\n",
    "    log_print(f\"  SNR (all): {snr_all:.2f}×\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "        log_print(f\"  Total excluded: {len(excluded_pairs)} cells\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_meaningful)}\")\n",
    "    if off_diagonal_meaningful:\n",
    "        log_print(f\"  Mean: {signal_meaningful_mean:.2e}\")\n",
    "        log_print(f\"  SNR (meaningful): {snr_meaningful:.2f}×\")\n",
    "    else:\n",
    "        log_print(\"  No meaningful pairs (all configs are equivalent)\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'baseline_mean': float(baseline_mean),\n",
    "        'signal_all_mean': float(signal_all_mean),\n",
    "        'signal_meaningful_mean': float(signal_meaningful_mean),\n",
    "        'snr_all': float(snr_all),\n",
    "        'snr_meaningful': float(snr_meaningful),\n",
    "        'excluded_pairs': equivalent_pairs,\n",
    "        'n_excluded_cells': len(excluded_pairs),\n",
    "        'n_meaningful_pairs': len(off_diagonal_meaningful)\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TOKEN CONSISTENCY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(measurements, config_names, tokenizer):\n",
    "    \"\"\"Verify generated tokens across quantization configs.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        tokens_by_cfg = {}\n",
    "        for cfg in config_names:\n",
    "            tokens_by_cfg[cfg] = measurements[cfg][ref_idx]['generated_ids']\n",
    "\n",
    "        reference_tokens = tokens_by_cfg[config_names[0]]\n",
    "\n",
    "        for cfg in config_names:\n",
    "            tokens = tokens_by_cfg[cfg]\n",
    "            match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "            decoded_text = tokenizer.decode(tokens[:30])\n",
    "            log_print(f\"  {cfg}:\")\n",
    "            log_print(f\"    Tokens: {len(tokens)}\")\n",
    "            log_print(f\"    First 30: {repr(decoded_text)}...\")\n",
    "            log_print(f\"    {match_str}\")\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def run_reproducibility_check(config_names, prompts):\n",
    "    \"\"\"Measure within-format noise floor from atomics/non-deterministic kernels.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"REPRODUCIBILITY CHECK (NOISE FLOOR MEASUREMENT)\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"Running {REPRODUCIBILITY_RUNS} identical inference passes per config\")\n",
    "    log_print(\"Measures within-format variance from atomics/non-deterministic kernels.\\n\")\n",
    "    \n",
    "    noise_floors = {}\n",
    "    test_prompt = prompts[0]  # Use first prompt for reproducibility check\n",
    "    \n",
    "    for cfg_name in config_names:\n",
    "        log_print(f\"\\n--- Checking: {cfg_name} ---\")\n",
    "        \n",
    "        llm, tokenizer = load_vllm_model(cfg_name)\n",
    "        \n",
    "        # Run multiple times\n",
    "        run_signals = []\n",
    "        for run_idx in range(REPRODUCIBILITY_RUNS):\n",
    "            log_print(f\"  Run {run_idx + 1}: \", end=\"\")\n",
    "            gen_data = run_generation(llm, tokenizer, test_prompt)\n",
    "            run_signals.append({\n",
    "                'generated_ids': gen_data['generated_ids'],\n",
    "                'decode_signals': gen_data['decode_signals'],\n",
    "                'prefill_signals': gen_data['prefill_signals']\n",
    "            })\n",
    "            log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "        \n",
    "        # Compute pairwise distances between all runs\n",
    "        log_print(f\"\\n  Pairwise distances:\")\n",
    "        decode_dists = []\n",
    "        prefill_dists = []\n",
    "        \n",
    "        for i in range(REPRODUCIBILITY_RUNS):\n",
    "            for j in range(i + 1, REPRODUCIBILITY_RUNS):\n",
    "                decode_dist = compare_signals(\n",
    "                    run_signals[i]['decode_signals'],\n",
    "                    run_signals[j]['decode_signals']\n",
    "                )\n",
    "                prefill_dist = compare_signals(\n",
    "                    run_signals[i]['prefill_signals'],\n",
    "                    run_signals[j]['prefill_signals']\n",
    "                )\n",
    "                decode_dists.append(decode_dist['logprobs_mean'])\n",
    "                prefill_dists.append(prefill_dist['logprobs_mean'])\n",
    "                \n",
    "                log_print(f\"    Run {i+1} vs {j+1}: decode={decode_dist['logprobs_mean']:.2e}, prefill={prefill_dist['logprobs_mean']:.2e}\")\n",
    "        \n",
    "        # Compute noise floor stats\n",
    "        finite_decode = [d for d in decode_dists if d != float('inf')]\n",
    "        finite_prefill = [d for d in prefill_dists if d != float('inf')]\n",
    "        \n",
    "        decode_noise = np.mean(finite_decode) if finite_decode else 0\n",
    "        prefill_noise = np.mean(finite_prefill) if finite_prefill else 0\n",
    "        \n",
    "        noise_floors[cfg_name] = {\n",
    "            'decode': decode_noise,\n",
    "            'prefill': prefill_noise\n",
    "        }\n",
    "        \n",
    "        if decode_noise < EQUIVALENCE_THRESHOLD:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: DETERMINISTIC (decode={decode_noise:.2e})\")\n",
    "        else:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: decode={decode_noise:.2e}, prefill={prefill_noise:.2e}\")\n",
    "        \n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(\"\\n\" + \"-\"*40)\n",
    "    log_print(\"NOISE FLOOR SUMMARY\")\n",
    "    log_print(\"-\"*40)\n",
    "    for cfg_name, nf in noise_floors.items():\n",
    "        log_print(f\"  {cfg_name}: decode={nf['decode']:.2e}, prefill={nf['prefill']:.2e}\")\n",
    "    log_print(\"\\nCross-format signal must exceed this noise floor to be detectable.\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    return noise_floors\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    import torch\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    config_names = list(MODEL_CONFIGS.keys())\n",
    "\n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"QUANTIZATION FORMAT DETECTION EXPERIMENT - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"vLLM: {system_info['vllm_version']}\")\n",
    "    log_print(f\"PyTorch: {system_info['torch_version']}\")\n",
    "    log_print(f\"CUDA: {system_info['cuda_version']}\")\n",
    "\n",
    "    log_print(f\"\\nConfigurations:\")\n",
    "    for cfg_name, cfg in MODEL_CONFIGS.items():\n",
    "        log_print(f\"  {cfg_name}: {cfg['model_name']} (quant={cfg['quantization']})\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        log_print(f\"\\nReference file: {REFERENCE_FILE}\")\n",
    "    log_print()\n",
    "\n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if not TEACHER_FORCING:\n",
    "        # ================================================================\n",
    "        # GENERATION MODE\n",
    "        # ================================================================\n",
    "        \n",
    "        # First, load one model to get tokenizer for prompt creation\n",
    "        log_print(\"Loading first model to create prompts...\")\n",
    "        first_llm, tokenizer = load_vllm_model(config_names[0])\n",
    "        prompts = create_prompts_from_pdf(tokenizer, NUM_REFERENCES)\n",
    "        log_print(f\"Created {len(prompts)} prompts\\n\")\n",
    "        del first_llm\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Reproducibility check\n",
    "        noise_floors = {}\n",
    "        if REPRODUCIBILITY_CHECK:\n",
    "            noise_floors = run_reproducibility_check(config_names, prompts)\n",
    "\n",
    "        # Main generation\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"GENERATION MODE\")\n",
    "        log_print(\"=\"*80)\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'max_new_tokens': MAX_NEW_TOKENS,\n",
    "                'tokens_per_slice': TOKENS_PER_SLICE,\n",
    "                'num_references': NUM_REFERENCES,\n",
    "                'top_k_logprobs': TOP_K_LOGPROBS,\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'generations': {}\n",
    "        }\n",
    "\n",
    "        measurements = {}\n",
    "\n",
    "        for cfg_name in config_names:\n",
    "            log_print(f\"\\n--- Config: {cfg_name} ---\")\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(cfg_name)\n",
    "\n",
    "            results['generations'][cfg_name] = []\n",
    "            measurements[cfg_name] = []\n",
    "\n",
    "            for ref_idx, prompt_ids in enumerate(prompts):\n",
    "                log_print(f\"  ref_{ref_idx}: \", end=\"\")\n",
    "                gen_data = run_generation(llm, tokenizer, prompt_ids)\n",
    "\n",
    "                results['generations'][cfg_name].append({\n",
    "                    'ref_idx': ref_idx,\n",
    "                    'prompt_ids': gen_data['prompt_ids'],\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prompt_length': gen_data['prompt_length'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals'],\n",
    "                    'num_generated': gen_data['num_generated']\n",
    "                })\n",
    "\n",
    "                measurements[cfg_name].append({\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals']\n",
    "                })\n",
    "\n",
    "                log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "                decoded = tokenizer.decode(gen_data['generated_ids'][:20])\n",
    "                log_print(f\"    -> {decoded}...\")\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Token consistency check\n",
    "        check_token_consistency(measurements, config_names, tokenizer)\n",
    "\n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(measurements, config_names, 'prefill')\n",
    "        decode_sanity = analyze_within_hardware(measurements, config_names, 'decode')\n",
    "\n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "        \n",
    "        # Store noise floors\n",
    "        if noise_floors:\n",
    "            results['noise_floors'] = noise_floors\n",
    "            \n",
    "            # Compute signal-to-noise ratio\n",
    "            avg_noise_decode = np.mean([nf['decode'] for nf in noise_floors.values()])\n",
    "            cross_format_decode = decode_sanity.get('off_diagonal_mean', 0)\n",
    "            \n",
    "            if avg_noise_decode > 0 and cross_format_decode:\n",
    "                snr_decode = cross_format_decode / avg_noise_decode\n",
    "                log_print(\"\\n\" + \"=\"*80)\n",
    "                log_print(\"SIGNAL-TO-NOISE ANALYSIS\")\n",
    "                log_print(\"=\"*80)\n",
    "                log_print(f\"\\nNoise floor (within-format variance): {avg_noise_decode:.2e}\")\n",
    "                log_print(f\"Cross-format distance: {cross_format_decode:.2e}\")\n",
    "                log_print(f\"SNR: {snr_decode:.1f}×\")\n",
    "                \n",
    "                if snr_decode > 10:\n",
    "                    log_print(f\"→ Quantization format is DETECTABLE (SNR > 10)\")\n",
    "                elif snr_decode > 3:\n",
    "                    log_print(f\"→ Quantization format is MARGINALLY detectable (3 < SNR < 10)\")\n",
    "                else:\n",
    "                    log_print(f\"→ Quantization format is NOT reliably detectable (SNR < 3)\")\n",
    "                \n",
    "                results['snr_analysis'] = {\n",
    "                    'noise_floor': avg_noise_decode,\n",
    "                    'signal': cross_format_decode,\n",
    "                    'snr': snr_decode\n",
    "                }\n",
    "\n",
    "        # Save\n",
    "        filepath = os.path.join(output_dir, f\"quant_generate_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Generation results saved to: {filepath}\")\n",
    "        log_print(f\"\\nNext step: Copy {filepath} to verifier machine\")\n",
    "        log_print(f\"Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\")\n",
    "\n",
    "        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        log_print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    else:\n",
    "        # ================================================================\n",
    "        # VERIFICATION MODE\n",
    "        # ================================================================\n",
    "        log_print(\"Loading reference file...\")\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "            content = content.replace('Infinity', '1e309')\n",
    "            reference = json.loads(content)\n",
    "\n",
    "        ref_env = reference['metadata']['environment']\n",
    "        ref_gpu = ref_env['gpu_name']\n",
    "        log_print(f\"Reference GPU: {ref_gpu}\")\n",
    "        log_print(f\"Verifier GPU:  {system_info['gpu_name']}\")\n",
    "\n",
    "        env_validation = validate_environment_match(ref_env, system_info)\n",
    "\n",
    "        # Load equivalent pairs from reference\n",
    "        prefill_equiv_pairs = reference.get('prefill_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        decode_equiv_pairs = reference.get('decode_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        \n",
    "        # Convert to tuples if stored as lists\n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_equiv_pairs]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_equiv_pairs]\n",
    "        \n",
    "        log_print(f\"\\nLoaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "\n",
    "        comparison_results = []\n",
    "\n",
    "        for verify_cfg in config_names:\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"VERIFYING WITH: {verify_cfg}\")\n",
    "            log_print(\"=\"*80)\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(verify_cfg)\n",
    "\n",
    "            for claimed_cfg in config_names:\n",
    "                log_print(f\"\\n  Claimed config: {claimed_cfg}\")\n",
    "\n",
    "                for ref_idx, gen_data in enumerate(reference['generations'][claimed_cfg]):\n",
    "                    is_diagonal = (claimed_cfg == verify_cfg)\n",
    "\n",
    "                    log_print(f\"    ref_{ref_idx} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "\n",
    "                    verify_result = run_teacher_forced_verification(\n",
    "                        llm, tokenizer, gen_data, is_diagonal\n",
    "                    )\n",
    "\n",
    "                    prefill_distances = compare_signals(\n",
    "                        gen_data['prefill_signals'],\n",
    "                        verify_result['prefill_signals']\n",
    "                    )\n",
    "\n",
    "                    decode_distances = compare_signals(\n",
    "                        gen_data['decode_signals'],\n",
    "                        verify_result['decode_signals']\n",
    "                    )\n",
    "\n",
    "                    log_print(f\"      Prefill: {prefill_distances['logprobs_mean']:.2e}, Decode: {decode_distances['logprobs_mean']:.2e}\")\n",
    "\n",
    "                    comparison_results.append({\n",
    "                        'ref_idx': ref_idx,\n",
    "                        'claimed_config': claimed_cfg,\n",
    "                        'verify_config': verify_cfg,\n",
    "                        'is_diagonal': is_diagonal,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances\n",
    "                    })\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Analyze with equivalent pair exclusion\n",
    "        prefill_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='prefill',\n",
    "            equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='decode',\n",
    "            equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill SNR (meaningful): {prefill_analysis['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode SNR (meaningful):  {decode_analysis['snr_meaningful']:.2f}×\")\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_gpu': ref_gpu,\n",
    "                'verifier_gpu': system_info['gpu_name'],\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'environment_validation': env_validation,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'timestamp': timestamp,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "\n",
    "        filepath = os.path.join(output_dir, f\"quant_verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"EXPERIMENT COMPLETE\")\n",
    "    log_print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916a843-2f06-4a53-a874-b7b3415a4080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
