{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a1940a0-6239-4f57-902c-1c9575953fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUANTIZATION FORMAT DETECTION EXPERIMENT - GENERATION\n",
      "================================================================================\n",
      "\n",
      "System: 637c3157512e\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "vLLM: 0.11.2\n",
      "PyTorch: 2.9.1+cu128\n",
      "CUDA: 12.8\n",
      "\n",
      "Configurations:\n",
      "  awq: Qwen/Qwen3-8B-AWQ (quant=awq)\n",
      "  awq_marlin: Qwen/Qwen3-8B-AWQ (quant=awq_marlin)\n",
      "  gptq_marlin: AlphaGaO/Qwen3-8B-GPTQ (quant=gptq_marlin)\n",
      "\n",
      "Loading first model to create prompts...\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:24:50 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:24:50 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:24:50 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:24:51 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 09:24:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50e006731c904eadb3bb1ad5d5e11958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-30 09:24:52 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:24:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:24:58 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:35489 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:24:58 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:24:58 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:24:59 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:24:59 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.66s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  1.85s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.12s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:03 [default_loader.py:314] Loading weights took 4.29 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:04 [gpu_model_runner.py:3338] Model loading took 5.7086 GiB memory and 4.842724 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:11 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/951c718543/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:11 [backends.py:647] Dynamo bytecode transform time: 6.64 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:18 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.676 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:19 [monitor.py:34] torch.compile takes 13.31 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:21 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:21 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:21 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 12.39it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 13.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:28 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.95 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5273)\u001b[0;0m INFO 11-30 09:25:28 [core.py:250] init engine (profile, create kv cache, warmup model) took 24.35 seconds\n",
      "INFO 11-30 09:25:29 [llm.py:352] Supported tasks: ['generate']\n",
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n",
      "Total source tokens: 120215\n",
      "Prompt structure: 33 prefix + 8000 snippet + 34 suffix = 8067 tokens\n",
      "Created 4 prompts\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:25:35.655557648 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REPRODUCIBILITY CHECK (NOISE FLOOR MEASUREMENT)\n",
      "================================================================================\n",
      "Running 3 identical inference passes per config\n",
      "Measures within-format variance from atomics/non-deterministic kernels.\n",
      "\n",
      "\n",
      "--- Checking: awq ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:25:36 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:25:36 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:25:36 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:25:36 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 09:25:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea28013cc89b45c8bad296d4bea93513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:42 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:42239 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:43 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:43 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.70s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  1.88s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.15s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:48 [default_loader.py:314] Loading weights took 4.34 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:48 [gpu_model_runner.py:3338] Model loading took 5.7086 GiB memory and 4.860261 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:55 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/951c718543/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:25:55 [backends.py:647] Dynamo bytecode transform time: 6.72 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:26:03 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.789 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:26:04 [monitor.py:34] torch.compile takes 13.51 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:26:05 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:26:05 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:26:05 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 12.38it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:26:13 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.95 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5563)\u001b[0;0m INFO 11-30 09:26:13 [core.py:250] init engine (profile, create kv cache, warmup model) took 24.60 seconds\n",
      "INFO 11-30 09:26:14 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70a23b855d0a474ab1b6921414c899b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c0b233480724d2fa82e4288272d95f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a86da0dca1141f9ae83d8076c096281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a78195769924fd39383a2e265d13582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dcdd71501724e058672f8c7cbf4ac62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c0e955e36764e459e6dbf313b78160b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "  awq noise floor: DETERMINISTIC (decode=0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:26:32.353739529 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking: awq_marlin ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:26:32 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq_marlin', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:26:32 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:26:33 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:26:33 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-30 09:26:33 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45055a5f3a834ba59fb6e85368d4ea87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:39 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:39 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:45365 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:39 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:40 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:40 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:40 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  1.81s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.06s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:44 [default_loader.py:314] Loading weights took 4.16 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:45 [gpu_model_runner.py:3338] Model loading took 5.7087 GiB memory and 5.078252 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:52 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/b8c5808a6c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:52 [backends.py:647] Dynamo bytecode transform time: 6.92 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:26:58 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.383 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:27:00 [monitor.py:34] torch.compile takes 12.31 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:27:01 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:27:02 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:27:02 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 21.21it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 24.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:27:06 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took 0.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=5843)\u001b[0;0m INFO 11-30 09:27:06 [core.py:250] init engine (profile, create kv cache, warmup model) took 20.93 seconds\n",
      "INFO 11-30 09:27:07 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5443af215e547e38d1fc06d547b77ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6969165abd7c4a6ba3f9976f8dccfd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42de637cdb064e719bb4c2b113d5232d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b57cc092df54166a15449f6e1355c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f683f03f714e4d2798f5ac498c54ec66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acf5f014906d4e6194afedb9d41089d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "  awq_marlin noise floor: DETERMINISTIC (decode=0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:27:12.691374946 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking: gptq_marlin ---\n",
      "Loading model: AlphaGaO/Qwen3-8B-GPTQ\n",
      "  Quantization: gptq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:27:13 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq_marlin', 'model': 'AlphaGaO/Qwen3-8B-GPTQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98c92f4df124b948d35c52be4dfc1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:27:13 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 09:27:13 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 09:27:13 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:27:13 [gptq_marlin.py:228] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 11-30 09:27:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d803b0aaaf43bd9237a8b14f7e6445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2213e30fa5a48518f4c1b55fa7b618a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2e59ce0ea540eaa496ecc8a816b2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd1f4eacc0e4e8489881703fc5a0397",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ad724a743843579b29634675fd1ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8ce54e58bc48de823080ae00e747ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b02080eafbd46eb89060ce802191c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29da928670f143a5a54b424c3f9bd3f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "615571461af543dc8e37b0ebaf2b67c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='AlphaGaO/Qwen3-8B-GPTQ', speculative_config=None, tokenizer='AlphaGaO/Qwen3-8B-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=AlphaGaO/Qwen3-8B-GPTQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:38801 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:21 [gpu_model_runner.py:3259] Starting to load model AlphaGaO/Qwen3-8B-GPTQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:21 [gptq_marlin.py:359] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:22 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:22 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:31 [weight_utils.py:441] Time spent downloading weights for AlphaGaO/Qwen3-8B-GPTQ: 8.802477 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.26it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.37it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.35it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:32 [default_loader.py:314] Loading weights took 1.56 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:33 [gpu_model_runner.py:3338] Model loading took 5.6900 GiB memory and 10.996772 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:40 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/95bc7cde1c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:40 [backends.py:647] Dynamo bytecode transform time: 7.29 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:42 [backends.py:251] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:51 [backends.py:282] Compiling a graph for dynamic shape takes 9.83 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:53 [monitor.py:34] torch.compile takes 17.12 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:55 [gpu_worker.py:359] Available KV cache memory: 48.30 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:55 [kv_cache_utils.py:1229] GPU KV cache size: 351,680 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:55 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.59x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 21.16it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:59 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took 0.76 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6140)\u001b[0;0m INFO 11-30 09:27:59 [core.py:250] init engine (profile, create kv cache, warmup model) took 26.80 seconds\n",
      "INFO 11-30 09:28:00 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d81b42188584c309493a1f2f8f526a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b13f011bb64dd492010405fa6dfc58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2d7fe6e2f42425494ce27606a7200b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b17b46300c400e9f56caf7e1915e74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1272d1ec5f104c6eba764900c54bfa73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da574bf0b3d4a729e05644cf04c4d79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "  gptq_marlin noise floor: DETERMINISTIC (decode=0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:28:06.350162585 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "NOISE FLOOR SUMMARY\n",
      "----------------------------------------\n",
      "  awq: decode=0.00e+00, prefill=0.00e+00\n",
      "  awq_marlin: decode=0.00e+00, prefill=0.00e+00\n",
      "  gptq_marlin: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "Cross-format signal must exceed this noise floor to be detectable.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GENERATION MODE\n",
      "================================================================================\n",
      "\n",
      "--- Config: awq ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:28:06 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:28:06 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:28:06 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:28:06 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 09:28:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d97abb7df05427c9faf2056fb992933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:13 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:13 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:46613 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:13 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:13 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:14 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:14 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.25s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.68s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.91s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:18 [default_loader.py:314] Loading weights took 3.87 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:18 [gpu_model_runner.py:3338] Model loading took 5.7086 GiB memory and 4.392062 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:25 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/951c718543/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:25 [backends.py:647] Dynamo bytecode transform time: 6.64 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:32 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.801 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:34 [monitor.py:34] torch.compile takes 13.44 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:35 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:35 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:35 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 12.40it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 13.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:43 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.95 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6453)\u001b[0;0m INFO 11-30 09:28:43 [core.py:250] init engine (profile, create kv cache, warmup model) took 24.48 seconds\n",
      "INFO 11-30 09:28:44 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0948003a25f54b389aed347e6f34fa83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d35b5be2a2124eb7b72197a2ab519316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bfed0c74c8f44d9ab0c66af9e9048e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43d8f73a0ea45cbabe7b129103678c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ed8043e1d441db9e8a63cdb1a78976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74937c3dbe684b00a2e7aef5dddd2117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ec0f2ed6ac44afbb650ef2865e0865",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249e13d0986f4be69d34b761c1abdc77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's tackle this query. The user provided a lengthy excerpt from a document and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:29:07.004373543 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Config: awq_marlin ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:29:08 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq_marlin', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:29:08 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:29:08 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:29:08 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-30 09:29:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba6bcc711c4c41a0874374879b90d6fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:14 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:15 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:43179 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:15 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:15 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:15 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.39s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.73s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.98s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:20 [default_loader.py:314] Loading weights took 4.01 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:21 [gpu_model_runner.py:3338] Model loading took 5.7087 GiB memory and 4.940562 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:28 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/b8c5808a6c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:28 [backends.py:647] Dynamo bytecode transform time: 7.00 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:34 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.545 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:35 [monitor.py:34] torch.compile takes 12.54 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:37 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:37 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:37 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 20.26it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 23.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:42 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took 0.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=6732)\u001b[0;0m INFO 11-30 09:29:42 [core.py:250] init engine (profile, create kv cache, warmup model) took 21.46 seconds\n",
      "INFO 11-30 09:29:43 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff2d38c0f4f45a39a4a73f4f9c39952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61c5819eef764b6e81294cebd0c2b015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b50a0459aa42be89c0a9d78f96e84d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a6954486ed4c789c506bd51b5333aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b015f95a55024ad2880f81f270f46b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b8464ec64744687ad5fd998761bdd56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09888f2063764a9dabdd5e79e3d94f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935f8271b3b146e8903783b30e6600c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's tackle this query. The user provided a lengthy excerpt from a document and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:29:50.259157241 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Config: gptq_marlin ---\n",
      "Loading model: AlphaGaO/Qwen3-8B-GPTQ\n",
      "  Quantization: gptq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:29:50 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq_marlin', 'model': 'AlphaGaO/Qwen3-8B-GPTQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:29:50 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 09:29:50 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 09:29:50 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:29:50 [gptq_marlin.py:228] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 11-30 09:29:50 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4860d4343c9e47c1a8699fe090fd5a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:29:56 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='AlphaGaO/Qwen3-8B-GPTQ', speculative_config=None, tokenizer='AlphaGaO/Qwen3-8B-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=AlphaGaO/Qwen3-8B-GPTQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:29:57 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:51381 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:29:57 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:29:57 [gpu_model_runner.py:3259] Starting to load model AlphaGaO/Qwen3-8B-GPTQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:29:57 [gptq_marlin.py:359] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:29:58 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:29:58 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.26it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:00 [default_loader.py:314] Loading weights took 1.76 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:00 [gpu_model_runner.py:3338] Model loading took 5.6900 GiB memory and 2.412623 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:08 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/95bc7cde1c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:08 [backends.py:647] Dynamo bytecode transform time: 7.39 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:14 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.463 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:16 [monitor.py:34] torch.compile takes 12.85 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:17 [gpu_worker.py:359] Available KV cache memory: 48.30 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:18 [kv_cache_utils.py:1229] GPU KV cache size: 351,680 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:18 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.59x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 20.23it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 23.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:22 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took 0.76 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=7005)\u001b[0;0m INFO 11-30 09:30:22 [core.py:250] init engine (profile, create kv cache, warmup model) took 22.01 seconds\n",
      "INFO 11-30 09:30:23 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a757d9c432d0434baac9f758d7fa7b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69872e278214cafa3c068cb62d3d0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's see. The user provided an excerpt from a document and wants to know...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a262df99d424cda823d26fe9cbb1e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39e1fd066cf4c1eb375385acdba08c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's see. The user provided an excerpt from a document and wants to know...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e60b26542153433c9677e916ebcd8727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3776172307d410caf5bca2bb5bd6c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8924efcd509545929ce744f48b0fcb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15c2e892b89b441695c54215868b38a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's see. The user provided an excerpt from a document and wants to know...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:30:30.879882057 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's see. The user provided an excerpt from a document and wants to know what type of document it is and its subject matter\"...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "--- ref_1 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided a long excerpt from a\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided a long excerpt from a\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's see. The user provided an excerpt from a document and wants to know what type of document it is and its subject matter\"...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "--- ref_2 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "--- ref_3 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's tackle this query. The user provided a lengthy excerpt from a document and is asking about the type of document and its subject\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's tackle this query. The user provided a lengthy excerpt from a document and is asking about the type of document and its subject\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's see. The user provided an excerpt from a document and wants to know what type of document it is and its subject matter\"...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE QUANTIZATION EFFECTS (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     3.50e-02     4.75e+00\n",
      "  awq_marlin     3.50e-02     0.00e+00     4.73e+00\n",
      " gptq_marlin     4.75e+00     4.73e+00     0.00e+00\n",
      "\n",
      "--- ref_1 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     4.16e-02     3.97e+00\n",
      "  awq_marlin     4.16e-02     0.00e+00     3.95e+00\n",
      " gptq_marlin     3.81e+00     3.79e+00     0.00e+00\n",
      "\n",
      "--- ref_2 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     3.82e-02     4.01e+00\n",
      "  awq_marlin     3.82e-02     0.00e+00     4.00e+00\n",
      " gptq_marlin     3.87e+00     3.87e+00     0.00e+00\n",
      "\n",
      "--- ref_3 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     4.46e-02     4.28e+00\n",
      "  awq_marlin     4.46e-02     0.00e+00     4.30e+00\n",
      " gptq_marlin     4.28e+00     4.30e+00     0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     3.99e-02     4.25e+00\n",
      "  awq_marlin     3.99e-02     0.00e+00     4.25e+00\n",
      " gptq_marlin     4.18e+00     4.17e+00     0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Mean: 2.82e+00\n",
      "  Range: [3.99e-02, 4.25e+00]\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: ['awq']\n",
      "  Class 2: ['awq_marlin']\n",
      "  Class 3: ['gptq_marlin']\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE QUANTIZATION EFFECTS (DECODE)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     2.73e-02          inf\n",
      "  awq_marlin     2.73e-02     0.00e+00          inf\n",
      " gptq_marlin          inf          inf     0.00e+00\n",
      "\n",
      "--- ref_1 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     2.13e-02     5.63e+00\n",
      "  awq_marlin     2.13e-02     0.00e+00     5.63e+00\n",
      " gptq_marlin     5.63e+00     5.63e+00     0.00e+00\n",
      "\n",
      "--- ref_2 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     3.21e-02          inf\n",
      "  awq_marlin     3.21e-02     0.00e+00          inf\n",
      " gptq_marlin          inf          inf     0.00e+00\n",
      "\n",
      "--- ref_3 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     3.19e-02     6.13e+00\n",
      "  awq_marlin     3.19e-02     0.00e+00     6.15e+00\n",
      " gptq_marlin     6.13e+00     6.15e+00     0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     0.00e+00     2.82e-02          inf\n",
      "  awq_marlin     2.82e-02     0.00e+00          inf\n",
      " gptq_marlin          inf          inf     0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Mean: 2.82e-02\n",
      "  Range: [2.82e-02, 2.82e-02]\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: ['awq']\n",
      "  Class 2: ['awq_marlin']\n",
      "  Class 3: ['gptq_marlin']\n",
      "\n",
      "✓ Generation results saved to: /workspace/experiments/quant_generate_20251130_092450.json\n",
      "\n",
      "Next step: Copy /workspace/experiments/quant_generate_20251130_092450.json to verifier machine\n",
      "Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\n",
      "File size: 1.5 MB\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quantization Format Detection Experiment using vLLM.\n",
    "Compares INT4 quantization methods: AWQ vs GPTQ, with and without Marlin kernels.\n",
    "\n",
    "Tests whether quantization format claims can be verified across different GPU architectures\n",
    "using logprob forensics with vLLM inference engine.\n",
    "\n",
    "Signal: logprobs only (vLLM limitation - no key vector access)\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts logprobs, saves to JSON\n",
    "\n",
    "2. Copy JSON to Machine B\n",
    "\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, compares logprobs\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = False\n",
    "REFERENCE_FILE = \"/workspace/experiments/quant_reference.json\"\n",
    "\n",
    "# Model configurations - all INT4, different quantization methods + kernels\n",
    "# \n",
    "# Available Qwen3-8B INT4 models for vLLM:\n",
    "#   - Qwen/Qwen3-8B-AWQ: AWQ quantization, supports awq/awq_marlin kernels\n",
    "#   - AlphaGaO/Qwen3-8B-GPTQ: GPTQ quantization, pre-converted to Marlin format\n",
    "#   - pytorch/Qwen3-8B-INT4: TorchAO HQQ (requires nightly vllm/torchao)\n",
    "#\n",
    "# Note: OpenVINO/Qwen3-8B-int4-ov is NOT vLLM compatible (OpenVINO backend only)\n",
    "#\n",
    "MODEL_CONFIGS = {\n",
    "    'awq': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'awq_marlin': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'gptq_marlin': {\n",
    "        'model_name': 'AlphaGaO/Qwen3-8B-GPTQ',\n",
    "        'quantization': 'gptq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    # Uncomment if you have nightly vllm + torchao installed:\n",
    "    # 'torchao_hqq': {\n",
    "    #     'model_name': 'pytorch/Qwen3-8B-INT4',\n",
    "    #     'quantization': 'torchao',\n",
    "    #     'dtype': 'bfloat16',\n",
    "    # },\n",
    "}\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "MAX_NEW_TOKENS = 100\n",
    "TOKENS_PER_SLICE = 8000\n",
    "NUM_REFERENCES = 4\n",
    "TOP_K_LOGPROBS = 5\n",
    "\n",
    "# Threshold for considering two configs \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Reproducibility check settings\n",
    "REPRODUCIBILITY_CHECK = True\n",
    "REPRODUCIBILITY_RUNS = 3\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"quant_experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_prompts_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load PDFs and create prompts with different content slices.\n",
    "    Returns list of prompt token ID lists.\n",
    "    \"\"\"\n",
    "    pdf_files = sorted(glob.glob(\"/workspace/*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        pdf_files = sorted(glob.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    content_tokens = tokenizer.encode(all_text)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    tokens_needed = num_references * TOKENS_PER_SLICE\n",
    "    if len(content_tokens) < tokens_needed:\n",
    "        raise ValueError(f\"Need {tokens_needed} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    # Build chat-formatted prompts\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "\n",
    "    suffix = f\"\"\"\\\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    suffix_tokens = tokenizer.encode(suffix)\n",
    "\n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    prompts = []\n",
    "    for i in range(num_references):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt_ids = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        prompts.append(prompt_ids)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import torch\n",
    "    import transformers\n",
    "\n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        import vllm\n",
    "        info[\"vllm_version\"] = vllm.__version__\n",
    "    except (ImportError, AttributeError):\n",
    "        info[\"vllm_version\"] = \"unknown\"\n",
    "\n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    critical_fields = ['vllm_version', 'torch_version', 'cuda_version']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "\n",
    "    mismatches = []\n",
    "\n",
    "    log_print(\"\\nCritical dependencies:\")\n",
    "    for field in critical_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not mismatches:\n",
    "        log_print(\"\\n✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    else:\n",
    "        log_print(\"\\n⚠ ENVIRONMENT MISMATCHES DETECTED\")\n",
    "        log_print(\"  Results may be affected by software differences, not just hardware.\")\n",
    "        return {'valid': False, 'mismatches': mismatches}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_vllm_model(config_name):\n",
    "    \"\"\"Load vLLM model with specified configuration.\"\"\"\n",
    "    config = MODEL_CONFIGS[config_name]\n",
    "    \n",
    "    log_print(f\"Loading model: {config['model_name']}\")\n",
    "    log_print(f\"  Quantization: {config['quantization']}\")\n",
    "    log_print(f\"  Dtype: {config['dtype']}\")\n",
    "    \n",
    "    kwargs = {\n",
    "        'model': config['model_name'],\n",
    "        'download_dir': CACHE_DIR,\n",
    "        'dtype': config['dtype'],\n",
    "        'trust_remote_code': True,\n",
    "        'gpu_memory_utilization': 0.7,\n",
    "        'quantization': config['quantization'],\n",
    "    }\n",
    "    \n",
    "    llm = LLM(**kwargs)\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    \n",
    "    return llm, tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# LOGPROB EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_logprobs_from_output(output, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from vLLM output at specified positions.\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    logprobs_list = output.outputs[0].logprobs\n",
    "    \n",
    "    if logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    num_generated = len(logprobs_list)\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else num_generated + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= num_generated:\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = logprobs_list[actual_idx]\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from prompt positions (for prefill analysis).\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "        \n",
    "        if token_logprobs is None:\n",
    "            continue\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION MODE\n",
    "# ============================================================================\n",
    "\n",
    "def run_generation(llm, tokenizer, prompt_ids):\n",
    "    \"\"\"Run generation and extract prefill + decode signals.\"\"\"\n",
    "    prompt_text = tokenizer.decode(prompt_ids)\n",
    "    prompt_length = len(prompt_ids)\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=0.0,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([prompt_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    generated_ids = list(output.outputs[0].token_ids)\n",
    "    num_generated = len(generated_ids)\n",
    "    \n",
    "    prefill_signals = extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1])\n",
    "    decode_signals = extract_logprobs_from_output(output, positions=[-3, -2, -1])\n",
    "    \n",
    "    return {\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'generated_ids': generated_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION MODE (TEACHER FORCING)\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_verification(llm, tokenizer, reference_data, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced verification.\n",
    "    For diagonal (same config): use exact reference\n",
    "    For off-diagonal: verify with different config\n",
    "    \n",
    "    vLLM doesn't support true teacher forcing, so we prefill the full sequence\n",
    "    (prompt + generated tokens) and extract logprobs.\n",
    "    \"\"\"\n",
    "    ref_prompt_ids = reference_data['prompt_ids']\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    \n",
    "    prompt_length = len(ref_prompt_ids)\n",
    "    num_generated = len(ref_generated_ids)\n",
    "    \n",
    "    # Full sequence = prompt + generated\n",
    "    full_ids = ref_prompt_ids + ref_generated_ids\n",
    "    full_text = tokenizer.decode(full_ids)\n",
    "    \n",
    "    log_print(f\"      Prompt: {prompt_length}, Gen: {num_generated}\", end=\"\")\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=1,  # Minimal generation, we just want logprobs\n",
    "        temperature=0.0,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([full_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract prefill signals from prompt portion\n",
    "    prefill_signals = {}\n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "            \n",
    "            if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            prefill_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    # Extract decode signals from generated portion\n",
    "    decode_signals = {}\n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            # Position in full sequence (prompt + generated)\n",
    "            full_pos = pos if pos >= 0 else (prompt_length + num_generated) + pos\n",
    "            \n",
    "            if full_pos < 0 or full_pos >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[full_pos]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            decode_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    log_print(f\" → verified\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_logprob_distance_canonical(logprobs1, logprobs2, canonical_ids):\n",
    "    \"\"\"Compute L2 distance between logprobs for a canonical set of token IDs.\"\"\"\n",
    "    lp1 = dict(zip(logprobs1['token_ids'], logprobs1['log_probs']))\n",
    "    lp2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "\n",
    "    for tid in canonical_ids:\n",
    "        if tid in lp1 and tid in lp2:\n",
    "            vec1.append(lp1[tid])\n",
    "            vec2.append(lp2[tid])\n",
    "\n",
    "    if len(vec1) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2):\n",
    "    \"\"\"Compare two signal sets using top 5 token IDs from first signal as canonical.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "\n",
    "    all_dists = []\n",
    "\n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]\n",
    "\n",
    "        # Use top 5 for comparison\n",
    "        canonical_ids = sig1['logprobs']['token_ids'][:5]\n",
    "        dist = compute_logprob_distance_canonical(\n",
    "            sig1['logprobs'], sig2['logprobs'], canonical_ids\n",
    "        )\n",
    "        all_dists.append(dist)\n",
    "\n",
    "    finite_dists = [d for d in all_dists if d != float('inf')]\n",
    "    \n",
    "    return {\n",
    "        'logprobs_mean': np.mean(finite_dists) if finite_dists else float('inf'),\n",
    "        'logprobs_max': max(finite_dists) if finite_dists else float('inf')\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# EQUIVALENCE DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def find_equivalent_pairs(matrix, config_names, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"Find pairs of configs that produce equivalent results (same kernel).\"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n = len(config_names)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((config_names[i], config_names[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, config_names):\n",
    "    \"\"\"Group configs into kernel equivalence classes.\"\"\"\n",
    "    parent = {cfg: cfg for cfg in config_names}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        union(cfg1, cfg2)\n",
    "    \n",
    "    groups = {}\n",
    "    for cfg in config_names:\n",
    "        root = find(cfg)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(cfg)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "# ============================================================================\n",
    "# WITHIN-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_within_hardware(measurements, config_names, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware quantization effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE QUANTIZATION EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    n = len(config_names)\n",
    "    all_matrices = []\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "\n",
    "        matrix = np.zeros((n, n))\n",
    "\n",
    "        for i, cfg_i in enumerate(config_names):\n",
    "            for j, cfg_j in enumerate(config_names):\n",
    "                signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "                sig_i = measurements[cfg_i][ref_idx][signals_key]\n",
    "                sig_j = measurements[cfg_j][ref_idx][signals_key]\n",
    "\n",
    "                if i == j:\n",
    "                    matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    distances = compare_signals(sig_i, sig_j)\n",
    "                    matrix[i, j] = distances['logprobs_mean']\n",
    "\n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, cfg in enumerate(config_names):\n",
    "            row = f\"{cfg:>12} \" + \" \".join(f\"{matrix[i,j]:12.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "\n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, cfg in enumerate(config_names):\n",
    "        row = f\"{cfg:>12} \" + \" \".join(f\"{avg_matrix[i,j]:12.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "\n",
    "    # Find equivalent pairs\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_matrix, config_names)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, config_names)\n",
    "\n",
    "    # Off-diagonal stats\n",
    "    off_diag = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag.append(avg_matrix[i, j])\n",
    "\n",
    "    finite_off_diag = [d for d in off_diag if d != float('inf')]\n",
    "\n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    if finite_off_diag:\n",
    "        log_print(f\"  Mean: {np.mean(finite_off_diag):.2e}\")\n",
    "        log_print(f\"  Range: [{np.min(finite_off_diag):.2e}, {np.max(finite_off_diag):.2e}]\")\n",
    "    \n",
    "    # Check if all zeros\n",
    "    zero_count = sum(1 for d in finite_off_diag if d < EQUIVALENCE_THRESHOLD)\n",
    "    if zero_count == len(finite_off_diag):\n",
    "        log_print(f\"\\n⚠ WARNING: All comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All configs produce identical results (single kernel class)\")\n",
    "\n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "\n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs:\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "\n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'off_diagonal_mean': float(np.mean(finite_off_diag)) if finite_off_diag else 0,\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes]\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_cross_hardware(comparison_results, config_names, signal_source='decode',\n",
    "                           equivalent_pairs=None):\n",
    "    \"\"\"Analyze the comparison matrix and determine detectability.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE QUANTIZATION DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    # Convert to set of both orderings for easy lookup\n",
    "    equiv_set = set()\n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        equiv_set.add((cfg1, cfg2))\n",
    "        equiv_set.add((cfg2, cfg1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_idx']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_config'], result['verify_config'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_matrices = []\n",
    "    n = len(config_names)\n",
    "\n",
    "    # Per-reference matrices\n",
    "    for ref_idx in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        ref_data = by_ref[ref_idx]\n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            for j, verify_cfg in enumerate(config_names):\n",
    "                key = (claimed_cfg, verify_cfg)\n",
    "                if key in ref_data:\n",
    "                    matrix[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(header)\n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            row = f\"{claimed_cfg:>12} \"\n",
    "            for j in range(n):\n",
    "                row += f\"{matrix[i,j]:12.2e} \"\n",
    "            log_print(row)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate matrix\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"  Rows = claimed config, Cols = verified config\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, claimed_cfg in enumerate(config_names):\n",
    "        row = f\"{claimed_cfg:>12} \"\n",
    "        for j in range(n):\n",
    "            row += f\"{avg_matrix[i,j]:12.2e} \"\n",
    "        log_print(row)\n",
    "    \n",
    "    # Compute statistics\n",
    "    diagonal = [avg_matrix[i, i] for i in range(n)]\n",
    "    \n",
    "    # Off-diagonal: exclude equivalent pairs\n",
    "    off_diagonal_all = []\n",
    "    off_diagonal_meaningful = []\n",
    "    excluded_pairs = []\n",
    "    \n",
    "    for i, cfg1 in enumerate(config_names):\n",
    "        for j, cfg2 in enumerate(config_names):\n",
    "            if i != j:\n",
    "                off_diagonal_all.append(avg_matrix[i, j])\n",
    "                if (cfg1, cfg2) in equiv_set:\n",
    "                    excluded_pairs.append((cfg1, cfg2))\n",
    "                else:\n",
    "                    off_diagonal_meaningful.append(avg_matrix[i, j])\n",
    "    \n",
    "    baseline_mean = np.mean(diagonal)\n",
    "    signal_all_mean = np.mean(off_diagonal_all) if off_diagonal_all else 0.0\n",
    "    signal_meaningful_mean = np.mean(off_diagonal_meaningful) if off_diagonal_meaningful else 0.0\n",
    "    \n",
    "    snr_all = signal_all_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    snr_meaningful = signal_meaningful_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same config):\")\n",
    "    log_print(f\"  Mean: {baseline_mean:.2e}\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_all)}\")\n",
    "    log_print(f\"  Mean: {signal_all_mean:.2e}\")\n",
    "    log_print(f\"  SNR (all): {snr_all:.2f}×\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "        log_print(f\"  Total excluded: {len(excluded_pairs)} cells\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_meaningful)}\")\n",
    "    if off_diagonal_meaningful:\n",
    "        log_print(f\"  Mean: {signal_meaningful_mean:.2e}\")\n",
    "        log_print(f\"  SNR (meaningful): {snr_meaningful:.2f}×\")\n",
    "    else:\n",
    "        log_print(\"  No meaningful pairs (all configs are equivalent)\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'baseline_mean': float(baseline_mean),\n",
    "        'signal_all_mean': float(signal_all_mean),\n",
    "        'signal_meaningful_mean': float(signal_meaningful_mean),\n",
    "        'snr_all': float(snr_all),\n",
    "        'snr_meaningful': float(snr_meaningful),\n",
    "        'excluded_pairs': equivalent_pairs,\n",
    "        'n_excluded_cells': len(excluded_pairs),\n",
    "        'n_meaningful_pairs': len(off_diagonal_meaningful)\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TOKEN CONSISTENCY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(measurements, config_names, tokenizer):\n",
    "    \"\"\"Verify generated tokens across quantization configs.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        tokens_by_cfg = {}\n",
    "        for cfg in config_names:\n",
    "            tokens_by_cfg[cfg] = measurements[cfg][ref_idx]['generated_ids']\n",
    "\n",
    "        reference_tokens = tokens_by_cfg[config_names[0]]\n",
    "\n",
    "        for cfg in config_names:\n",
    "            tokens = tokens_by_cfg[cfg]\n",
    "            match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "            decoded_text = tokenizer.decode(tokens[:30])\n",
    "            log_print(f\"  {cfg}:\")\n",
    "            log_print(f\"    Tokens: {len(tokens)}\")\n",
    "            log_print(f\"    First 30: {repr(decoded_text)}...\")\n",
    "            log_print(f\"    {match_str}\")\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def run_reproducibility_check(config_names, prompts):\n",
    "    \"\"\"Measure within-format noise floor from atomics/non-deterministic kernels.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"REPRODUCIBILITY CHECK (NOISE FLOOR MEASUREMENT)\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"Running {REPRODUCIBILITY_RUNS} identical inference passes per config\")\n",
    "    log_print(\"Measures within-format variance from atomics/non-deterministic kernels.\\n\")\n",
    "    \n",
    "    noise_floors = {}\n",
    "    test_prompt = prompts[0]  # Use first prompt for reproducibility check\n",
    "    \n",
    "    for cfg_name in config_names:\n",
    "        log_print(f\"\\n--- Checking: {cfg_name} ---\")\n",
    "        \n",
    "        llm, tokenizer = load_vllm_model(cfg_name)\n",
    "        \n",
    "        # Run multiple times\n",
    "        run_signals = []\n",
    "        for run_idx in range(REPRODUCIBILITY_RUNS):\n",
    "            log_print(f\"  Run {run_idx + 1}: \", end=\"\")\n",
    "            gen_data = run_generation(llm, tokenizer, test_prompt)\n",
    "            run_signals.append({\n",
    "                'generated_ids': gen_data['generated_ids'],\n",
    "                'decode_signals': gen_data['decode_signals'],\n",
    "                'prefill_signals': gen_data['prefill_signals']\n",
    "            })\n",
    "            log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "        \n",
    "        # Compute pairwise distances between all runs\n",
    "        log_print(f\"\\n  Pairwise distances:\")\n",
    "        decode_dists = []\n",
    "        prefill_dists = []\n",
    "        \n",
    "        for i in range(REPRODUCIBILITY_RUNS):\n",
    "            for j in range(i + 1, REPRODUCIBILITY_RUNS):\n",
    "                decode_dist = compare_signals(\n",
    "                    run_signals[i]['decode_signals'],\n",
    "                    run_signals[j]['decode_signals']\n",
    "                )\n",
    "                prefill_dist = compare_signals(\n",
    "                    run_signals[i]['prefill_signals'],\n",
    "                    run_signals[j]['prefill_signals']\n",
    "                )\n",
    "                decode_dists.append(decode_dist['logprobs_mean'])\n",
    "                prefill_dists.append(prefill_dist['logprobs_mean'])\n",
    "                \n",
    "                log_print(f\"    Run {i+1} vs {j+1}: decode={decode_dist['logprobs_mean']:.2e}, prefill={prefill_dist['logprobs_mean']:.2e}\")\n",
    "        \n",
    "        # Compute noise floor stats\n",
    "        finite_decode = [d for d in decode_dists if d != float('inf')]\n",
    "        finite_prefill = [d for d in prefill_dists if d != float('inf')]\n",
    "        \n",
    "        decode_noise = np.mean(finite_decode) if finite_decode else 0\n",
    "        prefill_noise = np.mean(finite_prefill) if finite_prefill else 0\n",
    "        \n",
    "        noise_floors[cfg_name] = {\n",
    "            'decode': decode_noise,\n",
    "            'prefill': prefill_noise\n",
    "        }\n",
    "        \n",
    "        if decode_noise < EQUIVALENCE_THRESHOLD:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: DETERMINISTIC (decode={decode_noise:.2e})\")\n",
    "        else:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: decode={decode_noise:.2e}, prefill={prefill_noise:.2e}\")\n",
    "        \n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(\"\\n\" + \"-\"*40)\n",
    "    log_print(\"NOISE FLOOR SUMMARY\")\n",
    "    log_print(\"-\"*40)\n",
    "    for cfg_name, nf in noise_floors.items():\n",
    "        log_print(f\"  {cfg_name}: decode={nf['decode']:.2e}, prefill={nf['prefill']:.2e}\")\n",
    "    log_print(\"\\nCross-format signal must exceed this noise floor to be detectable.\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    return noise_floors\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    import torch\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    config_names = list(MODEL_CONFIGS.keys())\n",
    "\n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"QUANTIZATION FORMAT DETECTION EXPERIMENT - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"vLLM: {system_info['vllm_version']}\")\n",
    "    log_print(f\"PyTorch: {system_info['torch_version']}\")\n",
    "    log_print(f\"CUDA: {system_info['cuda_version']}\")\n",
    "\n",
    "    log_print(f\"\\nConfigurations:\")\n",
    "    for cfg_name, cfg in MODEL_CONFIGS.items():\n",
    "        log_print(f\"  {cfg_name}: {cfg['model_name']} (quant={cfg['quantization']})\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        log_print(f\"\\nReference file: {REFERENCE_FILE}\")\n",
    "    log_print()\n",
    "\n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if not TEACHER_FORCING:\n",
    "        # ================================================================\n",
    "        # GENERATION MODE\n",
    "        # ================================================================\n",
    "        \n",
    "        # First, load one model to get tokenizer for prompt creation\n",
    "        log_print(\"Loading first model to create prompts...\")\n",
    "        first_llm, tokenizer = load_vllm_model(config_names[0])\n",
    "        prompts = create_prompts_from_pdf(tokenizer, NUM_REFERENCES)\n",
    "        log_print(f\"Created {len(prompts)} prompts\\n\")\n",
    "        del first_llm\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Reproducibility check\n",
    "        noise_floors = {}\n",
    "        if REPRODUCIBILITY_CHECK:\n",
    "            noise_floors = run_reproducibility_check(config_names, prompts)\n",
    "\n",
    "        # Main generation\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"GENERATION MODE\")\n",
    "        log_print(\"=\"*80)\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'max_new_tokens': MAX_NEW_TOKENS,\n",
    "                'tokens_per_slice': TOKENS_PER_SLICE,\n",
    "                'num_references': NUM_REFERENCES,\n",
    "                'top_k_logprobs': TOP_K_LOGPROBS,\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'generations': {}\n",
    "        }\n",
    "\n",
    "        measurements = {}\n",
    "\n",
    "        for cfg_name in config_names:\n",
    "            log_print(f\"\\n--- Config: {cfg_name} ---\")\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(cfg_name)\n",
    "\n",
    "            results['generations'][cfg_name] = []\n",
    "            measurements[cfg_name] = []\n",
    "\n",
    "            for ref_idx, prompt_ids in enumerate(prompts):\n",
    "                log_print(f\"  ref_{ref_idx}: \", end=\"\")\n",
    "                gen_data = run_generation(llm, tokenizer, prompt_ids)\n",
    "\n",
    "                results['generations'][cfg_name].append({\n",
    "                    'ref_idx': ref_idx,\n",
    "                    'prompt_ids': gen_data['prompt_ids'],\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prompt_length': gen_data['prompt_length'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals'],\n",
    "                    'num_generated': gen_data['num_generated']\n",
    "                })\n",
    "\n",
    "                measurements[cfg_name].append({\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals']\n",
    "                })\n",
    "\n",
    "                log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "                decoded = tokenizer.decode(gen_data['generated_ids'][:20])\n",
    "                log_print(f\"    -> {decoded}...\")\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Token consistency check\n",
    "        check_token_consistency(measurements, config_names, tokenizer)\n",
    "\n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(measurements, config_names, 'prefill')\n",
    "        decode_sanity = analyze_within_hardware(measurements, config_names, 'decode')\n",
    "\n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "        \n",
    "        # Store noise floors\n",
    "        if noise_floors:\n",
    "            results['noise_floors'] = noise_floors\n",
    "            \n",
    "            # Compute signal-to-noise ratio\n",
    "            avg_noise_decode = np.mean([nf['decode'] for nf in noise_floors.values()])\n",
    "            cross_format_decode = decode_sanity.get('off_diagonal_mean', 0)\n",
    "            \n",
    "            if avg_noise_decode > 0 and cross_format_decode:\n",
    "                snr_decode = cross_format_decode / avg_noise_decode\n",
    "                log_print(\"\\n\" + \"=\"*80)\n",
    "                log_print(\"SIGNAL-TO-NOISE ANALYSIS\")\n",
    "                log_print(\"=\"*80)\n",
    "                log_print(f\"\\nNoise floor (within-format variance): {avg_noise_decode:.2e}\")\n",
    "                log_print(f\"Cross-format distance: {cross_format_decode:.2e}\")\n",
    "                log_print(f\"SNR: {snr_decode:.1f}×\")\n",
    "                \n",
    "                if snr_decode > 10:\n",
    "                    log_print(f\"→ Quantization format is DETECTABLE (SNR > 10)\")\n",
    "                elif snr_decode > 3:\n",
    "                    log_print(f\"→ Quantization format is MARGINALLY detectable (3 < SNR < 10)\")\n",
    "                else:\n",
    "                    log_print(f\"→ Quantization format is NOT reliably detectable (SNR < 3)\")\n",
    "                \n",
    "                results['snr_analysis'] = {\n",
    "                    'noise_floor': avg_noise_decode,\n",
    "                    'signal': cross_format_decode,\n",
    "                    'snr': snr_decode\n",
    "                }\n",
    "\n",
    "        # Save\n",
    "        filepath = os.path.join(output_dir, f\"quant_generate_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Generation results saved to: {filepath}\")\n",
    "        log_print(f\"\\nNext step: Copy {filepath} to verifier machine\")\n",
    "        log_print(f\"Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\")\n",
    "\n",
    "        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        log_print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    else:\n",
    "        # ================================================================\n",
    "        # VERIFICATION MODE\n",
    "        # ================================================================\n",
    "        log_print(\"Loading reference file...\")\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "            content = content.replace('Infinity', '1e309')\n",
    "            reference = json.loads(content)\n",
    "\n",
    "        ref_env = reference['metadata']['environment']\n",
    "        ref_gpu = ref_env['gpu_name']\n",
    "        log_print(f\"Reference GPU: {ref_gpu}\")\n",
    "        log_print(f\"Verifier GPU:  {system_info['gpu_name']}\")\n",
    "\n",
    "        env_validation = validate_environment_match(ref_env, system_info)\n",
    "\n",
    "        # Load equivalent pairs from reference\n",
    "        prefill_equiv_pairs = reference.get('prefill_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        decode_equiv_pairs = reference.get('decode_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        \n",
    "        # Convert to tuples if stored as lists\n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_equiv_pairs]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_equiv_pairs]\n",
    "        \n",
    "        log_print(f\"\\nLoaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "\n",
    "        comparison_results = []\n",
    "\n",
    "        for verify_cfg in config_names:\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"VERIFYING WITH: {verify_cfg}\")\n",
    "            log_print(\"=\"*80)\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(verify_cfg)\n",
    "\n",
    "            for claimed_cfg in config_names:\n",
    "                log_print(f\"\\n  Claimed config: {claimed_cfg}\")\n",
    "\n",
    "                for ref_idx, gen_data in enumerate(reference['generations'][claimed_cfg]):\n",
    "                    is_diagonal = (claimed_cfg == verify_cfg)\n",
    "\n",
    "                    log_print(f\"    ref_{ref_idx} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "\n",
    "                    verify_result = run_teacher_forced_verification(\n",
    "                        llm, tokenizer, gen_data, is_diagonal\n",
    "                    )\n",
    "\n",
    "                    prefill_distances = compare_signals(\n",
    "                        gen_data['prefill_signals'],\n",
    "                        verify_result['prefill_signals']\n",
    "                    )\n",
    "\n",
    "                    decode_distances = compare_signals(\n",
    "                        gen_data['decode_signals'],\n",
    "                        verify_result['decode_signals']\n",
    "                    )\n",
    "\n",
    "                    log_print(f\"      Prefill: {prefill_distances['logprobs_mean']:.2e}, Decode: {decode_distances['logprobs_mean']:.2e}\")\n",
    "\n",
    "                    comparison_results.append({\n",
    "                        'ref_idx': ref_idx,\n",
    "                        'claimed_config': claimed_cfg,\n",
    "                        'verify_config': verify_cfg,\n",
    "                        'is_diagonal': is_diagonal,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances\n",
    "                    })\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Analyze with equivalent pair exclusion\n",
    "        prefill_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='prefill',\n",
    "            equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='decode',\n",
    "            equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill SNR (meaningful): {prefill_analysis['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode SNR (meaningful):  {decode_analysis['snr_meaningful']:.2f}×\")\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_gpu': ref_gpu,\n",
    "                'verifier_gpu': system_info['gpu_name'],\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'environment_validation': env_validation,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'timestamp': timestamp,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "\n",
    "        filepath = os.path.join(output_dir, f\"quant_verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"EXPERIMENT COMPLETE\")\n",
    "    log_print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf20678-9274-46b1-9c28-af6da6e1c82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
