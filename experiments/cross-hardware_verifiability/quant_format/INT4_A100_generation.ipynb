{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0ab2597-11ef-40c0-a274-207d4613909a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUANTIZATION FORMAT DETECTION EXPERIMENT - GENERATION\n",
      "================================================================================\n",
      "\n",
      "System: 637c3157512e\n",
      "GPU: NVIDIA A100-SXM4-80GB\n",
      "vLLM: 0.11.2\n",
      "PyTorch: 2.9.1+cu128\n",
      "CUDA: 12.8\n",
      "\n",
      "Configurations:\n",
      "  awq: Qwen/Qwen3-8B-AWQ (quant=awq)\n",
      "  awq_marlin: Qwen/Qwen3-8B-AWQ (quant=awq_marlin)\n",
      "  gptq_marlin: JunHowie/Qwen3-8B-GPTQ-Int4 (quant=gptq_marlin)\n",
      "  gptq: JunHowie/Qwen3-8B-GPTQ-Int4 (quant=gptq)\n",
      "\n",
      "Loading first model to create prompts...\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 19:16:33 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 19:16:34 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 19:16:34 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 19:16:35 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 19:16:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "067f7410e2e2437187ebcf714383ef3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-30 19:16:37 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:42 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:35765 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:43 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:43 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.25s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.75s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.97s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:47 [default_loader.py:314] Loading weights took 3.99 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:48 [gpu_model_runner.py:3338] Model loading took 5.7086 GiB memory and 4.516657 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:55 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/951c718543/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:16:55 [backends.py:647] Dynamo bytecode transform time: 6.60 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:17:02 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.826 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:17:03 [monitor.py:34] torch.compile takes 13.42 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:17:05 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:17:05 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:17:05 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 12.37it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 13.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:17:12 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.95 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1384)\u001b[0;0m INFO 11-30 19:17:12 [core.py:250] init engine (profile, create kv cache, warmup model) took 24.58 seconds\n",
      "INFO 11-30 19:17:13 [llm.py:352] Supported tasks: ['generate']\n",
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n",
      "Total source tokens: 120215\n",
      "Prompt structure: 33 prefix + 8000 snippet + 34 suffix = 8067 tokens\n",
      "Created 4 prompts\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 19:17:19.710584355 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "REPRODUCIBILITY CHECK (NOISE FLOOR MEASUREMENT)\n",
      "================================================================================\n",
      "Running 3 identical inference passes per config\n",
      "Measures within-format variance from atomics/non-deterministic kernels.\n",
      "\n",
      "\n",
      "--- Checking: awq ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 19:17:20 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 19:17:20 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 19:17:20 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 19:17:20 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 19:17:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec166071ffe248698f5dcb51065ed7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:42467 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:27 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:27 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.36s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.71s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.96s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:32 [default_loader.py:314] Loading weights took 3.97 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:32 [gpu_model_runner.py:3338] Model loading took 5.7086 GiB memory and 4.514797 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:39 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/951c718543/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:39 [backends.py:647] Dynamo bytecode transform time: 6.57 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:46 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.659 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:47 [monitor.py:34] torch.compile takes 13.23 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:49 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:49 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:49 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 12.39it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 13.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:56 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.95 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1677)\u001b[0;0m INFO 11-30 19:17:56 [core.py:250] init engine (profile, create kv cache, warmup model) took 24.25 seconds\n",
      "INFO 11-30 19:17:57 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c48770bb0849259d0aea0807331d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77f8b41e2074ea895ca0d2bf67956bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af912db7a414eefbfeae2de582646a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543bed0626934be1a386691d912628ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8311f910a2b4fa69fc643e4be61cced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a884f88a0952475890f8d3a56f219652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "  awq noise floor: DETERMINISTIC (decode=0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 19:18:15.761118471 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking: awq_marlin ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 19:18:16 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq_marlin', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 19:18:16 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 19:18:16 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 19:18:16 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-30 19:18:16 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24325c4f8aa34bd292590e753e093cd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:39775 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:24 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:24 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:24 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.43s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  1.78s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:04<00:00,  2.03s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:28 [default_loader.py:314] Loading weights took 4.11 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:29 [gpu_model_runner.py:3338] Model loading took 5.7087 GiB memory and 5.047723 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:36 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/b8c5808a6c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:36 [backends.py:647] Dynamo bytecode transform time: 6.92 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:42 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.447 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:44 [monitor.py:34] torch.compile takes 12.37 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:45 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:46 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:46 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 20.09it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 23.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:50 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took 0.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1965)\u001b[0;0m INFO 11-30 19:18:50 [core.py:250] init engine (profile, create kv cache, warmup model) took 21.18 seconds\n",
      "INFO 11-30 19:18:51 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050e051206274adeb72ca0b50e67dbef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d5e8dee171f4da4be3968fda92e7b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6b66938e734fdaa4c7d6ef6375db43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8969255071e48f2b9a30803ee80e121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abea697534754276ae2cda956a0f57d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364b3dbe97064135a3738a708c732adb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "  awq_marlin noise floor: DETERMINISTIC (decode=0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 19:18:56.989145919 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking: gptq_marlin ---\n",
      "Loading model: JunHowie/Qwen3-8B-GPTQ-Int4\n",
      "  Quantization: gptq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 19:18:57 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq_marlin', 'model': 'JunHowie/Qwen3-8B-GPTQ-Int4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37ca4ceabb54946a2742c21000d2e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 19:18:57 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 19:18:57 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 19:18:57 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 19:18:57 [gptq_marlin.py:228] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 11-30 19:18:57 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167e88e52b3c4b25801b757da25541cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2b8e1380d84c70b78a45168f3d0e5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a1989a85b744a3cb12bce27ddd4a668",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f662a8141f47bbb4e296e139947a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4748621e6c694e94945a09d94db6cacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8251c14e9f4a8b8b050a083b87d605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "007113c546e349699d9d8223ab0bb228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be60427230e647ae9df612e6beb99fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c68c8bb4eee64820ae5953902289ffa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56908793544a463fb37075cd0e589c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:05 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='JunHowie/Qwen3-8B-GPTQ-Int4', speculative_config=None, tokenizer='JunHowie/Qwen3-8B-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=JunHowie/Qwen3-8B-GPTQ-Int4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:06 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:59825 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:06 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:06 [gpu_model_runner.py:3259] Starting to load model JunHowie/Qwen3-8B-GPTQ-Int4...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:06 [gptq_marlin.py:359] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:06 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:06 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:16 [weight_utils.py:441] Time spent downloading weights for JunHowie/Qwen3-8B-GPTQ-Int4: 9.752970 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.30it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.48it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.45it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:18 [default_loader.py:314] Loading weights took 1.46 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:18 [gpu_model_runner.py:3338] Model loading took 5.6834 GiB memory and 11.820137 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:26 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/e9a62dd63f/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:26 [backends.py:647] Dynamo bytecode transform time: 7.37 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:28 [backends.py:251] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:36 [backends.py:282] Compiling a graph for dynamic shape takes 9.59 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:39 [monitor.py:34] torch.compile takes 16.96 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:40 [gpu_worker.py:359] Available KV cache memory: 48.30 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:41 [kv_cache_utils.py:1229] GPU KV cache size: 351,728 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:41 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.59x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 22.21it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 26.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:45 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took 0.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2260)\u001b[0;0m INFO 11-30 19:19:45 [core.py:250] init engine (profile, create kv cache, warmup model) took 26.88 seconds\n",
      "INFO 11-30 19:19:46 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7b8ae1b63e4492b87ccf23ede85124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2ae231f9784df3a06a62d283e3a2a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f958bfda7b4d39857a17a0c2b936db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755999dc1aed4630afa9f838ee239361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d268d318b954de28049b20ee50daf0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6da4509dfbf4de98c9764fd9f55f1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=0.00e+00, prefill=0.00e+00\n",
      "\n",
      "  gptq_marlin noise floor: DETERMINISTIC (decode=0.00e+00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 19:19:51.759206238 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking: gptq ---\n",
      "Loading model: JunHowie/Qwen3-8B-GPTQ-Int4\n",
      "  Quantization: gptq\n",
      "  Dtype: float16\n",
      "INFO 11-30 19:19:52 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq', 'model': 'JunHowie/Qwen3-8B-GPTQ-Int4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 19:19:52 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 19:19:52 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 19:19:52 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 19:19:52 [gptq_marlin.py:232] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "INFO 11-30 19:19:52 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "WARNING 11-30 19:19:52 [gptq.py:99] Currently, the 4-bit gptq_gemm kernel for GPTQ is buggy. Please switch to gptq_marlin or gptq_bitblas.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037f59ecc6134dd792dbf6b3511257f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:19:58 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='JunHowie/Qwen3-8B-GPTQ-Int4', speculative_config=None, tokenizer='JunHowie/Qwen3-8B-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=JunHowie/Qwen3-8B-GPTQ-Int4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:19:59 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:34397 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:19:59 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:19:59 [gpu_model_runner.py:3259] Starting to load model JunHowie/Qwen3-8B-GPTQ-Int4...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:00 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:00 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:02 [default_loader.py:314] Loading weights took 1.72 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:02 [gpu_model_runner.py:3338] Model loading took 5.7086 GiB memory and 2.334543 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:09 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/82bddc70ff/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:09 [backends.py:647] Dynamo bytecode transform time: 6.60 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:11 [backends.py:251] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:21 [backends.py:282] Compiling a graph for dynamic shape takes 11.39 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:22 [monitor.py:34] torch.compile takes 17.99 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:24 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:24 [kv_cache_utils.py:1229] GPU KV cache size: 351,536 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:24 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 18.31it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 21.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:29 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took 0.92 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2580)\u001b[0;0m INFO 11-30 19:20:29 [core.py:250] init engine (profile, create kv cache, warmup model) took 27.40 seconds\n",
      "INFO 11-30 19:20:31 [llm.py:352] Supported tasks: ['generate']\n",
      "  Run 1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93ecf360d5e44e7a5f0b6097df77552",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3460202ec4fa4b6097847cc8bf08186e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af98838b79d40f4a1d353a16dbf8221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc0da1ea5d924fa8b6305c729e00a8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "  Run 3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43b915937d641e3ae3c42c9ca5c6b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca406a30a98e4bcfb0b0059b5036181c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "\n",
      "  Pairwise distances:\n",
      "    Run 1 vs 2: decode=4.67e-02, prefill=0.00e+00\n",
      "    Run 1 vs 3: decode=inf, prefill=0.00e+00\n",
      "    Run 2 vs 3: decode=inf, prefill=0.00e+00\n",
      "\n",
      "  gptq noise floor: decode=4.67e-02, prefill=0.00e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 19:20:35.506649359 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----------------------------------------\n",
      "NOISE FLOOR SUMMARY\n",
      "----------------------------------------\n",
      "  awq: decode=0.00e+00, prefill=0.00e+00\n",
      "  awq_marlin: decode=0.00e+00, prefill=0.00e+00\n",
      "  gptq_marlin: decode=0.00e+00, prefill=0.00e+00\n",
      "  gptq: decode=4.67e-02, prefill=0.00e+00\n",
      "\n",
      "Cross-format signal must exceed this noise floor to be detectable.\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "GENERATION MODE\n",
      "================================================================================\n",
      "\n",
      "--- Config: awq ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 19:20:35 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 19:20:36 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 19:20:36 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 19:20:36 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 19:20:36 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191278830eaa4417b0f0482cd350ef1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:42 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:42 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:41395 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:42 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:43 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:43 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:43 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.42s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.71s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.97s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:47 [default_loader.py:314] Loading weights took 3.98 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:48 [gpu_model_runner.py:3338] Model loading took 5.7086 GiB memory and 4.537122 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:54 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/951c718543/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:20:54 [backends.py:647] Dynamo bytecode transform time: 6.62 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:21:02 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.754 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:21:03 [monitor.py:34] torch.compile takes 13.38 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:21:04 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:21:05 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:21:05 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:04<00:00, 12.14it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:02<00:00, 13.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:21:12 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took 0.95 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2892)\u001b[0;0m INFO 11-30 19:21:12 [core.py:250] init engine (profile, create kv cache, warmup model) took 24.56 seconds\n",
      "INFO 11-30 19:21:13 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb18837c82084fc9be7c05e1db5c49b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813363d783c24bada6a7677e263d9f36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81e7f1dea2764c7f835b6b4d0b164380",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f4fdab31764695b683e5b4c9830b9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45651e1d61e8428f9b74bfdb0102046c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365d9062bbfe4826b25fb7fa691dda24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07101a10a04647188dd9efbaf04cb2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9969e874a7407081436f4d8c929192",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's tackle this query. The user provided a lengthy excerpt from a document and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 19:21:37.519944441 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Config: awq_marlin ---\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 19:21:37 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq_marlin', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 19:21:38 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 19:21:38 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 19:21:38 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-30 19:21:38 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a214fe4969b84809986c515b25e42451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:44 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:44 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:54113 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:44 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:45 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:45 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:45 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:03<00:03,  3.40s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.75s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  2.00s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:49 [default_loader.py:314] Loading weights took 4.04 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:50 [gpu_model_runner.py:3338] Model loading took 5.7087 GiB memory and 4.950577 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:57 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/b8c5808a6c/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:21:57 [backends.py:647] Dynamo bytecode transform time: 6.94 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:22:03 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.409 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:22:05 [monitor.py:34] torch.compile takes 12.35 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:22:06 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:22:07 [kv_cache_utils.py:1229] GPU KV cache size: 351,552 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:22:07 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 20.03it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 23.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:22:11 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took 0.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3171)\u001b[0;0m INFO 11-30 19:22:11 [core.py:250] init engine (profile, create kv cache, warmup model) took 21.20 seconds\n",
      "INFO 11-30 19:22:12 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5051e0e34495492a9d3a831ebb33c581",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1566b6e37564401482979dc7a4b0b430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaadc0e0914f49a2952085e8c45a08a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1096f534f4a46c29454537ee8b165a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbac243537be47029b9696d2ee66ee36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bae3220ce874920ae7b073c51d54d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9ac4f0c06a74183a39512ab55eeac93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7159bd18e76e409f84047e251b504b71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let's tackle this query. The user provided a lengthy excerpt from a document and...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 19:22:19.610785059 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Config: gptq_marlin ---\n",
      "Loading model: JunHowie/Qwen3-8B-GPTQ-Int4\n",
      "  Quantization: gptq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 19:22:20 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq_marlin', 'model': 'JunHowie/Qwen3-8B-GPTQ-Int4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 19:22:20 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 19:22:20 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 19:22:20 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 19:22:20 [gptq_marlin.py:228] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 11-30 19:22:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fe5fd8ed664ef9bc093db05b053092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='JunHowie/Qwen3-8B-GPTQ-Int4', speculative_config=None, tokenizer='JunHowie/Qwen3-8B-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=JunHowie/Qwen3-8B-GPTQ-Int4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:26 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:55407 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:26 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:27 [gpu_model_runner.py:3259] Starting to load model JunHowie/Qwen3-8B-GPTQ-Int4...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:27 [gptq_marlin.py:359] Using MarlinLinearKernel for GPTQMarlinLinearMethod\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:27 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.31it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.51it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.47it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:29 [default_loader.py:314] Loading weights took 1.44 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:29 [gpu_model_runner.py:3338] Model loading took 5.6834 GiB memory and 2.040188 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:37 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/e9a62dd63f/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:37 [backends.py:647] Dynamo bytecode transform time: 7.36 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:43 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.076 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:45 [monitor.py:34] torch.compile takes 12.44 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:46 [gpu_worker.py:359] Available KV cache memory: 48.30 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:47 [kv_cache_utils.py:1229] GPU KV cache size: 351,728 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:47 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.59x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 21.35it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:51 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took 0.66 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3452)\u001b[0;0m INFO 11-30 19:22:51 [core.py:250] init engine (profile, create kv cache, warmup model) took 21.74 seconds\n",
      "INFO 11-30 19:22:52 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "327b55da63034ccaba5e1206b36c1fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b8ae612e8b4e9b8cca3c60d9fb7edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what this document is about. The user provided an...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc65ec7383cd42608b36d73dbc4093d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23f37852abd4eb69af0cb5713f52fd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what this document is about. The user provided an...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc588834b27a4c3180cd8df580533820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b13037769824eeeb900949f1ba2d586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what this document is about. The user provided an...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19f2e0539654a6b9d4e82ef51351b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d70e8aaadf28473b8d73231fb9e0a5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 19:22:59.317912443 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Config: gptq ---\n",
      "Loading model: JunHowie/Qwen3-8B-GPTQ-Int4\n",
      "  Quantization: gptq\n",
      "  Dtype: float16\n",
      "INFO 11-30 19:22:59 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq', 'model': 'JunHowie/Qwen3-8B-GPTQ-Int4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 19:22:59 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 19:22:59 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 19:22:59 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 19:22:59 [gptq_marlin.py:232] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "INFO 11-30 19:22:59 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a0201cca623499a84cd2e2a1a40757f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:06 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='JunHowie/Qwen3-8B-GPTQ-Int4', speculative_config=None, tokenizer='JunHowie/Qwen3-8B-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=JunHowie/Qwen3-8B-GPTQ-Int4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:06 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.2:44779 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:06 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:06 [gpu_model_runner.py:3259] Starting to load model JunHowie/Qwen3-8B-GPTQ-Int4...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:07 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:07 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.36it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.54it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.51it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:09 [default_loader.py:314] Loading weights took 1.41 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:09 [gpu_model_runner.py:3338] Model loading took 5.7086 GiB memory and 2.067772 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:16 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/82bddc70ff/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:16 [backends.py:647] Dynamo bytecode transform time: 6.69 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:23 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.367 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:24 [monitor.py:34] torch.compile takes 13.06 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:26 [gpu_worker.py:359] Available KV cache memory: 48.28 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:26 [kv_cache_utils.py:1229] GPU KV cache size: 351,536 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:26 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 8.58x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 18.81it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 21.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:31 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took 0.92 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3722)\u001b[0;0m INFO 11-30 19:23:31 [core.py:250] init engine (profile, create kv cache, warmup model) took 21.80 seconds\n",
      "INFO 11-30 19:23:32 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14edbf491ce94f22b923d9b8e078e3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e83958a19f40f4b04d4d51ae6b6c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what this document is about. The user provided an...\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5005292bc22429b97e297321c293d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34410b58798b44fdb33d4d259332c26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what this document is about. The user provided an...\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a877aa21874df29e54bb45535d847c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b8465f08004431ad8a96f3462ea531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what this document is about. The user provided an...\n",
      "  ref_3: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec1be2e1406748f5bea76f465d66f03d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "736f366523d3418fa7e04a4b557c01e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tokens\n",
      "    -> <think>\n",
      "Okay, let me try to figure out what kind of document this is and what it's...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 19:23:38.206403909 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TOKEN GENERATION CONSISTENCY CHECK\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: '<think>\\nOkay, let me try to figure out what this document is about. The user provided an excerpt from a document, and I need to determine'...\n",
      "    ✗ DIFFERENT\n",
      "  gptq:\n",
      "    Tokens: 100\n",
      "    First 30: '<think>\\nOkay, let me try to figure out what this document is about. The user provided an excerpt from a document, and I need to determine'...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "--- ref_1 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided a long excerpt from a\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided a long excerpt from a\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: '<think>\\nOkay, let me try to figure out what this document is about. The user provided an excerpt from a document and wants to know the type'...\n",
      "    ✗ DIFFERENT\n",
      "  gptq:\n",
      "    Tokens: 100\n",
      "    First 30: '<think>\\nOkay, let me try to figure out what this document is about. The user provided an excerpt from a document and wants to know the type'...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "--- ref_2 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: '<think>\\nOkay, let me try to figure out what this document is about. The user provided an excerpt from a document, and I need to determine'...\n",
      "    ✗ DIFFERENT\n",
      "  gptq:\n",
      "    Tokens: 100\n",
      "    First 30: '<think>\\nOkay, let me try to figure out what this document is about. The user provided an excerpt from a document, and I need to determine'...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "--- ref_3 ---\n",
      "  awq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's tackle this query. The user provided a lengthy excerpt from a document and is asking about the type of document and its subject\"...\n",
      "    ✓\n",
      "  awq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let's tackle this query. The user provided a lengthy excerpt from a document and is asking about the type of document and its subject\"...\n",
      "    ✓\n",
      "  gptq_marlin:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✗ DIFFERENT\n",
      "  gptq:\n",
      "    Tokens: 100\n",
      "    First 30: \"<think>\\nOkay, let me try to figure out what kind of document this is and what it's about. The user provided an excerpt from a document\"...\n",
      "    ✗ DIFFERENT\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE QUANTIZATION EFFECTS (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     3.50e-02     5.94e+00     5.96e+00\n",
      "  awq_marlin     3.50e-02     0.00e+00     5.94e+00     5.96e+00\n",
      " gptq_marlin     5.71e+00     5.71e+00     0.00e+00     1.63e-01\n",
      "        gptq     5.75e+00     5.75e+00     1.63e-01     0.00e+00\n",
      "\n",
      "--- ref_1 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     4.16e-02     6.36e+00     6.38e+00\n",
      "  awq_marlin     4.16e-02     0.00e+00     6.36e+00     6.38e+00\n",
      " gptq_marlin     5.48e+00     5.48e+00     0.00e+00     2.01e-01\n",
      "        gptq     5.51e+00     5.51e+00     2.01e-01     0.00e+00\n",
      "\n",
      "--- ref_2 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     3.82e-02     6.13e+00     5.98e+00\n",
      "  awq_marlin     3.82e-02     0.00e+00     6.12e+00     5.97e+00\n",
      " gptq_marlin     5.14e+00     5.14e+00     0.00e+00     2.52e-01\n",
      "        gptq     5.98e+00     5.97e+00     2.58e-01     0.00e+00\n",
      "\n",
      "--- ref_3 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     4.46e-02     5.36e+00     5.52e+00\n",
      "  awq_marlin     4.46e-02     0.00e+00     5.38e+00     5.54e+00\n",
      " gptq_marlin     4.20e+00     4.23e+00     0.00e+00     4.07e-01\n",
      "        gptq     4.42e+00     4.45e+00     4.07e-01     0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     3.99e-02     5.95e+00     5.96e+00\n",
      "  awq_marlin     3.99e-02     0.00e+00     5.95e+00     5.96e+00\n",
      " gptq_marlin     5.13e+00     5.14e+00     0.00e+00     2.56e-01\n",
      "        gptq     5.42e+00     5.42e+00     2.57e-01     0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Mean: 3.79e+00\n",
      "  Range: [3.99e-02, 5.96e+00]\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: ['awq']\n",
      "  Class 2: ['awq_marlin']\n",
      "  Class 3: ['gptq_marlin']\n",
      "  Class 4: ['gptq']\n",
      "\n",
      "----------------------------------------\n",
      "PAIRWISE DISTANCE SUMMARY\n",
      "----------------------------------------\n",
      "  awq_vs_awq_marlin: 3.99e-02 → SMALL difference\n",
      "  gptq_marlin_vs_gptq: 2.57e-01 → MODERATE difference\n",
      "  awq_vs_gptq_marlin: 5.54e+00 → LARGE difference\n",
      "  awq_marlin_vs_gptq_marlin: 5.54e+00 → LARGE difference\n",
      "  awq_vs_gptq: 5.69e+00 → LARGE difference\n",
      "  awq_marlin_vs_gptq: 5.69e+00 → LARGE difference\n",
      "\n",
      "================================================================================\n",
      "WITHIN-HARDWARE QUANTIZATION EFFECTS (DECODE)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     2.73e-02          inf          inf\n",
      "  awq_marlin     2.73e-02     0.00e+00          inf          inf\n",
      " gptq_marlin          inf          inf     0.00e+00     9.59e-02\n",
      "        gptq          inf          inf     9.59e-02     0.00e+00\n",
      "\n",
      "--- ref_1 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     2.13e-02     1.27e+00     1.27e+00\n",
      "  awq_marlin     2.13e-02     0.00e+00     1.27e+00     1.27e+00\n",
      " gptq_marlin     1.27e+00     1.27e+00     0.00e+00     1.04e-01\n",
      "        gptq     1.27e+00     1.27e+00     1.04e-01     0.00e+00\n",
      "\n",
      "--- ref_2 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     3.21e-02     2.34e+00     2.36e+00\n",
      "  awq_marlin     3.21e-02     0.00e+00     2.34e+00     2.36e+00\n",
      " gptq_marlin     2.34e+00     2.34e+00     0.00e+00     6.97e-02\n",
      "        gptq     2.36e+00     2.36e+00     6.97e-02     0.00e+00\n",
      "\n",
      "--- ref_3 ---\n",
      "\n",
      "Logprobs (L2 distance):\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     3.19e-02          inf          inf\n",
      "  awq_marlin     3.19e-02     0.00e+00          inf          inf\n",
      " gptq_marlin          inf          inf     0.00e+00     7.42e-02\n",
      "        gptq          inf          inf     7.42e-02     0.00e+00\n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     0.00e+00     2.82e-02          inf          inf\n",
      "  awq_marlin     2.82e-02     0.00e+00          inf          inf\n",
      " gptq_marlin          inf          inf     0.00e+00     8.58e-02\n",
      "        gptq          inf          inf     8.58e-02     0.00e+00\n",
      "\n",
      "Off-diagonal stats:\n",
      "  Mean: 5.70e-02\n",
      "  Range: [2.82e-02, 8.58e-02]\n",
      "\n",
      "Kernel equivalence classes:\n",
      "  Class 1: ['awq']\n",
      "  Class 2: ['awq_marlin']\n",
      "  Class 3: ['gptq_marlin']\n",
      "  Class 4: ['gptq']\n",
      "\n",
      "----------------------------------------\n",
      "PAIRWISE DISTANCE SUMMARY\n",
      "----------------------------------------\n",
      "  awq_vs_awq_marlin: 2.82e-02 → SMALL difference\n",
      "  gptq_marlin_vs_gptq: 8.58e-02 → SMALL difference\n",
      "  awq_vs_gptq_marlin: inf → LARGE difference\n",
      "  awq_vs_gptq: inf → LARGE difference\n",
      "  awq_marlin_vs_gptq_marlin: inf → LARGE difference\n",
      "  awq_marlin_vs_gptq: inf → LARGE difference\n",
      "\n",
      "================================================================================\n",
      "SIGNAL-TO-NOISE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Noise floor (within-format variance): 1.17e-02\n",
      "Cross-format distance: 5.70e-02\n",
      "SNR: 4.9×\n",
      "→ Quantization format is MARGINALLY detectable (3 < SNR < 10)\n",
      "\n",
      "✓ Generation results saved to: /workspace/experiments/quant_generate_20251130_191633.json\n",
      "\n",
      "Next step: Copy /workspace/experiments/quant_generate_20251130_191633.json to verifier machine\n",
      "Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\n",
      "File size: 2.0 MB\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quantization Format Detection Experiment using vLLM.\n",
    "Compares INT4 quantization methods: AWQ vs GPTQ, with and without Marlin kernels.\n",
    "\n",
    "Tests whether quantization format claims can be verified across different GPU architectures\n",
    "using logprob forensics with vLLM inference engine.\n",
    "\n",
    "Signal: logprobs only (vLLM limitation - no key vector access)\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts logprobs, saves to JSON\n",
    "\n",
    "2. Copy JSON to Machine B\n",
    "\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, compares logprobs\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = False\n",
    "REFERENCE_FILE = \"INT4_A100_generation.json\"\n",
    "\n",
    "# Model configurations - all INT4, different quantization methods + kernels\n",
    "# \n",
    "# Available Qwen3-8B INT4 models for vLLM:\n",
    "#   - Qwen/Qwen3-8B-AWQ: AWQ quantization, supports awq/awq_marlin kernels\n",
    "#   - AlphaGaO/Qwen3-8B-GPTQ: GPTQ quantization, pre-converted to Marlin format\n",
    "#   - pytorch/Qwen3-8B-INT4: TorchAO HQQ (requires nightly vllm/torchao)\n",
    "#\n",
    "# Note: OpenVINO/Qwen3-8B-int4-ov is NOT vLLM compatible (OpenVINO backend only)\n",
    "#\n",
    "MODEL_CONFIGS = {\n",
    "    'awq': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'awq_marlin': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'gptq_marlin': {\n",
    "        'model_name': 'JunHowie/Qwen3-8B-GPTQ-Int4',\n",
    "        'quantization': 'gptq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'gptq': {\n",
    "        'model_name': 'JunHowie/Qwen3-8B-GPTQ-Int4',\n",
    "        'quantization': 'gptq',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    # Uncomment if you have nightly vllm + torchao installed:\n",
    "    # 'torchao_hqq': {\n",
    "    #     'model_name': 'pytorch/Qwen3-8B-INT4',\n",
    "    #     'quantization': 'torchao',\n",
    "    #     'dtype': 'bfloat16',\n",
    "    # },\n",
    "}\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "MAX_NEW_TOKENS = 100\n",
    "TOKENS_PER_SLICE = 8000\n",
    "NUM_REFERENCES = 4\n",
    "TOP_K_LOGPROBS = 5\n",
    "\n",
    "# Threshold for considering two configs \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Reproducibility check settings\n",
    "REPRODUCIBILITY_CHECK = True\n",
    "REPRODUCIBILITY_RUNS = 3\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"quant_experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_prompts_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load PDFs and create prompts with different content slices.\n",
    "    Returns list of prompt token ID lists.\n",
    "    \"\"\"\n",
    "    pdf_files = sorted(glob.glob(\"/workspace/*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        pdf_files = sorted(glob.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    content_tokens = tokenizer.encode(all_text)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    tokens_needed = num_references * TOKENS_PER_SLICE\n",
    "    if len(content_tokens) < tokens_needed:\n",
    "        raise ValueError(f\"Need {tokens_needed} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    # Build chat-formatted prompts\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "\n",
    "    suffix = f\"\"\"\\\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    suffix_tokens = tokenizer.encode(suffix)\n",
    "\n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    prompts = []\n",
    "    for i in range(num_references):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt_ids = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        prompts.append(prompt_ids)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import torch\n",
    "    import transformers\n",
    "\n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        import vllm\n",
    "        info[\"vllm_version\"] = vllm.__version__\n",
    "    except (ImportError, AttributeError):\n",
    "        info[\"vllm_version\"] = \"unknown\"\n",
    "\n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    critical_fields = ['vllm_version', 'torch_version', 'cuda_version']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "\n",
    "    mismatches = []\n",
    "\n",
    "    log_print(\"\\nCritical dependencies:\")\n",
    "    for field in critical_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not mismatches:\n",
    "        log_print(\"\\n✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    else:\n",
    "        log_print(\"\\n⚠ ENVIRONMENT MISMATCHES DETECTED\")\n",
    "        log_print(\"  Results may be affected by software differences, not just hardware.\")\n",
    "        return {'valid': False, 'mismatches': mismatches}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_vllm_model(config_name):\n",
    "    \"\"\"Load vLLM model with specified configuration.\"\"\"\n",
    "    config = MODEL_CONFIGS[config_name]\n",
    "    \n",
    "    log_print(f\"Loading model: {config['model_name']}\")\n",
    "    log_print(f\"  Quantization: {config['quantization']}\")\n",
    "    log_print(f\"  Dtype: {config['dtype']}\")\n",
    "    \n",
    "    kwargs = {\n",
    "        'model': config['model_name'],\n",
    "        'download_dir': CACHE_DIR,\n",
    "        'dtype': config['dtype'],\n",
    "        'trust_remote_code': True,\n",
    "        'gpu_memory_utilization': 0.7,\n",
    "        'quantization': config['quantization'],\n",
    "    }\n",
    "    \n",
    "    llm = LLM(**kwargs)\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    \n",
    "    return llm, tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# LOGPROB EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_logprobs_from_output(output, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from vLLM output at specified positions.\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    logprobs_list = output.outputs[0].logprobs\n",
    "    \n",
    "    if logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    num_generated = len(logprobs_list)\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else num_generated + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= num_generated:\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = logprobs_list[actual_idx]\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from prompt positions (for prefill analysis).\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "        \n",
    "        if token_logprobs is None:\n",
    "            continue\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION MODE\n",
    "# ============================================================================\n",
    "\n",
    "def run_generation(llm, tokenizer, prompt_ids):\n",
    "    \"\"\"Run generation and extract prefill + decode signals.\"\"\"\n",
    "    prompt_text = tokenizer.decode(prompt_ids)\n",
    "    prompt_length = len(prompt_ids)\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=0.0,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([prompt_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    generated_ids = list(output.outputs[0].token_ids)\n",
    "    num_generated = len(generated_ids)\n",
    "    \n",
    "    prefill_signals = extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1])\n",
    "    decode_signals = extract_logprobs_from_output(output, positions=[-3, -2, -1])\n",
    "    \n",
    "    return {\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'generated_ids': generated_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION MODE (TEACHER FORCING)\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_verification(llm, tokenizer, reference_data, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced verification.\n",
    "    For diagonal (same config): use exact reference\n",
    "    For off-diagonal: verify with different config\n",
    "    \n",
    "    vLLM doesn't support true teacher forcing, so we prefill the full sequence\n",
    "    (prompt + generated tokens) and extract logprobs.\n",
    "    \"\"\"\n",
    "    ref_prompt_ids = reference_data['prompt_ids']\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    \n",
    "    prompt_length = len(ref_prompt_ids)\n",
    "    num_generated = len(ref_generated_ids)\n",
    "    \n",
    "    # Full sequence = prompt + generated\n",
    "    full_ids = ref_prompt_ids + ref_generated_ids\n",
    "    full_text = tokenizer.decode(full_ids)\n",
    "    \n",
    "    log_print(f\"      Prompt: {prompt_length}, Gen: {num_generated}\", end=\"\")\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=1,  # Minimal generation, we just want logprobs\n",
    "        temperature=0.0,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([full_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract prefill signals from prompt portion\n",
    "    prefill_signals = {}\n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "            \n",
    "            if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            prefill_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    # Extract decode signals from generated portion\n",
    "    decode_signals = {}\n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            # Position in full sequence (prompt + generated)\n",
    "            full_pos = pos if pos >= 0 else (prompt_length + num_generated) + pos\n",
    "            \n",
    "            if full_pos < 0 or full_pos >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[full_pos]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            decode_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    log_print(f\" → verified\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_logprob_distance_canonical(logprobs1, logprobs2, canonical_ids):\n",
    "    \"\"\"Compute L2 distance between logprobs for a canonical set of token IDs.\"\"\"\n",
    "    lp1 = dict(zip(logprobs1['token_ids'], logprobs1['log_probs']))\n",
    "    lp2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "\n",
    "    for tid in canonical_ids:\n",
    "        if tid in lp1 and tid in lp2:\n",
    "            vec1.append(lp1[tid])\n",
    "            vec2.append(lp2[tid])\n",
    "\n",
    "    if len(vec1) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2):\n",
    "    \"\"\"Compare two signal sets using top 5 token IDs from first signal as canonical.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "\n",
    "    all_dists = []\n",
    "\n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]\n",
    "\n",
    "        # Use top 5 for comparison\n",
    "        canonical_ids = sig1['logprobs']['token_ids'][:5]\n",
    "        dist = compute_logprob_distance_canonical(\n",
    "            sig1['logprobs'], sig2['logprobs'], canonical_ids\n",
    "        )\n",
    "        all_dists.append(dist)\n",
    "\n",
    "    finite_dists = [d for d in all_dists if d != float('inf')]\n",
    "    \n",
    "    return {\n",
    "        'logprobs_mean': np.mean(finite_dists) if finite_dists else float('inf'),\n",
    "        'logprobs_max': max(finite_dists) if finite_dists else float('inf')\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# EQUIVALENCE DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def find_equivalent_pairs(matrix, config_names, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"Find pairs of configs that produce equivalent results (same kernel).\"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n = len(config_names)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((config_names[i], config_names[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, config_names):\n",
    "    \"\"\"Group configs into kernel equivalence classes.\"\"\"\n",
    "    parent = {cfg: cfg for cfg in config_names}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        union(cfg1, cfg2)\n",
    "    \n",
    "    groups = {}\n",
    "    for cfg in config_names:\n",
    "        root = find(cfg)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(cfg)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "# ============================================================================\n",
    "# WITHIN-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_within_hardware(measurements, config_names, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware quantization effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE QUANTIZATION EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    n = len(config_names)\n",
    "    all_matrices = []\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "\n",
    "        matrix = np.zeros((n, n))\n",
    "\n",
    "        for i, cfg_i in enumerate(config_names):\n",
    "            for j, cfg_j in enumerate(config_names):\n",
    "                signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "                sig_i = measurements[cfg_i][ref_idx][signals_key]\n",
    "                sig_j = measurements[cfg_j][ref_idx][signals_key]\n",
    "\n",
    "                if i == j:\n",
    "                    matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    distances = compare_signals(sig_i, sig_j)\n",
    "                    matrix[i, j] = distances['logprobs_mean']\n",
    "\n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, cfg in enumerate(config_names):\n",
    "            row = f\"{cfg:>12} \" + \" \".join(f\"{matrix[i,j]:12.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "\n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, cfg in enumerate(config_names):\n",
    "        row = f\"{cfg:>12} \" + \" \".join(f\"{avg_matrix[i,j]:12.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "\n",
    "    # Find equivalent pairs\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_matrix, config_names)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, config_names)\n",
    "\n",
    "    # Off-diagonal stats\n",
    "    off_diag = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag.append(avg_matrix[i, j])\n",
    "\n",
    "    finite_off_diag = [d for d in off_diag if d != float('inf')]\n",
    "\n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    if finite_off_diag:\n",
    "        log_print(f\"  Mean: {np.mean(finite_off_diag):.2e}\")\n",
    "        log_print(f\"  Range: [{np.min(finite_off_diag):.2e}, {np.max(finite_off_diag):.2e}]\")\n",
    "    \n",
    "    # Check if all zeros\n",
    "    zero_count = sum(1 for d in finite_off_diag if d < EQUIVALENCE_THRESHOLD)\n",
    "    if zero_count == len(finite_off_diag):\n",
    "        log_print(f\"\\n⚠ WARNING: All comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All configs produce identical results (single kernel class)\")\n",
    "\n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "\n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs:\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "\n",
    "    # Pairwise distance summary\n",
    "    log_print(f\"\\n\" + \"-\"*40)\n",
    "    log_print(\"PAIRWISE DISTANCE SUMMARY\")\n",
    "    log_print(\"-\"*40)\n",
    "    \n",
    "    pairwise_distances = {}\n",
    "    for i, cfg_i in enumerate(config_names):\n",
    "        for j, cfg_j in enumerate(config_names):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            pair_key = f\"{cfg_i}_vs_{cfg_j}\"\n",
    "            avg_dist = (avg_matrix[i, j] + avg_matrix[j, i]) / 2\n",
    "            pairwise_distances[pair_key] = float(avg_dist)\n",
    "    \n",
    "    for pair_key, dist in sorted(pairwise_distances.items(), key=lambda x: x[1]):\n",
    "        if dist < EQUIVALENCE_THRESHOLD:\n",
    "            status = \"≈ EQUIVALENT (same kernel?)\"\n",
    "        elif dist < 0.1:\n",
    "            status = \"SMALL difference\"\n",
    "        elif dist < 1.0:\n",
    "            status = \"MODERATE difference\"\n",
    "        else:\n",
    "            status = \"LARGE difference\"\n",
    "        log_print(f\"  {pair_key}: {dist:.2e} → {status}\")\n",
    "\n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'off_diagonal_mean': float(np.mean(finite_off_diag)) if finite_off_diag else 0,\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes],\n",
    "        'pairwise_distances': pairwise_distances\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_cross_hardware(comparison_results, config_names, signal_source='decode',\n",
    "                           equivalent_pairs=None):\n",
    "    \"\"\"Analyze the comparison matrix and determine detectability.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE QUANTIZATION DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    # Convert to set of both orderings for easy lookup\n",
    "    equiv_set = set()\n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        equiv_set.add((cfg1, cfg2))\n",
    "        equiv_set.add((cfg2, cfg1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_idx']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_config'], result['verify_config'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_matrices = []\n",
    "    n = len(config_names)\n",
    "\n",
    "    # Per-reference matrices\n",
    "    for ref_idx in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        ref_data = by_ref[ref_idx]\n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            for j, verify_cfg in enumerate(config_names):\n",
    "                key = (claimed_cfg, verify_cfg)\n",
    "                if key in ref_data:\n",
    "                    matrix[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(header)\n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            row = f\"{claimed_cfg:>12} \"\n",
    "            for j in range(n):\n",
    "                row += f\"{matrix[i,j]:12.2e} \"\n",
    "            log_print(row)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate matrix\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"  Rows = claimed config, Cols = verified config\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, claimed_cfg in enumerate(config_names):\n",
    "        row = f\"{claimed_cfg:>12} \"\n",
    "        for j in range(n):\n",
    "            row += f\"{avg_matrix[i,j]:12.2e} \"\n",
    "        log_print(row)\n",
    "    \n",
    "    # Compute statistics\n",
    "    diagonal = [avg_matrix[i, i] for i in range(n)]\n",
    "    \n",
    "    # Off-diagonal: exclude equivalent pairs\n",
    "    off_diagonal_all = []\n",
    "    off_diagonal_meaningful = []\n",
    "    excluded_pairs = []\n",
    "    \n",
    "    for i, cfg1 in enumerate(config_names):\n",
    "        for j, cfg2 in enumerate(config_names):\n",
    "            if i != j:\n",
    "                off_diagonal_all.append(avg_matrix[i, j])\n",
    "                if (cfg1, cfg2) in equiv_set:\n",
    "                    excluded_pairs.append((cfg1, cfg2))\n",
    "                else:\n",
    "                    off_diagonal_meaningful.append(avg_matrix[i, j])\n",
    "    \n",
    "    baseline_mean = np.mean(diagonal)\n",
    "    signal_all_mean = np.mean(off_diagonal_all) if off_diagonal_all else 0.0\n",
    "    signal_meaningful_mean = np.mean(off_diagonal_meaningful) if off_diagonal_meaningful else 0.0\n",
    "    \n",
    "    snr_all = signal_all_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    snr_meaningful = signal_meaningful_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same config):\")\n",
    "    log_print(f\"  Mean: {baseline_mean:.2e}\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_all)}\")\n",
    "    log_print(f\"  Mean: {signal_all_mean:.2e}\")\n",
    "    log_print(f\"  SNR (all): {snr_all:.2f}×\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "        log_print(f\"  Total excluded: {len(excluded_pairs)} cells\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_meaningful)}\")\n",
    "    if off_diagonal_meaningful:\n",
    "        log_print(f\"  Mean: {signal_meaningful_mean:.2e}\")\n",
    "        log_print(f\"  SNR (meaningful): {snr_meaningful:.2f}×\")\n",
    "    else:\n",
    "        log_print(\"  No meaningful pairs (all configs are equivalent)\")\n",
    "    \n",
    "    # Pairwise SNR analysis\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"PAIRWISE SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(\"\\nFor each pair (A, B): signal = distance(A→B), noise = diagonal baseline\")\n",
    "    log_print(\"SNR = signal / noise_floor\\n\")\n",
    "    \n",
    "    pairwise_snr = {}\n",
    "    \n",
    "    for i, cfg_i in enumerate(config_names):\n",
    "        for j, cfg_j in enumerate(config_names):\n",
    "            if i >= j:  # Skip diagonal and lower triangle (symmetric)\n",
    "                continue\n",
    "            \n",
    "            pair_key = f\"{cfg_i}_vs_{cfg_j}\"\n",
    "            \n",
    "            # Get distances in both directions and average\n",
    "            dist_ij = avg_matrix[i, j]\n",
    "            dist_ji = avg_matrix[j, i]\n",
    "            avg_dist = (dist_ij + dist_ji) / 2\n",
    "            \n",
    "            # Use average of the two diagonals as noise floor for this pair\n",
    "            noise_i = avg_matrix[i, i]\n",
    "            noise_j = avg_matrix[j, j]\n",
    "            noise_floor = (noise_i + noise_j) / 2\n",
    "            \n",
    "            snr = avg_dist / noise_floor if noise_floor > 0 else float('inf')\n",
    "            \n",
    "            pairwise_snr[pair_key] = {\n",
    "                'signal': float(avg_dist),\n",
    "                'noise_floor': float(noise_floor),\n",
    "                'snr': float(snr)\n",
    "            }\n",
    "            \n",
    "            # Interpretation\n",
    "            if snr > 100:\n",
    "                interpretation = \"TRIVIALLY DETECTABLE\"\n",
    "            elif snr > 10:\n",
    "                interpretation = \"CLEARLY DETECTABLE\"\n",
    "            elif snr > 3:\n",
    "                interpretation = \"MARGINALLY DETECTABLE\"\n",
    "            else:\n",
    "                interpretation = \"NOT RELIABLY DETECTABLE\"\n",
    "            \n",
    "            log_print(f\"  {cfg_i} vs {cfg_j}:\")\n",
    "            log_print(f\"    Signal: {avg_dist:.2e}, Noise: {noise_floor:.2e}\")\n",
    "            log_print(f\"    SNR: {snr:.1f}× → {interpretation}\")\n",
    "    \n",
    "    # Summary table\n",
    "    log_print(\"\\n\" + \"-\"*40)\n",
    "    log_print(\"PAIRWISE SNR SUMMARY TABLE\")\n",
    "    log_print(\"-\"*40)\n",
    "    log_print(f\"{'Pair':<30} {'Signal':>10} {'Noise':>10} {'SNR':>8}\")\n",
    "    log_print(\"-\"*60)\n",
    "    for pair_key, data in sorted(pairwise_snr.items(), key=lambda x: -x[1]['snr']):\n",
    "        log_print(f\"{pair_key:<30} {data['signal']:>10.2e} {data['noise_floor']:>10.2e} {data['snr']:>8.2f}×\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'baseline_mean': float(baseline_mean),\n",
    "        'signal_all_mean': float(signal_all_mean),\n",
    "        'signal_meaningful_mean': float(signal_meaningful_mean),\n",
    "        'snr_all': float(snr_all),\n",
    "        'snr_meaningful': float(snr_meaningful),\n",
    "        'excluded_pairs': equivalent_pairs,\n",
    "        'n_excluded_cells': len(excluded_pairs),\n",
    "        'n_meaningful_pairs': len(off_diagonal_meaningful),\n",
    "        'pairwise_snr': pairwise_snr\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TOKEN CONSISTENCY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(measurements, config_names, tokenizer):\n",
    "    \"\"\"Verify generated tokens across quantization configs.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        tokens_by_cfg = {}\n",
    "        for cfg in config_names:\n",
    "            tokens_by_cfg[cfg] = measurements[cfg][ref_idx]['generated_ids']\n",
    "\n",
    "        reference_tokens = tokens_by_cfg[config_names[0]]\n",
    "\n",
    "        for cfg in config_names:\n",
    "            tokens = tokens_by_cfg[cfg]\n",
    "            match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "            decoded_text = tokenizer.decode(tokens[:30])\n",
    "            log_print(f\"  {cfg}:\")\n",
    "            log_print(f\"    Tokens: {len(tokens)}\")\n",
    "            log_print(f\"    First 30: {repr(decoded_text)}...\")\n",
    "            log_print(f\"    {match_str}\")\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def run_reproducibility_check(config_names, prompts):\n",
    "    \"\"\"Measure within-format noise floor from atomics/non-deterministic kernels.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"REPRODUCIBILITY CHECK (NOISE FLOOR MEASUREMENT)\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"Running {REPRODUCIBILITY_RUNS} identical inference passes per config\")\n",
    "    log_print(\"Measures within-format variance from atomics/non-deterministic kernels.\\n\")\n",
    "    \n",
    "    noise_floors = {}\n",
    "    test_prompt = prompts[0]  # Use first prompt for reproducibility check\n",
    "    \n",
    "    for cfg_name in config_names:\n",
    "        log_print(f\"\\n--- Checking: {cfg_name} ---\")\n",
    "        \n",
    "        llm, tokenizer = load_vllm_model(cfg_name)\n",
    "        \n",
    "        # Run multiple times\n",
    "        run_signals = []\n",
    "        for run_idx in range(REPRODUCIBILITY_RUNS):\n",
    "            log_print(f\"  Run {run_idx + 1}: \", end=\"\")\n",
    "            gen_data = run_generation(llm, tokenizer, test_prompt)\n",
    "            run_signals.append({\n",
    "                'generated_ids': gen_data['generated_ids'],\n",
    "                'decode_signals': gen_data['decode_signals'],\n",
    "                'prefill_signals': gen_data['prefill_signals']\n",
    "            })\n",
    "            log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "        \n",
    "        # Compute pairwise distances between all runs\n",
    "        log_print(f\"\\n  Pairwise distances:\")\n",
    "        decode_dists = []\n",
    "        prefill_dists = []\n",
    "        \n",
    "        for i in range(REPRODUCIBILITY_RUNS):\n",
    "            for j in range(i + 1, REPRODUCIBILITY_RUNS):\n",
    "                decode_dist = compare_signals(\n",
    "                    run_signals[i]['decode_signals'],\n",
    "                    run_signals[j]['decode_signals']\n",
    "                )\n",
    "                prefill_dist = compare_signals(\n",
    "                    run_signals[i]['prefill_signals'],\n",
    "                    run_signals[j]['prefill_signals']\n",
    "                )\n",
    "                decode_dists.append(decode_dist['logprobs_mean'])\n",
    "                prefill_dists.append(prefill_dist['logprobs_mean'])\n",
    "                \n",
    "                log_print(f\"    Run {i+1} vs {j+1}: decode={decode_dist['logprobs_mean']:.2e}, prefill={prefill_dist['logprobs_mean']:.2e}\")\n",
    "        \n",
    "        # Compute noise floor stats\n",
    "        finite_decode = [d for d in decode_dists if d != float('inf')]\n",
    "        finite_prefill = [d for d in prefill_dists if d != float('inf')]\n",
    "        \n",
    "        decode_noise = np.mean(finite_decode) if finite_decode else 0\n",
    "        prefill_noise = np.mean(finite_prefill) if finite_prefill else 0\n",
    "        \n",
    "        noise_floors[cfg_name] = {\n",
    "            'decode': decode_noise,\n",
    "            'prefill': prefill_noise\n",
    "        }\n",
    "        \n",
    "        if decode_noise < EQUIVALENCE_THRESHOLD:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: DETERMINISTIC (decode={decode_noise:.2e})\")\n",
    "        else:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: decode={decode_noise:.2e}, prefill={prefill_noise:.2e}\")\n",
    "        \n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(\"\\n\" + \"-\"*40)\n",
    "    log_print(\"NOISE FLOOR SUMMARY\")\n",
    "    log_print(\"-\"*40)\n",
    "    for cfg_name, nf in noise_floors.items():\n",
    "        log_print(f\"  {cfg_name}: decode={nf['decode']:.2e}, prefill={nf['prefill']:.2e}\")\n",
    "    log_print(\"\\nCross-format signal must exceed this noise floor to be detectable.\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    return noise_floors\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    import torch\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    config_names = list(MODEL_CONFIGS.keys())\n",
    "\n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"QUANTIZATION FORMAT DETECTION EXPERIMENT - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"vLLM: {system_info['vllm_version']}\")\n",
    "    log_print(f\"PyTorch: {system_info['torch_version']}\")\n",
    "    log_print(f\"CUDA: {system_info['cuda_version']}\")\n",
    "\n",
    "    log_print(f\"\\nConfigurations:\")\n",
    "    for cfg_name, cfg in MODEL_CONFIGS.items():\n",
    "        log_print(f\"  {cfg_name}: {cfg['model_name']} (quant={cfg['quantization']})\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        log_print(f\"\\nReference file: {REFERENCE_FILE}\")\n",
    "    log_print()\n",
    "\n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if not TEACHER_FORCING:\n",
    "        # ================================================================\n",
    "        # GENERATION MODE\n",
    "        # ================================================================\n",
    "        \n",
    "        # First, load one model to get tokenizer for prompt creation\n",
    "        log_print(\"Loading first model to create prompts...\")\n",
    "        first_llm, tokenizer = load_vllm_model(config_names[0])\n",
    "        prompts = create_prompts_from_pdf(tokenizer, NUM_REFERENCES)\n",
    "        log_print(f\"Created {len(prompts)} prompts\\n\")\n",
    "        del first_llm\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Reproducibility check\n",
    "        noise_floors = {}\n",
    "        if REPRODUCIBILITY_CHECK:\n",
    "            noise_floors = run_reproducibility_check(config_names, prompts)\n",
    "\n",
    "        # Main generation\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"GENERATION MODE\")\n",
    "        log_print(\"=\"*80)\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'max_new_tokens': MAX_NEW_TOKENS,\n",
    "                'tokens_per_slice': TOKENS_PER_SLICE,\n",
    "                'num_references': NUM_REFERENCES,\n",
    "                'top_k_logprobs': TOP_K_LOGPROBS,\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'generations': {}\n",
    "        }\n",
    "\n",
    "        measurements = {}\n",
    "\n",
    "        for cfg_name in config_names:\n",
    "            log_print(f\"\\n--- Config: {cfg_name} ---\")\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(cfg_name)\n",
    "\n",
    "            results['generations'][cfg_name] = []\n",
    "            measurements[cfg_name] = []\n",
    "\n",
    "            for ref_idx, prompt_ids in enumerate(prompts):\n",
    "                log_print(f\"  ref_{ref_idx}: \", end=\"\")\n",
    "                gen_data = run_generation(llm, tokenizer, prompt_ids)\n",
    "\n",
    "                results['generations'][cfg_name].append({\n",
    "                    'ref_idx': ref_idx,\n",
    "                    'prompt_ids': gen_data['prompt_ids'],\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prompt_length': gen_data['prompt_length'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals'],\n",
    "                    'num_generated': gen_data['num_generated']\n",
    "                })\n",
    "\n",
    "                measurements[cfg_name].append({\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals']\n",
    "                })\n",
    "\n",
    "                log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "                decoded = tokenizer.decode(gen_data['generated_ids'][:20])\n",
    "                log_print(f\"    -> {decoded}...\")\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Token consistency check\n",
    "        check_token_consistency(measurements, config_names, tokenizer)\n",
    "\n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(measurements, config_names, 'prefill')\n",
    "        decode_sanity = analyze_within_hardware(measurements, config_names, 'decode')\n",
    "\n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "        \n",
    "        # Store noise floors\n",
    "        if noise_floors:\n",
    "            results['noise_floors'] = noise_floors\n",
    "            \n",
    "            # Compute signal-to-noise ratio\n",
    "            avg_noise_decode = np.mean([nf['decode'] for nf in noise_floors.values()])\n",
    "            cross_format_decode = decode_sanity.get('off_diagonal_mean', 0)\n",
    "            \n",
    "            if avg_noise_decode > 0 and cross_format_decode:\n",
    "                snr_decode = cross_format_decode / avg_noise_decode\n",
    "                log_print(\"\\n\" + \"=\"*80)\n",
    "                log_print(\"SIGNAL-TO-NOISE ANALYSIS\")\n",
    "                log_print(\"=\"*80)\n",
    "                log_print(f\"\\nNoise floor (within-format variance): {avg_noise_decode:.2e}\")\n",
    "                log_print(f\"Cross-format distance: {cross_format_decode:.2e}\")\n",
    "                log_print(f\"SNR: {snr_decode:.1f}×\")\n",
    "                \n",
    "                if snr_decode > 10:\n",
    "                    log_print(f\"→ Quantization format is DETECTABLE (SNR > 10)\")\n",
    "                elif snr_decode > 3:\n",
    "                    log_print(f\"→ Quantization format is MARGINALLY detectable (3 < SNR < 10)\")\n",
    "                else:\n",
    "                    log_print(f\"→ Quantization format is NOT reliably detectable (SNR < 3)\")\n",
    "                \n",
    "                results['snr_analysis'] = {\n",
    "                    'noise_floor': avg_noise_decode,\n",
    "                    'signal': cross_format_decode,\n",
    "                    'snr': snr_decode\n",
    "                }\n",
    "\n",
    "        # Save\n",
    "        filepath = os.path.join(output_dir, f\"quant_generate_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Generation results saved to: {filepath}\")\n",
    "        log_print(f\"\\nNext step: Copy {filepath} to verifier machine\")\n",
    "        log_print(f\"Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\")\n",
    "\n",
    "        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        log_print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    else:\n",
    "        # ================================================================\n",
    "        # VERIFICATION MODE\n",
    "        # ================================================================\n",
    "        log_print(\"Loading reference file...\")\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "            content = content.replace('Infinity', '1e309')\n",
    "            reference = json.loads(content)\n",
    "\n",
    "        ref_env = reference['metadata']['environment']\n",
    "        ref_gpu = ref_env['gpu_name']\n",
    "        log_print(f\"Reference GPU: {ref_gpu}\")\n",
    "        log_print(f\"Verifier GPU:  {system_info['gpu_name']}\")\n",
    "\n",
    "        env_validation = validate_environment_match(ref_env, system_info)\n",
    "\n",
    "        # Load equivalent pairs from reference\n",
    "        prefill_equiv_pairs = reference.get('prefill_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        decode_equiv_pairs = reference.get('decode_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        \n",
    "        # Convert to tuples if stored as lists\n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_equiv_pairs]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_equiv_pairs]\n",
    "        \n",
    "        log_print(f\"\\nLoaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "\n",
    "        comparison_results = []\n",
    "\n",
    "        for verify_cfg in config_names:\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"VERIFYING WITH: {verify_cfg}\")\n",
    "            log_print(\"=\"*80)\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(verify_cfg)\n",
    "\n",
    "            for claimed_cfg in config_names:\n",
    "                log_print(f\"\\n  Claimed config: {claimed_cfg}\")\n",
    "\n",
    "                for ref_idx, gen_data in enumerate(reference['generations'][claimed_cfg]):\n",
    "                    is_diagonal = (claimed_cfg == verify_cfg)\n",
    "\n",
    "                    log_print(f\"    ref_{ref_idx} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "\n",
    "                    verify_result = run_teacher_forced_verification(\n",
    "                        llm, tokenizer, gen_data, is_diagonal\n",
    "                    )\n",
    "\n",
    "                    prefill_distances = compare_signals(\n",
    "                        gen_data['prefill_signals'],\n",
    "                        verify_result['prefill_signals']\n",
    "                    )\n",
    "\n",
    "                    decode_distances = compare_signals(\n",
    "                        gen_data['decode_signals'],\n",
    "                        verify_result['decode_signals']\n",
    "                    )\n",
    "\n",
    "                    log_print(f\"      Prefill: {prefill_distances['logprobs_mean']:.2e}, Decode: {decode_distances['logprobs_mean']:.2e}\")\n",
    "\n",
    "                    comparison_results.append({\n",
    "                        'ref_idx': ref_idx,\n",
    "                        'claimed_config': claimed_cfg,\n",
    "                        'verify_config': verify_cfg,\n",
    "                        'is_diagonal': is_diagonal,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances\n",
    "                    })\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Analyze with equivalent pair exclusion\n",
    "        prefill_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='prefill',\n",
    "            equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='decode',\n",
    "            equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill SNR (meaningful): {prefill_analysis['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode SNR (meaningful):  {decode_analysis['snr_meaningful']:.2f}×\")\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_gpu': ref_gpu,\n",
    "                'verifier_gpu': system_info['gpu_name'],\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'environment_validation': env_validation,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'timestamp': timestamp,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "\n",
    "        filepath = os.path.join(output_dir, f\"quant_verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"EXPERIMENT COMPLETE\")\n",
    "    log_print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
