{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ccd1798-0be6-4d26-b542-00a4b71f1848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUANTIZATION FORMAT DETECTION EXPERIMENT - VERIFICATION (teacher-forcing)\n",
      "================================================================================\n",
      "\n",
      "System: 0ce493a31677\n",
      "GPU: NVIDIA H100 80GB HBM3\n",
      "vLLM: 0.11.2\n",
      "PyTorch: 2.9.0+cu128\n",
      "CUDA: 12.8\n",
      "\n",
      "Configurations:\n",
      "  awq: Qwen/Qwen3-8B-AWQ (quant=awq)\n",
      "  awq_marlin: Qwen/Qwen3-8B-AWQ (quant=awq_marlin)\n",
      "  gptq_marlin: AlphaGaO/Qwen3-8B-GPTQ (quant=gptq_marlin)\n",
      "\n",
      "Reference file: INT4_A100_generation.json\n",
      "\n",
      "Loading reference file...\n",
      "Reference GPU: NVIDIA A100-SXM4-80GB\n",
      "Verifier GPU:  NVIDIA H100 80GB HBM3\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Critical dependencies:\n",
      "  ✓ vllm_version: 0.11.2\n",
      "  ✗ torch_version: reference=2.9.1+cu128, verifier=2.9.0+cu128\n",
      "  ✓ cuda_version: 12.8\n",
      "\n",
      "Expected differences (hardware):\n",
      "  ✓ gpu_name: reference=NVIDIA A100-SXM4-80GB, verifier=NVIDIA H100 80GB HBM3\n",
      "  ✓ hostname: reference=637c3157512e, verifier=0ce493a31677\n",
      "\n",
      "⚠ ENVIRONMENT MISMATCHES DETECTED\n",
      "  Results may be affected by software differences, not just hardware.\n",
      "\n",
      "Loaded equivalent pairs from reference:\n",
      "  Prefill: []\n",
      "  Decode: []\n",
      "\n",
      "================================================================================\n",
      "VERIFYING WITH: awq\n",
      "================================================================================\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:42:50 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:42:51 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:42:51 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:42:51 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 09:42:51 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36789c70e94941529cdbcaa478768dda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-30 09:42:53 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:42:57 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:42:58 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:56107 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:42:58 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:42:59 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:42:59 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:42:59 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.19s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.10s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.27s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:02 [default_loader.py:314] Loading weights took 2.57 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:03 [gpu_model_runner.py:3338] Model loading took 5.7071 GiB memory and 3.505089 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:08 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/b0d34aaec5/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:08 [backends.py:647] Dynamo bytecode transform time: 5.43 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:14 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.449 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:15 [monitor.py:34] torch.compile takes 10.88 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:16 [gpu_worker.py:359] Available KV cache memory: 43.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:17 [kv_cache_utils.py:1229] GPU KV cache size: 319,728 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:17 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.81x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m 2025-11-30 09:43:17,146 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m 2025-11-30 09:43:17,159 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:03<00:00, 15.09it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:24 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took -0.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11111)\u001b[0;0m INFO 11-30 09:43:24 [core.py:250] init engine (profile, create kv cache, warmup model) took 20.91 seconds\n",
      "INFO 11-30 09:43:25 [llm.py:352] Supported tasks: ['generate']\n",
      "\n",
      "  Claimed config: awq\n",
      "    ref_0 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b11f98df025d417aa34dda26678241df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bfc2dbd88b4043a60144ddb8ed72ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.41e-02, Decode: 3.46e-02\n",
      "    ref_1 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82faa02b54f4aac8392a2e2a55e6e57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5340565e012444c1a69876da636b8e0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.70e-02, Decode: 2.82e-02\n",
      "    ref_2 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "087d5d38c9554f05a7e0e68feff11a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee85d6007874ff59ef5f74f7ffab2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.12e-02, Decode: 2.54e-02\n",
      "    ref_3 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd2d515ef504f04bc52855f7d8406cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80073009343e4b02940584f4c43d4fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.20e-02, Decode: 4.66e-02\n",
      "\n",
      "  Claimed config: awq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197ca4e7f8b44b68a112897522a62a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe27ca6f8c34e75b2a03ad7be34a54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.86e-02, Decode: 4.33e-02\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a81efc35ff146c3bf9f375660e7b2f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256ec10bd585466cb1bdbc7da5839b6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.26e-02, Decode: 1.98e-02\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6088ca8890e40b1bb4ffc1941487bde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd98fd9e21742dc8af9e2b7f5c1c26a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.44e-02, Decode: 6.72e-03\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18074119084041968ff153009a68a361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca2623acd0940aab3a7bfb8c69d6a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 7.11e-02, Decode: 4.30e-02\n",
      "\n",
      "  Claimed config: gptq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7159c32ea1714c27aee4a5b4c93f4cce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878b4b87313c49d59126aa151ce60031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.72e+00, Decode: 3.24e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce29847a4c849f19196a11d15fa036e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694431a9e23d48b1b63b87a332b42bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.80e+00, Decode: 2.39e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c4c080a373e47a9ade9e5ed5dca0438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c865af35d244488eafe5b5fdd9df485c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.85e+00, Decode: 2.21e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2601694b808f4a4caac9e747d0a1ba69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de320a8bff54b9f9321be61b89efc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.27e+00, Decode: 2.40e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:43:36.421807370 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFYING WITH: awq_marlin\n",
      "================================================================================\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:43:37 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq_marlin', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:43:37 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 09:43:37 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:43:37 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-30 09:43:37 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c3c1442bfe4fe7b52d3cb93e187339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:43 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:44 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:42579 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:44 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:44 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:44 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:44 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.12s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.12s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:02<00:00,  1.27s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:47 [default_loader.py:314] Loading weights took 2.58 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:48 [gpu_model_runner.py:3338] Model loading took 5.7073 GiB memory and 3.485737 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:54 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/d85c3e3665/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:54 [backends.py:647] Dynamo bytecode transform time: 5.83 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:43:59 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.342 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:44:00 [monitor.py:34] torch.compile takes 10.18 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:44:02 [gpu_worker.py:359] Available KV cache memory: 43.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:44:02 [kv_cache_utils.py:1229] GPU KV cache size: 319,728 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:44:02 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.81x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m 2025-11-30 09:44:02,417 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m 2025-11-30 09:44:02,635 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 22.85it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 31.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:44:07 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took -0.75 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=11659)\u001b[0;0m INFO 11-30 09:44:07 [core.py:250] init engine (profile, create kv cache, warmup model) took 18.75 seconds\n",
      "INFO 11-30 09:44:08 [llm.py:352] Supported tasks: ['generate']\n",
      "\n",
      "  Claimed config: awq\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4007c2f5d9674548a065ddfcdfd38cb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547d3f1e17984ed8a6fdc9de574b31f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.56e-02, Decode: 1.26e-02\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1aac957cee843a38bee5b470baa38fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4f54e104d5458f87e46949d4739f74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.25e-02, Decode: 4.43e-02\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb21f2d0a138440b9be1ccda59cc992a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "672698443bd649afac020e71c28b6f69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.96e-02, Decode: 2.19e-02\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67aa2114e6324b1398ae2c0e4f8f34bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccfe65c1e6a745ed97baec6d8aa93ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.61e-02, Decode: 5.01e-02\n",
      "\n",
      "  Claimed config: awq_marlin\n",
      "    ref_0 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a63124c6377f48a0a88f1fed686fc858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9001a5d8aba343a482926af66d900f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 8.24e-02, Decode: 2.42e-02\n",
      "    ref_1 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e61c7ed9854f22b8f31758060a3c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec2c06eb8b8484fb25581ea072c9e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.50e-02, Decode: 4.63e-02\n",
      "    ref_2 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "724a67fed0c04c93aef9edb4832eff5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c6841e3014449ab999b9fa05ee1f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.09e-02, Decode: 2.20e-02\n",
      "    ref_3 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48049282dd3a43d48ecc67585e44f141",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa76f2bd8334a90af4baebcb5beedea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.35e-02, Decode: 2.65e-02\n",
      "\n",
      "  Claimed config: gptq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee804ca000804137b1eeb49c804b239f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5fc61f2267479baad9a147442f03fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.70e+00, Decode: 3.23e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89120958a004d169ace2227ca14d898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71f99cb7b2d449c9170a3e1f2fd0bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.79e+00, Decode: 2.38e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28824170da8e43aab671bd63a024a557",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1f3dc9b68134c839620cbd19d336f14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.87e+00, Decode: 2.24e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a098457cb0534a279ec3b69f7c2186b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2df5b8c0f3482786ad3293a2f2d3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.26e+00, Decode: 2.39e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:44:21.366160573 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFYING WITH: gptq_marlin\n",
      "================================================================================\n",
      "Loading model: AlphaGaO/Qwen3-8B-GPTQ\n",
      "  Quantization: gptq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 09:44:22 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq_marlin', 'model': 'AlphaGaO/Qwen3-8B-GPTQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 09:44:22 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 09:44:22 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 09:44:22 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 09:44:22 [gptq_marlin.py:228] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 11-30 09:44:22 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db5ac704536c4cdf839900a2b0c5ee43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:28 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='AlphaGaO/Qwen3-8B-GPTQ', speculative_config=None, tokenizer='AlphaGaO/Qwen3-8B-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=AlphaGaO/Qwen3-8B-GPTQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:29 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.5:50259 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:29 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:29 [gpu_model_runner.py:3259] Starting to load model AlphaGaO/Qwen3-8B-GPTQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:29 [gptq_marlin.py:359] Using MacheteLinearKernel for GPTQMarlinLinearMethod\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:29 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.69it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.88it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.85it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:31 [default_loader.py:314] Loading weights took 1.16 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:32 [gpu_model_runner.py:3338] Model loading took 5.7137 GiB memory and 2.543057 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:38 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/c543182331/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:38 [backends.py:647] Dynamo bytecode transform time: 6.14 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:44 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.412 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:46 [monitor.py:34] torch.compile takes 11.55 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:47 [gpu_worker.py:359] Available KV cache memory: 43.90 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:47 [kv_cache_utils.py:1229] GPU KV cache size: 319,680 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:47 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 7.80x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m 2025-11-30 09:44:47,759 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m 2025-11-30 09:44:47,771 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 32.70it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 33.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:51 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took -0.78 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=12177)\u001b[0;0m INFO 11-30 09:44:51 [core.py:250] init engine (profile, create kv cache, warmup model) took 18.90 seconds\n",
      "INFO 11-30 09:44:52 [llm.py:352] Supported tasks: ['generate']\n",
      "\n",
      "  Claimed config: awq\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09807fdfb27840598e77cb1fb24cae65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "165878b815154df1b802ad7154762e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.74e+00, Decode: 2.55e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cd7f437d1fc4283afa7914a05449a0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f519475f587947fcb320090355742982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.01e+00, Decode: 2.07e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c68d32f0eb435295f676798162ed7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faf77cf96a63422d94e7dba1c67e0cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.01e+00, Decode: 1.90e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc9a520f41b4eef84c43c25dd6b3ab9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0089e9c1398460481f55f1be88a35d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.26e+00, Decode: 2.86e+00\n",
      "\n",
      "  Claimed config: awq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6846b3e8704bb2b639441646fcf73e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80f8684b0dc34561b644142b689de821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.73e+00, Decode: 2.56e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb46136b81c473090c5d69ebfa1559a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04354713509d4de88f150df82a7a9ceb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.98e+00, Decode: 2.06e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac126e196a504338a9b7433751ee4b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb2ad7c17224673b3633c50e3ee1bc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.01e+00, Decode: 1.89e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6eb3e02b5e54a33a6941d95a43e5391",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8ddc353efa4fb08515babbb1b77429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.28e+00, Decode: 2.86e+00\n",
      "\n",
      "  Claimed config: gptq_marlin\n",
      "    ref_0 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818cf1ea9541439a8963c67fd378b962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78514c001cf48acbf7731e05501d228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 2.29e-02, Decode: 2.26e-02\n",
      "    ref_1 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b74bd543e6c451989a962ba9ff5c27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99f589be247427dbe36fb0f32886407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.34e-02, Decode: 1.95e-02\n",
      "    ref_2 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b84f04d1bd4c41dfb49638fd5a7ab1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153aee1a232c4f28b7b602675d7d017f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.14e-02, Decode: 4.16e-02\n",
      "    ref_3 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cc8f39d861b458c806575119c7e7fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98081c25acf3474aa91df0584b8929c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.51e-02, Decode: 3.91e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 09:45:04.903444074 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CROSS-HARDWARE QUANTIZATION DETECTABILITY (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     4.41e-02     6.56e-02     4.74e+00 \n",
      "  awq_marlin     4.86e-02     8.24e-02     4.73e+00 \n",
      " gptq_marlin     4.72e+00     4.70e+00     2.29e-02 \n",
      "\n",
      "--- ref_1 ---\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     5.70e-02     5.25e-02     4.01e+00 \n",
      "  awq_marlin     3.26e-02     3.50e-02     3.98e+00 \n",
      " gptq_marlin     3.80e+00     3.79e+00     6.34e-02 \n",
      "\n",
      "--- ref_2 ---\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     4.12e-02     3.96e-02     4.01e+00 \n",
      "  awq_marlin     4.44e-02     3.09e-02     4.01e+00 \n",
      " gptq_marlin     3.85e+00     3.87e+00     4.14e-02 \n",
      "\n",
      "--- ref_3 ---\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     4.20e-02     3.61e-02     4.26e+00 \n",
      "  awq_marlin     7.11e-02     6.35e-02     4.28e+00 \n",
      " gptq_marlin     4.27e+00     4.26e+00     5.51e-02 \n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "  Rows = claimed config, Cols = verified config\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     4.61e-02     4.85e-02     4.25e+00 \n",
      "  awq_marlin     4.92e-02     5.29e-02     4.25e+00 \n",
      " gptq_marlin     4.16e+00     4.15e+00     4.57e-02 \n",
      "\n",
      "================================================================================\n",
      "SNR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Diagonal (baseline = cross-hardware, same config):\n",
      "  Mean: 4.82e-02\n",
      "\n",
      "Off-diagonal (all pairs):\n",
      "  Count: 6\n",
      "  Mean: 2.82e+00\n",
      "  SNR (all): 58.44×\n",
      "\n",
      "Off-diagonal (meaningful pairs only):\n",
      "  Count: 6\n",
      "  Mean: 2.82e+00\n",
      "  SNR (meaningful): 58.44×\n",
      "\n",
      "================================================================================\n",
      "PAIRWISE SNR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "For each pair (A, B): signal = distance(A→B), noise = diagonal baseline\n",
      "SNR = signal / noise_floor\n",
      "\n",
      "  awq vs awq_marlin:\n",
      "    Signal: 4.88e-02, Noise: 4.95e-02\n",
      "    SNR: 1.0× → NOT RELIABLY DETECTABLE\n",
      "  awq vs gptq_marlin:\n",
      "    Signal: 4.21e+00, Noise: 4.59e-02\n",
      "    SNR: 91.7× → CLEARLY DETECTABLE\n",
      "  awq_marlin vs gptq_marlin:\n",
      "    Signal: 4.20e+00, Noise: 4.93e-02\n",
      "    SNR: 85.2× → CLEARLY DETECTABLE\n",
      "\n",
      "----------------------------------------\n",
      "PAIRWISE SNR SUMMARY TABLE\n",
      "----------------------------------------\n",
      "Pair                               Signal      Noise      SNR\n",
      "------------------------------------------------------------\n",
      "awq_vs_gptq_marlin               4.21e+00   4.59e-02     91.7×\n",
      "awq_marlin_vs_gptq_marlin        4.20e+00   4.93e-02     85.2×\n",
      "awq_vs_awq_marlin                4.88e-02   4.95e-02      1.0×\n",
      "\n",
      "================================================================================\n",
      "CROSS-HARDWARE QUANTIZATION DETECTABILITY (DECODE)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     3.46e-02     1.26e-02     2.55e+00 \n",
      "  awq_marlin     4.33e-02     2.42e-02     2.56e+00 \n",
      " gptq_marlin     3.24e+00     3.23e+00     2.26e-02 \n",
      "\n",
      "--- ref_1 ---\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     2.82e-02     4.43e-02     2.07e+00 \n",
      "  awq_marlin     1.98e-02     4.63e-02     2.06e+00 \n",
      " gptq_marlin     2.39e+00     2.38e+00     1.95e-02 \n",
      "\n",
      "--- ref_2 ---\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     2.54e-02     2.19e-02     1.90e+00 \n",
      "  awq_marlin     6.72e-03     2.20e-02     1.89e+00 \n",
      " gptq_marlin     2.21e+00     2.24e+00     4.16e-02 \n",
      "\n",
      "--- ref_3 ---\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     4.66e-02     5.01e-02     2.86e+00 \n",
      "  awq_marlin     4.30e-02     2.65e-02     2.86e+00 \n",
      " gptq_marlin     2.40e+00     2.39e+00     3.91e-02 \n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "  Rows = claimed config, Cols = verified config\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin\n",
      "         awq     3.37e-02     3.22e-02     2.34e+00 \n",
      "  awq_marlin     2.82e-02     2.97e-02     2.34e+00 \n",
      " gptq_marlin     2.56e+00     2.56e+00     3.07e-02 \n",
      "\n",
      "================================================================================\n",
      "SNR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Diagonal (baseline = cross-hardware, same config):\n",
      "  Mean: 3.14e-02\n",
      "\n",
      "Off-diagonal (all pairs):\n",
      "  Count: 6\n",
      "  Mean: 1.64e+00\n",
      "  SNR (all): 52.40×\n",
      "\n",
      "Off-diagonal (meaningful pairs only):\n",
      "  Count: 6\n",
      "  Mean: 1.64e+00\n",
      "  SNR (meaningful): 52.40×\n",
      "\n",
      "================================================================================\n",
      "PAIRWISE SNR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "For each pair (A, B): signal = distance(A→B), noise = diagonal baseline\n",
      "SNR = signal / noise_floor\n",
      "\n",
      "  awq vs awq_marlin:\n",
      "    Signal: 3.02e-02, Noise: 3.17e-02\n",
      "    SNR: 1.0× → NOT RELIABLY DETECTABLE\n",
      "  awq vs gptq_marlin:\n",
      "    Signal: 2.45e+00, Noise: 3.22e-02\n",
      "    SNR: 76.1× → CLEARLY DETECTABLE\n",
      "  awq_marlin vs gptq_marlin:\n",
      "    Signal: 2.45e+00, Noise: 3.02e-02\n",
      "    SNR: 81.1× → CLEARLY DETECTABLE\n",
      "\n",
      "----------------------------------------\n",
      "PAIRWISE SNR SUMMARY TABLE\n",
      "----------------------------------------\n",
      "Pair                               Signal      Noise      SNR\n",
      "------------------------------------------------------------\n",
      "awq_marlin_vs_gptq_marlin        2.45e+00   3.02e-02     81.1×\n",
      "awq_vs_gptq_marlin               2.45e+00   3.22e-02     76.1×\n",
      "awq_vs_awq_marlin                3.02e-02   3.17e-02      1.0×\n",
      "\n",
      "================================================================================\n",
      "PREFILL vs DECODE COMPARISON\n",
      "================================================================================\n",
      "Prefill SNR (meaningful): 58.44×\n",
      "Decode SNR (meaningful):  52.40×\n",
      "\n",
      "✓ Results saved to: /workspace/experiments/quant_verify_20251130_094250.json\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quantization Format Detection Experiment using vLLM.\n",
    "Compares INT4 quantization methods: AWQ vs GPTQ, with and without Marlin kernels.\n",
    "\n",
    "Tests whether quantization format claims can be verified across different GPU architectures\n",
    "using logprob forensics with vLLM inference engine.\n",
    "\n",
    "Signal: logprobs only (vLLM limitation - no key vector access)\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts logprobs, saves to JSON\n",
    "\n",
    "2. Copy JSON to Machine B\n",
    "\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, compares logprobs\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = True\n",
    "REFERENCE_FILE = \"INT4_A100_generation.json\"\n",
    "\n",
    "# Model configurations - all INT4, different quantization methods + kernels\n",
    "# \n",
    "# Available Qwen3-8B INT4 models for vLLM:\n",
    "#   - Qwen/Qwen3-8B-AWQ: AWQ quantization, supports awq/awq_marlin kernels\n",
    "#   - AlphaGaO/Qwen3-8B-GPTQ: GPTQ quantization, pre-converted to Marlin format\n",
    "#   - pytorch/Qwen3-8B-INT4: TorchAO HQQ (requires nightly vllm/torchao)\n",
    "#\n",
    "# Note: OpenVINO/Qwen3-8B-int4-ov is NOT vLLM compatible (OpenVINO backend only)\n",
    "#\n",
    "MODEL_CONFIGS = {\n",
    "    'awq': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'awq_marlin': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'gptq_marlin': {\n",
    "        'model_name': 'AlphaGaO/Qwen3-8B-GPTQ',\n",
    "        'quantization': 'gptq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    # Uncomment if you have nightly vllm + torchao installed:\n",
    "    # 'torchao_hqq': {\n",
    "    #     'model_name': 'pytorch/Qwen3-8B-INT4',\n",
    "    #     'quantization': 'torchao',\n",
    "    #     'dtype': 'bfloat16',\n",
    "    # },\n",
    "}\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "MAX_NEW_TOKENS = 100\n",
    "TOKENS_PER_SLICE = 8000\n",
    "NUM_REFERENCES = 4\n",
    "TOP_K_LOGPROBS = 5\n",
    "\n",
    "# Threshold for considering two configs \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Reproducibility check settings\n",
    "REPRODUCIBILITY_CHECK = True\n",
    "REPRODUCIBILITY_RUNS = 3\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"quant_experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_prompts_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load PDFs and create prompts with different content slices.\n",
    "    Returns list of prompt token ID lists.\n",
    "    \"\"\"\n",
    "    pdf_files = sorted(glob.glob(\"/workspace/*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        pdf_files = sorted(glob.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    content_tokens = tokenizer.encode(all_text)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    tokens_needed = num_references * TOKENS_PER_SLICE\n",
    "    if len(content_tokens) < tokens_needed:\n",
    "        raise ValueError(f\"Need {tokens_needed} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    # Build chat-formatted prompts\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "\n",
    "    suffix = f\"\"\"\\\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    suffix_tokens = tokenizer.encode(suffix)\n",
    "\n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    prompts = []\n",
    "    for i in range(num_references):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt_ids = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        prompts.append(prompt_ids)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import torch\n",
    "    import transformers\n",
    "\n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        import vllm\n",
    "        info[\"vllm_version\"] = vllm.__version__\n",
    "    except (ImportError, AttributeError):\n",
    "        info[\"vllm_version\"] = \"unknown\"\n",
    "\n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    critical_fields = ['vllm_version', 'torch_version', 'cuda_version']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "\n",
    "    mismatches = []\n",
    "\n",
    "    log_print(\"\\nCritical dependencies:\")\n",
    "    for field in critical_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not mismatches:\n",
    "        log_print(\"\\n✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    else:\n",
    "        log_print(\"\\n⚠ ENVIRONMENT MISMATCHES DETECTED\")\n",
    "        log_print(\"  Results may be affected by software differences, not just hardware.\")\n",
    "        return {'valid': False, 'mismatches': mismatches}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_vllm_model(config_name):\n",
    "    \"\"\"Load vLLM model with specified configuration.\"\"\"\n",
    "    config = MODEL_CONFIGS[config_name]\n",
    "    \n",
    "    log_print(f\"Loading model: {config['model_name']}\")\n",
    "    log_print(f\"  Quantization: {config['quantization']}\")\n",
    "    log_print(f\"  Dtype: {config['dtype']}\")\n",
    "    \n",
    "    kwargs = {\n",
    "        'model': config['model_name'],\n",
    "        'download_dir': CACHE_DIR,\n",
    "        'dtype': config['dtype'],\n",
    "        'trust_remote_code': True,\n",
    "        'gpu_memory_utilization': 0.7,\n",
    "        'quantization': config['quantization'],\n",
    "    }\n",
    "    \n",
    "    llm = LLM(**kwargs)\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    \n",
    "    return llm, tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# LOGPROB EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_logprobs_from_output(output, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from vLLM output at specified positions.\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    logprobs_list = output.outputs[0].logprobs\n",
    "    \n",
    "    if logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    num_generated = len(logprobs_list)\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else num_generated + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= num_generated:\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = logprobs_list[actual_idx]\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from prompt positions (for prefill analysis).\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "        \n",
    "        if token_logprobs is None:\n",
    "            continue\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION MODE\n",
    "# ============================================================================\n",
    "\n",
    "def run_generation(llm, tokenizer, prompt_ids):\n",
    "    \"\"\"Run generation and extract prefill + decode signals.\"\"\"\n",
    "    prompt_text = tokenizer.decode(prompt_ids)\n",
    "    prompt_length = len(prompt_ids)\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=0.0,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([prompt_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    generated_ids = list(output.outputs[0].token_ids)\n",
    "    num_generated = len(generated_ids)\n",
    "    \n",
    "    prefill_signals = extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1])\n",
    "    decode_signals = extract_logprobs_from_output(output, positions=[-3, -2, -1])\n",
    "    \n",
    "    return {\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'generated_ids': generated_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION MODE (TEACHER FORCING)\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_verification(llm, tokenizer, reference_data, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced verification.\n",
    "    For diagonal (same config): use exact reference\n",
    "    For off-diagonal: verify with different config\n",
    "    \n",
    "    vLLM doesn't support true teacher forcing, so we prefill the full sequence\n",
    "    (prompt + generated tokens) and extract logprobs.\n",
    "    \"\"\"\n",
    "    ref_prompt_ids = reference_data['prompt_ids']\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    \n",
    "    prompt_length = len(ref_prompt_ids)\n",
    "    num_generated = len(ref_generated_ids)\n",
    "    \n",
    "    # Full sequence = prompt + generated\n",
    "    full_ids = ref_prompt_ids + ref_generated_ids\n",
    "    full_text = tokenizer.decode(full_ids)\n",
    "    \n",
    "    log_print(f\"      Prompt: {prompt_length}, Gen: {num_generated}\", end=\"\")\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=1,  # Minimal generation, we just want logprobs\n",
    "        temperature=0.0,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([full_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract prefill signals from prompt portion\n",
    "    prefill_signals = {}\n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "            \n",
    "            if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            prefill_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    # Extract decode signals from generated portion\n",
    "    decode_signals = {}\n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            # Position in full sequence (prompt + generated)\n",
    "            full_pos = pos if pos >= 0 else (prompt_length + num_generated) + pos\n",
    "            \n",
    "            if full_pos < 0 or full_pos >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[full_pos]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            decode_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    log_print(f\" → verified\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_logprob_distance_canonical(logprobs1, logprobs2, canonical_ids):\n",
    "    \"\"\"Compute L2 distance between logprobs for a canonical set of token IDs.\"\"\"\n",
    "    lp1 = dict(zip(logprobs1['token_ids'], logprobs1['log_probs']))\n",
    "    lp2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "\n",
    "    for tid in canonical_ids:\n",
    "        if tid in lp1 and tid in lp2:\n",
    "            vec1.append(lp1[tid])\n",
    "            vec2.append(lp2[tid])\n",
    "\n",
    "    if len(vec1) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2):\n",
    "    \"\"\"Compare two signal sets using top 5 token IDs from first signal as canonical.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "\n",
    "    all_dists = []\n",
    "\n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]\n",
    "\n",
    "        # Use top 5 for comparison\n",
    "        canonical_ids = sig1['logprobs']['token_ids'][:5]\n",
    "        dist = compute_logprob_distance_canonical(\n",
    "            sig1['logprobs'], sig2['logprobs'], canonical_ids\n",
    "        )\n",
    "        all_dists.append(dist)\n",
    "\n",
    "    finite_dists = [d for d in all_dists if d != float('inf')]\n",
    "    \n",
    "    return {\n",
    "        'logprobs_mean': np.mean(finite_dists) if finite_dists else float('inf'),\n",
    "        'logprobs_max': max(finite_dists) if finite_dists else float('inf')\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# EQUIVALENCE DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def find_equivalent_pairs(matrix, config_names, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"Find pairs of configs that produce equivalent results (same kernel).\"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n = len(config_names)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((config_names[i], config_names[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, config_names):\n",
    "    \"\"\"Group configs into kernel equivalence classes.\"\"\"\n",
    "    parent = {cfg: cfg for cfg in config_names}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        union(cfg1, cfg2)\n",
    "    \n",
    "    groups = {}\n",
    "    for cfg in config_names:\n",
    "        root = find(cfg)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(cfg)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "# ============================================================================\n",
    "# WITHIN-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_within_hardware(measurements, config_names, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware quantization effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE QUANTIZATION EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    n = len(config_names)\n",
    "    all_matrices = []\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "\n",
    "        matrix = np.zeros((n, n))\n",
    "\n",
    "        for i, cfg_i in enumerate(config_names):\n",
    "            for j, cfg_j in enumerate(config_names):\n",
    "                signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "                sig_i = measurements[cfg_i][ref_idx][signals_key]\n",
    "                sig_j = measurements[cfg_j][ref_idx][signals_key]\n",
    "\n",
    "                if i == j:\n",
    "                    matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    distances = compare_signals(sig_i, sig_j)\n",
    "                    matrix[i, j] = distances['logprobs_mean']\n",
    "\n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, cfg in enumerate(config_names):\n",
    "            row = f\"{cfg:>12} \" + \" \".join(f\"{matrix[i,j]:12.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "\n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, cfg in enumerate(config_names):\n",
    "        row = f\"{cfg:>12} \" + \" \".join(f\"{avg_matrix[i,j]:12.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "\n",
    "    # Find equivalent pairs\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_matrix, config_names)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, config_names)\n",
    "\n",
    "    # Off-diagonal stats\n",
    "    off_diag = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag.append(avg_matrix[i, j])\n",
    "\n",
    "    finite_off_diag = [d for d in off_diag if d != float('inf')]\n",
    "\n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    if finite_off_diag:\n",
    "        log_print(f\"  Mean: {np.mean(finite_off_diag):.2e}\")\n",
    "        log_print(f\"  Range: [{np.min(finite_off_diag):.2e}, {np.max(finite_off_diag):.2e}]\")\n",
    "    \n",
    "    # Check if all zeros\n",
    "    zero_count = sum(1 for d in finite_off_diag if d < EQUIVALENCE_THRESHOLD)\n",
    "    if zero_count == len(finite_off_diag):\n",
    "        log_print(f\"\\n⚠ WARNING: All comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All configs produce identical results (single kernel class)\")\n",
    "\n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "\n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs:\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "\n",
    "    # Pairwise distance summary\n",
    "    log_print(f\"\\n\" + \"-\"*40)\n",
    "    log_print(\"PAIRWISE DISTANCE SUMMARY\")\n",
    "    log_print(\"-\"*40)\n",
    "    \n",
    "    pairwise_distances = {}\n",
    "    for i, cfg_i in enumerate(config_names):\n",
    "        for j, cfg_j in enumerate(config_names):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            pair_key = f\"{cfg_i}_vs_{cfg_j}\"\n",
    "            avg_dist = (avg_matrix[i, j] + avg_matrix[j, i]) / 2\n",
    "            pairwise_distances[pair_key] = float(avg_dist)\n",
    "    \n",
    "    for pair_key, dist in sorted(pairwise_distances.items(), key=lambda x: x[1]):\n",
    "        if dist < EQUIVALENCE_THRESHOLD:\n",
    "            status = \"≈ EQUIVALENT (same kernel?)\"\n",
    "        elif dist < 0.1:\n",
    "            status = \"SMALL difference\"\n",
    "        elif dist < 1.0:\n",
    "            status = \"MODERATE difference\"\n",
    "        else:\n",
    "            status = \"LARGE difference\"\n",
    "        log_print(f\"  {pair_key}: {dist:.2e} → {status}\")\n",
    "\n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'off_diagonal_mean': float(np.mean(finite_off_diag)) if finite_off_diag else 0,\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes],\n",
    "        'pairwise_distances': pairwise_distances\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_cross_hardware(comparison_results, config_names, signal_source='decode',\n",
    "                           equivalent_pairs=None):\n",
    "    \"\"\"Analyze the comparison matrix and determine detectability.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE QUANTIZATION DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    # Convert to set of both orderings for easy lookup\n",
    "    equiv_set = set()\n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        equiv_set.add((cfg1, cfg2))\n",
    "        equiv_set.add((cfg2, cfg1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_idx']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_config'], result['verify_config'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_matrices = []\n",
    "    n = len(config_names)\n",
    "\n",
    "    # Per-reference matrices\n",
    "    for ref_idx in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        ref_data = by_ref[ref_idx]\n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            for j, verify_cfg in enumerate(config_names):\n",
    "                key = (claimed_cfg, verify_cfg)\n",
    "                if key in ref_data:\n",
    "                    matrix[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(header)\n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            row = f\"{claimed_cfg:>12} \"\n",
    "            for j in range(n):\n",
    "                row += f\"{matrix[i,j]:12.2e} \"\n",
    "            log_print(row)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate matrix\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"  Rows = claimed config, Cols = verified config\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, claimed_cfg in enumerate(config_names):\n",
    "        row = f\"{claimed_cfg:>12} \"\n",
    "        for j in range(n):\n",
    "            row += f\"{avg_matrix[i,j]:12.2e} \"\n",
    "        log_print(row)\n",
    "    \n",
    "    # Compute statistics\n",
    "    diagonal = [avg_matrix[i, i] for i in range(n)]\n",
    "    \n",
    "    # Off-diagonal: exclude equivalent pairs\n",
    "    off_diagonal_all = []\n",
    "    off_diagonal_meaningful = []\n",
    "    excluded_pairs = []\n",
    "    \n",
    "    for i, cfg1 in enumerate(config_names):\n",
    "        for j, cfg2 in enumerate(config_names):\n",
    "            if i != j:\n",
    "                off_diagonal_all.append(avg_matrix[i, j])\n",
    "                if (cfg1, cfg2) in equiv_set:\n",
    "                    excluded_pairs.append((cfg1, cfg2))\n",
    "                else:\n",
    "                    off_diagonal_meaningful.append(avg_matrix[i, j])\n",
    "    \n",
    "    baseline_mean = np.mean(diagonal)\n",
    "    signal_all_mean = np.mean(off_diagonal_all) if off_diagonal_all else 0.0\n",
    "    signal_meaningful_mean = np.mean(off_diagonal_meaningful) if off_diagonal_meaningful else 0.0\n",
    "    \n",
    "    snr_all = signal_all_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    snr_meaningful = signal_meaningful_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same config):\")\n",
    "    log_print(f\"  Mean: {baseline_mean:.2e}\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_all)}\")\n",
    "    log_print(f\"  Mean: {signal_all_mean:.2e}\")\n",
    "    log_print(f\"  SNR (all): {snr_all:.2f}×\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "        log_print(f\"  Total excluded: {len(excluded_pairs)} cells\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_meaningful)}\")\n",
    "    if off_diagonal_meaningful:\n",
    "        log_print(f\"  Mean: {signal_meaningful_mean:.2e}\")\n",
    "        log_print(f\"  SNR (meaningful): {snr_meaningful:.2f}×\")\n",
    "    else:\n",
    "        log_print(\"  No meaningful pairs (all configs are equivalent)\")\n",
    "    \n",
    "    # Pairwise SNR analysis\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"PAIRWISE SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(\"\\nFor each pair (A, B): signal = distance(A→B), noise = diagonal baseline\")\n",
    "    log_print(\"SNR = signal / noise_floor\\n\")\n",
    "    \n",
    "    pairwise_snr = {}\n",
    "    \n",
    "    for i, cfg_i in enumerate(config_names):\n",
    "        for j, cfg_j in enumerate(config_names):\n",
    "            if i >= j:  # Skip diagonal and lower triangle (symmetric)\n",
    "                continue\n",
    "            \n",
    "            pair_key = f\"{cfg_i}_vs_{cfg_j}\"\n",
    "            \n",
    "            # Get distances in both directions and average\n",
    "            dist_ij = avg_matrix[i, j]\n",
    "            dist_ji = avg_matrix[j, i]\n",
    "            avg_dist = (dist_ij + dist_ji) / 2\n",
    "            \n",
    "            # Use average of the two diagonals as noise floor for this pair\n",
    "            noise_i = avg_matrix[i, i]\n",
    "            noise_j = avg_matrix[j, j]\n",
    "            noise_floor = (noise_i + noise_j) / 2\n",
    "            \n",
    "            snr = avg_dist / noise_floor if noise_floor > 0 else float('inf')\n",
    "            \n",
    "            pairwise_snr[pair_key] = {\n",
    "                'signal': float(avg_dist),\n",
    "                'noise_floor': float(noise_floor),\n",
    "                'snr': float(snr)\n",
    "            }\n",
    "            \n",
    "            # Interpretation\n",
    "            if snr > 100:\n",
    "                interpretation = \"TRIVIALLY DETECTABLE\"\n",
    "            elif snr > 10:\n",
    "                interpretation = \"CLEARLY DETECTABLE\"\n",
    "            elif snr > 3:\n",
    "                interpretation = \"MARGINALLY DETECTABLE\"\n",
    "            else:\n",
    "                interpretation = \"NOT RELIABLY DETECTABLE\"\n",
    "            \n",
    "            log_print(f\"  {cfg_i} vs {cfg_j}:\")\n",
    "            log_print(f\"    Signal: {avg_dist:.2e}, Noise: {noise_floor:.2e}\")\n",
    "            log_print(f\"    SNR: {snr:.1f}× → {interpretation}\")\n",
    "    \n",
    "    # Summary table\n",
    "    log_print(\"\\n\" + \"-\"*40)\n",
    "    log_print(\"PAIRWISE SNR SUMMARY TABLE\")\n",
    "    log_print(\"-\"*40)\n",
    "    log_print(f\"{'Pair':<30} {'Signal':>10} {'Noise':>10} {'SNR':>8}\")\n",
    "    log_print(\"-\"*60)\n",
    "    for pair_key, data in sorted(pairwise_snr.items(), key=lambda x: -x[1]['snr']):\n",
    "        log_print(f\"{pair_key:<30} {data['signal']:>10.2e} {data['noise_floor']:>10.2e} {data['snr']:>8.1f}×\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'baseline_mean': float(baseline_mean),\n",
    "        'signal_all_mean': float(signal_all_mean),\n",
    "        'signal_meaningful_mean': float(signal_meaningful_mean),\n",
    "        'snr_all': float(snr_all),\n",
    "        'snr_meaningful': float(snr_meaningful),\n",
    "        'excluded_pairs': equivalent_pairs,\n",
    "        'n_excluded_cells': len(excluded_pairs),\n",
    "        'n_meaningful_pairs': len(off_diagonal_meaningful),\n",
    "        'pairwise_snr': pairwise_snr\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TOKEN CONSISTENCY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(measurements, config_names, tokenizer):\n",
    "    \"\"\"Verify generated tokens across quantization configs.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        tokens_by_cfg = {}\n",
    "        for cfg in config_names:\n",
    "            tokens_by_cfg[cfg] = measurements[cfg][ref_idx]['generated_ids']\n",
    "\n",
    "        reference_tokens = tokens_by_cfg[config_names[0]]\n",
    "\n",
    "        for cfg in config_names:\n",
    "            tokens = tokens_by_cfg[cfg]\n",
    "            match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "            decoded_text = tokenizer.decode(tokens[:30])\n",
    "            log_print(f\"  {cfg}:\")\n",
    "            log_print(f\"    Tokens: {len(tokens)}\")\n",
    "            log_print(f\"    First 30: {repr(decoded_text)}...\")\n",
    "            log_print(f\"    {match_str}\")\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def run_reproducibility_check(config_names, prompts):\n",
    "    \"\"\"Measure within-format noise floor from atomics/non-deterministic kernels.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"REPRODUCIBILITY CHECK (NOISE FLOOR MEASUREMENT)\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"Running {REPRODUCIBILITY_RUNS} identical inference passes per config\")\n",
    "    log_print(\"Measures within-format variance from atomics/non-deterministic kernels.\\n\")\n",
    "    \n",
    "    noise_floors = {}\n",
    "    test_prompt = prompts[0]  # Use first prompt for reproducibility check\n",
    "    \n",
    "    for cfg_name in config_names:\n",
    "        log_print(f\"\\n--- Checking: {cfg_name} ---\")\n",
    "        \n",
    "        llm, tokenizer = load_vllm_model(cfg_name)\n",
    "        \n",
    "        # Run multiple times\n",
    "        run_signals = []\n",
    "        for run_idx in range(REPRODUCIBILITY_RUNS):\n",
    "            log_print(f\"  Run {run_idx + 1}: \", end=\"\")\n",
    "            gen_data = run_generation(llm, tokenizer, test_prompt)\n",
    "            run_signals.append({\n",
    "                'generated_ids': gen_data['generated_ids'],\n",
    "                'decode_signals': gen_data['decode_signals'],\n",
    "                'prefill_signals': gen_data['prefill_signals']\n",
    "            })\n",
    "            log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "        \n",
    "        # Compute pairwise distances between all runs\n",
    "        log_print(f\"\\n  Pairwise distances:\")\n",
    "        decode_dists = []\n",
    "        prefill_dists = []\n",
    "        \n",
    "        for i in range(REPRODUCIBILITY_RUNS):\n",
    "            for j in range(i + 1, REPRODUCIBILITY_RUNS):\n",
    "                decode_dist = compare_signals(\n",
    "                    run_signals[i]['decode_signals'],\n",
    "                    run_signals[j]['decode_signals']\n",
    "                )\n",
    "                prefill_dist = compare_signals(\n",
    "                    run_signals[i]['prefill_signals'],\n",
    "                    run_signals[j]['prefill_signals']\n",
    "                )\n",
    "                decode_dists.append(decode_dist['logprobs_mean'])\n",
    "                prefill_dists.append(prefill_dist['logprobs_mean'])\n",
    "                \n",
    "                log_print(f\"    Run {i+1} vs {j+1}: decode={decode_dist['logprobs_mean']:.2e}, prefill={prefill_dist['logprobs_mean']:.2e}\")\n",
    "        \n",
    "        # Compute noise floor stats\n",
    "        finite_decode = [d for d in decode_dists if d != float('inf')]\n",
    "        finite_prefill = [d for d in prefill_dists if d != float('inf')]\n",
    "        \n",
    "        decode_noise = np.mean(finite_decode) if finite_decode else 0\n",
    "        prefill_noise = np.mean(finite_prefill) if finite_prefill else 0\n",
    "        \n",
    "        noise_floors[cfg_name] = {\n",
    "            'decode': decode_noise,\n",
    "            'prefill': prefill_noise\n",
    "        }\n",
    "        \n",
    "        if decode_noise < EQUIVALENCE_THRESHOLD:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: DETERMINISTIC (decode={decode_noise:.2e})\")\n",
    "        else:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: decode={decode_noise:.2e}, prefill={prefill_noise:.2e}\")\n",
    "        \n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(\"\\n\" + \"-\"*40)\n",
    "    log_print(\"NOISE FLOOR SUMMARY\")\n",
    "    log_print(\"-\"*40)\n",
    "    for cfg_name, nf in noise_floors.items():\n",
    "        log_print(f\"  {cfg_name}: decode={nf['decode']:.2e}, prefill={nf['prefill']:.2e}\")\n",
    "    log_print(\"\\nCross-format signal must exceed this noise floor to be detectable.\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    return noise_floors\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    import torch\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    config_names = list(MODEL_CONFIGS.keys())\n",
    "\n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"QUANTIZATION FORMAT DETECTION EXPERIMENT - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"vLLM: {system_info['vllm_version']}\")\n",
    "    log_print(f\"PyTorch: {system_info['torch_version']}\")\n",
    "    log_print(f\"CUDA: {system_info['cuda_version']}\")\n",
    "\n",
    "    log_print(f\"\\nConfigurations:\")\n",
    "    for cfg_name, cfg in MODEL_CONFIGS.items():\n",
    "        log_print(f\"  {cfg_name}: {cfg['model_name']} (quant={cfg['quantization']})\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        log_print(f\"\\nReference file: {REFERENCE_FILE}\")\n",
    "    log_print()\n",
    "\n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if not TEACHER_FORCING:\n",
    "        # ================================================================\n",
    "        # GENERATION MODE\n",
    "        # ================================================================\n",
    "        \n",
    "        # First, load one model to get tokenizer for prompt creation\n",
    "        log_print(\"Loading first model to create prompts...\")\n",
    "        first_llm, tokenizer = load_vllm_model(config_names[0])\n",
    "        prompts = create_prompts_from_pdf(tokenizer, NUM_REFERENCES)\n",
    "        log_print(f\"Created {len(prompts)} prompts\\n\")\n",
    "        del first_llm\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Reproducibility check\n",
    "        noise_floors = {}\n",
    "        if REPRODUCIBILITY_CHECK:\n",
    "            noise_floors = run_reproducibility_check(config_names, prompts)\n",
    "\n",
    "        # Main generation\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"GENERATION MODE\")\n",
    "        log_print(\"=\"*80)\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'max_new_tokens': MAX_NEW_TOKENS,\n",
    "                'tokens_per_slice': TOKENS_PER_SLICE,\n",
    "                'num_references': NUM_REFERENCES,\n",
    "                'top_k_logprobs': TOP_K_LOGPROBS,\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'generations': {}\n",
    "        }\n",
    "\n",
    "        measurements = {}\n",
    "\n",
    "        for cfg_name in config_names:\n",
    "            log_print(f\"\\n--- Config: {cfg_name} ---\")\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(cfg_name)\n",
    "\n",
    "            results['generations'][cfg_name] = []\n",
    "            measurements[cfg_name] = []\n",
    "\n",
    "            for ref_idx, prompt_ids in enumerate(prompts):\n",
    "                log_print(f\"  ref_{ref_idx}: \", end=\"\")\n",
    "                gen_data = run_generation(llm, tokenizer, prompt_ids)\n",
    "\n",
    "                results['generations'][cfg_name].append({\n",
    "                    'ref_idx': ref_idx,\n",
    "                    'prompt_ids': gen_data['prompt_ids'],\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prompt_length': gen_data['prompt_length'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals'],\n",
    "                    'num_generated': gen_data['num_generated']\n",
    "                })\n",
    "\n",
    "                measurements[cfg_name].append({\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals']\n",
    "                })\n",
    "\n",
    "                log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "                decoded = tokenizer.decode(gen_data['generated_ids'][:20])\n",
    "                log_print(f\"    -> {decoded}...\")\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Token consistency check\n",
    "        check_token_consistency(measurements, config_names, tokenizer)\n",
    "\n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(measurements, config_names, 'prefill')\n",
    "        decode_sanity = analyze_within_hardware(measurements, config_names, 'decode')\n",
    "\n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "        \n",
    "        # Store noise floors\n",
    "        if noise_floors:\n",
    "            results['noise_floors'] = noise_floors\n",
    "            \n",
    "            # Compute signal-to-noise ratio\n",
    "            avg_noise_decode = np.mean([nf['decode'] for nf in noise_floors.values()])\n",
    "            cross_format_decode = decode_sanity.get('off_diagonal_mean', 0)\n",
    "            \n",
    "            if avg_noise_decode > 0 and cross_format_decode:\n",
    "                snr_decode = cross_format_decode / avg_noise_decode\n",
    "                log_print(\"\\n\" + \"=\"*80)\n",
    "                log_print(\"SIGNAL-TO-NOISE ANALYSIS\")\n",
    "                log_print(\"=\"*80)\n",
    "                log_print(f\"\\nNoise floor (within-format variance): {avg_noise_decode:.2e}\")\n",
    "                log_print(f\"Cross-format distance: {cross_format_decode:.2e}\")\n",
    "                log_print(f\"SNR: {snr_decode:.1f}×\")\n",
    "                \n",
    "                if snr_decode > 10:\n",
    "                    log_print(f\"→ Quantization format is DETECTABLE (SNR > 10)\")\n",
    "                elif snr_decode > 3:\n",
    "                    log_print(f\"→ Quantization format is MARGINALLY detectable (3 < SNR < 10)\")\n",
    "                else:\n",
    "                    log_print(f\"→ Quantization format is NOT reliably detectable (SNR < 3)\")\n",
    "                \n",
    "                results['snr_analysis'] = {\n",
    "                    'noise_floor': avg_noise_decode,\n",
    "                    'signal': cross_format_decode,\n",
    "                    'snr': snr_decode\n",
    "                }\n",
    "\n",
    "        # Save\n",
    "        filepath = os.path.join(output_dir, f\"quant_generate_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Generation results saved to: {filepath}\")\n",
    "        log_print(f\"\\nNext step: Copy {filepath} to verifier machine\")\n",
    "        log_print(f\"Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\")\n",
    "\n",
    "        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        log_print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    else:\n",
    "        # ================================================================\n",
    "        # VERIFICATION MODE\n",
    "        # ================================================================\n",
    "        log_print(\"Loading reference file...\")\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "            content = content.replace('Infinity', '1e309')\n",
    "            reference = json.loads(content)\n",
    "\n",
    "        ref_env = reference['metadata']['environment']\n",
    "        ref_gpu = ref_env['gpu_name']\n",
    "        log_print(f\"Reference GPU: {ref_gpu}\")\n",
    "        log_print(f\"Verifier GPU:  {system_info['gpu_name']}\")\n",
    "\n",
    "        env_validation = validate_environment_match(ref_env, system_info)\n",
    "\n",
    "        # Load equivalent pairs from reference\n",
    "        prefill_equiv_pairs = reference.get('prefill_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        decode_equiv_pairs = reference.get('decode_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        \n",
    "        # Convert to tuples if stored as lists\n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_equiv_pairs]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_equiv_pairs]\n",
    "        \n",
    "        log_print(f\"\\nLoaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "\n",
    "        comparison_results = []\n",
    "\n",
    "        for verify_cfg in config_names:\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"VERIFYING WITH: {verify_cfg}\")\n",
    "            log_print(\"=\"*80)\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(verify_cfg)\n",
    "\n",
    "            for claimed_cfg in config_names:\n",
    "                log_print(f\"\\n  Claimed config: {claimed_cfg}\")\n",
    "\n",
    "                for ref_idx, gen_data in enumerate(reference['generations'][claimed_cfg]):\n",
    "                    is_diagonal = (claimed_cfg == verify_cfg)\n",
    "\n",
    "                    log_print(f\"    ref_{ref_idx} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "\n",
    "                    verify_result = run_teacher_forced_verification(\n",
    "                        llm, tokenizer, gen_data, is_diagonal\n",
    "                    )\n",
    "\n",
    "                    prefill_distances = compare_signals(\n",
    "                        gen_data['prefill_signals'],\n",
    "                        verify_result['prefill_signals']\n",
    "                    )\n",
    "\n",
    "                    decode_distances = compare_signals(\n",
    "                        gen_data['decode_signals'],\n",
    "                        verify_result['decode_signals']\n",
    "                    )\n",
    "\n",
    "                    log_print(f\"      Prefill: {prefill_distances['logprobs_mean']:.2e}, Decode: {decode_distances['logprobs_mean']:.2e}\")\n",
    "\n",
    "                    comparison_results.append({\n",
    "                        'ref_idx': ref_idx,\n",
    "                        'claimed_config': claimed_cfg,\n",
    "                        'verify_config': verify_cfg,\n",
    "                        'is_diagonal': is_diagonal,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances\n",
    "                    })\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Analyze with equivalent pair exclusion\n",
    "        prefill_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='prefill',\n",
    "            equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='decode',\n",
    "            equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill SNR (meaningful): {prefill_analysis['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode SNR (meaningful):  {decode_analysis['snr_meaningful']:.2f}×\")\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_gpu': ref_gpu,\n",
    "                'verifier_gpu': system_info['gpu_name'],\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'environment_validation': env_validation,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'timestamp': timestamp,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "\n",
    "        filepath = os.path.join(output_dir, f\"quant_verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"EXPERIMENT COMPLETE\")\n",
    "    log_print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b916a843-2f06-4a53-a874-b7b3415a4080",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
