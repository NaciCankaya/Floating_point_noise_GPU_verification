{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff8297e0-f772-4d75-90e3-a481ac531be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "QUANTIZATION FORMAT DETECTION EXPERIMENT - VERIFICATION (teacher-forcing)\n",
      "================================================================================\n",
      "\n",
      "System: 17ae938f3494\n",
      "GPU: NVIDIA H100 NVL\n",
      "vLLM: 0.11.2\n",
      "PyTorch: 2.9.0+cu128\n",
      "CUDA: 12.8\n",
      "\n",
      "Configurations:\n",
      "  awq: Qwen/Qwen3-8B-AWQ (quant=awq)\n",
      "  awq_marlin: Qwen/Qwen3-8B-AWQ (quant=awq_marlin)\n",
      "  gptq_marlin: JunHowie/Qwen3-8B-GPTQ-Int4 (quant=gptq_marlin)\n",
      "  gptq: JunHowie/Qwen3-8B-GPTQ-Int4 (quant=gptq)\n",
      "\n",
      "Reference file: INT4_A100_generation.json\n",
      "\n",
      "Loading reference file...\n",
      "Reference GPU: NVIDIA A100-SXM4-80GB\n",
      "Verifier GPU:  NVIDIA H100 NVL\n",
      "\n",
      "================================================================================\n",
      "ENVIRONMENT VALIDATION\n",
      "================================================================================\n",
      "\n",
      "Critical dependencies:\n",
      "  ✓ vllm_version: 0.11.2\n",
      "  ✗ torch_version: reference=2.9.1+cu128, verifier=2.9.0+cu128\n",
      "  ✓ cuda_version: 12.8\n",
      "\n",
      "Expected differences (hardware):\n",
      "  ✓ gpu_name: reference=NVIDIA A100-SXM4-80GB, verifier=NVIDIA H100 NVL\n",
      "  ✓ hostname: reference=637c3157512e, verifier=17ae938f3494\n",
      "\n",
      "⚠ ENVIRONMENT MISMATCHES DETECTED\n",
      "  Results may be affected by software differences, not just hardware.\n",
      "\n",
      "Loaded equivalent pairs from reference:\n",
      "  Prefill: []\n",
      "  Decode: []\n",
      "\n",
      "================================================================================\n",
      "VERIFYING WITH: awq\n",
      "================================================================================\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq\n",
      "  Dtype: float16\n",
      "INFO 11-30 21:58:27 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0b6fdce1ea4251bb4edb1b4a49ef0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 21:58:34 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 21:58:34 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 21:58:34 [awq_marlin.py:166] Detected that the model can run with awq_marlin, however you specified quantization=awq explicitly, so forcing awq. Use quantization=awq_marlin for faster inference\n",
      "INFO 11-30 21:58:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b45597b9b5044a69e421cc1b894f5da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06f06fc8a4248e4aba5a88a46813ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b3ca3190d640fba4713858b35f85bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff155108672d4661955908ac1908649f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8323f99cd99941b1bc80ea781642396e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1545b598213c45c88899838150d99369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "252fc0606f804d9aa12142360d7b8906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-30 21:58:38 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:58:43 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:58:43 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.6:46365 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:58:43 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:58:44 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m [2025-11-30 21:58:44] INFO _optional_torch_c_dlpack.py:119: JIT-compiling torch-c-dlpack-ext to cache...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:01 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:01 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:08 [weight_utils.py:441] Time spent downloading weights for Qwen/Qwen3-8B-AWQ: 7.134800 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.71s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.43s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.62s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:11 [default_loader.py:314] Loading weights took 3.27 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:12 [gpu_model_runner.py:3338] Model loading took 5.7071 GiB memory and 27.448767 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:18 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/3818bcf7fb/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:18 [backends.py:647] Dynamo bytecode transform time: 6.18 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:25 [backends.py:251] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:36 [backends.py:282] Compiling a graph for dynamic shape takes 17.53 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:39 [monitor.py:34] torch.compile takes 23.71 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:40 [gpu_worker.py:359] Available KV cache memory: 53.64 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:40 [kv_cache_utils.py:1229] GPU KV cache size: 390,624 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:40 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 9.54x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m 2025-11-30 21:59:40,950 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m 2025-11-30 21:59:40,964 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:03<00:00, 14.69it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:02<00:00, 17.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:48 [gpu_model_runner.py:4244] Graph capturing finished in 7 secs, took -0.89 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=1827)\u001b[0;0m INFO 11-30 21:59:48 [core.py:250] init engine (profile, create kv cache, warmup model) took 35.92 seconds\n",
      "INFO 11-30 21:59:49 [llm.py:352] Supported tasks: ['generate']\n",
      "\n",
      "  Claimed config: awq\n",
      "    ref_0 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5ea8560c0a4282800e21fe254e8186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1460dc53b146b894af57d25da5a656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.41e-02, Decode: 3.46e-02\n",
      "    ref_1 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "112c81164e4543169d2d9892b26928ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa335b1150340918be25a88555b709f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.70e-02, Decode: 2.82e-02\n",
      "    ref_2 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e8cb27ca8b45e8a025e359cc5f5cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aace70c1f6a4641a584d57567e9faaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.12e-02, Decode: 2.54e-02\n",
      "    ref_3 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d41363543cf54fb9914f6616b1384f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6800fad6e1247378cf9df544db95c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.20e-02, Decode: 4.66e-02\n",
      "\n",
      "  Claimed config: awq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f1d8da2ea3441487ae429944960da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc1e8afd9f6487d94bee383c87531f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.86e-02, Decode: 4.33e-02\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ffaf6ed91b4d919061159638045db9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b09137af0149bb9040d452097c6270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.26e-02, Decode: 1.98e-02\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7687f9db18094da4b9055c738d83d1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93a4a96d4a154aeda673ae706c13b3f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.44e-02, Decode: 6.72e-03\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e41b4c9e11e489d95aa9773c6b43d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3397a292964156a7777c6dcf2c633e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 7.11e-02, Decode: 4.30e-02\n",
      "\n",
      "  Claimed config: gptq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc4fb3fcd02b427ab0b763e1d5c8b6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9922376c9f342e883a7dffb315ff506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.70e+00, Decode: 2.14e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa21339c5a5141669ec33b31a3100564",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4a086ed36b4c2d8791f3de909a28a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.50e+00, Decode: 1.38e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf9133f36474f458faf906c431b37e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53cece1d08c4b91902a544a9be1c1c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.12e+00, Decode: 1.59e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cf7c0436bf0406199e425385de39e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f0be29c6744ba3a6a79788ded6908a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.19e+00, Decode: 2.76e+00\n",
      "\n",
      "  Claimed config: gptq\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c553ad36232c42898df7c30bfb85f491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b1329fcc5c484aa7ccb156a93cb644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.75e+00, Decode: 2.14e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89613730144742c5a1e291065f8be51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837dccd0461b444ba10777f58412493e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.53e+00, Decode: 1.36e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49b1b9d391364cf788f1b05fcf67544a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce1f2dc25eb646c2a6e98e96e48f1ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.95e+00, Decode: 1.60e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9586a6b16e684b14b24b8fea8973ebd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85255ea422c8413f9a7b9d2356fb8aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.41e+00, Decode: 2.75e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 22:00:10.698402546 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFYING WITH: awq_marlin\n",
      "================================================================================\n",
      "Loading model: Qwen/Qwen3-8B-AWQ\n",
      "  Quantization: awq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 22:00:11 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'awq_marlin', 'model': 'Qwen/Qwen3-8B-AWQ'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 22:00:11 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-30 22:00:11 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 22:00:11 [awq_marlin.py:162] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\n",
      "INFO 11-30 22:00:11 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1551a15d5cb44ac79cea76edd1bf902a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:18 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-8B-AWQ', speculative_config=None, tokenizer='Qwen/Qwen3-8B-AWQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-8B-AWQ, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:18 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.6:60841 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:18 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:18 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-8B-AWQ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:19 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:19 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:02<00:02,  2.67s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.45s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:03<00:00,  1.63s/it]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:23 [default_loader.py:314] Loading weights took 3.30 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:23 [gpu_model_runner.py:3338] Model loading took 5.7073 GiB memory and 4.247288 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:30 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/69dba1f5c9/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:30 [backends.py:647] Dynamo bytecode transform time: 6.33 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:32 [backends.py:251] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:41 [backends.py:282] Compiling a graph for dynamic shape takes 10.61 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:43 [monitor.py:34] torch.compile takes 16.94 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:45 [gpu_worker.py:359] Available KV cache memory: 53.64 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:45 [kv_cache_utils.py:1229] GPU KV cache size: 390,608 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:45 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 9.54x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m 2025-11-30 22:00:45,969 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m 2025-11-30 22:00:46,280 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 22.09it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 27.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:51 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took -1.04 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2862)\u001b[0;0m INFO 11-30 22:00:51 [core.py:250] init engine (profile, create kv cache, warmup model) took 27.37 seconds\n",
      "INFO 11-30 22:00:52 [llm.py:352] Supported tasks: ['generate']\n",
      "\n",
      "  Claimed config: awq\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0453e7c8a4a74bcb95c27857245a0e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62a04f21b0894cdd9a25e47c95c48da4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.56e-02, Decode: 1.26e-02\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab16c47a2dbd42b99bf595747957b052",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ca6e1973f94008b303870b386a33b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.25e-02, Decode: 4.43e-02\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3675df6966b0460d8cbfb9dda39cc9cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65509180dc634dfdb5e7d1a7462c49bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.96e-02, Decode: 2.19e-02\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ca276a2f8146969b5836348ddd0e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da24b57d4a2f47dca27c2d8f9ed567c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.61e-02, Decode: 5.01e-02\n",
      "\n",
      "  Claimed config: awq_marlin\n",
      "    ref_0 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ed3ce2fc514f82acb8249c40e53e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16f1028cb5c42c786528a6027f16c43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 8.24e-02, Decode: 2.42e-02\n",
      "    ref_1 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a819574e97e481493b370b767ef2c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59031d664bdc42718ff66069f66664df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.50e-02, Decode: 4.63e-02\n",
      "    ref_2 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb6e494ccc84a72b5e451d3e465f5ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e1b0dca2264b0c8f429d8fd0fd891a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.09e-02, Decode: 2.20e-02\n",
      "    ref_3 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f428fff0f2430ba794a8a85cd57435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea08877ba7a404e83eea830db51e924",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.35e-02, Decode: 2.65e-02\n",
      "\n",
      "  Claimed config: gptq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89aec8cec328423ead243465730c1d5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96be6b000b42463ea628ca14159181f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.70e+00, Decode: 2.12e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66fb528baff4df78d72e4e5c22a1857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e3a4f2685f48928d72392494ba7b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.49e+00, Decode: 1.38e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca0b2947bfa54875ae74bc328944222a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8537a8dff2b74bb895e415ba8eb22d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.14e+00, Decode: 1.58e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973b6cc41855476492f988a9b3b2a2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e11f8d2451a420f8e2d01a3e7ac934c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.20e+00, Decode: 2.73e+00\n",
      "\n",
      "  Claimed config: gptq\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0042fe51f6ef4beb86f72872769c3938",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9646102aee3243928d175412fd396118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.74e+00, Decode: 2.12e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "706fceb954634677905875ec11f2609b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1912810ef0345ac8a54869cd4bde417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.52e+00, Decode: 1.36e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6a621f56c4e446f950fdc92f0ec39ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda7e2defd654754b902b9452ab5032f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.97e+00, Decode: 1.59e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e090baf4e294ee28afd6c2836685d8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6a5ed10c8d4e4aaa0de947430ef1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.42e+00, Decode: 2.72e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 22:01:12.903209313 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFYING WITH: gptq_marlin\n",
      "================================================================================\n",
      "Loading model: JunHowie/Qwen3-8B-GPTQ-Int4\n",
      "  Quantization: gptq_marlin\n",
      "  Dtype: float16\n",
      "INFO 11-30 22:01:13 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq_marlin', 'model': 'JunHowie/Qwen3-8B-GPTQ-Int4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ba3f7e48414eab8cd4f78c2fdf9f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 22:01:13 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 22:01:13 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 22:01:13 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 22:01:13 [gptq_marlin.py:228] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\n",
      "INFO 11-30 22:01:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0bb9431f8b34788a81260c42c57fa92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a659872ee1449c8317a6543040c929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db5f570c6c3466b870d669ae4b1b3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f84d2dc8a964da584ff53362c449060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89827158593a4193941dac7a94314f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aeab852938441759bef71e6ea9a45be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e30cf776964961864f19df69cead36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c6ed9ea51954b9ea339989c80f320a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/491 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d1b2a49df2044d892436ab08a577c7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91a1e7dddc146a299aa22bbcbe2a6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:23 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='JunHowie/Qwen3-8B-GPTQ-Int4', speculative_config=None, tokenizer='JunHowie/Qwen3-8B-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=JunHowie/Qwen3-8B-GPTQ-Int4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:23 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.6:54257 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:23 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:23 [gpu_model_runner.py:3259] Starting to load model JunHowie/Qwen3-8B-GPTQ-Int4...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:24 [gptq_marlin.py:359] Using MacheteLinearKernel for GPTQMarlinLinearMethod\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:24 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:24 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:34 [weight_utils.py:441] Time spent downloading weights for JunHowie/Qwen3-8B-GPTQ-Int4: 9.539635 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.17it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.37it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.34it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:36 [default_loader.py:314] Loading weights took 1.56 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:36 [gpu_model_runner.py:3338] Model loading took 5.7104 GiB memory and 11.807435 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:42 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/c0f1b68528/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:42 [backends.py:647] Dynamo bytecode transform time: 6.14 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:44 [backends.py:251] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:54 [backends.py:282] Compiling a graph for dynamic shape takes 10.81 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:56 [monitor.py:34] torch.compile takes 16.94 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:57 [gpu_worker.py:359] Available KV cache memory: 53.64 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:58 [kv_cache_utils.py:1229] GPU KV cache size: 390,592 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:01:58 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 9.54x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m 2025-11-30 22:01:58,169 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m 2025-11-30 22:01:58,180 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:01<00:00, 32.39it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 36.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:02:01 [gpu_model_runner.py:4244] Graph capturing finished in 4 secs, took -1.13 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3350)\u001b[0;0m INFO 11-30 22:02:01 [core.py:250] init engine (profile, create kv cache, warmup model) took 25.32 seconds\n",
      "INFO 11-30 22:02:03 [llm.py:352] Supported tasks: ['generate']\n",
      "\n",
      "  Claimed config: awq\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0888f39438042b2a4cbc3b2368e695b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7944846fc86347b8aae22dbf60e3196c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.95e+00, Decode: 1.54e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "831e6dd9b7264c918f3e2908b977d3c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3ae8a2f420440ab3bbb843f8876850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.39e+00, Decode: 2.68e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32c81deb8e034771be0c70d765b615a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2536d17ae84f4797a06e8e415ff543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.11e+00, Decode: 2.55e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1023513981cd4790bb7135c7c61a539b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a34c98feabc435aba95f4517a48feb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.43e+00, Decode: 2.86e+00\n",
      "\n",
      "  Claimed config: awq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38e1a510312a435c975ec0a3fc6e4a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1191306c27bc4cc29b3a99a9bd523d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.95e+00, Decode: 1.54e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877b7e7d536e4e32ac2c0906b132a4d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdafd09e34747e2a97b94f7f7f8fcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.39e+00, Decode: 2.68e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2520b365c7d345008cc105b1ffb46bc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d363e288c9a2413c86c49a17c377e14b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.10e+00, Decode: 2.54e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7c04a62ca624bb080d0a8e8fa040f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9c5c748211e48e5bf7872dc12a394b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.45e+00, Decode: 2.85e+00\n",
      "\n",
      "  Claimed config: gptq_marlin\n",
      "    ref_0 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1e11f13fabd4890abb65840a4b16032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f738034f3d1f4355852584c0e766a72c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.41e-02, Decode: 1.26e-02\n",
      "    ref_1 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88306a78786b4e60b7c6c8a53a49688a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6df2253be02c413fa14f2e681e1c95d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.62e-02, Decode: 2.65e-02\n",
      "    ref_2 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9298354cb2c4af6becebfe7c967d683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6092b96f6ad64f35a6d3dae5fabfcca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 4.23e-02, Decode: 2.90e-02\n",
      "    ref_3 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a63b142f4b4814b07e83c00999a949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9d878f89e64f7380ac1fbaedf16097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 9.51e-02, Decode: 2.50e-02\n",
      "\n",
      "  Claimed config: gptq\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da3197f946944d639927b4c51456826c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f437f576c374e5ab3e87dbec521886b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 1.78e-01, Decode: 9.41e-02\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdafeb30a10b4db8bbe7dbe557b67570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda1d85c6d53487dbd9c3e8233b10d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 1.97e-01, Decode: 1.20e-01\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1201fd57a3664756b393d5f0f08d9506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff071b000e774bb292c203657e520460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 2.32e-01, Decode: 8.01e-02\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d65160a173b4c47bda57d314ed5eb5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99748e2303da4ac1a134ac099ef61481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.33e-01, Decode: 8.20e-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 22:02:19.833632669 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VERIFYING WITH: gptq\n",
      "================================================================================\n",
      "Loading model: JunHowie/Qwen3-8B-GPTQ-Int4\n",
      "  Quantization: gptq\n",
      "  Dtype: float16\n",
      "INFO 11-30 22:02:20 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'float16', 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'quantization': 'gptq', 'model': 'JunHowie/Qwen3-8B-GPTQ-Int4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-30 22:02:20 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "WARNING 11-30 22:02:20 [model.py:1971] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-30 22:02:20 [model.py:1745] Using max model len 40960\n",
      "INFO 11-30 22:02:20 [gptq_marlin.py:232] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "INFO 11-30 22:02:20 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "WARNING 11-30 22:02:20 [gptq.py:99] Currently, the 4-bit gptq_gemm kernel for GPTQ is buggy. Please switch to gptq_marlin or gptq_bitblas.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074f54217218497b8a37dfcc43ac712a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parse safetensors files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/venv/main/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:26 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='JunHowie/Qwen3-8B-GPTQ-Int4', speculative_config=None, tokenizer='JunHowie/Qwen3-8B-GPTQ-Int4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=40960, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=JunHowie/Qwen3-8B-GPTQ-Int4, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:27 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.0.6:48287 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:27 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:27 [gpu_model_runner.py:3259] Starting to load model JunHowie/Qwen3-8B-GPTQ-Int4...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:27 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:27 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.10it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.38it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.33it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:30 [default_loader.py:314] Loading weights took 1.57 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:30 [gpu_model_runner.py:3338] Model loading took 5.7071 GiB memory and 2.412707 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:36 [backends.py:631] Using cache directory: /root/.cache/vllm/torch_compile_cache/e192f1fd03/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:36 [backends.py:647] Dynamo bytecode transform time: 5.81 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:38 [backends.py:251] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:48 [backends.py:282] Compiling a graph for dynamic shape takes 11.67 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:50 [monitor.py:34] torch.compile takes 17.49 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:51 [gpu_worker.py:359] Available KV cache memory: 53.64 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:52 [kv_cache_utils.py:1229] GPU KV cache size: 390,624 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:52 [kv_cache_utils.py:1234] Maximum concurrency for 40,960 tokens per request: 9.54x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m 2025-11-30 22:02:52,065 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m 2025-11-30 22:02:52,078 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 51/51 [00:02<00:00, 22.37it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 51/51 [00:01<00:00, 25.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:57 [gpu_model_runner.py:4244] Graph capturing finished in 5 secs, took 0.23 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=3863)\u001b[0;0m INFO 11-30 22:02:57 [core.py:250] init engine (profile, create kv cache, warmup model) took 26.70 seconds\n",
      "INFO 11-30 22:02:58 [llm.py:352] Supported tasks: ['generate']\n",
      "\n",
      "  Claimed config: awq\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a70439d578b42e69c0e9ed5187d08f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0c6a27ae3e14365af6d968d5853fe1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.22e+00, Decode: 1.52e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c804d68dae426cb0a04b7d92dd1e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e32dc34c9ac490986a2161531f3e6d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.40e+00, Decode: 2.67e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c2d9270bea4cfc9531c66fc4fa99aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f13112850904d38ab01086ebe4e7717",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.17e+00, Decode: 2.56e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "774bf1033e294027aa399d4766fe891c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45bce4983762467b9faf5e37b1851efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.31e+00, Decode: 2.91e+00\n",
      "\n",
      "  Claimed config: awq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6028117b1914033835fbfe82a42af7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b87e51d0cc4e6388dbb52644872c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.22e+00, Decode: 1.53e+00\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d574b6aa646425f99a5eeb7029e8fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d256578b38b41808e401bbf4a94bd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.40e+00, Decode: 2.68e+00\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5903c925c67f4f078e25fefaddda50e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3225b3b612364e908e21451bd018978d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 6.16e+00, Decode: 2.55e+00\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22456946fb924d88857a87fa8dcede5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977195eeaf6c4fc1b7aaf20b6c2a397a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.33e+00, Decode: 2.89e+00\n",
      "\n",
      "  Claimed config: gptq_marlin\n",
      "    ref_0 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ffaf333103d4a17a7f8f45a1b0608c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af9c75e5e1d4b6cb41f857d7df1323f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.60e-01, Decode: 1.98e-01\n",
      "    ref_1 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "483a2c4c0388475e8b1f4a3463f8a4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c323ee1c00843ed847fac4906a0991f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 2.16e-01, Decode: 7.47e-02\n",
      "    ref_2 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3331160ea4764fd2bc4b8dd397145112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1494da089744737986c6db774a74432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 1.32e-01, Decode: 8.16e-02\n",
      "    ref_3 (off):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2888fa08f64ebe838c993eddc932b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b3595e75cce4d84aab374bd7890d1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 1.75e-01, Decode: 2.71e-01\n",
      "\n",
      "  Claimed config: gptq\n",
      "    ref_0 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0cf42ae8ac40dc9c5fee66368431c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a582fb80b1c4cb4850a7904f5edb35c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.98e-01, Decode: 1.56e-01\n",
      "    ref_1 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bc43f0fd294a128cd6733a1e833ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b278c2dd6d98482eaa1f860d46afb319",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.21e-01, Decode: 1.24e-01\n",
      "    ref_2 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd28b397757d43b9a4d014d1f1ebf512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bd8eed308b427aaef2d666f2e1f074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 3.05e-01, Decode: 1.21e-01\n",
      "    ref_3 (diag):      Prompt: 8067, Gen: 100"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "396498a6d02d44a7bf7efacdb4750eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42ecc059ec7a4820ada7281ac21ea39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → verified\n",
      "      Prefill: 5.02e-01, Decode: 2.99e-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1130 22:03:14.693408081 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CROSS-HARDWARE QUANTIZATION DETECTABILITY (PREFILL)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     4.41e-02     6.56e-02     5.95e+00     6.22e+00 \n",
      "  awq_marlin     4.86e-02     8.24e-02     5.95e+00     6.22e+00 \n",
      " gptq_marlin     5.70e+00     5.70e+00     4.41e-02     3.60e-01 \n",
      "        gptq     5.75e+00     5.74e+00     1.78e-01     3.98e-01 \n",
      "\n",
      "--- ref_1 ---\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     5.70e-02     5.25e-02     6.39e+00     6.40e+00 \n",
      "  awq_marlin     3.26e-02     3.50e-02     6.39e+00     6.40e+00 \n",
      " gptq_marlin     5.50e+00     5.49e+00     5.62e-02     2.16e-01 \n",
      "        gptq     5.53e+00     5.52e+00     1.97e-01     3.21e-01 \n",
      "\n",
      "--- ref_2 ---\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     4.12e-02     3.96e-02     6.11e+00     6.17e+00 \n",
      "  awq_marlin     4.44e-02     3.09e-02     6.10e+00     6.16e+00 \n",
      " gptq_marlin     5.12e+00     5.14e+00     4.23e-02     1.32e-01 \n",
      "        gptq     5.95e+00     5.97e+00     2.32e-01     3.05e-01 \n",
      "\n",
      "--- ref_3 ---\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     4.20e-02     3.61e-02     5.43e+00     5.31e+00 \n",
      "  awq_marlin     7.11e-02     6.35e-02     5.45e+00     5.33e+00 \n",
      " gptq_marlin     4.19e+00     4.20e+00     9.51e-02     1.75e-01 \n",
      "        gptq     4.41e+00     4.42e+00     3.33e-01     5.02e-01 \n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "  Rows = claimed config, Cols = verified config\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     4.61e-02     4.85e-02     5.97e+00     6.02e+00 \n",
      "  awq_marlin     4.92e-02     5.29e-02     5.97e+00     6.03e+00 \n",
      " gptq_marlin     5.13e+00     5.13e+00     5.94e-02     2.21e-01 \n",
      "        gptq     5.41e+00     5.41e+00     2.35e-01     3.82e-01 \n",
      "\n",
      "================================================================================\n",
      "SNR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Diagonal (baseline = cross-hardware, same config):\n",
      "  Mean: 1.35e-01\n",
      "\n",
      "Off-diagonal (all pairs):\n",
      "  Count: 12\n",
      "  Mean: 3.80e+00\n",
      "  SNR (all): 28.16×\n",
      "\n",
      "Off-diagonal (meaningful pairs only):\n",
      "  Count: 12\n",
      "  Mean: 3.80e+00\n",
      "  SNR (meaningful): 28.16×\n",
      "\n",
      "================================================================================\n",
      "PAIRWISE SNR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "For each pair (A, B): signal = distance(A→B), noise = diagonal baseline\n",
      "SNR = signal / noise_floor\n",
      "\n",
      "  awq vs awq_marlin:\n",
      "    Signal: 4.88e-02, Noise: 4.95e-02\n",
      "    SNR: 1.0× → NOT RELIABLY DETECTABLE\n",
      "  awq vs gptq_marlin:\n",
      "    Signal: 5.55e+00, Noise: 5.27e-02\n",
      "    SNR: 105.2× → TRIVIALLY DETECTABLE\n",
      "  awq vs gptq:\n",
      "    Signal: 5.72e+00, Noise: 2.14e-01\n",
      "    SNR: 26.7× → CLEARLY DETECTABLE\n",
      "  awq_marlin vs gptq_marlin:\n",
      "    Signal: 5.55e+00, Noise: 5.62e-02\n",
      "    SNR: 98.9× → CLEARLY DETECTABLE\n",
      "  awq_marlin vs gptq:\n",
      "    Signal: 5.72e+00, Noise: 2.17e-01\n",
      "    SNR: 26.3× → CLEARLY DETECTABLE\n",
      "  gptq_marlin vs gptq:\n",
      "    Signal: 2.28e-01, Noise: 2.21e-01\n",
      "    SNR: 1.0× → NOT RELIABLY DETECTABLE\n",
      "\n",
      "----------------------------------------\n",
      "PAIRWISE SNR SUMMARY TABLE\n",
      "----------------------------------------\n",
      "Pair                               Signal      Noise      SNR\n",
      "------------------------------------------------------------\n",
      "awq_vs_gptq_marlin               5.55e+00   5.27e-02   105.20×\n",
      "awq_marlin_vs_gptq_marlin        5.55e+00   5.62e-02    98.86×\n",
      "awq_vs_gptq                      5.72e+00   2.14e-01    26.73×\n",
      "awq_marlin_vs_gptq               5.72e+00   2.17e-01    26.32×\n",
      "gptq_marlin_vs_gptq              2.28e-01   2.21e-01     1.03×\n",
      "awq_vs_awq_marlin                4.88e-02   4.95e-02     0.99×\n",
      "\n",
      "================================================================================\n",
      "CROSS-HARDWARE QUANTIZATION DETECTABILITY (DECODE)\n",
      "================================================================================\n",
      "\n",
      "--- ref_0 ---\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     3.46e-02     1.26e-02     1.54e+00     1.52e+00 \n",
      "  awq_marlin     4.33e-02     2.42e-02     1.54e+00     1.53e+00 \n",
      " gptq_marlin     2.14e+00     2.12e+00     1.26e-02     1.98e-01 \n",
      "        gptq     2.14e+00     2.12e+00     9.41e-02     1.56e-01 \n",
      "\n",
      "--- ref_1 ---\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     2.82e-02     4.43e-02     2.68e+00     2.67e+00 \n",
      "  awq_marlin     1.98e-02     4.63e-02     2.68e+00     2.68e+00 \n",
      " gptq_marlin     1.38e+00     1.38e+00     2.65e-02     7.47e-02 \n",
      "        gptq     1.36e+00     1.36e+00     1.20e-01     1.24e-01 \n",
      "\n",
      "--- ref_2 ---\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     2.54e-02     2.19e-02     2.55e+00     2.56e+00 \n",
      "  awq_marlin     6.72e-03     2.20e-02     2.54e+00     2.55e+00 \n",
      " gptq_marlin     1.59e+00     1.58e+00     2.90e-02     8.16e-02 \n",
      "        gptq     1.60e+00     1.59e+00     8.01e-02     1.21e-01 \n",
      "\n",
      "--- ref_3 ---\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     4.66e-02     5.01e-02     2.86e+00     2.91e+00 \n",
      "  awq_marlin     4.30e-02     2.65e-02     2.85e+00     2.89e+00 \n",
      " gptq_marlin     2.76e+00     2.73e+00     2.50e-02     2.71e-01 \n",
      "        gptq     2.75e+00     2.72e+00     8.20e-02     2.99e-01 \n",
      "\n",
      "================================================================================\n",
      "AGGREGATE (average across references):\n",
      "  Rows = claimed config, Cols = verified config\n",
      "================================================================================\n",
      "                       awq   awq_marlin  gptq_marlin         gptq\n",
      "         awq     3.37e-02     3.22e-02     2.41e+00     2.42e+00 \n",
      "  awq_marlin     2.82e-02     2.97e-02     2.40e+00     2.41e+00 \n",
      " gptq_marlin     1.97e+00     1.95e+00     2.33e-02     1.56e-01 \n",
      "        gptq     1.96e+00     1.95e+00     9.40e-02     1.75e-01 \n",
      "\n",
      "================================================================================\n",
      "SNR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Diagonal (baseline = cross-hardware, same config):\n",
      "  Mean: 6.54e-02\n",
      "\n",
      "Off-diagonal (all pairs):\n",
      "  Count: 12\n",
      "  Mean: 1.48e+00\n",
      "  SNR (all): 22.65×\n",
      "\n",
      "Off-diagonal (meaningful pairs only):\n",
      "  Count: 12\n",
      "  Mean: 1.48e+00\n",
      "  SNR (meaningful): 22.65×\n",
      "\n",
      "================================================================================\n",
      "PAIRWISE SNR ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "For each pair (A, B): signal = distance(A→B), noise = diagonal baseline\n",
      "SNR = signal / noise_floor\n",
      "\n",
      "  awq vs awq_marlin:\n",
      "    Signal: 3.02e-02, Noise: 3.17e-02\n",
      "    SNR: 1.0× → NOT RELIABLY DETECTABLE\n",
      "  awq vs gptq_marlin:\n",
      "    Signal: 2.19e+00, Noise: 2.85e-02\n",
      "    SNR: 76.8× → CLEARLY DETECTABLE\n",
      "  awq vs gptq:\n",
      "    Signal: 2.19e+00, Noise: 1.04e-01\n",
      "    SNR: 21.0× → CLEARLY DETECTABLE\n",
      "  awq_marlin vs gptq_marlin:\n",
      "    Signal: 2.18e+00, Noise: 2.65e-02\n",
      "    SNR: 82.2× → CLEARLY DETECTABLE\n",
      "  awq_marlin vs gptq:\n",
      "    Signal: 2.18e+00, Noise: 1.02e-01\n",
      "    SNR: 21.3× → CLEARLY DETECTABLE\n",
      "  gptq_marlin vs gptq:\n",
      "    Signal: 1.25e-01, Noise: 9.91e-02\n",
      "    SNR: 1.3× → NOT RELIABLY DETECTABLE\n",
      "\n",
      "----------------------------------------\n",
      "PAIRWISE SNR SUMMARY TABLE\n",
      "----------------------------------------\n",
      "Pair                               Signal      Noise      SNR\n",
      "------------------------------------------------------------\n",
      "awq_marlin_vs_gptq_marlin        2.18e+00   2.65e-02    82.16×\n",
      "awq_vs_gptq_marlin               2.19e+00   2.85e-02    76.80×\n",
      "awq_marlin_vs_gptq               2.18e+00   1.02e-01    21.30×\n",
      "awq_vs_gptq                      2.19e+00   1.04e-01    20.99×\n",
      "gptq_marlin_vs_gptq              1.25e-01   9.91e-02     1.26×\n",
      "awq_vs_awq_marlin                3.02e-02   3.17e-02     0.95×\n",
      "\n",
      "================================================================================\n",
      "PREFILL vs DECODE COMPARISON\n",
      "================================================================================\n",
      "Prefill SNR (meaningful): 28.16×\n",
      "Decode SNR (meaningful):  22.65×\n",
      "\n",
      "✓ Results saved to: /workspace/experiments/quant_verify_20251130_215827.json\n",
      "\n",
      "================================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Quantization Format Detection Experiment using vLLM.\n",
    "Compares INT4 quantization methods: AWQ vs GPTQ, with and without Marlin kernels.\n",
    "\n",
    "Tests whether quantization format claims can be verified across different GPU architectures\n",
    "using logprob forensics with vLLM inference engine.\n",
    "\n",
    "Signal: logprobs only (vLLM limitation - no key vector access)\n",
    "\n",
    "Workflow:\n",
    "1. Run on Machine A with TEACHER_FORCING = False\n",
    "   → Generates tokens, extracts logprobs, saves to JSON\n",
    "\n",
    "2. Copy JSON to Machine B\n",
    "\n",
    "3. Run on Machine B with TEACHER_FORCING = True\n",
    "   → Teacher-forces A's tokens, compares logprobs\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "import PyPDF2\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TEACHER_FORCING = True\n",
    "REFERENCE_FILE = \"INT4_A100_generation.json\"\n",
    "\n",
    "# Model configurations - all INT4, different quantization methods + kernels\n",
    "# \n",
    "# Available Qwen3-8B INT4 models for vLLM:\n",
    "#   - Qwen/Qwen3-8B-AWQ: AWQ quantization, supports awq/awq_marlin kernels\n",
    "#   - AlphaGaO/Qwen3-8B-GPTQ: GPTQ quantization, pre-converted to Marlin format\n",
    "#   - pytorch/Qwen3-8B-INT4: TorchAO HQQ (requires nightly vllm/torchao)\n",
    "#\n",
    "# Note: OpenVINO/Qwen3-8B-int4-ov is NOT vLLM compatible (OpenVINO backend only)\n",
    "#\n",
    "MODEL_CONFIGS = {\n",
    "    'awq': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'awq_marlin': {\n",
    "        'model_name': 'Qwen/Qwen3-8B-AWQ',\n",
    "        'quantization': 'awq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'gptq_marlin': {\n",
    "        'model_name': 'JunHowie/Qwen3-8B-GPTQ-Int4',\n",
    "        'quantization': 'gptq_marlin',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    'gptq': {\n",
    "        'model_name': 'JunHowie/Qwen3-8B-GPTQ-Int4',\n",
    "        'quantization': 'gptq',\n",
    "        'dtype': 'float16',\n",
    "    },\n",
    "    # Uncomment if you have nightly vllm + torchao installed:\n",
    "    # 'torchao_hqq': {\n",
    "    #     'model_name': 'pytorch/Qwen3-8B-INT4',\n",
    "    #     'quantization': 'torchao',\n",
    "    #     'dtype': 'bfloat16',\n",
    "    # },\n",
    "}\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "MAX_NEW_TOKENS = 100\n",
    "TOKENS_PER_SLICE = 8000\n",
    "NUM_REFERENCES = 4\n",
    "TOP_K_LOGPROBS = 5\n",
    "\n",
    "# Threshold for considering two configs \"equivalent\" (same kernel)\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "# Reproducibility check settings\n",
    "REPRODUCIBILITY_CHECK = True\n",
    "REPRODUCIBILITY_RUNS = 3\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING SETUP\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    mode = \"verify\" if TEACHER_FORCING else \"generate\"\n",
    "    log_path = os.path.join(output_dir, f\"quant_experiment_{mode}_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_prompts_from_pdf(tokenizer, num_references=NUM_REFERENCES):\n",
    "    \"\"\"\n",
    "    Load PDFs and create prompts with different content slices.\n",
    "    Returns list of prompt token ID lists.\n",
    "    \"\"\"\n",
    "    pdf_files = sorted(glob.glob(\"/workspace/*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        pdf_files = sorted(glob.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    content_tokens = tokenizer.encode(all_text)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    tokens_needed = num_references * TOKENS_PER_SLICE\n",
    "    if len(content_tokens) < tokens_needed:\n",
    "        raise ValueError(f\"Need {tokens_needed} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    # Build chat-formatted prompts\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "\n",
    "    suffix = f\"\"\"\\\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "\n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    suffix_tokens = tokenizer.encode(suffix)\n",
    "\n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    prompts = []\n",
    "    for i in range(num_references):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt_ids = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        prompts.append(prompt_ids)\n",
    "\n",
    "    return prompts\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info():\n",
    "    \"\"\"Collect comprehensive environment information.\"\"\"\n",
    "    import torch\n",
    "    import transformers\n",
    "\n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        import vllm\n",
    "        info[\"vllm_version\"] = vllm.__version__\n",
    "    except (ImportError, AttributeError):\n",
    "        info[\"vllm_version\"] = \"unknown\"\n",
    "\n",
    "    return info\n",
    "\n",
    "def validate_environment_match(reference_env, verifier_env):\n",
    "    \"\"\"\n",
    "    Validate that software environments match between reference and verifier.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"ENVIRONMENT VALIDATION\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    critical_fields = ['vllm_version', 'torch_version', 'cuda_version']\n",
    "    expected_different = ['gpu_name', 'hostname']\n",
    "\n",
    "    mismatches = []\n",
    "\n",
    "    log_print(\"\\nCritical dependencies:\")\n",
    "    for field in critical_fields:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val == ver_val:\n",
    "            log_print(f\"  ✓ {field}: {ref_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ✗ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "            mismatches.append((field, ref_val, ver_val))\n",
    "\n",
    "    log_print(\"\\nExpected differences (hardware):\")\n",
    "    for field in expected_different:\n",
    "        ref_val = reference_env.get(field, 'N/A')\n",
    "        ver_val = verifier_env.get(field, 'N/A')\n",
    "\n",
    "        if ref_val != ver_val:\n",
    "            log_print(f\"  ✓ {field}: reference={ref_val}, verifier={ver_val}\")\n",
    "        else:\n",
    "            log_print(f\"  ⚠ {field}: SAME ({ref_val}) - are you on different hardware?\")\n",
    "\n",
    "    if not mismatches:\n",
    "        log_print(\"\\n✓ ENVIRONMENT VALIDATION PASSED\")\n",
    "        return {'valid': True, 'mismatches': []}\n",
    "    else:\n",
    "        log_print(\"\\n⚠ ENVIRONMENT MISMATCHES DETECTED\")\n",
    "        log_print(\"  Results may be affected by software differences, not just hardware.\")\n",
    "        return {'valid': False, 'mismatches': mismatches}\n",
    "\n",
    "# ============================================================================\n",
    "# MODEL LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_vllm_model(config_name):\n",
    "    \"\"\"Load vLLM model with specified configuration.\"\"\"\n",
    "    config = MODEL_CONFIGS[config_name]\n",
    "    \n",
    "    log_print(f\"Loading model: {config['model_name']}\")\n",
    "    log_print(f\"  Quantization: {config['quantization']}\")\n",
    "    log_print(f\"  Dtype: {config['dtype']}\")\n",
    "    \n",
    "    kwargs = {\n",
    "        'model': config['model_name'],\n",
    "        'download_dir': CACHE_DIR,\n",
    "        'dtype': config['dtype'],\n",
    "        'trust_remote_code': True,\n",
    "        'gpu_memory_utilization': 0.7,\n",
    "        'quantization': config['quantization'],\n",
    "    }\n",
    "    \n",
    "    llm = LLM(**kwargs)\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    \n",
    "    return llm, tokenizer\n",
    "\n",
    "# ============================================================================\n",
    "# LOGPROB EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_logprobs_from_output(output, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from vLLM output at specified positions.\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    logprobs_list = output.outputs[0].logprobs\n",
    "    \n",
    "    if logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    num_generated = len(logprobs_list)\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else num_generated + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= num_generated:\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = logprobs_list[actual_idx]\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract logprobs from prompt positions (for prefill analysis).\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is None:\n",
    "        return signals\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "        \n",
    "        if token_logprobs is None:\n",
    "            continue\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            log_probs.append(logprob_obj.logprob)\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION MODE\n",
    "# ============================================================================\n",
    "\n",
    "def run_generation(llm, tokenizer, prompt_ids):\n",
    "    \"\"\"Run generation and extract prefill + decode signals.\"\"\"\n",
    "    prompt_text = tokenizer.decode(prompt_ids)\n",
    "    prompt_length = len(prompt_ids)\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=MAX_NEW_TOKENS,\n",
    "        temperature=0.0,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([prompt_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    generated_ids = list(output.outputs[0].token_ids)\n",
    "    num_generated = len(generated_ids)\n",
    "    \n",
    "    prefill_signals = extract_prompt_logprobs(output, prompt_length, positions=[-3, -2, -1])\n",
    "    decode_signals = extract_logprobs_from_output(output, positions=[-3, -2, -1])\n",
    "    \n",
    "    return {\n",
    "        'prompt_ids': prompt_ids,\n",
    "        'generated_ids': generated_ids,\n",
    "        'prompt_length': prompt_length,\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# VERIFICATION MODE (TEACHER FORCING)\n",
    "# ============================================================================\n",
    "\n",
    "def run_teacher_forced_verification(llm, tokenizer, reference_data, is_diagonal):\n",
    "    \"\"\"\n",
    "    Teacher-forced verification.\n",
    "    For diagonal (same config): use exact reference\n",
    "    For off-diagonal: verify with different config\n",
    "    \n",
    "    vLLM doesn't support true teacher forcing, so we prefill the full sequence\n",
    "    (prompt + generated tokens) and extract logprobs.\n",
    "    \"\"\"\n",
    "    ref_prompt_ids = reference_data['prompt_ids']\n",
    "    ref_generated_ids = reference_data['generated_ids']\n",
    "    \n",
    "    prompt_length = len(ref_prompt_ids)\n",
    "    num_generated = len(ref_generated_ids)\n",
    "    \n",
    "    # Full sequence = prompt + generated\n",
    "    full_ids = ref_prompt_ids + ref_generated_ids\n",
    "    full_text = tokenizer.decode(full_ids)\n",
    "    \n",
    "    log_print(f\"      Prompt: {prompt_length}, Gen: {num_generated}\", end=\"\")\n",
    "    \n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=1,  # Minimal generation, we just want logprobs\n",
    "        temperature=0.0,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([full_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    # Extract prefill signals from prompt portion\n",
    "    prefill_signals = {}\n",
    "    prompt_logprobs_list = output.prompt_logprobs\n",
    "    \n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            actual_idx = pos if pos >= 0 else prompt_length + pos\n",
    "            \n",
    "            if actual_idx < 0 or actual_idx >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[actual_idx]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            prefill_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    # Extract decode signals from generated portion\n",
    "    decode_signals = {}\n",
    "    if prompt_logprobs_list is not None:\n",
    "        for pos in [-3, -2, -1]:\n",
    "            # Position in full sequence (prompt + generated)\n",
    "            full_pos = pos if pos >= 0 else (prompt_length + num_generated) + pos\n",
    "            \n",
    "            if full_pos < 0 or full_pos >= len(prompt_logprobs_list):\n",
    "                continue\n",
    "            \n",
    "            pos_label = f\"pos_{pos}\"\n",
    "            token_logprobs = prompt_logprobs_list[full_pos]\n",
    "            \n",
    "            if token_logprobs is None:\n",
    "                continue\n",
    "            \n",
    "            token_ids = list(token_logprobs.keys())\n",
    "            log_probs = [token_logprobs[tid].logprob for tid in token_ids]\n",
    "            \n",
    "            decode_signals[pos_label] = {\n",
    "                'logprobs': {\n",
    "                    'token_ids': token_ids,\n",
    "                    'log_probs': log_probs\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    log_print(f\" → verified\")\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals,\n",
    "        'decode_signals': decode_signals,\n",
    "        'num_generated': num_generated\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_logprob_distance_canonical(logprobs1, logprobs2, canonical_ids):\n",
    "    \"\"\"Compute L2 distance between logprobs for a canonical set of token IDs.\"\"\"\n",
    "    lp1 = dict(zip(logprobs1['token_ids'], logprobs1['log_probs']))\n",
    "    lp2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "\n",
    "    for tid in canonical_ids:\n",
    "        if tid in lp1 and tid in lp2:\n",
    "            vec1.append(lp1[tid])\n",
    "            vec2.append(lp2[tid])\n",
    "\n",
    "    if len(vec1) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals(signals1, signals2):\n",
    "    \"\"\"Compare two signal sets using top 5 token IDs from first signal as canonical.\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "\n",
    "    all_dists = []\n",
    "\n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]\n",
    "\n",
    "        # Use top 5 for comparison\n",
    "        canonical_ids = sig1['logprobs']['token_ids'][:5]\n",
    "        dist = compute_logprob_distance_canonical(\n",
    "            sig1['logprobs'], sig2['logprobs'], canonical_ids\n",
    "        )\n",
    "        all_dists.append(dist)\n",
    "\n",
    "    finite_dists = [d for d in all_dists if d != float('inf')]\n",
    "    \n",
    "    return {\n",
    "        'logprobs_mean': np.mean(finite_dists) if finite_dists else float('inf'),\n",
    "        'logprobs_max': max(finite_dists) if finite_dists else float('inf')\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# EQUIVALENCE DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "def find_equivalent_pairs(matrix, config_names, threshold=EQUIVALENCE_THRESHOLD):\n",
    "    \"\"\"Find pairs of configs that produce equivalent results (same kernel).\"\"\"\n",
    "    equivalent_pairs = []\n",
    "    n = len(config_names)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if matrix[i, j] < threshold:\n",
    "                equivalent_pairs.append((config_names[i], config_names[j]))\n",
    "    \n",
    "    return equivalent_pairs\n",
    "\n",
    "def format_kernel_classes(equivalent_pairs, config_names):\n",
    "    \"\"\"Group configs into kernel equivalence classes.\"\"\"\n",
    "    parent = {cfg: cfg for cfg in config_names}\n",
    "    \n",
    "    def find(x):\n",
    "        if parent[x] != x:\n",
    "            parent[x] = find(parent[x])\n",
    "        return parent[x]\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        union(cfg1, cfg2)\n",
    "    \n",
    "    groups = {}\n",
    "    for cfg in config_names:\n",
    "        root = find(cfg)\n",
    "        if root not in groups:\n",
    "            groups[root] = set()\n",
    "        groups[root].add(cfg)\n",
    "    \n",
    "    return list(groups.values())\n",
    "\n",
    "# ============================================================================\n",
    "# WITHIN-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_within_hardware(measurements, config_names, signal_source='decode'):\n",
    "    \"\"\"Analyze within-hardware quantization effects.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"WITHIN-HARDWARE QUANTIZATION EFFECTS ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    n = len(config_names)\n",
    "    all_matrices = []\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "\n",
    "        matrix = np.zeros((n, n))\n",
    "\n",
    "        for i, cfg_i in enumerate(config_names):\n",
    "            for j, cfg_j in enumerate(config_names):\n",
    "                signals_key = 'prefill_signals' if signal_source == 'prefill' else 'decode_signals'\n",
    "                sig_i = measurements[cfg_i][ref_idx][signals_key]\n",
    "                sig_j = measurements[cfg_j][ref_idx][signals_key]\n",
    "\n",
    "                if i == j:\n",
    "                    matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    distances = compare_signals(sig_i, sig_j)\n",
    "                    matrix[i, j] = distances['logprobs_mean']\n",
    "\n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, cfg in enumerate(config_names):\n",
    "            row = f\"{cfg:>12} \" + \" \".join(f\"{matrix[i,j]:12.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "\n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "\n",
    "    log_print(f\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, cfg in enumerate(config_names):\n",
    "        row = f\"{cfg:>12} \" + \" \".join(f\"{avg_matrix[i,j]:12.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "\n",
    "    # Find equivalent pairs\n",
    "    equivalent_pairs = find_equivalent_pairs(avg_matrix, config_names)\n",
    "    kernel_classes = format_kernel_classes(equivalent_pairs, config_names)\n",
    "\n",
    "    # Off-diagonal stats\n",
    "    off_diag = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag.append(avg_matrix[i, j])\n",
    "\n",
    "    finite_off_diag = [d for d in off_diag if d != float('inf')]\n",
    "\n",
    "    log_print(f\"\\nOff-diagonal stats:\")\n",
    "    if finite_off_diag:\n",
    "        log_print(f\"  Mean: {np.mean(finite_off_diag):.2e}\")\n",
    "        log_print(f\"  Range: [{np.min(finite_off_diag):.2e}, {np.max(finite_off_diag):.2e}]\")\n",
    "    \n",
    "    # Check if all zeros\n",
    "    zero_count = sum(1 for d in finite_off_diag if d < EQUIVALENCE_THRESHOLD)\n",
    "    if zero_count == len(finite_off_diag):\n",
    "        log_print(f\"\\n⚠ WARNING: All comparisons are EXACTLY ZERO\")\n",
    "        log_print(\"  All configs produce identical results (single kernel class)\")\n",
    "\n",
    "    log_print(f\"\\nKernel equivalence classes:\")\n",
    "    for i, cls in enumerate(kernel_classes):\n",
    "        log_print(f\"  Class {i+1}: {sorted(cls)}\")\n",
    "\n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs:\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "\n",
    "    # Pairwise distance summary\n",
    "    log_print(f\"\\n\" + \"-\"*40)\n",
    "    log_print(\"PAIRWISE DISTANCE SUMMARY\")\n",
    "    log_print(\"-\"*40)\n",
    "    \n",
    "    pairwise_distances = {}\n",
    "    for i, cfg_i in enumerate(config_names):\n",
    "        for j, cfg_j in enumerate(config_names):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            pair_key = f\"{cfg_i}_vs_{cfg_j}\"\n",
    "            avg_dist = (avg_matrix[i, j] + avg_matrix[j, i]) / 2\n",
    "            pairwise_distances[pair_key] = float(avg_dist)\n",
    "    \n",
    "    for pair_key, dist in sorted(pairwise_distances.items(), key=lambda x: x[1]):\n",
    "        if dist < EQUIVALENCE_THRESHOLD:\n",
    "            status = \"≈ EQUIVALENT (same kernel?)\"\n",
    "        elif dist < 0.1:\n",
    "            status = \"SMALL difference\"\n",
    "        elif dist < 1.0:\n",
    "            status = \"MODERATE difference\"\n",
    "        else:\n",
    "            status = \"LARGE difference\"\n",
    "        log_print(f\"  {pair_key}: {dist:.2e} → {status}\")\n",
    "\n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'off_diagonal_mean': float(np.mean(finite_off_diag)) if finite_off_diag else 0,\n",
    "        'equivalent_pairs': equivalent_pairs,\n",
    "        'kernel_classes': [sorted(list(cls)) for cls in kernel_classes],\n",
    "        'pairwise_distances': pairwise_distances\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# CROSS-HARDWARE ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_cross_hardware(comparison_results, config_names, signal_source='decode',\n",
    "                           equivalent_pairs=None):\n",
    "    \"\"\"Analyze the comparison matrix and determine detectability.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(f\"CROSS-HARDWARE QUANTIZATION DETECTABILITY ({signal_source.upper()})\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    if equivalent_pairs is None:\n",
    "        equivalent_pairs = []\n",
    "    \n",
    "    # Convert to set of both orderings for easy lookup\n",
    "    equiv_set = set()\n",
    "    for cfg1, cfg2 in equivalent_pairs:\n",
    "        equiv_set.add((cfg1, cfg2))\n",
    "        equiv_set.add((cfg2, cfg1))\n",
    "    \n",
    "    dist_key = 'prefill_distances' if signal_source == 'prefill' else 'decode_distances'\n",
    "    \n",
    "    by_ref = {}\n",
    "    for result in comparison_results:\n",
    "        ref = result['ref_idx']\n",
    "        if ref not in by_ref:\n",
    "            by_ref[ref] = {}\n",
    "        key = (result['claimed_config'], result['verify_config'])\n",
    "        by_ref[ref][key] = result\n",
    "\n",
    "    all_matrices = []\n",
    "    n = len(config_names)\n",
    "\n",
    "    # Per-reference matrices\n",
    "    for ref_idx in sorted(by_ref.keys()):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        ref_data = by_ref[ref_idx]\n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            for j, verify_cfg in enumerate(config_names):\n",
    "                key = (claimed_cfg, verify_cfg)\n",
    "                if key in ref_data:\n",
    "                    matrix[i, j] = ref_data[key][dist_key]['logprobs_mean']\n",
    "        \n",
    "        # Display matrix\n",
    "        header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "        log_print(header)\n",
    "        for i, claimed_cfg in enumerate(config_names):\n",
    "            row = f\"{claimed_cfg:>12} \"\n",
    "            for j in range(n):\n",
    "                row += f\"{matrix[i,j]:12.2e} \"\n",
    "            log_print(row)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "\n",
    "    # Aggregate matrix\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"  Rows = claimed config, Cols = verified config\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"              \" + \" \".join(f\"{c:>12}\" for c in config_names)\n",
    "    log_print(header)\n",
    "    for i, claimed_cfg in enumerate(config_names):\n",
    "        row = f\"{claimed_cfg:>12} \"\n",
    "        for j in range(n):\n",
    "            row += f\"{avg_matrix[i,j]:12.2e} \"\n",
    "        log_print(row)\n",
    "    \n",
    "    # Compute statistics\n",
    "    diagonal = [avg_matrix[i, i] for i in range(n)]\n",
    "    \n",
    "    # Off-diagonal: exclude equivalent pairs\n",
    "    off_diagonal_all = []\n",
    "    off_diagonal_meaningful = []\n",
    "    excluded_pairs = []\n",
    "    \n",
    "    for i, cfg1 in enumerate(config_names):\n",
    "        for j, cfg2 in enumerate(config_names):\n",
    "            if i != j:\n",
    "                off_diagonal_all.append(avg_matrix[i, j])\n",
    "                if (cfg1, cfg2) in equiv_set:\n",
    "                    excluded_pairs.append((cfg1, cfg2))\n",
    "                else:\n",
    "                    off_diagonal_meaningful.append(avg_matrix[i, j])\n",
    "    \n",
    "    baseline_mean = np.mean(diagonal)\n",
    "    signal_all_mean = np.mean(off_diagonal_all) if off_diagonal_all else 0.0\n",
    "    signal_meaningful_mean = np.mean(off_diagonal_meaningful) if off_diagonal_meaningful else 0.0\n",
    "    \n",
    "    snr_all = signal_all_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    snr_meaningful = signal_meaningful_mean / baseline_mean if baseline_mean > 0 else float('inf')\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same config):\")\n",
    "    log_print(f\"  Mean: {baseline_mean:.2e}\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (all pairs):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_all)}\")\n",
    "    log_print(f\"  Mean: {signal_all_mean:.2e}\")\n",
    "    log_print(f\"  SNR (all): {snr_all:.2f}×\")\n",
    "    \n",
    "    if equivalent_pairs:\n",
    "        log_print(f\"\\nExcluded equivalent pairs (same kernel within-hardware):\")\n",
    "        for cfg1, cfg2 in equivalent_pairs:\n",
    "            log_print(f\"  ({cfg1}, {cfg2})\")\n",
    "        log_print(f\"  Total excluded: {len(excluded_pairs)} cells\")\n",
    "    \n",
    "    log_print(f\"\\nOff-diagonal (meaningful pairs only):\")\n",
    "    log_print(f\"  Count: {len(off_diagonal_meaningful)}\")\n",
    "    if off_diagonal_meaningful:\n",
    "        log_print(f\"  Mean: {signal_meaningful_mean:.2e}\")\n",
    "        log_print(f\"  SNR (meaningful): {snr_meaningful:.2f}×\")\n",
    "    else:\n",
    "        log_print(\"  No meaningful pairs (all configs are equivalent)\")\n",
    "    \n",
    "    # Pairwise SNR analysis\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"PAIRWISE SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(\"\\nFor each pair (A, B): signal = distance(A→B), noise = diagonal baseline\")\n",
    "    log_print(\"SNR = signal / noise_floor\\n\")\n",
    "    \n",
    "    pairwise_snr = {}\n",
    "    \n",
    "    for i, cfg_i in enumerate(config_names):\n",
    "        for j, cfg_j in enumerate(config_names):\n",
    "            if i >= j:  # Skip diagonal and lower triangle (symmetric)\n",
    "                continue\n",
    "            \n",
    "            pair_key = f\"{cfg_i}_vs_{cfg_j}\"\n",
    "            \n",
    "            # Get distances in both directions and average\n",
    "            dist_ij = avg_matrix[i, j]\n",
    "            dist_ji = avg_matrix[j, i]\n",
    "            avg_dist = (dist_ij + dist_ji) / 2\n",
    "            \n",
    "            # Use average of the two diagonals as noise floor for this pair\n",
    "            noise_i = avg_matrix[i, i]\n",
    "            noise_j = avg_matrix[j, j]\n",
    "            noise_floor = (noise_i + noise_j) / 2\n",
    "            \n",
    "            snr = avg_dist / noise_floor if noise_floor > 0 else float('inf')\n",
    "            \n",
    "            pairwise_snr[pair_key] = {\n",
    "                'signal': float(avg_dist),\n",
    "                'noise_floor': float(noise_floor),\n",
    "                'snr': float(snr)\n",
    "            }\n",
    "            \n",
    "            # Interpretation\n",
    "            if snr > 100:\n",
    "                interpretation = \"TRIVIALLY DETECTABLE\"\n",
    "            elif snr > 10:\n",
    "                interpretation = \"CLEARLY DETECTABLE\"\n",
    "            elif snr > 3:\n",
    "                interpretation = \"MARGINALLY DETECTABLE\"\n",
    "            else:\n",
    "                interpretation = \"NOT RELIABLY DETECTABLE\"\n",
    "            \n",
    "            log_print(f\"  {cfg_i} vs {cfg_j}:\")\n",
    "            log_print(f\"    Signal: {avg_dist:.2e}, Noise: {noise_floor:.2e}\")\n",
    "            log_print(f\"    SNR: {snr:.1f}× → {interpretation}\")\n",
    "    \n",
    "    # Summary table\n",
    "    log_print(\"\\n\" + \"-\"*40)\n",
    "    log_print(\"PAIRWISE SNR SUMMARY TABLE\")\n",
    "    log_print(\"-\"*40)\n",
    "    log_print(f\"{'Pair':<30} {'Signal':>10} {'Noise':>10} {'SNR':>8}\")\n",
    "    log_print(\"-\"*60)\n",
    "    for pair_key, data in sorted(pairwise_snr.items(), key=lambda x: -x[1]['snr']):\n",
    "        log_print(f\"{pair_key:<30} {data['signal']:>10.2e} {data['noise_floor']:>10.2e} {data['snr']:>8.2f}×\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'baseline_mean': float(baseline_mean),\n",
    "        'signal_all_mean': float(signal_all_mean),\n",
    "        'signal_meaningful_mean': float(signal_meaningful_mean),\n",
    "        'snr_all': float(snr_all),\n",
    "        'snr_meaningful': float(snr_meaningful),\n",
    "        'excluded_pairs': equivalent_pairs,\n",
    "        'n_excluded_cells': len(excluded_pairs),\n",
    "        'n_meaningful_pairs': len(off_diagonal_meaningful),\n",
    "        'pairwise_snr': pairwise_snr\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# TOKEN CONSISTENCY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def check_token_consistency(measurements, config_names, tokenizer):\n",
    "    \"\"\"Verify generated tokens across quantization configs.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"TOKEN GENERATION CONSISTENCY CHECK\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        tokens_by_cfg = {}\n",
    "        for cfg in config_names:\n",
    "            tokens_by_cfg[cfg] = measurements[cfg][ref_idx]['generated_ids']\n",
    "\n",
    "        reference_tokens = tokens_by_cfg[config_names[0]]\n",
    "\n",
    "        for cfg in config_names:\n",
    "            tokens = tokens_by_cfg[cfg]\n",
    "            match_str = \"✓\" if tokens == reference_tokens else \"✗ DIFFERENT\"\n",
    "            decoded_text = tokenizer.decode(tokens[:30])\n",
    "            log_print(f\"  {cfg}:\")\n",
    "            log_print(f\"    Tokens: {len(tokens)}\")\n",
    "            log_print(f\"    First 30: {repr(decoded_text)}...\")\n",
    "            log_print(f\"    {match_str}\")\n",
    "\n",
    "# ============================================================================\n",
    "# REPRODUCIBILITY CHECK\n",
    "# ============================================================================\n",
    "\n",
    "def run_reproducibility_check(config_names, prompts):\n",
    "    \"\"\"Measure within-format noise floor from atomics/non-deterministic kernels.\"\"\"\n",
    "    import torch\n",
    "    \n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"REPRODUCIBILITY CHECK (NOISE FLOOR MEASUREMENT)\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"Running {REPRODUCIBILITY_RUNS} identical inference passes per config\")\n",
    "    log_print(\"Measures within-format variance from atomics/non-deterministic kernels.\\n\")\n",
    "    \n",
    "    noise_floors = {}\n",
    "    test_prompt = prompts[0]  # Use first prompt for reproducibility check\n",
    "    \n",
    "    for cfg_name in config_names:\n",
    "        log_print(f\"\\n--- Checking: {cfg_name} ---\")\n",
    "        \n",
    "        llm, tokenizer = load_vllm_model(cfg_name)\n",
    "        \n",
    "        # Run multiple times\n",
    "        run_signals = []\n",
    "        for run_idx in range(REPRODUCIBILITY_RUNS):\n",
    "            log_print(f\"  Run {run_idx + 1}: \", end=\"\")\n",
    "            gen_data = run_generation(llm, tokenizer, test_prompt)\n",
    "            run_signals.append({\n",
    "                'generated_ids': gen_data['generated_ids'],\n",
    "                'decode_signals': gen_data['decode_signals'],\n",
    "                'prefill_signals': gen_data['prefill_signals']\n",
    "            })\n",
    "            log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "        \n",
    "        # Compute pairwise distances between all runs\n",
    "        log_print(f\"\\n  Pairwise distances:\")\n",
    "        decode_dists = []\n",
    "        prefill_dists = []\n",
    "        \n",
    "        for i in range(REPRODUCIBILITY_RUNS):\n",
    "            for j in range(i + 1, REPRODUCIBILITY_RUNS):\n",
    "                decode_dist = compare_signals(\n",
    "                    run_signals[i]['decode_signals'],\n",
    "                    run_signals[j]['decode_signals']\n",
    "                )\n",
    "                prefill_dist = compare_signals(\n",
    "                    run_signals[i]['prefill_signals'],\n",
    "                    run_signals[j]['prefill_signals']\n",
    "                )\n",
    "                decode_dists.append(decode_dist['logprobs_mean'])\n",
    "                prefill_dists.append(prefill_dist['logprobs_mean'])\n",
    "                \n",
    "                log_print(f\"    Run {i+1} vs {j+1}: decode={decode_dist['logprobs_mean']:.2e}, prefill={prefill_dist['logprobs_mean']:.2e}\")\n",
    "        \n",
    "        # Compute noise floor stats\n",
    "        finite_decode = [d for d in decode_dists if d != float('inf')]\n",
    "        finite_prefill = [d for d in prefill_dists if d != float('inf')]\n",
    "        \n",
    "        decode_noise = np.mean(finite_decode) if finite_decode else 0\n",
    "        prefill_noise = np.mean(finite_prefill) if finite_prefill else 0\n",
    "        \n",
    "        noise_floors[cfg_name] = {\n",
    "            'decode': decode_noise,\n",
    "            'prefill': prefill_noise\n",
    "        }\n",
    "        \n",
    "        if decode_noise < EQUIVALENCE_THRESHOLD:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: DETERMINISTIC (decode={decode_noise:.2e})\")\n",
    "        else:\n",
    "            log_print(f\"\\n  {cfg_name} noise floor: decode={decode_noise:.2e}, prefill={prefill_noise:.2e}\")\n",
    "        \n",
    "        del llm\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    log_print(\"\\n\" + \"-\"*40)\n",
    "    log_print(\"NOISE FLOOR SUMMARY\")\n",
    "    log_print(\"-\"*40)\n",
    "    for cfg_name, nf in noise_floors.items():\n",
    "        log_print(f\"  {cfg_name}: decode={nf['decode']:.2e}, prefill={nf['prefill']:.2e}\")\n",
    "    log_print(\"\\nCross-format signal must exceed this noise floor to be detectable.\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    return noise_floors\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    import torch\n",
    "    \n",
    "    log_path = setup_logging()\n",
    "    system_info = collect_system_info()\n",
    "    config_names = list(MODEL_CONFIGS.keys())\n",
    "\n",
    "    mode = \"VERIFICATION (teacher-forcing)\" if TEACHER_FORCING else \"GENERATION\"\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"QUANTIZATION FORMAT DETECTION EXPERIMENT - {mode}\")\n",
    "    log_print(\"=\"*80)\n",
    "\n",
    "    log_print(f\"\\nSystem: {system_info['hostname']}\")\n",
    "    log_print(f\"GPU: {system_info['gpu_name']}\")\n",
    "    log_print(f\"vLLM: {system_info['vllm_version']}\")\n",
    "    log_print(f\"PyTorch: {system_info['torch_version']}\")\n",
    "    log_print(f\"CUDA: {system_info['cuda_version']}\")\n",
    "\n",
    "    log_print(f\"\\nConfigurations:\")\n",
    "    for cfg_name, cfg in MODEL_CONFIGS.items():\n",
    "        log_print(f\"  {cfg_name}: {cfg['model_name']} (quant={cfg['quantization']})\")\n",
    "    \n",
    "    if TEACHER_FORCING:\n",
    "        log_print(f\"\\nReference file: {REFERENCE_FILE}\")\n",
    "    log_print()\n",
    "\n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    if not TEACHER_FORCING:\n",
    "        # ================================================================\n",
    "        # GENERATION MODE\n",
    "        # ================================================================\n",
    "        \n",
    "        # First, load one model to get tokenizer for prompt creation\n",
    "        log_print(\"Loading first model to create prompts...\")\n",
    "        first_llm, tokenizer = load_vllm_model(config_names[0])\n",
    "        prompts = create_prompts_from_pdf(tokenizer, NUM_REFERENCES)\n",
    "        log_print(f\"Created {len(prompts)} prompts\\n\")\n",
    "        del first_llm\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # Reproducibility check\n",
    "        noise_floors = {}\n",
    "        if REPRODUCIBILITY_CHECK:\n",
    "            noise_floors = run_reproducibility_check(config_names, prompts)\n",
    "\n",
    "        # Main generation\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"GENERATION MODE\")\n",
    "        log_print(\"=\"*80)\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'environment': system_info,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'max_new_tokens': MAX_NEW_TOKENS,\n",
    "                'tokens_per_slice': TOKENS_PER_SLICE,\n",
    "                'num_references': NUM_REFERENCES,\n",
    "                'top_k_logprobs': TOP_K_LOGPROBS,\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'generations': {}\n",
    "        }\n",
    "\n",
    "        measurements = {}\n",
    "\n",
    "        for cfg_name in config_names:\n",
    "            log_print(f\"\\n--- Config: {cfg_name} ---\")\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(cfg_name)\n",
    "\n",
    "            results['generations'][cfg_name] = []\n",
    "            measurements[cfg_name] = []\n",
    "\n",
    "            for ref_idx, prompt_ids in enumerate(prompts):\n",
    "                log_print(f\"  ref_{ref_idx}: \", end=\"\")\n",
    "                gen_data = run_generation(llm, tokenizer, prompt_ids)\n",
    "\n",
    "                results['generations'][cfg_name].append({\n",
    "                    'ref_idx': ref_idx,\n",
    "                    'prompt_ids': gen_data['prompt_ids'],\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prompt_length': gen_data['prompt_length'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals'],\n",
    "                    'num_generated': gen_data['num_generated']\n",
    "                })\n",
    "\n",
    "                measurements[cfg_name].append({\n",
    "                    'generated_ids': gen_data['generated_ids'],\n",
    "                    'prefill_signals': gen_data['prefill_signals'],\n",
    "                    'decode_signals': gen_data['decode_signals']\n",
    "                })\n",
    "\n",
    "                log_print(f\"{gen_data['num_generated']} tokens\")\n",
    "                decoded = tokenizer.decode(gen_data['generated_ids'][:20])\n",
    "                log_print(f\"    -> {decoded}...\")\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Token consistency check\n",
    "        check_token_consistency(measurements, config_names, tokenizer)\n",
    "\n",
    "        # Within-hardware analysis\n",
    "        prefill_sanity = analyze_within_hardware(measurements, config_names, 'prefill')\n",
    "        decode_sanity = analyze_within_hardware(measurements, config_names, 'decode')\n",
    "\n",
    "        results['prefill_sanity_check'] = prefill_sanity\n",
    "        results['decode_sanity_check'] = decode_sanity\n",
    "        \n",
    "        # Store noise floors\n",
    "        if noise_floors:\n",
    "            results['noise_floors'] = noise_floors\n",
    "            \n",
    "            # Compute signal-to-noise ratio\n",
    "            avg_noise_decode = np.mean([nf['decode'] for nf in noise_floors.values()])\n",
    "            cross_format_decode = decode_sanity.get('off_diagonal_mean', 0)\n",
    "            \n",
    "            if avg_noise_decode > 0 and cross_format_decode:\n",
    "                snr_decode = cross_format_decode / avg_noise_decode\n",
    "                log_print(\"\\n\" + \"=\"*80)\n",
    "                log_print(\"SIGNAL-TO-NOISE ANALYSIS\")\n",
    "                log_print(\"=\"*80)\n",
    "                log_print(f\"\\nNoise floor (within-format variance): {avg_noise_decode:.2e}\")\n",
    "                log_print(f\"Cross-format distance: {cross_format_decode:.2e}\")\n",
    "                log_print(f\"SNR: {snr_decode:.1f}×\")\n",
    "                \n",
    "                if snr_decode > 10:\n",
    "                    log_print(f\"→ Quantization format is DETECTABLE (SNR > 10)\")\n",
    "                elif snr_decode > 3:\n",
    "                    log_print(f\"→ Quantization format is MARGINALLY detectable (3 < SNR < 10)\")\n",
    "                else:\n",
    "                    log_print(f\"→ Quantization format is NOT reliably detectable (SNR < 3)\")\n",
    "                \n",
    "                results['snr_analysis'] = {\n",
    "                    'noise_floor': avg_noise_decode,\n",
    "                    'signal': cross_format_decode,\n",
    "                    'snr': snr_decode\n",
    "                }\n",
    "\n",
    "        # Save\n",
    "        filepath = os.path.join(output_dir, f\"quant_generate_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Generation results saved to: {filepath}\")\n",
    "        log_print(f\"\\nNext step: Copy {filepath} to verifier machine\")\n",
    "        log_print(f\"Then set TEACHER_FORCING = True and REFERENCE_FILE = '<path>'\")\n",
    "\n",
    "        file_size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        log_print(f\"File size: {file_size_mb:.1f} MB\")\n",
    "\n",
    "    else:\n",
    "        # ================================================================\n",
    "        # VERIFICATION MODE\n",
    "        # ================================================================\n",
    "        log_print(\"Loading reference file...\")\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "            content = content.replace('Infinity', '1e309')\n",
    "            reference = json.loads(content)\n",
    "\n",
    "        ref_env = reference['metadata']['environment']\n",
    "        ref_gpu = ref_env['gpu_name']\n",
    "        log_print(f\"Reference GPU: {ref_gpu}\")\n",
    "        log_print(f\"Verifier GPU:  {system_info['gpu_name']}\")\n",
    "\n",
    "        env_validation = validate_environment_match(ref_env, system_info)\n",
    "\n",
    "        # Load equivalent pairs from reference\n",
    "        prefill_equiv_pairs = reference.get('prefill_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        decode_equiv_pairs = reference.get('decode_sanity_check', {}).get('equivalent_pairs', [])\n",
    "        \n",
    "        # Convert to tuples if stored as lists\n",
    "        prefill_equiv_pairs = [tuple(p) for p in prefill_equiv_pairs]\n",
    "        decode_equiv_pairs = [tuple(p) for p in decode_equiv_pairs]\n",
    "        \n",
    "        log_print(f\"\\nLoaded equivalent pairs from reference:\")\n",
    "        log_print(f\"  Prefill: {prefill_equiv_pairs}\")\n",
    "        log_print(f\"  Decode: {decode_equiv_pairs}\")\n",
    "\n",
    "        comparison_results = []\n",
    "\n",
    "        for verify_cfg in config_names:\n",
    "            log_print(f\"\\n{'='*80}\")\n",
    "            log_print(f\"VERIFYING WITH: {verify_cfg}\")\n",
    "            log_print(\"=\"*80)\n",
    "\n",
    "            llm, tokenizer = load_vllm_model(verify_cfg)\n",
    "\n",
    "            for claimed_cfg in config_names:\n",
    "                log_print(f\"\\n  Claimed config: {claimed_cfg}\")\n",
    "\n",
    "                for ref_idx, gen_data in enumerate(reference['generations'][claimed_cfg]):\n",
    "                    is_diagonal = (claimed_cfg == verify_cfg)\n",
    "\n",
    "                    log_print(f\"    ref_{ref_idx} ({'diag' if is_diagonal else 'off'}):\", end=\"\")\n",
    "\n",
    "                    verify_result = run_teacher_forced_verification(\n",
    "                        llm, tokenizer, gen_data, is_diagonal\n",
    "                    )\n",
    "\n",
    "                    prefill_distances = compare_signals(\n",
    "                        gen_data['prefill_signals'],\n",
    "                        verify_result['prefill_signals']\n",
    "                    )\n",
    "\n",
    "                    decode_distances = compare_signals(\n",
    "                        gen_data['decode_signals'],\n",
    "                        verify_result['decode_signals']\n",
    "                    )\n",
    "\n",
    "                    log_print(f\"      Prefill: {prefill_distances['logprobs_mean']:.2e}, Decode: {decode_distances['logprobs_mean']:.2e}\")\n",
    "\n",
    "                    comparison_results.append({\n",
    "                        'ref_idx': ref_idx,\n",
    "                        'claimed_config': claimed_cfg,\n",
    "                        'verify_config': verify_cfg,\n",
    "                        'is_diagonal': is_diagonal,\n",
    "                        'prefill_distances': prefill_distances,\n",
    "                        'decode_distances': decode_distances\n",
    "                    })\n",
    "\n",
    "            del llm\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # Analyze with equivalent pair exclusion\n",
    "        prefill_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='prefill',\n",
    "            equivalent_pairs=prefill_equiv_pairs\n",
    "        )\n",
    "        decode_analysis = analyze_cross_hardware(\n",
    "            comparison_results, config_names, signal_source='decode',\n",
    "            equivalent_pairs=decode_equiv_pairs\n",
    "        )\n",
    "\n",
    "        log_print(\"\\n\" + \"=\"*80)\n",
    "        log_print(\"PREFILL vs DECODE COMPARISON\")\n",
    "        log_print(\"=\"*80)\n",
    "        log_print(f\"Prefill SNR (meaningful): {prefill_analysis['snr_meaningful']:.2f}×\")\n",
    "        log_print(f\"Decode SNR (meaningful):  {decode_analysis['snr_meaningful']:.2f}×\")\n",
    "\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'reference_gpu': ref_gpu,\n",
    "                'verifier_gpu': system_info['gpu_name'],\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reference_environment': ref_env,\n",
    "                'verifier_environment': system_info,\n",
    "                'environment_validation': env_validation,\n",
    "                'model_configs': {k: v for k, v in MODEL_CONFIGS.items()},\n",
    "                'timestamp': timestamp,\n",
    "                'prefill_equivalent_pairs': prefill_equiv_pairs,\n",
    "                'decode_equivalent_pairs': decode_equiv_pairs\n",
    "            },\n",
    "            'comparisons': comparison_results,\n",
    "            'prefill_analysis': prefill_analysis,\n",
    "            'decode_analysis': decode_analysis\n",
    "        }\n",
    "\n",
    "        filepath = os.path.join(output_dir, f\"quant_verify_{timestamp}.json\")\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        log_print(f\"\\n✓ Results saved to: {filepath}\")\n",
    "\n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"EXPERIMENT COMPLETE\")\n",
    "    log_print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097364d-98d0-494d-9982-ac5247671ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
