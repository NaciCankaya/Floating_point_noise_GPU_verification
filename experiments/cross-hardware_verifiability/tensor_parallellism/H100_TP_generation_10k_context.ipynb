{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a28db-273a-4ff1-a889-9ba1b08999e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TENSOR PARALLELISM DETECTABILITY EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Hostname: 552ddec725ff\n",
      "GPUs: 4x NVIDIA H100 80GB HBM3\n",
      "vLLM: 0.11.2\n",
      "Model: Qwen/Qwen3-14B\n",
      "TP sizes to test: [1, 2, 4]\n",
      "Mode: LOCAL\n",
      "\n",
      "Loading tokenizer...\n",
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n",
      "Total source tokens: 120215\n",
      "Prompt structure: 33 prefix + 10000 snippet + 34 suffix = 10067 tokens\n",
      "Created 3 prompts\n",
      "\n",
      "================================================================================\n",
      "TENSOR PARALLEL = 1\n",
      "================================================================================\n",
      "Loading model with TP=1...\n",
      "INFO 11-26 23:39:23 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'max_model_len': 32768, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen3-14B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:39:24 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-26 23:39:24 [model.py:1745] Using max model len 32768\n",
      "INFO 11-26 23:39:26 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 11-26 23:39:26 [vllm.py:500] Cudagraph is disabled under eager mode\n",
      "WARNING 11-26 23:39:28 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:32 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:33 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.23.0.2:57375 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:33 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:33 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:33 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:33 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:04,  1.71it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:04,  1.36it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:03,  1.29it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.25it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:03<00:02,  1.23it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:04<00:01,  1.22it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:05<00:00,  1.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.40it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.32it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:40 [default_loader.py:314] Loading weights took 6.17 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:41 [gpu_model_runner.py:3338] Model loading took 27.5185 GiB memory and 7.194715 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:42 [gpu_worker.py:359] Available KV cache memory: 14.15 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:42 [kv_cache_utils.py:1229] GPU KV cache size: 92,752 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:42 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 2.83x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m 2025-11-26 23:39:42,586 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m 2025-11-26 23:39:42,597 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:43 [core.py:250] init engine (profile, create kv cache, warmup model) took 2.02 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=13380)\u001b[0;0m INFO 11-26 23:39:44 [vllm.py:500] Cudagraph is disabled under eager mode\n",
      "INFO 11-26 23:39:44 [llm.py:352] Supported tasks: ['generate']\n",
      "\n",
      "================================================================================\n",
      "REPRODUCIBILITY TEST\n",
      "================================================================================\n",
      "Running same prompt 5 times...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8bcc03d160f40a9945bcb02feae460e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f38456e9bae4683a107183f652983d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 1: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbc9e780165d44d1bda28cca7ac2d0df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91ccc6fc899a402b938abe525504eece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 2: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2930858a3de346fdb6e467c9ae28bc08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d920124e2d3d475fa6d8e3ee2db54de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 3: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f398e15280249b3818425b89bd0ed92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29add273dc6a4e65bd181f98b4ed2748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 4: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb22f6906574f0ca77800533ceaf015",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03bb386b2b844ca08214cba968015992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 5: done\n",
      "\n",
      "âœ“ REPRODUCIBILITY TEST PASSED\n",
      "  All 5 runs identical\n",
      "  Max logprob difference: 0.00e+00\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8505efb79b8843e7b34f503dd6bcbb8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22a023476b464520bf39ba254ceefbab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcd268d8ff8413ebe6dc9f36e8b074c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5bc56acc2434fed916cab1d3ff76bd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a290d7990b14c99802b434f696d544e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b7ff365d3845ac93dd4d5a9a2a3b62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1126 23:40:05.865436584 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TENSOR PARALLEL = 2\n",
      "================================================================================\n",
      "Loading model with TP=2...\n",
      "INFO 11-26 23:40:05 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'max_model_len': 32768, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen3-14B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:40:06 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-26 23:40:06 [model.py:1745] Using max model len 32768\n",
      "INFO 11-26 23:40:06 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 11-26 23:40:06 [vllm.py:500] Cudagraph is disabled under eager mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=14073)\u001b[0;0m INFO 11-26 23:40:12 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14073)\u001b[0;0m WARNING 11-26 23:40:12 [multiproc_executor.py:869] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:40:17 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:59615 backend=nccl\n",
      "INFO 11-26 23:40:18 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:59615 backend=nccl\n",
      "[Gloo] Rank [Gloo] Rank 1 is connected to 01 is connected to  peer ranks. 1Expected number of connected peer ranks is :  peer ranks. 1Expected number of connected peer ranks is : 1\n",
      "\n",
      "[Gloo] Rank 0 is connected to [Gloo] Rank 1 peer ranks. Expected number of connected peer ranks is : 11 is connected to \n",
      "1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-26 23:40:18 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0[Gloo] Rank  is connected to 1 peer ranks. 1Expected number of connected peer ranks is :  is connected to 11 peer ranks. \n",
      "Expected number of connected peer ranks is : 1\n",
      "INFO 11-26 23:40:19 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-26 23:40:19 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m INFO 11-26 23:40:20 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...\n",
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m INFO 11-26 23:40:20 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m INFO 11-26 23:40:20 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP1 pid=14208)\u001b[0;0m INFO 11-26 23:40:20 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP1 pid=14208)\u001b[0;0m INFO 11-26 23:40:20 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:03,  2.27it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:03,  1.75it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:01<00:03,  1.64it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:02<00:02,  1.59it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:03<00:01,  1.58it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:03<00:01,  1.57it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:04<00:00,  1.56it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:04<00:00,  1.94it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:04<00:00,  1.74it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m INFO 11-26 23:40:25 [default_loader.py:314] Loading weights took 4.63 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m INFO 11-26 23:40:26 [gpu_model_runner.py:3338] Model loading took 13.8818 GiB memory and 5.658196 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m INFO 11-26 23:40:39 [gpu_worker.py:359] Available KV cache memory: 27.32 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14073)\u001b[0;0m INFO 11-26 23:40:39 [kv_cache_utils.py:1229] GPU KV cache size: 358,048 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14073)\u001b[0;0m INFO 11-26 23:40:39 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 10.93x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14073)\u001b[0;0m INFO 11-26 23:40:39 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 10.93x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m 2025-11-26 23:40:39,812 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP1 pid=14208)\u001b[0;0m 2025-11-26 23:40:39,812 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m 2025-11-26 23:40:39,828 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP1 pid=14208)\u001b[0;0m 2025-11-26 23:40:39,835 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=14073)\u001b[0;0m INFO 11-26 23:40:40 [core.py:250] init engine (profile, create kv cache, warmup model) took 13.55 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14073)\u001b[0;0m INFO 11-26 23:40:41 [vllm.py:500] Cudagraph is disabled under eager mode\n",
      "INFO 11-26 23:40:41 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a18a90e5f641bb95628e0b52c63251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd6a7cadf7db4835926a306201852105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89c14b6ec7d843818bdf798bb59e2987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ff9a4fbddec42d8b60a5159eb8468ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0e097c1c254b99bf6a1ab59e482fb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab272001fac344ca87f542e9038d1e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "\u001b[1;36m(Worker_TP0 pid=14207)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=14208)\u001b[0;0m INFO 11-26 23:40:44 [multiproc_executor.py:702] Parent process exited, terminating worker\n",
      "INFO 11-26 23:40:44 [multiproc_executor.py:702] Parent process exited, terminating worker\n",
      "\n",
      "================================================================================\n",
      "TENSOR PARALLEL = 4\n",
      "================================================================================\n",
      "Loading model with TP=4...\n",
      "INFO 11-26 23:40:47 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'max_model_len': 32768, 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen3-14B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:40:47 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-26 23:40:47 [model.py:1745] Using max model len 32768\n",
      "INFO 11-26 23:40:47 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 11-26 23:40:47 [vllm.py:500] Cudagraph is disabled under eager mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=14386)\u001b[0;0m INFO 11-26 23:40:53 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14386)\u001b[0;0m WARNING 11-26 23:40:53 [multiproc_executor.py:869] Reducing Torch parallelism from 104 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:40:59 [parallel_state.py:1208] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:57705 backend=nccl\n",
      "INFO 11-26 23:40:59 [parallel_state.py:1208] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:57705 backend=nccl\n",
      "INFO 11-26 23:40:59 [parallel_state.py:1208] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:57705 backend=nccl\n",
      "INFO 11-26 23:40:59 [parallel_state.py:1208] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:57705 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. [Gloo] Rank Expected number of connected peer ranks is : 3\n",
      "2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 11-26 23:41:00 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3[Gloo] Rank  is connected to 32 peer ranks.  is connected to Expected number of connected peer ranks is : 33 peer ranks. Expected number of connected peer ranks is : \n",
      "3\n",
      "INFO 11-26 23:41:02 [parallel_state.py:1394] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-26 23:41:02 [parallel_state.py:1394] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 11-26 23:41:02 [parallel_state.py:1394] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 11-26 23:41:02 [parallel_state.py:1394] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "\u001b[1;36m(Worker_TP0 pid=14519)\u001b[0;0m INFO 11-26 23:41:02 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...\n",
      "\u001b[1;36m(Worker_TP3 pid=14522)\u001b[0;0m INFO 11-26 23:41:02 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP3 pid=14522)\u001b[0;0m INFO 11-26 23:41:02 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=14519)\u001b[0;0m INFO 11-26 23:41:02 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP0 pid=14519)\u001b[0;0m INFO 11-26 23:41:02 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP1 pid=14520)\u001b[0;0m INFO 11-26 23:41:03 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP1 pid=14520)\u001b[0;0m INFO 11-26 23:41:03 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP2 pid=14521)\u001b[0;0m INFO 11-26 23:41:03 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP2 pid=14521)\u001b[0;0m INFO 11-26 23:41:03 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:01,  3.93it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:00<00:01,  3.05it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:01<00:01,  2.81it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:01<00:01,  2.70it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:01<00:01,  2.63it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:02<00:00,  2.61it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:02<00:00,  2.59it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:02<00:00,  3.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:02<00:00,  2.92it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=14519)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=14519)\u001b[0;0m INFO 11-26 23:41:07 [default_loader.py:314] Loading weights took 2.76 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=14519)\u001b[0;0m INFO 11-26 23:41:08 [gpu_model_runner.py:3338] Model loading took 6.9456 GiB memory and 4.923804 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=14519)\u001b[0;0m INFO 11-26 23:41:25 [gpu_worker.py:359] Available KV cache memory: 34.20 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14386)\u001b[0;0m INFO 11-26 23:41:26 [kv_cache_utils.py:1229] GPU KV cache size: 896,656 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14386)\u001b[0;0m INFO 11-26 23:41:26 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 27.36x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14386)\u001b[0;0m INFO 11-26 23:41:26 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 27.36x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14386)\u001b[0;0m INFO 11-26 23:41:26 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 27.36x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14386)\u001b[0;0m INFO 11-26 23:41:26 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 27.36x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP3 pid=14522)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=14519)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=14520)\u001b[0;0m 2025-11-26 23:41:26,226 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2025-11-26 23:41:26,226 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "2025-11-26 23:41:26,226 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP2 pid=14521)\u001b[0;0m 2025-11-26 23:41:26,226 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...\n",
      "\u001b[1;36m(Worker_TP3 pid=14522)\u001b[0;0m 2025-11-26 23:41:26,243 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP0 pid=14519)\u001b[0;0m 2025-11-26 23:41:26,243 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP1 pid=14520)\u001b[0;0m 2025-11-26 23:41:26,244 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n",
      "\u001b[1;36m(Worker_TP2 pid=14521)\u001b[0;0m 2025-11-26 23:41:26,250 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=14386)\u001b[0;0m INFO 11-26 23:41:26 [core.py:250] init engine (profile, create kv cache, warmup model) took 18.22 seconds\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Tensor Parallelism (TP) detectability experiment using vLLM.\n",
    "Compares tensor_parallel=1, 2, 4 on dense model in BF16.\n",
    "Prefill-only, logprobs-only.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "import gc\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-14B\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "# Mode: False = run locally and save, True = load reference and compare cross-hardware\n",
    "CROSS_HARDWARE_MODE = False\n",
    "REFERENCE_FILE = \"/workspace/tp_reference.json\"\n",
    "\n",
    "TP_SIZES = [1, 2, 4]  # Tensor parallel sizes to compare\n",
    "\n",
    "TOKENS_PER_SLICE = 10000\n",
    "NUM_REFERENCES = 3\n",
    "TOP_K_LOGPROBS = 5\n",
    "\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_path = os.path.join(output_dir, f\"tp_experiment_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_prompts(tokenizer, num_references=NUM_REFERENCES):\n",
    "    pdf_files = sorted(glob.glob(\"/workspace/*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        pdf_files = sorted(glob.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    content_tokens = tokenizer.encode(all_text)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    if len(content_tokens) < num_references * TOKENS_PER_SLICE:\n",
    "        raise ValueError(f\"Need {num_references * TOKENS_PER_SLICE} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "    \n",
    "    suffix = f\"\"\"\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    suffix_tokens = tokenizer.encode(suffix)\n",
    "    \n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    prompts = []\n",
    "    prompt_texts = []\n",
    "    for i in range(num_references):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt_tokens = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        prompts.append(prompt_tokens)\n",
    "        prompt_texts.append(tokenizer.decode(prompt_tokens))\n",
    "    \n",
    "    return prompts, prompt_texts\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info(tp_size=None):\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        \"tensor_parallel_size\": tp_size,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import vllm\n",
    "        info[\"vllm_version\"] = vllm.__version__\n",
    "    except (ImportError, AttributeError):\n",
    "        info[\"vllm_version\"] = \"unknown\"\n",
    "    \n",
    "    return info\n",
    "\n",
    "# ============================================================================\n",
    "# PREFILL LOGPROB EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_prefill_logprobs(output, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract prompt logprobs at specified positions.\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    prompt_logprobs = output.prompt_logprobs\n",
    "    if prompt_logprobs is None:\n",
    "        return signals\n",
    "    \n",
    "    num_positions = len(prompt_logprobs)\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else num_positions + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= num_positions:\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = prompt_logprobs[actual_idx]\n",
    "        \n",
    "        if token_logprobs is None:\n",
    "            continue\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            if hasattr(logprob_obj, 'logprob'):\n",
    "                log_probs.append(logprob_obj.logprob)\n",
    "            else:\n",
    "                log_probs.append(float(logprob_obj))\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def run_prefill(llm, prompt_text):\n",
    "    \"\"\"Run prefill and extract logprobs.\"\"\"\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=1,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([prompt_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    prefill_signals = extract_prefill_logprobs(output, positions=[-3, -2, -1])\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals\n",
    "    }\n",
    "\n",
    "def test_reproducibility(llm, prompt_text, num_runs=5):\n",
    "    \"\"\"\n",
    "    Test that the model produces deterministic outputs.\n",
    "    Run the same prompt multiple times and compare logprobs.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"REPRODUCIBILITY TEST\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"Running same prompt {num_runs} times...\")\n",
    "    \n",
    "    results = []\n",
    "    for i in range(num_runs):\n",
    "        result = run_prefill(llm, prompt_text)\n",
    "        results.append(result)\n",
    "        log_print(f\"  Run {i+1}: done\")\n",
    "        # Clear any accumulated cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compare all runs to the first\n",
    "    all_identical = True\n",
    "    max_diffs_seen = []\n",
    "    \n",
    "    for i in range(1, num_runs):\n",
    "        for pos in results[0]['prefill_signals'].keys():\n",
    "            lp0 = results[0]['prefill_signals'][pos]['logprobs']\n",
    "            lpi = results[i]['prefill_signals'][pos]['logprobs']\n",
    "            \n",
    "            # Compare token IDs\n",
    "            if lp0['token_ids'] != lpi['token_ids']:\n",
    "                log_print(f\"  âœ— Run {i+1} {pos}: token_ids differ\")\n",
    "                all_identical = False\n",
    "                continue\n",
    "            \n",
    "            # Compare log_probs values\n",
    "            diffs = [abs(a - b) for a, b in zip(lp0['log_probs'], lpi['log_probs'])]\n",
    "            max_diff = max(diffs) if diffs else 0\n",
    "            max_diffs_seen.append(max_diff)\n",
    "            \n",
    "            if max_diff > 1e-9:\n",
    "                log_print(f\"  âœ— Run {i+1} {pos}: max logprob diff = {max_diff:.2e}\")\n",
    "                all_identical = False\n",
    "    \n",
    "    if all_identical:\n",
    "        overall_max = max(max_diffs_seen) if max_diffs_seen else 0\n",
    "        log_print(f\"\\nâœ“ REPRODUCIBILITY TEST PASSED\")\n",
    "        log_print(f\"  All {num_runs} runs identical\")\n",
    "        log_print(f\"  Max logprob difference: {overall_max:.2e}\")\n",
    "    else:\n",
    "        log_print(f\"\\nâš  REPRODUCIBILITY TEST FAILED\")\n",
    "        log_print(f\"  Non-deterministic outputs detected\")\n",
    "        log_print(f\"  Results may vary between runs even with same TP setting\")\n",
    "    \n",
    "    return all_identical\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_logprob_distance_canonical(logprobs1, logprobs2, canonical_ids):\n",
    "    \"\"\"\n",
    "    Compute L2 distance between logprobs for a canonical set of token IDs.\n",
    "    \"\"\"\n",
    "    lp1 = dict(zip(logprobs1['token_ids'], logprobs1['log_probs']))\n",
    "    lp2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "\n",
    "    for tid in canonical_ids:\n",
    "        if tid in lp1 and tid in lp2:\n",
    "            vec1.append(lp1[tid])\n",
    "            vec2.append(lp2[tid])\n",
    "\n",
    "    if len(vec1) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals_with_canonical_ids(signals1, signals2, canonical_token_ids):\n",
    "    \"\"\"Compare using pre-specified canonical token IDs (top 5).\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "\n",
    "    all_dists = []\n",
    "\n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]\n",
    "\n",
    "        # Use top 5 for comparison\n",
    "        canonical_ids = canonical_token_ids.get(pos_label, sig1['logprobs']['token_ids'][:5])[:5]\n",
    "        dist = compute_logprob_distance_canonical(\n",
    "            sig1['logprobs'], sig2['logprobs'], canonical_ids\n",
    "        )\n",
    "        all_dists.append(dist)\n",
    "\n",
    "    return {\n",
    "        'logprobs_mean': np.mean(all_dists) if all_dists else 0.0\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_results(measurements, tp_sizes):\n",
    "    \"\"\"Analyze prefill logprob differences across TP sizes (within-hardware).\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"WITHIN-HARDWARE TP COMPARISON (PREFILL)\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    n = len(tp_sizes)\n",
    "    all_matrices = []\n",
    "    \n",
    "    # Use TP=1 (first) as canonical for token IDs\n",
    "    canonical_tp = tp_sizes[0]\n",
    "    \n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        # Get canonical token IDs from first TP size\n",
    "        canonical_signals = measurements[canonical_tp][ref_idx]['prefill_signals']\n",
    "        canonical_token_ids = {}\n",
    "        for pos_label, pos_data in canonical_signals.items():\n",
    "            canonical_token_ids[pos_label] = pos_data['logprobs']['token_ids']\n",
    "        \n",
    "        for i, tp_i in enumerate(tp_sizes):\n",
    "            for j, tp_j in enumerate(tp_sizes):\n",
    "                if i == j:\n",
    "                    matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    sig_i = measurements[tp_i][ref_idx]['prefill_signals']\n",
    "                    sig_j = measurements[tp_j][ref_idx]['prefill_signals']\n",
    "                    \n",
    "                    distances = compare_signals_with_canonical_ids(sig_i, sig_j, canonical_token_ids)\n",
    "                    matrix[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        header = \"       \" + \" \".join(f\"TP={tp:>3}\" for tp in tp_sizes)\n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, tp in enumerate(tp_sizes):\n",
    "            row = f\"TP={tp:<3}\" + \" \".join(f\"  {matrix[i,j]:6.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "    \n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"       \" + \" \".join(f\"TP={tp:>3}\" for tp in tp_sizes)\n",
    "    log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, tp in enumerate(tp_sizes):\n",
    "        row = f\"TP={tp:<3}\" + \" \".join(f\"  {avg_matrix[i,j]:6.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "    \n",
    "    # Off-diagonal stats\n",
    "    off_diag = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag.append(avg_matrix[i, j])\n",
    "    \n",
    "    diagonal = np.mean([avg_matrix[i, i] for i in range(n)])\n",
    "    off_diagonal = np.mean(off_diag)\n",
    "    \n",
    "    log_print(f\"\\nDiagonal (same TP): {diagonal:.2e}\")\n",
    "    log_print(f\"Off-diagonal (different TP): {off_diagonal:.2e}\")\n",
    "    \n",
    "    if diagonal > 0:\n",
    "        snr = off_diagonal / diagonal\n",
    "        log_print(f\"SNR: {snr:.2f}Ã—\")\n",
    "    else:\n",
    "        snr = float('inf') if off_diagonal > 0 else 1.0\n",
    "        log_print(f\"SNR: {snr}\")\n",
    "    \n",
    "    # Check equivalences\n",
    "    equiv_pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if avg_matrix[i, j] < EQUIVALENCE_THRESHOLD:\n",
    "                equiv_pairs.append((tp_sizes[i], tp_sizes[j]))\n",
    "    \n",
    "    if equiv_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs:\")\n",
    "        for p in equiv_pairs:\n",
    "            log_print(f\"  TP={p[0]} â‰ˆ TP={p[1]}\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'diagonal': float(diagonal),\n",
    "        'off_diagonal': float(off_diagonal),\n",
    "        'snr': float(snr) if snr != float('inf') else None,\n",
    "        'equivalent_pairs': equiv_pairs\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_cross_hardware(ref_measurements, ver_measurements, tp_sizes):\n",
    "    \"\"\"Analyze cross-hardware TP comparison.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"CROSS-HARDWARE TP COMPARISON (PREFILL)\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(\"  Rows = reference TP, Cols = verifier TP\")\n",
    "    \n",
    "    n = len(tp_sizes)\n",
    "    all_matrices = []\n",
    "    \n",
    "    # Use reference TP=1 as canonical for token IDs\n",
    "    canonical_tp = str(tp_sizes[0])\n",
    "    \n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        # Get canonical token IDs from reference TP=1\n",
    "        canonical_signals = ref_measurements[canonical_tp][ref_idx]['prefill_signals']\n",
    "        canonical_token_ids = {}\n",
    "        for pos_label, pos_data in canonical_signals.items():\n",
    "            canonical_token_ids[pos_label] = pos_data['logprobs']['token_ids']\n",
    "        \n",
    "        for i, tp_ref in enumerate(tp_sizes):\n",
    "            for j, tp_ver in enumerate(tp_sizes):\n",
    "                sig_ref = ref_measurements[str(tp_ref)][ref_idx]['prefill_signals']\n",
    "                sig_ver = ver_measurements[tp_ver][ref_idx]['prefill_signals']\n",
    "                \n",
    "                distances = compare_signals_with_canonical_ids(sig_ref, sig_ver, canonical_token_ids)\n",
    "                matrix[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        header = \"       \" + \" \".join(f\"v_TP={tp:>2}\" for tp in tp_sizes)\n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, tp in enumerate(tp_sizes):\n",
    "            row = f\"r_TP={tp:<2}\" + \" \".join(f\"  {matrix[i,j]:6.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "    \n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"       \" + \" \".join(f\"v_TP={tp:>2}\" for tp in tp_sizes)\n",
    "    log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, tp in enumerate(tp_sizes):\n",
    "        row = f\"r_TP={tp:<2}\" + \" \".join(f\"  {avg_matrix[i,j]:6.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "    \n",
    "    # SNR calculation\n",
    "    diagonal = np.mean([avg_matrix[i, i] for i in range(n)])\n",
    "    off_diag = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag.append(avg_matrix[i, j])\n",
    "    off_diagonal = np.mean(off_diag)\n",
    "    \n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same TP):\")\n",
    "    log_print(f\"  Logprobs: {diagonal:.2e}\")\n",
    "    log_print(f\"\\nOff-diagonal (different TP):\")\n",
    "    log_print(f\"  Logprobs - Mean: {off_diagonal:.2e}\", end=\"\")\n",
    "    \n",
    "    if diagonal > 0:\n",
    "        snr = off_diagonal / diagonal\n",
    "        log_print(f\", SNR: {snr:.2f}Ã—\")\n",
    "    else:\n",
    "        snr = float('inf') if off_diagonal > 0 else 1.0\n",
    "        log_print(f\", SNR: {snr}\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'diagonal': float(diagonal),\n",
    "        'off_diagonal': float(off_diagonal),\n",
    "        'snr': float(snr) if snr != float('inf') else None\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    log_path = setup_logging()\n",
    "    \n",
    "    log_print(\"=\" * 80)\n",
    "    log_print(\"TENSOR PARALLELISM DETECTABILITY EXPERIMENT\")\n",
    "    log_print(\"=\" * 80)\n",
    "    \n",
    "    system_info = collect_system_info()\n",
    "    log_print(f\"\\nHostname: {system_info['hostname']}\")\n",
    "    log_print(f\"GPUs: {system_info['gpu_count']}x {system_info['gpu_name']}\")\n",
    "    log_print(f\"vLLM: {system_info['vllm_version']}\")\n",
    "    log_print(f\"Model: {MODEL_NAME}\")\n",
    "    log_print(f\"TP sizes to test: {TP_SIZES}\")\n",
    "    log_print(f\"Mode: {'CROSS-HARDWARE' if CROSS_HARDWARE_MODE else 'LOCAL'}\")\n",
    "    \n",
    "    # Load reference file first if in cross-hardware mode (to get prompts)\n",
    "    reference = None\n",
    "    if CROSS_HARDWARE_MODE:\n",
    "        log_print(f\"\\nLoading reference: {REFERENCE_FILE}\")\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "            content = content.replace('Infinity', '1e309')\n",
    "            reference = json.loads(content)\n",
    "        prompt_texts = reference['prompt_texts']\n",
    "        log_print(f\"Loaded {len(prompt_texts)} prompts from reference\")\n",
    "    else:\n",
    "        # Use transformers tokenizer (much faster than loading vLLM)\n",
    "        log_print(\"\\nLoading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        \n",
    "        prompts, prompt_texts = create_prompts(tokenizer, NUM_REFERENCES)\n",
    "        log_print(f\"Created {len(prompts)} prompts\")\n",
    "    \n",
    "    # Run for each TP size\n",
    "    measurements = {}\n",
    "    environments = {}\n",
    "    reproducibility_passed = None\n",
    "    \n",
    "    for tp_size in TP_SIZES:\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"TENSOR PARALLEL = {tp_size}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        log_print(f\"Loading model with TP={tp_size}...\")\n",
    "        llm = LLM(\n",
    "            model=MODEL_NAME,\n",
    "            download_dir=CACHE_DIR,\n",
    "            dtype=\"bfloat16\",\n",
    "            trust_remote_code=True,\n",
    "            tensor_parallel_size=tp_size,\n",
    "            gpu_memory_utilization=0.6,\n",
    "            max_model_len=32768,\n",
    "            enforce_eager=True,\n",
    "        )\n",
    "        \n",
    "        # Run reproducibility test on first TP size\n",
    "        if reproducibility_passed is None:\n",
    "            reproducibility_passed = test_reproducibility(llm, prompt_texts[0], num_runs=5)\n",
    "        \n",
    "        environments[tp_size] = collect_system_info(tp_size)\n",
    "        measurements[tp_size] = []\n",
    "        \n",
    "        for ref_idx, prompt_text in enumerate(prompt_texts):\n",
    "            log_print(f\"  ref_{ref_idx}: \", end=\"\")\n",
    "            \n",
    "            result = run_prefill(llm, prompt_text)\n",
    "            measurements[tp_size].append(result)\n",
    "            \n",
    "            num_positions = len(result['prefill_signals'])\n",
    "            log_print(f\"{num_positions} positions captured\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del llm\n",
    "        destroy_model_parallel()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Within-hardware analysis\n",
    "    within_analysis = analyze_results(measurements, TP_SIZES)\n",
    "    \n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if not CROSS_HARDWARE_MODE:\n",
    "        # Save results for potential cross-hardware comparison later\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'model': MODEL_NAME,\n",
    "                'dtype': 'bfloat16',\n",
    "                'tp_sizes': TP_SIZES,\n",
    "                'tokens_per_slice': TOKENS_PER_SLICE,\n",
    "                'num_references': NUM_REFERENCES,\n",
    "                'top_k_logprobs': TOP_K_LOGPROBS,\n",
    "                'reproducibility_test_passed': reproducibility_passed,\n",
    "                'environments': {str(k): v for k, v in environments.items()},\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'prompt_texts': prompt_texts,\n",
    "            'measurements': {str(k): v for k, v in measurements.items()},\n",
    "            'within_hardware_analysis': within_analysis\n",
    "        }\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"tp_experiment_{timestamp}.json\")\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        log_print(f\"\\nSaved to: {filepath}\")\n",
    "        log_print(f\"\\nFor cross-hardware comparison:\")\n",
    "        log_print(f\"  1. Copy {filepath} to second machine\")\n",
    "        log_print(f\"  2. Set CROSS_HARDWARE_MODE = True\")\n",
    "        log_print(f\"  3. Set REFERENCE_FILE = '<path to json>'\")\n",
    "        \n",
    "    else:\n",
    "        # Cross-hardware comparison (reference already loaded at start)\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(\"CROSS-HARDWARE COMPARISON\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        # Environment comparison\n",
    "        ref_env = reference['metadata']['environments']['1']\n",
    "        ver_env = environments[1]\n",
    "        \n",
    "        log_print(\"\\nEnvironment comparison:\")\n",
    "        log_print(f\"  Reference GPU: {ref_env['gpu_name']}\")\n",
    "        log_print(f\"  Verifier GPU:  {ver_env['gpu_name']}\")\n",
    "        \n",
    "        check_fields = ['vllm_version', 'torch_version', 'cuda_version']\n",
    "        for field in check_fields:\n",
    "            ref_val = ref_env.get(field, 'N/A')\n",
    "            ver_val = ver_env.get(field, 'N/A')\n",
    "            match = \"âœ“\" if ref_val == ver_val else \"âœ—\"\n",
    "            log_print(f\"  {field}: {ref_val} vs {ver_val} {match}\")\n",
    "        \n",
    "        ref_measurements = reference['measurements']\n",
    "        \n",
    "        cross_analysis = analyze_cross_hardware(ref_measurements, measurements, TP_SIZES)\n",
    "        \n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'model': MODEL_NAME,\n",
    "                'dtype': 'bfloat16',\n",
    "                'tp_sizes': TP_SIZES,\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reproducibility_test_passed': reproducibility_passed,\n",
    "                'reference_environments': reference['metadata']['environments'],\n",
    "                'verifier_environments': {str(k): v for k, v in environments.items()},\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'within_hardware_analysis': within_analysis,\n",
    "            'cross_hardware_analysis': cross_analysis\n",
    "        }\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"tp_cross_hardware_{timestamp}.json\")\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        log_print(f\"\\nSaved to: {filepath}\")\n",
    "    \n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5ac87-29b5-4dab-93c5-b7acefe591d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
