{
  "metadata": {
    "model": "Qwen/Qwen3-14B",
    "dtype": "bfloat16",
    "tp_sizes": [
      1,
      2,
      4
    ],
    "tokens_per_slice": 10000,
    "num_references": 3,
    "top_k_logprobs": 5,
    "reproducibility_test_passed": true,
    "environments": {
      "1": {
        "hostname": "63ddf43f81b3",
        "platform": "Linux-6.8.0-52-generic-x86_64-with-glibc2.39",
        "python_version": "3.12.3",
        "torch_version": "2.9.0+cu128",
        "cuda_version": "12.8",
        "cudnn_version": "91002",
        "transformers_version": "4.57.3",
        "numpy_version": "2.1.2",
        "gpu_name": "NVIDIA A100-SXM4-80GB",
        "gpu_count": 4,
        "tensor_parallel_size": 1,
        "vllm_version": "0.11.2"
      },
      "2": {
        "hostname": "63ddf43f81b3",
        "platform": "Linux-6.8.0-52-generic-x86_64-with-glibc2.39",
        "python_version": "3.12.3",
        "torch_version": "2.9.0+cu128",
        "cuda_version": "12.8",
        "cudnn_version": "91002",
        "transformers_version": "4.57.3",
        "numpy_version": "2.1.2",
        "gpu_name": "NVIDIA A100-SXM4-80GB",
        "gpu_count": 4,
        "tensor_parallel_size": 2,
        "vllm_version": "0.11.2"
      },
      "4": {
        "hostname": "63ddf43f81b3",
        "platform": "Linux-6.8.0-52-generic-x86_64-with-glibc2.39",
        "python_version": "3.12.3",
        "torch_version": "2.9.0+cu128",
        "cuda_version": "12.8",
        "cudnn_version": "91002",
        "transformers_version": "4.57.3",
        "numpy_version": "2.1.2",
        "gpu_name": "NVIDIA A100-SXM4-80GB",
        "gpu_count": 4,
        "tensor_parallel_size": 4,
        "vllm_version": "0.11.2"
      }
    },
    "timestamp": "20251126_234742"
  },
  "prompt_texts": [
    "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere is an excerpt from a document:\n\n\"VerificationforInternational\nAIGovernance\nBenHarack1,RobertF.Trager1,2,AnkaReuel3,4,DavidManheim5,6,\nMilesBrundage7,OnniAarne8,AaronScher9,YanliangPan10,JennyXiao11,12,\nKristyLoke ,SumayaNurAdan1,13,GuillemBas14,15,NicholasA.Caputo1,\nJuliaC.Morse1,16,JanviAhuja1,17,IsabellaDuan18,JanetEgan19,BenBucknall1,\nBriannaRosen20,RenanAraujo1,8,VincentBoulanin21,RanjitLall22,FazlBarez1,\nSanaaAlvira ,CorinKatzke23,AhmadAtamli24,andAmroAwad1\n1Oxford Martin AI Governance Initiative,\n2Blavatnik School of Government, University of Oxford,3Stanford University,\n4Robert and Ren\u00e9e Belfer Center for Science and International Affairs,\nHarvard Kennedy School, Harvard University,\n5Association for Long Term Existence and Resilience,\n6Technion - Israel Institute of Technology,\n7AI Verification and Evaluation Research Institute,8Institute for AI Policy and Strategy,\n9Machine Intelligence Research Institute,\n10James Martin Center for Nonproliferation Studies,11Leonis Capital,\n12Columbia University,13Centre for AI Security and Access,\n14Observatorio de Riesgos Catastr\u00f3ficos Globales,15Future Impact Group,\n16University of California, Santa Barbara,17Big Data Institute, University of Oxford,\n18Safe AI Forum,19Center for a New American Security,\n20Oxford Programme for Cyber and Technology Policy,\n21Governance of AI Programme, Stockholm International Peace Research Institute,\n22University of Oxford,23Convergence Analysis,24University of Southampton\nEach author contributed ideas or writing to the report. However, being an author does not imply\nagreement with every claim made in the report. Authors are listed in approximately descending order\nof contribution. Abstract\nThe growing impacts of artificial intelligence (AI) are spurring states to con-\nsider international agreements that could help manage this rapidly evolving\ntechnology. The political feasibility of such agreements can hinge on their\nverifiability \u2014the extent to which the states involved can determine whether\nother states are complying. This report analyzes several potential interna-\ntional agreements and ways they could be verified. To improve the robustness\nof the conclusions, pessimistic assumptions are made about the technical and\npolitical parameters of the verification challenge.\nThis report has three primary findings. First, verification of many inter-\nnational AI agreements appears possible even without speculative advances\nin verification technology. Some agreements can be verified using existing\nhardware, while others will require major investments in developing and in-\nstalling verification infrastructure. In particular, verifying the regulation of\ndata center-based AI development and deployment appears to be possible\nwithin a few years if serious efforts are made toward that goal. One such\nscheme would require 1) constructing and installing narrow-purpose verifi-\ncation hardware in data centers and 2) creating a mutually verified data cen-\nter which can run privacy-preserving computations. Second, verification for\nsome kinds of AI-related activities is likely to face a combination of technical\nand political barriers, thus limiting prospects for agreement. In particular,\nthe detailed regulation of mobile AI-enabled devices in sensitive domains\u2014\nsuch as weapons\u2014faces severe political challenges. Third, near-term actions\nin several areas, including research and development as well as state policy,\ncan improve the prospects for future verification agreements by reducing\ncosts and security concerns. In sum, this report outlines workable approaches\nfor verifying international AI agreements and illustrates how investments in\nverification today can shape the political possibilities of tomorrow.\nCorresponding author : Ben Harack (ben.harack@gmail.com).\n2 Contents\nAbstract 2\nContents 3\nExecutiveSummary 7\nPessimistic assumptions about verification difficulty . . . . . . . . . . . . . . . . . . . 7\nAgreements and their verifiability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nThe challenge of verifying computational rules and tools for accomplishing it . . . 10\nTwo approaches to verifiable confidential computing . . . . . . . . . . . . . . . . . . 12\nPolitical options and tradeoffs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\nAreas for further work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n1 Introduction 17\n1.1 Scope . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n1.1.1 AI types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n1.1.2 Agreement types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n1.1.3 Verification mechanisms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n1.2 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n1.3 Political needs and political will . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n1.4 Importance of AI agreement verification . . . . . . . . . . . . . . . . . . . . . . . 27\n1.5 Challenges of AI agreement verification . . . . . . . . . . . . . . . . . . . . . . . 29\n1.5.1 Transparency-security tradeoff . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.5.2 Asymmetric burden of proof for verifying negative claims . . . . . . . . 31\n1.5.3 Why AI verification is particularly challenging . . . . . . . . . . . . . . . . 32\n1.6 AI verification may be workable . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n2 Surveyofverificationcomponents 39\n2.1 Personnel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n2.1.1 Verifiable personnel controls . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n3 2.1.2 Verifiable claims centered on access to personnel . . . . . . . . . . . . . . 39\n2.1.3 Human inspectors as a verification mechanism . . . . . . . . . . . . . . . 40\n2.2 Digital systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.2.1 Cryptography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n2.2.2 AI-specialized computational hardware . . . . . . . . . . . . . . . . . . . . 43\n2.2.3 Hardware verifiability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n2.2.4 Hardware-enabled mechanisms . . . . . . . . . . . . . . . . . . . . . . . . 48\n2.2.5 AI-enabled devices . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n2.3 Electrical infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n2.4 Socio-technical systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n2.4.1 Institutional digital infrastructure . . . . . . . . . . . . . . . . . . . . . . . 55\n2.5 Enclosures and security boundaries for AI hardware . . . . . . . . . . . . . . . . 57\n2.5.1 Security . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n2.5.2 Mutual verification of hardware and code . . . . . . . . . . . . . . . . . . . 59\n2.5.3 Verifiable confidential computing . . . . . . . . . . . . . . . . . . . . . . . 64\n2.5.4 Different enclosure sizes for AI verification . . . . . . . . . . . . . . . . . . 68\n3 PoliticaloptionsandtradeoffsinAIverification 73\n3.1 Which stages of the value chain should be governed? . . . . . . . . . . . . . . . . 73\n3.1.1 Enforcement shapes verification . . . . . . . . . . . . . . . . . . . . . . . . 74\n3.1.2 Strict governance of even one part of the AI value chain can be useful . . 75\n3.1.3 Timing of verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n3.2 What activities need to be verified? . . . . . . . . . . . . . . . . . . . . . . . . . . 76\n3.3 How certain must the Verifier be of the Prover\u2019s compliance? . . . . . . . . . . . 77\n3.4 On-chip vs off-chip hardware mechanisms . . . . . . . . . . . . . . . . . . . . . 78\n3.5 Security-preserving digital verification: Are humans needed in the loop? . . . . 79\n3.6 Location and physical control of crucial hardware . . . . . . . . . . . . . . . . . 83\n3.6.1 Neutral mutually verified data center . . . . . . . . . . . . . . . . . . . . . 84\n3.6.2 Licensing system locations . . . . . . . . . . . . . . . . . . . . . . . . . . . 86\n3.7 What kind of institution should oversee verification? . . . . . . . . . . . . . . . . 87\n3.7.1 Direct vs indirect verification . . . . . . . . . . . . . . . . . . . . . . . . . . 87\n4 3.7.2 Is an international institution desirable? . . . . . . . . . . . . . . . . . . . . 88\n4 InternationalAIgovernanceagreementsandtheirverificationrequirements 89\n4.1 Transfer knowledge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\n4.1.1 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n4.1.2 Verify that knowledge shared is authentic and complete . . . . . . . . . . 93\n4.1.3 Verify that transferred knowledge is safeguarded appropriately . . . . . . 94\n4.2 Transfer resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 94\n4.2.1 Transfer development resources . . . . . . . . . . . . . . . . . . . . . . . . 95\n4.2.2 Provide access to AI systems . . . . . . . . . . . . . . . . . . . . . . . . . . 96\n4.2.3 Share benefits . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n4.3 Pool resources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n4.3.1 Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n4.3.2 Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n4.4 Prepare for emergencies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n4.4.1 Computational emergency detection and response . . . . . . . . . . . . . 101\n4.5 Regulate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 102\n4.5.1 AI development . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103\n4.5.2 AI deployment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n5 Conclusion 127\n5.1 Research and development of particular importance . . . . . . . . . . . . . . . . 128\n5.2 Policy action of particular importance . . . . . . . . . . . . . . . . . . . . . . . . 130\nAcknowledgements 133\nAppendices 134\nAppendixA Verificationofpersonnelcontrols 134\nAppendixB Verificationofclaimscenteredonaccesstopersonnel 136\nAppendixC Bestpracticesinverification 138\nC.1 Agreement content that makes verification easier . . . . . . . . . . . . . . . . . . 138\nC.2 Gradual implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n5 C.3 Learning and iteration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\nC.4 Keep AI infrastructure away from sensitive sites when possible . . . . . . . . . . 140\nC.5 Additional certainty via compound or parallel verification . . . . . . . . . . . . 140\nC.5.1 Compound verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\nC.5.2 Parallel verification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\nC.6 Transparent infrastructure; secret test content . . . . . . . . . . . . . . . . . . . . 141\nAppendixD Reliabledigitalverificationtypicallyrequireslookingatthewholestack 143\nAppendixE Algorithmverification 145\nAppendixF The\u201cWhowatchesthewatchers?\u201d problemwithprivacy-preservingeval-\nuations 148\nAppendixG Modelevaluation 150\nG.1 Open-ended challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\nG.1.1 Sandbagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 151\nG.1.2 Aggregate models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nG.1.3 Data-centric distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\nAppendixH SystemicallyriskyAI 153\nAppendixI Hardwareconfigurationsfor\u201cfixedset\u201dpolicies 155\nAppendixJ Chipdensitycontrols 156\nAppendixK Potentialapproachesfordevice-modelmatingwithanencryptedmodel 157\nAppendixL Otherverificationmechanisms 158\nL.1 Model-specific training licensing . . . . . . . . . . . . . . . . . . . . . . . . . . . 158\nL.2 Model-specific inference licensing . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\nL.3 Model registry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\nL.4 Model fingerprint attestation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 159\nL.5 Model tenancy ledger attestation . . . . . . . . . . . . . . . . . . . . . . . . . . . 161\nBibliography 162\n6 ExecutiveSummary\nRapid changes in the artificial intelligence (AI) ecosystem have galvanized government ef-\nforts to understand this technology and shape its future. States may seek to create inter-\nnational agreements over AI to capture new economic opportunities, preserve peace, and\nmitigate risks created or exacerbated by AI. This report examines the potential for states to\nundertake verification of international agreements relating to AI\u2014where verification is any\nprocess by which member states can assess each other's compliance with an agreement. This\nanalysis aims to take into account both technological limits and political sensitivities. We\nfind that many AI agreements could be implemented verifiably today. Within a few years,\ntargeted technical and policy efforts may allow robust and politically viable verification for a\nmuch wider range of possible agreements. One notable exception is agreements seeking to\nprovide detailed regulation of the behavior of mobile AI-enabled devices, such as weapons,\nwhere a combination of domain-specific technical limitations and political sensitivities make\nit extremely challenging to design acceptable verification.\nVerification can be a crucial component of international agreements. Without verification,\nstates may find that otherwise desirable deals are unavailable, just as mutual cooperation is\noften unavailable in social dilemmas such as the prisoner\u2019s dilemma. Effective verification\nmechanisms can transform strategic deadlocks into viable compromises. Verification pro-\nvides states with more political room to maneuver and with reliable information about the\nactions of their fellows. Therefore, it generally is in the interest of states to improve their\nability to verify agreements. For example, toward the end of the Cold War, new approaches\nfor verifying missiles expanded the set of realistic political options for arms control, eventu-\nally enabling the conclusion of agreements such as the Intermediate-Range Nuclear Forces\n(INF) Treaty of 1987.1\nBoth unilateral and cooperative verification techniques are valuable. Unilateral techniques\u2014\nsuch as satellite imagery and intelligence operations\u2014allow the behavior of a state to be\nscrutinized without its cooperation. By contrast, cooperative verification requires the active\nparticipation of both states\u2014with the Prover attempting to credibly demonstrate that they\nare following an agreement and the Verifier examining all available information to make\ninferences about the Prover\u2019s compliance. Some agreements require states to take on both\nroles as they simultaneously try to demonstrate their own compliance while checking their\npeers\u2019 compliance.\nPessimisticassumptionsaboutverificationdifficulty\nThis report aims to make claims about the possibilities for AI verification that are robust to\ntechnical and political change. To do so, it makes four pessimistic assumptions about the dif-\n1Toivanen, \u2018The Significance of Strategic Foresight in Verification Technologies\u2019.\n7 ficulty of verification. First, we assume that decentralized training of large AI models (using\nmultiple geographically distributed data centers) will be tractable with minimal efficiency\nlosses compared to centralized training\u2014thus requiring that AI-development governance\nand verification be applied to small data centers as well as large ones. Second, we assume\nthat algorithmic progress may be rapid, thus drastically increasing the capabilities that may\nbe available with a given quantity of compute. Third, we assume that AI will be employed by\nsecurity-sensitive institutions such as intelligence agencies and militaries for sensitive pur-\nposes, including those relating to personal, corporate, and state secrets. Fourth, we assume\nthat future treaties might need to subject even very sensitive uses of AI to detailed and verifi-\nable governance, thus requiring that verification plans be compatible with secrecy and secu-\nrity.2The verification proposals described in this report are intended to be workable even if\nall of these pessimistic assumptions become reality. Moreover, since these mechanisms are\ndesigned to be robust in the face of evolving technical and political realities, they are also\nlikely to be workable in conditions that are less constrained.\nAgreementsandtheirverifiability\nThis report examines five families of agreements relating to AI:\n1.Transferknowledge : A state provides AI-related knowledge to another state.\n2.Transferresources : A state provides AI-related resources to another state.\n3.Poolresources : States combine AI-related resources toward a common goal.\n4.Prepareforemergencies : States prepare to detect AI emergencies and respond to them.\n5.Regulate : States regulate AI development and deployment according to shared rules.\nThese agreement families are verifiable to different degrees, as summarized in Figure 1.\nThe first two families of agreements\u2014\u201c transfer knowledge \u201d and \u201c transfer resources \u201d\u2014are similar\nto prior international agreements regarding the transfer of knowledge and resources. Just\nas with their precursors in domains such as weapons-production technologies or nuclear\nenergy, these agreements are primarily limited by the ability of the receiving state to credibly\ndemonstrate that transferred knowledge or resources will not be used against the interests\nof the sending state. Some marginal improvement in the verifiability of these agreements\nis possible in the next few years via privacy-preserving digital verification tools as discussed\nbelow. However, the risk of downstream misuse is expected to remain the crucial factor\nlimiting these agreements.\nThe third family of agreements\u2014\u201c pool resources \u201d\u2014is also similar to historical analogues, but\nit faces less severe political problems and is therefore quite verifiable even today. States reg-\nularly create institutions to solve shared problems. By design, such international institutions\ntend to provide enough information to states to demonstrate that pooled resources are not\n2These final two assumptions force us to confront the transparency-security tradeoff, where the transparency\nneeded to demonstrate compliance with an agreement is believed to make a state less secure. This tradeoff is the\ncentral political challenge for some of the international agreements examined below.\n8 V er if iability pr esuming \u2028\nf iv e y ear s of ser ious ef f or tV er if iability if im plement ed \nt oda yEx am ple ag r eementA g r eement famil yY es, wit h political \nlimit ations*Y es, wit h political \nlimit ations*Shar e kno w ledg e of AI r isk sT ransf er kno w ledg eY es, wit h political \nlimit ations*Y es, wit h political \nlimit ations*Shar e AI-specialized c hipsT ransf er r esour cesY esY esP ool r esour ces t o w ar d \nint er national goalP ool r esour cesMa ybeN oCom put ational emer g ency \nde t ection and r epsonsePr epar e f or emer g enciesY esN oR egulat e dat a cent er \ncom put ationsR egulat eLimit edV er y limit edR egulat e AI-enabled \nw eapons*The sending s t at e mus t deem t he r isk s of kno w ledg e or r esour ce misuse t o be t olerable.Figure1: The families of international agreements examined in this report and their approx-\nimate verifiability.\nbeing misused. While most prior agreements of this kind dealt with relatively low-stakes do-\nmains, some engaged directly with core state interests such as security and prosperity (e.g.,\nthe European Coal and Steel Community). Overall, agreements aiming to pool AI resources\nfor political purposes are highly verifiable.\nThe final two families of agreements\u2014\u201c prepare for emergencies \u201d and \u201c regulate \u201d\u2014are much more\ndifficult to verify today, but preparation may allow some agreements in this domain to be\nverified robustly in the coming few years. In particular, it appears likely that a few years\nof intense work could allow rules about data center-based computations to be verified at\nscale, including the development and deployment of AI. Challenges remain for AI-enabled\ndevices that are both mobile and are employed for sensitive purposes, such as weapons. The\nregulation of weapons in particular is likely to remain politically difficult even when using\nthe verification approaches best suited to the challenge.\nIn sum, AI-centered agreements that bear resemblance to historical agreements over simi-\nlarly sensitive technologies (such as civilian nuclear energy) are roughly as verifiable as those\nprior agreements were. However, some of the agreement families discussed in this report\u2014\nnamely the \u201cprepare for emergencies\u201d and \u201cregulate\u201d families\u2014pose fundamentally new\n9 challenges that are the key reason why these agreements are largely unverifiable today. The\ncentral challenge in verifying these agreements is the difficulty of making credible claims\nabout rules that are applied to computations . This challenge is the central topic of the report\nand will be explored further below.\nThechallengeofverifyingcomputationalrulesandtools\nforaccomplishingit\nDemonstrating that rules are being applied to computations (such as those central to AI devel-\nopment or deployment) is very challenging, but hardware-centric verification schemes make\nit possible. Detailed knowledge of computational activities undertaken by a state is not reli-\nably available to other states, and therefore unilateral verification is of limited use in this do-\nmain.3Cooperative verification has great potential, especially via verification processes that\nfocus on AI-specialized computational hardware\u2014hereafter called \u201cchips\u201d or \u201ccompute\u201d. Be-\ning a physical asset, compute is easier to place verifiable controls on than are the other crucial\ninputs into AI (data and algorithms). AI can only be developed and deployed on computa-\ntional hardware\u2014with compute, because of its tailoring, significantly outperforming other\nkinds of hardware. The anticipated centrality of compute for the future of AI means that\nhardware governance is likely to be relevant into the future regardless of how the relative\ncosts of inputs change or how AI paradigms evolve.4,5Compute-centric governance also en-\nables fine-grained controls which minimize disruption to compute applications. Contrary\nto the common notion that digital verification is more challenging than physical verification,\nglobe-spanning verification systems already enable the operation of the Internet thanks to\nthe remarkable abilities of modern cryptography. Similarly, civilization-scale verification\nof computational activities on compute hardware appears to be not only feasible but also\nachievable in a way that is designed to robustly protect the privacy and security of all parties.\nAll verification tools are imperfect, thus often requiring that tools be employed in combi-\nnations that address their individual weaknesses and achieve a desired effect. Four kinds of\nverification techniques are worth emphasizing:\n1.States can use unilateral verification mechanisms to help them understand the broad\nshape of activities undertaken by other states\u2014such as their compute or power infras-\ntructure investments. States may also have access to much more sensitive information\nvia intelligence operations, thus allowing them to double check that declarations are\ncorrect and complete. Overall, unilateral verification serves a crucial role in limiting\n3Even though cyber attacks and intelligence operations are very capable of revealing important information,\nthese techniques are unlikely to be sufficient for reliable and ongoing access to the details of all computations\noccurring in a rival state\u2019s data centers.\n4An important caveat here is consumer GPUs (graphics processing units), the most powerful of which are\nnearly as capable as AI-specialized chips. Depending on the political goals of the agreement, a hardware-centric\ngovernance approach may have to either govern the most powerful GPUs or limit their AI capabilities at the\nhardware level.\n5See also Konstantin Pilz, Lennart Heim, and Nicholas Brown, \u2018Increased Compute Efficiency and the Diffu-\nsion of AI Capabilities\u2019 (arXiv, 13 February 2024), https://doi.org/10.48550/arXiv.2311.15377 .\n10 the ability of states to get away with incomplete or inaccurate declarations (e.g., having\nundeclared sites conducting operations in violation of an agreement).\n2.Hardware-enabled on-chip mechanisms, such as confidential computing, are embed-\nded into the hardware undertaking the operations being verified.\n3.Hardware-enabled off-chip mechanisms are verification techniques centered on hard-\nware placed elsewhere in the stack (not on the AI chip).6For example, off-chip veri-\nfication hardware could be placed in networking equipment, thus allowing verifiable\nclaims to be made about network traffic. Off-chip hardware mechanisms can be used\nto provide nearly any kind of information, thus making them capable of either form-\ning a full-stack verification mechanism on their own or supporting verification schemes\ncentered on other techniques.\n4.Personnel -centered verification can be employed to provide information about AI ac-\ntivities and their institutional context. The security challenges and potential unrelia-\nbility of personnel-based verification make it less suitable to serve as an independent\nload-bearing aspect of an agreement. However, personnel-based schemes can be cre-\nated comparatively quickly and provide a parallel verification scheme which could be\npolitically useful either on its own or in combination with other verification efforts.\nThe deployment of new on-chip mechanisms faces significant challenges\u2014including ma-\njor issues with technical viability, political acceptability, and deployment timelines.7AI-\nspecialized chips are among the most complex objects produced by humanity, employing\na supply chain centered on extremely sensitive intellectual property and technologies\u2014all\nspanning several countries. Creating new mechanisms is technically challenging and might\nrequire the active participation of key incumbents such as NVIDIA and Huawei. Further-\nmore, proving that the mechanisms are not being used for other covert purposes may be\nextremely difficult, since the closely guarded chip design and production processes are hard\nto scrutinize from the outside\u2014and once a leading-node semiconductor has been built it is\ninfeasible to verify its entire structure.8Even if a new on-chip mechanism can be designed\nand made politically acceptable, it would take several years for new chips to be produced and\nto form a meaningful part of all AI compute. This path to improved verifiability is possible,\nbut it faces significant challenges that have no obvious solution.\nBy contrast, off-chip mechanisms might be designed, built, and deployed quickly\u2014perhaps\nrequiring as little as several months of focused efforts. Off-chip hardware-enabled mech-\nanisms can be designed for narrow purposes and mutually verified either through non-\ninvasive downstream tests or cooperative production (such as in trailing-node semiconduc-\ntor fabrication facilities). Unlike on-chip mechanisms, off-chip mechanisms do not neces-\n6For the sake of distinguishing these mechanisms from the commonly employed \u201con-chip mechanisms\u201d\nconcept, these mechanisms have a negative definition that essentially means any verification-related hardware\nplaced anywhere except on the chip.\n7Note that existing on-chip mechanisms\u2014such as the components underpinning confidential computing\u2014\nare extremely useful for verification, as detailed below. It is the prospect of adding new mechanisms that brings\nus up against these challenges.\n8As discussed in the report, techniques may exist to do this, but they would destroy the semiconductor in the\nprocess, thus placing limits on their usefulness.\n11 sarily need to be built using leading-node semiconductor fabrication techniques.9Overall,\noff-chip mechanisms appear to be capable of robustly contributing to the verifiability of\ncomputations and many other aspects of agreements.\nAll of these categories of verification mechanisms have potential roles in the verification\nof even the thorniest agreements. The next section outlines an ideal theoretical compu-\ntational verification system and two ways that it can be approximated with existing or\nnear-future technology.\nTwoapproachestoverifiableconfidentialcomputing\nThis report describes two technical approaches which can approximate verifiable confidential\ncomputing \u2014computing which perfectly protects an agent\u2019s privacy while also providing them\nwith the ability to credibly demonstrate that all of their computations adhered to a set of rules.\nIn theory, an approach that can achieve these information exchanges would allow verifiability\nwith no information leakage and thus would fully address the transparency-security tradeoff.\nIn reality, no approach will be perfect, but serious efforts to approximate this ideal appear\nto be possible. The two approaches described below share a common structure, with the\nfirst requiring existing on-chip mechanisms for \u201cconfidential computing\u201d and the second\nrequiring new verification hardware placed into networks.\nA schematic illustration of both approaches is shown in Figure 2.\nEncr yp t ed dat aPlaint e xt dat aLeg end:Secr e tsProver\nV er if iable operationsPr o v er dat a cent erV er if ication com put ationsN eutral dat a cent erCr yp t og raphic commitmentsV er if ication r esultsCommon kno w ledg eSecr e t t es tsV erifier\nFigure2: Schematic summary of a way to approximate verifiable confidential computing . This\nillustrates the basic structure of both of the approaches to verifiable confidential computing\ndescribed in this report, since those approaches differ only in how the verifiable operations\nproduce the cryptographic commitments.\n9While leading performance is required for a competitive AI chip, it is not required for most functions that\nhardware could potentially undertake (e.g., sensors, data transmission, etc.). If leading-node fabrication is desired\nfor a particular off-chip mechanism, it will be necessary to create a way to either verify or cooperatively produce\na leading-node semiconductor\u2014both of which appear to be open problems as of this writing.\n12 Both approaches share a dependence on 1) cryptographic commitments, 2) a neutral mutu-\nally verified data center, 3) hardware inspection and monitoring, and 4) cutting-edge secu-\nrity.10Cryptographic commitments are short strings that \u201ccommit\u201d to a piece of plaintext\ndata without revealing the actual data\u2014thus allowing an actor to later reveal the true data\nand simultaneously prove that it was the same data that was committed to earlier.11A neu-\ntral mutually verified data center is a data center that parties to an agreement cooperatively\nbuild, maintain, and secure with the express purpose of running high-sensitivity computa-\ntions with neither party having access to the data. Both schemes below involve a) computa-\ntional processes producing cryptographic commitments about the data involved (including\nall code and parameters), b) which can then be used to demonstrate within a neutral mutually\nverified data center that the true data has been revealed, c) thus allowing privacy-preserving\ncomputational processes to be run on the true data to determine whether they are compli-\nant with rules. To provide mutual assurance that all hardware throughout all relevant data\ncenters is configured correctly and not subject to physical circumvention attacks, all hard-\nware involved in these schemes is presumed to be mutually inspected and then subject to\ncontinuous monitoring.\nThe first approach employs \u201cconfidential computing\u201d features that are already available in\nleading compute hardware and will be more widely available in the future.12Confidential\ncomputing allows a set of actors (here termed the Prover and Verifier) to undertake com-\nputations using private data in which only the computational results are available to the ac-\ntors (not the data itself). Overall, technologies like confidential computing allow the Prover\nand Verifier to mutually review code and run tests against each other\u2019s data. This kind of\nprivacy-preserving information exchange has significant potential for technical verification.\nAspirationally, it may even provide a way to answer the age-old question of \u201cWho watches\nthe watchers?\u201d since mutual verification among a set of competent actors may be sufficient\nto demonstrate not only compliance with a set of rules, but also that all tools and data used\nwithin the system are true to purpose and not secret attempts to circumvent or undermine\nthe governance system. Given that confidential computing features are already available on\nsome of the leading AI chips, there is potential for a rapid rollout of this approach to govern\nan important portion of AI compute.\nThe second approach leverages networking hardware and hardware enclosures to make cryp-\ntographic commitments about all traffic. In theory, such cryptographic commitments can\nbe just as credible as those made via confidential computing, since hardware configurations\n10On security, the reader is encouraged to consult Sella Nevo et al., \u2018Securing AI Model Weights: Preventing\nTheft and Misuse of Frontier Models\u2019 (RAND Corporation, 30 May 2024), https://www.rand.org/pubs/rese\narch_reports/RRA2849-1.html .\n11Less technically, a commitment lets you prove that pieces of data are identical without revealing the\ndata itself.\n12The term \u201cconfidential computing\u201d is used in this report to refer to not only the existing confidential com-\nputing standard, but also the entire family of technologies that can similarly enable credible multi-agent remote\nattestation. A full exploration of this family of technologies and techniques is beyond the scope of this report.\nFor more on this subject, see Patrick Jauernig, Ahmad-Reza Sadeghi, and Emmanuel Stapf, \u2018Trusted Execution\nEnvironments: Properties, Applications, and Challenges\u2019, IEEE Security & Privacy 18, no. 2 (March 2020): 56\u201360,\nhttps://doi.org/10.1109/MSEC.2019.2947124 .\n13 could be inspected closely\u2014and then monitored\u2014to ensure that the verified networking\nhardware is the only such hardware available. While this scheme employs very mature tech-\nnology families such as networking technology and cryptographic commitments, there are\nmany unanswered questions about the difficulty, cost, and time involved in deploying this\nscheme at scale. This approach therefore requires further research and development.\nThese two approaches can each provide a way for a Prover to demonstrate that their computa-\ntions are compliant with rules. It also appears to be possible to implement both approaches in\nparallel, thus providing two independent mechanisms\u2014backed by different roots of trust\u2014\nfor making verifiable claims. Much more exploration is needed of these approaches and\npotential alternatives, but there is room for cautious optimism that at least one technical\napproach to verifiable confidential computing will be workable.\nPoliticaloptionsandtradeoffs\nIn addition to their presumed overall prioritization of security and prosperity, states will\nface a number of political tradeoffs in their negotiations of AI agreements. Several such\ntradeoffs relate directly to verification and are thus explored briefly in this report. In their\nconsideration of a complex AI-related agreement, such as an agreement to regulate data\ncenter-based AI development and inference, states must make several choices.\n1.Which parts of the AI value chain\u2014such as training, fine-tuning, and inference\u2014should\nbe governed and verified? The choice of stage to govern has ramifications for the AI\necosystem, including what AI tools can be created legally and which institutions might\nbear the financial and legal costs of compliance.\n2.Should aspects of enforcement be embedded in the agreement directly to change the\ndifficulty of the verification problem or to make the political agreement more robust?\n3.What level of certainty of compliance is needed\u2014and what costs in terms of money, se-\ncurity, delayed implementation, or slower AI computations are the various states willing\nto pay for a marginal improvement in certainty?\n4.How gradual must agreement implementation be\u2014and how delayed must verification\nprocesses be in relation to the activities they are verifying\u2014in order to sufficiently alle-\nviate the security concerns of all states?\n5.Would automated code-centric verification be sufficient for the political purposes of\nthe agreement or will human judgment be needed for all verification operations or as a\npathway for rare escalation when automated systems provide ambiguous signals?\n6.Where will crucial parts of the verification system\u2014such as data centers for running ver-\nification computations or licensing systems\u2014be located? The location of crucial hard-\nware can shape the political viability of agreements and how states perceive the possi-\nbility of one state (or all) exiting the agreement.\n7.How much AI hardware must be regulated? Key hardware families include new AI hard-\nware, legacy AI hardware, and commodity chips such as gaming GPUs. Exempting less-\n14 performant hardware from governance allows a significant proportion of AI work to be\nungoverned\u2014at least initially\u2014but makes the agreement much easier to implement.\nThe many dimensions of political choice can be daunting, but they are also a reason for cau-\ntious optimism. When bargaining can be undertaken in many dimensions simultaneously\u2014\nand when many of these dimensions allow for fine-grained choices\u2014there is likely to be a\ncombination of political choices that allow an agreement to fall within the zone of possible\nagreement for all states.13Similarly, there is reason to believe that a flexible set of parame-\nters will allow states to bargain their way toward a stable equilibrium that is near the Pareto\nfrontier, thus minimizing overall costs while maximizing overall gains.\nAreasforfurtherwork\nMeaningful further work on AI verification can be done on a number of different fronts, but\nonly a few will be highlighted in this summary. Four areas will be outlined: research and\ndevelopment, actions that any institution can take, unilateral state action, and cooperative\nstate action.\nResearch and development would be particularly valuable on seven general directions:\n1.exploring the potential and limits of confidential computing\u2014with a particular focus\non resilience to extremely sophisticated cyber attacks;\n2.designing and building hardware mechanisms for providing cryptographic commit-\nments about network data;\n3.collating general techniques for inspecting data center hardware as well as ways to pro-\nvide ongoing monitoring to ensure that local hardware attacks are infeasible;\n4.outlining a roadmap toward a neutral mutually verified data center that can realistically\nundertake politically sensitive computations while keeping that data robustly safe from\nall actors;\n5.exploring the possibility of verifying and monitoring containerized data centers which\ncan be moved to undisclosed locations to provide security assurances for security-\noptimizing institutions such as militaries;\n6.exploring both the limits of verifying hardware that has already been created and\nthe prospects for verifiable cooperative production of hardware at either leading- or\ntrailing-node fabs;\n7.expanding and diversifying the work on verifying whether rules are followed by digi-\ntal objects\u2014presuming plaintext access within existing privacy-preserving frameworks\nsuch as confidential computing\u2014to include not only models but also other information\nsuch as training data, algorithm code, and inference exchanges.\n13Paul Poast, \u2018Issue Linkage and International Cooperation: An Empirical Investigation\u2019, Conflict Management\nand Peace Science, no. 3 (2013): 286\u2013303.\n15 Any institution can support the development of the AI verification conversation. In particu-\nlar, it would be very valuable for one or more open projects to develop transparent verifica-\ntion infrastructure and processes which could be leveraged by other actors. Industry players\nmay wish to actively support such efforts in order to help foster regulatory interoperability\nbetween jurisdictions in which they do business\"\n\nBased on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n<|im_start|>assistant\n",
    "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere is an excerpt from a document:\n\n\". Another area for valuable open develop-\nment would be evaluations and meta-evaluations\u2014which are techniques for demonstrating\nthat private evaluation code and data actually accomplish their declared goal and not some\nsecret other goal. Even if meta-evaluations can never be perfect and even though adversaries\ncan certainly \u201cteach to the test\u201d against open standards, such work has the potential to make\nfalsified evaluations much harder to create and can thus help states as they seek to verify each\nother\u2019s testing frameworks. Finally, any institution, in addition to supporting direct work on\nthese problems, can galvanize broad interest via repeated contest-like incentives such as bug\nbounties to find issues with proposals\nUnilateral state efforts should focus on 1) building their domestic capacity to understand the\nAI ecosystem and the prospects for AI verification; 2) developing their own evaluation suites\n(see above) while keeping at least some aspects of these suites secret to avoid allowing AI\ndevelopers or other states too much ability to find ways to circumvent these tests; 3) support-\ning the creation and rollout of interoperable verification standards to either aid their own\ndomestic AI industry or galvanize foreign players to create secure and verifiable AI services\nthat they can use safely; and 4) avoiding colocation of AI facilities with other sensitive equip-\nment, such as cutting-edge military hardware, which can help allow future inspections and\nmonitoring of AI hardware without severe security concerns.\nCooperative state efforts can begin with the following three policies, each of which can grow\nfrom a unilateral or minilateral effort into a broader cooperative effort as the politics of verifi-\ncation evolve. First, states can expand their tracking of key inputs to AI such as AI-specialized\nchips and the equipment for producing them. Second, states can deliberately carve out space\nfor international academic and civil society discussions on verification to enable these non-\ngovernmental communities to build common understanding of the problems and potential\nsolutions. Third, states can monitor the situation and share information with trusted part-\nners as they consider when to engage more broadly. Progress in AI may be extremely rapid,\nso states may need to make substantial efforts not only to keep up with the changing techno-\nlogical frontier, but also to plan ahead for the potential political demands of tomorrow.\nIn closing, it should be noted that for the optimistic predictions of this report to come about,\npolitical will is required. Absent political will to create and deploy verification mechanisms\nacross key infrastructure, AI computations will remain largely unverifiable. If key states get\nserious about these problems, a combination of unilateral, collaborative, and open efforts\nshould be sufficient to enable the creation of a robust verification system within a few years.\n16 1Introduction\nAs the benefits and risks of artificial intelligence (AI) have become increasingly salient, discus-\nsions have begun about how governments can act to bolster markets and avert risks. Stake-\nholders in these governance conversations must grapple with a remarkably fast-moving,\ngeneral-purpose technology that has the potential to affect every area of human life. One\ncrucial thread of these conversations is the prospect of international agreements that relate\nto AI and what they can accomplish. For many such agreements, their potential hinges\non the extent to which states can verify whether other states are abiding by their commit-\nments.14,15This report examines the verification of international agreements relating to AI,\nwith a particular emphasis on approaches that might be technically and politically feasible\nin the near future.\nThe viability of international agreements can hinge on their verifiability because in many\ncases states would not want to bind their own actions unless their counterparties were sim-\nilarly bound. If defection from an agreement cannot be detected, then the agreement will\nbe unlikely to have a significant effect on state action.16However, if states can reliably de-\ntect defection, agreements might be possible. Since verification mechanisms are focused on\nrevealing information about compliance, these mechanisms can open up political options\u2014\nthus expanding the reach of the \u201cart of the possible\u201d.17\nThis report is structured around the idea that verification mechanisms are best understood in\nthe context of the agreements that they are supposed to be verifying. Myriad political factors\nwill affect states\u2019 choices about which verification techniques to implement for a given agree-\nment.18So while verification mechanisms\u2014and verification technologies in particular\u2014can\nindeed open up political options, politics will also determine which verification mechanisms\nare workable in a given circumstance. Therefore, the politics and the technological frontier\nof verification must be considered together when trying to map the space of possibilities.\nBoth unilateral and cooperative verification schemes are possible. Unilateral verification typ-\nically presumes that no formal or even tacit agreement exists that would enable or ease verifi-\n14In this report, a \u201cstate\u201d is a country.\n15Dialogues between global academics have recently highlighted the particular need for AI verification. Bengio,\nYoshua, Andrew Yao, Geoffrey Hinton, Zhang Ya-Qin, Stuart Russell, Gillian Hadfield, Mary Robinson, Xue Lan,\net al. \u2018IDAIS-Venice\u2019 (International Dialogues on AI Safety, 2024), idais.ai/dialogue/idais-venice/ .\n16A particularly challenging domain is agreements over cyber weaponry. Unfortunately, cyberweapons can\nbe developed and stockpiled in secret, thus making it infeasible to make robust agreements about their creation.\nEven worse, states often cannot discover who is attacking them in the cyber realm, since attackers can obfuscate\nthe source of attacks.\n17The broad definition of verification used in this report is inclusive of information activities that are also\nreferred to as \u201creporting\u201d or \u201cmonitoring\u201d. This report will use the term \u201cverification\u201d throughout to indicate its\nemphasis on the more robust end of the spectrum of such mechanisms.\n18In this report, verification \u201ctechniques\u201d are a more general category than \u201ctechnologies\u201d. Here, \u201ctechniques\u201d is\nsimilar to the concept of a \u201cmechanism\u201d, referring to any way in which things could work. Meanwhile, \u201ctechnolo-\ngies\u201d refer to ways of doing things, with an emphasis on enablers such as hardware and software. For example,\nsocial structures such as institutions are not referred to as technologies within this report.\n17 cation processes. Cooperative verification refers to scenarios where states have at least some\nincentive to demonstrate their compliance to one another. While unilateral verification is\nprimarily constrained by the available technologies and the domain of interest,19coopera-\ntive verification is typically a balancing act between diverging political needs. For cooperative\nverification, it's productive to consider the politics of verification in both directions. Not only\nis the Verifier trying to determine whether the Prover is complying with the agreement, but\nthe Prover might also be trying to credibly demonstrate their compliance in a way that they\ndeem to be politically acceptable.20\nThe scope of AI considered in this report includes both data center-based models and mo-\nbile AI-enabled devices.21Therefore, this report relates to a wide spectrum of automated\ncapabilities, including not only models that require large-scale computing hardware, but\nalso smaller models that can be deployed widely on devices for economic, industrial, or mil-\nitary purposes. Furthermore, some of the agreement and verification types discussed here\nare centered on key inputs such as computing hardware, data, and infrastructure\u2014both as\nmechanisms for governing AI and as targets for agreements in their own right.22\nThis report includes analysis of both the civilian domain and highly sensitive domains such\nas state military and intelligence efforts. While the fast-moving civilian domain has been the\ntarget of most governance proposals thus far, analogous governance discussions for military\nAI remain underexplored.23This allocation of attention is expected given that AI\u2019s frontier\nhas been driven forward by civilian institutions and purposes. However, as AI begins to be\nwoven into all aspects of life, it will also become increasingly important for highly sensitive\nparts of the state apparatus. These changes warrant attention. Moreover, these sensitive\ndomains present political difficulties that are substantially different from those in the civilian\nrealm. As will be explored at length in this report, states can face a transparency-security\ntradeoff in their attempts to make verifiable deals with each other, where relatively robust\nverification possibilities can come at the cost of state security.24On the other hand, it is also\npossible that concerns about state security will be the very issues that will bring states to the\ntable in the first place, just as was the case with nuclear arms control during the Cold War.25\n19In an extreme scenario, if all relevant activities are clearly visible from space and all states have satellites,\nthere would be no need for cooperative verification. Relatedly, early 20th-century naval agreements such as\nthe Washington Naval Conference had no need for cooperative verification mechanisms because it was deemed\ninfeasible to hide naval buildups from the other states.\n20In this report, a Prover is a state that claims to be attempting to demonstrate their compliance with an\nagreement. A Verifier is an institution (e.g., another state or an international institution) that is assessing the\nProver\u2019s compliance.\n21\u201cMobile\u201d here refers to devices that can be moved around. The key examples explored in this report are\nAI-enabled weapons.\n22For example, the report will discuss deals where states choose to apportion AI development and deployment\ninfrastructure according to an agreement.\n23Matthijs M. Maas and Jos\u00e9 Jaime Villalobos, \u2018International AI Institutions: A Literature Review of Models,\nExamples, and Proposals\u2019, SSRN Scholarly Paper (Rochester, NY, 22 September 2023), https://doi.org/10.2\n139/ssrn.4579773 ; Robert Trager et al., \u2018International Governance of Civilian AI: A Jurisdictional Certification\nApproach\u2019 (Oxford Martin AI Governance Initiative, 2023).\n24These institutions might also be particularly sensitive to regulation of their behavior due to a perception that\nany limitations are likely to handicap the state. This political factor is salient in the discussions in Section 1.3.\n25This is discussed further in Section 1.3.\n18 This report aims to make three contributions:\n1.It describes typesofinternationalagreements that have not yet been discussed widely\nin the literature.\n2.For each agreement type, it explores workable verificationmechanisms.\n3.Based on political needs and the technical frontier, it describes opportunitiesfornear-\ntermresearch,investment,andpolicytoimproveprospects for AI verification.\nThe report has five sections. First, the remainder of the introduction will clarify the report\u2019s\nscope and then go on to summarize why international agreements over AI are desirable,\nwhy some of them need to be verifiable, and why designing verifiable agreements over AI\nis challenging. Second, a curated set of verification components is surveyed and analyzed.\nThese are the ingredients for any recipe for AI verification, and they each have diverging\nlimitations and strengths. Third, a set of political options and tradeoffs are examined for AI\nverification. For any given international agreement relating to AI, political choices may need\nto be made on some or all of these dimensions. Fourth, families of potential international\nagreements are described along with particular implementation options, as well as verifica-\ntion options that could work for those implementations. The agreements analyzed include\nagreements that provide international regulation of AI development and deployment. Fifth,\nthe conclusion summarizes the key takeaways of the analysis and outlines priority areas for\nfuture work.\n1.1Scope\n1.1.1AItypes\nThis report aims for a broad perspective, but it does emphasize certain kinds and uses of AI\nmore than others. Specifically, we focus on a) large-scale AI26and b) highly sensitive uses of\nAI. The emphasis on large-scale AI is for two reasons. First, large models have been respon-\nsible for the majority of the rapid advances in AI capabilities in recent years, and they have\nbeen the focus of the corresponding discussions of risk and governance.27Second, large\nmodels and large-scale AI projects28are relatively easy to govern and verify compared to\n26Multiple terms exist for large-scale AI efforts that might be correspondingly powerful and risky. These\ninclude \u201cfrontier AI\u201d, \u201ctransformative AI\u201d, and \u201cadvanced AI\u201d as well as the more speculative \u201cartificial general\nintelligence\u201d and \u201csuperintelligence\u201d. This paper will employ none of these terms explicitly, since each term is\nprimarily useful in other domains (such as domestic regulation) or is oriented around technical or philosophical\nclaims rather than political impacts. This report emphasizes the international politics of AI and thus employs\nonly generic terminology with one exception: the introduction of systemically risky AI in Section 4.3(see also Ap-\npendix H) to denote AI systems with the potential to create extraordinary dangers for multiple states.\n27ChatGPT and GPT-4 from OpenAI were widely recognized as significantly shifting the frontier of AI capa-\nbility and usability. A flurry of similarly large models have since been released. Overall, these broadly capable\nmodels appear to be the primary impetus for much of the society-wide conversation about AI\u2019s opportunities\nand risks.\n28Note that large-scale AI projects can involve the intensive use of resources without necessarily involving\nthe creation of large models: for example, a project might require large amounts of AI inference using an\nexisting model.\n19 their smaller cousins.29However, it should be noted that this emphasis on large models\ndoes not imply an emphasis only on large clusters of AI-specialized compute such as large\ndata centers. While large clusters of compute will certainly be relevant for certain kinds of\ngovernance and verification, the fact that leading AI models can be trained across multiple\ndata centers means that, even for very large training runs, Verifiers will need to be able to ver-\nify whether trainings being completed within different data centers at different times are for\nthe same model or for unrelated models.30Concretely, this report assumes that distributed\ntraining is workable with minimal efficiency losses.31In the same vein, this report assumes\nthat algorithmic progress could be rapid, thus enabling greatly expanded capabilities per\nunit compute.32In sum, despite a slight emphasis on the governance of larger models, this\nreport aims to propose verification mechanisms that could scale up in a way that allows the\nverification of rules even for relatively small models that are produced in great quantities.33\nThe second emphasis is on relatively sensitive uses of AI. As will be explored in more detail\nlater, there are strong reasons to believe that in highly sensitive domains\u2014such as state mil-\nitary or intelligence activities\u2014verification of governance rules will be dramatically harder.\nThe nature of these domains means that verification mechanisms will need to be able to do\ntheir job effectively while minimizing the revelation of information that is unrelated to the\nagreement. Very few verification mechanisms have characteristics that make them workable\nin such domains, and by comparison, low-sensitivity domains can reasonably be verified us-\ning a broader array of tools. For that reason, and because of the importance of these domains,\nthe report conservatively emphasizes analysis of highly sensitive domains.\n1.1.2Agreementtypes\nThis report outlines a number of families of international agreements. Agreement types\nwere chosen for inclusion based on two filters: 1) that the agreements need to be international\n29The largest models as of this writing require tens of thousands of cutting-edge AI chips to create\u2014costing\na total of hundreds of millions of dollars. The creation of large models thus has a large associated footprint\nin computational power, money, energy, and physical space. By comparison, once created, open source large\nmodels can be modified and deployed using widely available consumer hardware, and therefore small changes\ncan be made and minimal inference conducted for only thousands of dollars. Therefore, the creation of small\nmodels, or the deployment (as opposed to creation) of larger open source models, may have footprints that are\ndrastically smaller. Thus these are much harder to govern.\n30\u2018Gemini 1.5: Unlocking Multimodal Understanding across Millions of Tokens of Context\u2019 (arXiv, 8 August\n2024), http://arxiv.org/abs/2403.05530 , page 7.\n31This choice is made for three major reasons. First, this assumption makes the verification problem harder,\nso if this assumption turns out to be wrong we will still have laid out plans that could work. Second, distributed\ntraining has already been demonstrated with Gemini (Georgiev et al., 2024) as well as with other recent research\nresults (e.g, Jaghouar et al., 2024 and Peng et al., 2024). Third, calculations in Scher and Thiergart (2024) indicate\nthat it is difficult to rule out the potential of distributed training, thus suggesting that relatively efficient distributed\ntraining is likely to be tractable.\n32This is also the conservative assumption, since if we assume no future algorithmic progress, we could be\nleft flailing if rapid advances occur. see Appendix Efor more about the challenges for verification and some\npotential solutions. Furthermore, there is ample evidence of rapid historical and ongoing progress in algorithmic\nefficiency. Anson Ho et al., \u2018Algorithmic Progress in Language Models\u2019 (arXiv, 9 March 2024), https://doi.or\ng/10.48550/arXiv.2403.05812 .\n33Important caveats to this point are described in the report, including important tradeoffs regarding the\namount of compute that might need to be governed (Section 3.2) and the underexplored challenge of verify-\ning either AI agents or AI systems that are designed to use external tools (Section 2.5.3 ).\n20 and 2) that verification of the agreement must be both possible in theory andprovide non-trivial\nassurances . Agreements are further organized into families on the basis of their verification\nrequirements, not other political dimensions. Each of these points will be expanded in turn.\nInternational agreements are agreements that involve at least two states .34This report does not\npresume that any particular states are involved in the agreements.\nOnly certain agreements are verifiable in theory but also have non-trivial verification needs.\nVerifiability-in-theory means that compliance or non-compliance with the agreement would\nlead to evidence that could in theory become known to the Verifier. For example, unverifi-\nable agreements could relate to claims about unobservable activities or about the intent of\nactions, rather than claims about the observable implications of actions. While declarations\nof intent can be valuable for building and maintaining norms, they are not verifiable claims.\nThus, this report will not consider agreements that are solely about mental states such as de-\nclared beliefs or intents. Instead, the report will focus on agreements with observable struc-\nture or outcomes which can be perceived.35For example, an agreement that stipulates rules\nabout the location and disposition of specific physical objects would be verifiable in theory.\nFurthermore, agreements that are trivially verifiable will not be explored in this report. Triv-\nially verified agreements include agreements with obvious and readily observable conse-\nquences, such as money being sent to an account, a model being published, or a shipment\nof chips being delivered to a state. Any action that can be fully understood quickly and\neasily due to its immediate and overt real-world consequences does not need a verification\nregime.36An important caveat is that some overt actions can contain subtle deception. For\nexample, a delivered set of chips might have been secretly modified to serve the sender\u2019s\nstrategic goals and undermine the receiver\u2019s goals.37Efforts to ascertain whether a declared\nagreement is being secretly undermined are certainly within the scope of this report. In sum,\nthis report considers agreements with consequences that can be legible to another state. It\ntherefore includes categories such as knowledge or resources transfers, the pooling of re-\nsources, preparation for emergencies, and regulation (see Section 4).38\nThe question of whether agreements are verifiable in theory should not be confused with the\nquestion of whether these agreements are verifiable in practice. As described above, verifiable-\nin-theory means that a perfectly privacy-preserving verification mechanism would have the\npotential to verify that agreement.39Whether an agreement is verifiable-in-practice is a more\nmultifaceted question, and grappling with this question is the primary goal of this report.\n34This includes agreements that are bilateral\u2014between two states; minilateral\u2014involving small groups of\nstates; and multilateral\u2014aiming for broader or universal participation.\n35There are many potential challenges in verifying consequences, including that consequences might be de-\nlayed, subtle, or deliberately hidden by the Prover.\n36These kinds of agreements may be very useful complements to the agreements discussed in this report.\n37Another example is Provers providing information that is deliberately falsified in a way that is difficult to\ncatch. See Section 4.1.\n38Tacit agreements could in theory be verified in some cases, but this report does not deeply explore this\ndomain other than a discussion of the limitations of unilateral verification for AI regulation (see Section 1.5.3).\n39As explored throughout the report, computational methods for such mechanisms already exist in\nsome domains.\n21 The report will focus on the potential for robust verification of the various agreement types\nin scenarios where states are paradoxically seeking to demonstrate compliance to each other\nwhile simultaneously seeking ways to circumvent verification.40Unilateral verification capa-\nbilities such as national technical means are also examined as part of the equation. Over-\nall, the emphasis is on verification approaches which can robustly, even if imperfectly,41\ndemonstrate compliance among states and other actors.42This report emphasizes families\nof verification techniques that appear to be well-suited for the political goals of states, and\nhave the potential to reassure both sides about their mutual compliance. Other mechanisms\nthat are less reliable, or that are likely to be politically impossible, will be mentioned but\nnot emphasized.\nAgreements are organized into families according to similarities in their verification require-\nments rather than other aspects of the agreement, such as their political structure, effective-\nness, distribution of benefits, political feasibility, or enforcement. Other work has explored\nthese questions,43and we provide a different way of organizing the discussion, which is not\nintended to be definitive or exhaustive.44This also means that the detailed political content\nof agreements\u2014such as specific regulatory rules\u2014will not be deeply explored in this report.\nAs noted earlier, verification capabilities open up a space for political options. Therefore, the\ntechniques discussed in this report are also intended to help future discussions about political\nagreements succeed.45Moreover, even within the realm of verification, political questions\nabound. These latter questions are expanded in Section 3.\n1.1.3Verificationmechanisms\nThis report contains a survey of some verification approaches for AI. This survey is broad\nbut not exhaustive, and aims to provide an overview of both a) the potential and limitations\nof the various kinds of mechanisms and b) why specific mechanisms appear to be well-suited\nfor verifying particular kinds of agreements.\n40This framing is sometimes termed the \u201ccovert adversary\u201d. A covert adversary is a theoretical actor who will\ncomply with verification protocols only to the extent that they could be caught attempting to circumvent those\nprotocols. In short, they\u2019ll cheat if they can get away with it. See Yonatan Aumann and Yehuda Lindell, \u2018Security\nAgainst Covert Adversaries: Efficient Protocols for Realistic Adversaries\u2019, in Theory of Cryptography, ed. Salil P.\nVadhan (Berlin, Heidelberg: Springer, 2007), 137\u201356, https://doi.org/10.1007/978-3-540-70936-7_8 .\n41US Secretary of State George Shultz testified in 1988 regarding the INF treaty that \u201cThere is no such thing\nas absolute, 100 percent verification. But it is our judgment that this treaty, through its successive layers of\nprocedures, contains the measures needed for effective verification\u2026. The bottom line is that the verification\nprovisions of this treaty get the job done.\u201d Rose Gottemoeller, \u2018Looking Back: The Intermediate-Range Nuclear\nForces Treaty\u2019 (Arms Control Today, 2007), https://www.armscontrol.org/act/2007-06/looking-back-i\nntermediate-range-nuclear-forces-treaty .\n42The presumption is that less robust mechanisms will be easier to design and implement. While these may\nbe very valuable to develop in order to serve particular political needs, this is not our focus.\n43In addition to specific proposals, work comparing approaches includes; Matthijs M. Maas and Jos\u00e9 Jaime Vil-\nlalobos, \u2018International AI Institutions: A Literature Review of Models, Examples, and Proposals\u2019, SSRN Scholarly\nPaper (Rochester, NY, 22 September 2023), https://doi.org/10.2139/ssrn.4579773 ; Robert Trager et al.,\n\u2018International Governance of Civilian AI: A Jurisdictional Certification Approach\u2019 (Oxford Martin AI Governance\nInitiative, 2023).\n44This report\u2019s focus on the verification of international agreements necessarily means that many other mean-\ningful political dimensions will be underexplored or left out entirely. This prioritization reflects the goals of this\nreport, not the overall importance of the different dimensions.\n45Inclusion of an agreement type in this report should not be regarded as an endorsement.\n22 Table1.1: Agreements examined in this report.\nAgreementfamily Agreementtypes Variants\nShare research\nShare knowledge of AI risks and oppor-\ntunities\nShare AI development knowledgeTransfer\nknowledge\nShare safety-enhancing technologies\nShare AI-specialized chips\nShare access to AI-specialized\ncompute Transfer development resources\nTraining programs for AI pro-\nfessionals\nTransfer completed models\nProvide access to AI systems\nProvide API access\nCash transfers\nDeploy AI-enabled devices as\naidTransfer\nresources\nShare benefits\nTransfer AI-enabled devices\nPool resources toward an international\ngoal\nPool resources toward defensive AIs\nPool resources toward transformative\nAIPool\nresources\nPursue systemically risky AI only in a\nsingular project\nPrepare for\nemergenciesComputational emergency detection\nand response\nRegulate data center-based AI\ndevelopment\nRegulate AI development\nRegulate fine-tuning and on-\nline learning\nRegulating data center infer-\nenceRegulate\nRegulate AI deploymentRegulating sensitive mobile\nAI-enabled devices\n23 We include both widely-used mature concepts and technologies, as well as approaches which\nare speculative to varying degrees. However, we limit discussion of the most speculative\nmethods, instead focusing on verification mechanisms that are available already or can po-\ntentially be made available soon (e.g., within a few years).\nMechanisms will also be examined in terms of their political characteristics, including a)\ntheir ability to verify agreement(s), b) their ability to address transparency-security tradeoffs,\nand c) their robustness against covert circumvention attempts. This report does not explicitly\nexamine confidence-building measures,46although many of the techniques described herein\nwould also be applicable to such efforts.\n1.2Methodology\nThe ideas in this report were generated through a review of the relevant literature and a se-\nries of informal cross-disciplinary idea exchanges and conversations. Overall, this document\nattempts to be a synthesis of the key areas of related disciplines (e.g., cryptography, machine\nlearning, and verification in International Relations) as well as an analysis of how the knowl-\nedge of these disciplines can be brought together to shape our sense of what is possible in\nAI verification today, as well as what might be possible in the future. Omissions and errors\nare inevitable in a work of this scope. Nonetheless, the authors hope that this report informs\nand broadens future conversations in this crucial field.\n1.3Politicalneedsandpoliticalwill\nDetailed discussions of the political needs for international AI governance are outside the\nscope of this report. Nonetheless, to highlight the importance of the topics at hand, we pro-\nvide a brief list of reasons why verifiable international AI governance might be desirable\nfor states. States might work together via verifiable international agreements to create new\neconomic opportunities, preserve peace, and mitigate other risks.\nCreating new economic opportunities is a clear and salient priority for many state govern-\nments today.47By creating effective and harmonized regulations, states can reap the poten-\ntially enormous benefits of AI.48Absent such harmonized regulations, cross-border trade in\nAI-related goods and services might be sharply limited, and entire sub-industries may fail to\n46Confidence building measures, such as used by the Biological Weapons Convention, are useful in the absence\nof an agreed-upon regime, and may function as adjuncts to verification, but are not themselves intended to\nbe verified or verifiable. See also Michael C Horowitz and Paul Scharre, \u2018AI and International Stability: Risks\nand Confidence-Building Measures\u2019 (Center for a New American Security), accessed 25 October 2024, https:\n//www.cnas.org/publications/reports/ai-and-international-stability-risks-and-confidence-bui\nlding-measures .\n47\u2018Statement on Inclusive and Sustainable Artificial Intelligence for People and the Planet\u2019 (Paris AI Action\nSummit, 11 February 2025), https://www.elysee.fr/en/emmanuel-macron/2025/02/11/statement-on-inc\nlusive-and-sustainable-artificial-intelligence-for-people-and-the-planet .\n48Claire Dennis et al., \u2018What Should Be Internationalised in AI Governance?\u2019 (Oxford Martin AI Governance\nInitiative, 2024).\n24 reach their full potential.49The creation of robust standardized rules for interoperable eco-\nnomic systems has been enormously influential in other domains, such as civil aviation or\nmaritime shipping. A failure to regulate and standardize internationally might have stymied\nthe growth of the networks for aviation and shipping that now span the globe. Similarly, the\nAI industry could be hampered by fragmentation, regulatory arbitrage, and the potential\nfor industry-wide reputational damage due to a disaster. This final possibility is particularly\ninstructive, given how the Three Mile Island and Chernobyl nuclear accidents affected the\nprospects for civilian nuclear energy.50\nPreventing war is also of central concern. At least two pathways to war are already salient\ntoday. First, arms races can be extraordinarily expensive and lead to rapidly increasing per-\nceptions of mutual threat. Today, a race toward leadership in civilian AI is already underway\nand AI is increasingly perceived to be central to military power.51An unconstrained arms\nrace in this domain could consume vast resources that states would otherwise prefer to put\ntoward other uses\u2014thus raising the risk of war due to the desire to reduce their need for arm-\ning in the future.52Furthermore, major investments in AI-enabled military hardware may\nsignificantly increase the threat that states perceive from each other\u2019s weaponry.53Second,\nracing can also create systemic pressures toward a major war: the pursuit of greater relative\npower by one state can incentivize other states to wage preventive war to stave off what they\nperceive to be a major shift in military power. Concerns of this general shape are certainly\nnot new,54but some policy elites consider this a real possibility for the near future.55In sum,\nthese two pathways mean that failing to adequately govern AI internationally could lead to\ncostly and dangerous arms races or even a major war.\n49Sub-industries that might fail to reach their full potential without AI verification in particular could include\nAI services that process sensitive personal, business, or state data or which provide high-sensitivity services (e.g.,\ngovernment services or military & intelligence capabilities), to states themselves.\n50Bulat Aytbaev et al., \u2018Don\u2019t Let Nuclear Accidents Scare You Away from Nuclear Power\u2019, Bulletin of the Atomic\nScientists, 31 August 2020, https://thebulletin.org/2020/08/dont-let-nuclear-accidents-scare-you\n-away-from-nuclear-power/ .\n51Nestor Maslej et al., \u2018The AI Index 2025 Annual Report\u2019 (AI Index Steering Committee, Institute for Human-\nCentered AI, Stanford University, April 2025); James Black et al., \u2018Strategic Competition in the Age of AI: Emerg-\ning Risks and Opportunities from Military Use of Artificial Intelligence\u2019 (RAND, 2024).\n52As explored in various parts of the International Relations literature, states might start wars because maintain-\ning the status quo is too costly for them. Robert Powell, \u2018Guns, Butter, and Anarchy\u2019, American Political Science\nReview 87, no. 1 (1993): 115\u201332; Andrew Coe, \u2018Costly Peace: A New Rationalist Explanation for War\u2019 (2011).\n53This is particularly true if AI has an effect on the overall offense-defense balance of strategic weaponry. Un-\nfortunately, there is no guarantee that the dominance of defensive nuclear deterrence that has undergirded peace\nfor decades will remain unchanged as AI enters the equation. See also Chris Meserole, \u2018Artificial Intelligence and\nthe Security Dilemma\u2019, Lawfare (Lawfare, 4 November 2018), https://www.lawfaremedia.org/article/art\nificial-intelligence-and-security-dilemma ; James Johnson, \u2018AI-Security Dilemma: Insecurity, Mistrust,\nand Misperception under the Nuclear Shadow\u2019, in AI and the Bomb: Nuclear Strategy and Risk in the Digital Age\n(Oxford University Press, 2023), https://doi.org/10.1093/oso/9780192858184.003.0005 .\n54In 1960, Thomas Schelling wrote, \u201cA nation known to be on the threshold of an absolutely potent surprise-\nattack weapon may have reason to forswear it unilaterally\u2014if there is any possible way to do so\u2014in order to\nforestall a desperate last-minute attempt by an enemy to strike first while he still has a chance.\u201d Thomas C\nSchelling, The Strategy of Conflict (Harvard University Press, 1960), p. 133.\n55Along those lines, Schmidt et al. argue that \u201c[i]n the event that the identity of a winner [of the AI race] does\ncrystallize, mere competition could devolve into conflicts driven by desperation and fear,\u201d to the extent that\n\u201csome states may seem the advent of AI threatening enough to demand a nuclear response.\u201d Henry Kissinger,\nEric Schmidt, and Craig Mundie, Genesis: Artificial Intelligence, Hope, and the Human Spirit (Little Brown and\nCompany, 2024).\n25 Other risks may also be a clear priority for many states, including the proliferation of dan-\ngerous capabilities, exacerbated inequality, and large-scale harms due to accidents or mis-\nuse. Terrorists or rogue states in possession of dangerous capabilities might be particularly\nlikely to use them against other states.56On the flip side, centralization of AI\u2019s benefits could\ngreatly exacerbate economic inequality both within and among states\u2014which in turn can\nlead to political destabilization and even conflict. In response to a similar set of political\nneeds with other technologies, states have constructed governance regimes that manage the\ndanger of proliferation while allowing civilian use of the same technologies.57In particular,\nInternational Atomic Energy Agency (IAEA) safeguards have facilitated civil nuclear transfers\nby providing assurances to states that no transferred technology or material will be used for\nnuclear weapon development. A set of related multilateral export control regimes for sen-\nsitive technologies and materials was built in the decades after the IAEA safeguards regime\ntook shape.58\nFinally, the potential for AI to have scalable effects on the world means that AI accidents\nor misuse might cause harm on a vast scale.59Preventing both accidents and misuse is\nextremely difficult due to fundamental aspects of the technology and how it is being em-\nployed.60Moreover, competition in the economic and military domains makes these prob-\nlems more likely.61Driven in part by increasingly urgent messaging from civil society and\nindustry leaders, governments are increasingly recognizing these issues.62To manage these\n56Markus Anderljung and Julian Hazell, \u2018Protecting Society from AI Misuse: When Are Restrictions on Capa-\nbilities Warranted?\u2019 (arXiv, 29 March 2023), https://doi.org/10.48550/arXiv.2303.09377 .\n57Jane Vaynman and Tristan A. Volpe, \u2018Dual Use Deception: How Technology Shapes Cooperation in Interna-\ntional Relations\u2019, International Organization 77, no. 3 (March 2023): 599\u2013632, https://doi.org/10.1017/S0\n020818323000140 .\n58In particular, such regimes were built for technologies and materials enabling the construction of nuclear,\nbiological, and chemical weapons as well as long-range missiles. See \u2018Guidelines\u2019, Nuclear Suppliers Group; \u2018Com-\nmon Control Lists\u2019, Australia Group; \u2018Equipment, Software And Technology Annex\u2019 (Missile Technology Control\nRegime, 14 March 2024).\n59While most technologies are limited to local effects, AI is poised to create global effects. Two pathways to\nsuch impact include 1) interactions with our existing globe-spanning digital infrastructure and 2) the increasing\nintelligence and autonomy we provide to some AI systems\u2014which might enable them to drastically expand\ntheir impacts.\n60Brian Christian, The Alignment Problem: Machine Learning and Human Values (WW Norton & Company,\n2020); Yoshua Bengio, et al., \u2018International AI Safety Report\u2019, 29 Jan 2025.\n61Stuart Armstrong, Nick Bostrom, and Carl Shulman, \u2018Racing to the Precipice: A Model of Artificial Intelli-\ngence Development\u2019, AI & Society 31, no. 2 (2016): 201\u20136; Amanda Askell, Miles Brundage, and Gillian Hadfield,\n\u2018The Role of Cooperation in Responsible AI Development\u2019 (arXiv, 10 July 2019), http://arxiv.org/abs/1907.0\n4534 ; Eoghan Stafford, Robert F Trager, and Allan Dafoe, \u2018Safety Not Guaranteed: International Races for Risky\nTechnologies\u2019 (November 2022), https://cdn.governance.ai/International_Races_for_Risky_Technolo\ngies_DRAFT_NOV_2022.pdf ; Robert F. Trager et al., \u2018Safety-Performance Tradeoff Model: Exploring Safety In-\nsights in AI Competition\u2019, Modeling Cooperation, December 2022, https://spt.modelingcooperation.com/ .\n62\u2018Statement on AI Risk\u2019, Center for AI Safety, 30 May 2023, https://www.safe.ai/statement-on-ai-risk ;\n\u2018The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023\u2019, GOV.UK, accessed\n2 November 2023, https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletc\nhley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2\n-november-2023 .\n26 risks, governments may take a variety of actions, including some that require international\ncooperation in order to be effective.63\nIn sum, the political needs for international cooperation over AI may include the creation of\nnew economic frontiers, the preservation of peace, and the mitigation of substantial risks to\nsociety. The remainder of this report aims to clarify how states can verifiably move together\ntoward some of these goals if they choose to do so.\n1.4ImportanceofAIagreementverification\nVerification is central to most high-stakes international agreements, and it serves three key\npurposes. First, a verification system aims to detect non-compliance, which is crucial to in-\nform enforcement. Second, verification deters parties from even contemplating a deliberate\nviolation by setting the expectation that such violations will be detected. And third, verifica-\ntion helps build confidence by permitting compliant parties to demonstrate their compliance\nin an open, official, systematic, and continuing way.64\nThese elements can also be illustrated by theories on how cooperative equilibria can be\nachieved within iterated social dilemmas. In a typical social dilemma such as the prisoner\u2019s\ndilemma, the all-cooperate outcome is preferred by all agents compared to the all-defect\noutcome\u2014but the cooperative equilibrium is only available in specific circumstances, such\nas an iterated game with patient players.65One of the main requirements for such an equi-\nlibrium is that each agent needs to be able to detect whether the other party is defecting.\nThis is precisely the kind of information provided by a robust verification protocol. The\ncomparative desirability of the cooperative equilibrium is what drives agents to try to make\nthemselves legible to one another. A further factor is the expectation of future interactions.\nIf agents expect numerous and frequent interactions in the future, they will also expect less\noverall utility if they defect and the other agent detects that defection, since the other agent\ncan be expected to punish them in subsequent rounds.66The expectation of punishment\n63For example, extremely dangerous AI capabilities can be banned or verifiably placed explicitly under the\ncommand of a state. Such capabilities might include the ability to produce chemical, biological, radiological,\nnuclear, or cyber weapons as well as extreme social manipulation capabilities. One analogy is that no private\ngroup in the world is legally allowed to hold nuclear weapons.\n64Coming to Terms with Security: A Handbook on Verification and Compliance (Geneva: United Nations\nInstitute for Disarmament Research, 2003).\n65Robert Axelrod, The Evolution of Cooperation (Basic Books, New York, 1984).\n66In technical terms, the discount factor \u03b4in an indefinite game can be stated as the probability of the two agents\ninteracting again in the future. In that sense, raising the effective frequency of interactions\u2014or the rapidity with\nwhich defection can be recognized and responded to\u2014can push \u03b4toward its theoretical maximum of 1. This\nframing highlights the rapidity of interaction and response as opposed to the common alternative framing of\n\u201cpatient\u201d agents. Even fairly impatient agents can achieve cooperative equilibria if their interactions occur with\nhigh frequency.\n27 in response to defection will increase the likelihood of cooperative behavior.67Ultimately,\nthese factors mean that agents need both verification and enforcement of agreements to solve\nsocial dilemmas. This report focuses on verification but acknowledges that enforcement will\nalso be needed.\nFurthermore, this report emphasizes both agreements and verification mechanisms that fo-\ncus on earlier rather than later aspects of the AI value chain. For example, we emphasize\nmechanisms for controlling the models that can be created in the first place, as opposed to\nmechanisms for controlling the use of those models. While in the analysis that follows we\nattempt to be systematic, we emphasize the governance of upstream factors in the produc-\ntion of AI systems because those factors tend to be more amenable to stable governance via\niterated agreements such as those described above. This emphasis is further reinforced by\ntheoretical and practical arguments that indicate that political agreements which focus on the\n\u201croots\u201d of power rather than power itself\u2014such as coal and steel resources rather than tanks\u2014\nallow states to achieve more robust and useful agreements.68Given that there is substantial\nuncertainty about how powerful AI will be\u2014either in an economic or military sense\u2014it is\nalso prudent to preferentially aim agreements primarily at the ways in which AI systems can\nbe produced, since those agreements are likely to be robust to a wider range of potential\nfutures than comparable agreements that focus only on the specific downstream effects and\nuses of AI.69The rest of this report emphasizes agreements centered on roots of power that\nare upstream of power itself, such as the creation rather than the deployment of models, AI-\nspecialized compute resources, and the supply chains that create both AI chips themselves\nand all of their requisite inputs. While the degree to which an agreement focuses on such up-\nstream resources is a political question and thus outside the scope of this report, this report\nwill nonetheless emphasize iteration on upstream resources because they allow more robust\nverifiable agreements than similar efforts aimed at controlling downstream effects.\nIn some cases, feasible verification may be a necessary condition for negotiating an interna-\ntional agreement in the first place, especially when undetected defection is deemed unac-\nceptable for a state\u2019s security. For example, states did not agree to the Chemical Weapons\n67Intuitive examples of two extremes of this dynamic are interstate war and cyber attacks. Interstate war tends\nto involve very little ambiguity over who the parties to the conflict are\u2014and punishment is implicit in the process\nvia the costs of war. A state considering an overt attack on another state will know that both the defending state\nwill correctly perceive that it is being attacked and will likely respond with its own use of force. Since war is\ncostly, the potential aggressor knows that it will face at least some punishment if it attacks (\u201cdefection\u201d in the\nlanguage of social dilemmas). By contrast, cyber attacks do not generally allow for reliable attribution, so a state\nunder attack generally does not know precisely who is attacking. This lack of attribution makes punishment\nmuch less credible, and thus aggressors are not as deterred by the costs of conflict in cyber attacks as they are\nin conventional war. These differing dynamics help clarify why interstate war is very rare while cyber attacks\nare ubiquitous.\n68Thomas Chadefaux, \u2018Bargaining over Power: When Do Shifts in Power Lead to War?\u2019, International Theory\n3, no. 2 (2011): 228\u201353.\n69For example\"\n\nBased on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n<|im_start|>assistant\n",
    "<|im_start|>system\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n<|im_start|>user\nHere is an excerpt from a document:\n\n\", there is no guarantee that the military use of AI will lead toward continued or renewed deter-\nrence dominance like nuclear weapons and their delivery systems did. This uncertainty will eventually fade as\nthe true shape of AI\u2019s effect on military affairs becomes discernible in the coming years, but for now strategic rea-\nsoning about such effects must acknowledge substantial uncertainty and thus consider a wide range of possible\neffects, including a substantial shift away from deterrence dominance.\n28 Convention until the problem of verification had been solved, which took decades.70By con-\ntrast, some agreements cannot be fully verified due to their nature, yet states may be willing\nto accept them. For instance, the Biological Weapons Convention does not have a verification\nprotocol, partly because great powers have found it unlikely that covert violations would se-\nriously threaten their security.71The kinds of regulation that might be acceptable for a state\nis also influenced by the perceived strategic value of the weapon, including the availability\nof substitutes. For instance, biological weapons were perceived to have lower military utility\nthan chemical weapons72and nuclear weapons.73\nIn the case of AI, demonstrating that verification is feasible may be crucial to unlocking in-\nternational agreements. Given AI\u2019s potentially high strategic value, states may require signif-\nicant certainty that their counterparties are not defecting from the agreement and therefore\nmay require robust verification protocols to be part of any major agreement. If the agree-\nment is perceived to be high-stakes, a covert defection might be regarded as an unaccept-\nable security danger, and thus the political need for reliable verification would increase. If\nsufficiently reliable verification mechanisms are unavailable or perceived to be politically\nunacceptable, such a strategic scenario might not lead to an agreement despite such an equi-\nlibrium being more desirable for all states than the default outcome.74However, if defection\ncan be quickly and accurately detected, the credible threat of reliable enforcement can create\na stable cooperative equilibrium.75\n1.5ChallengesofAIagreementverification\nThree areas of background knowledge are particularly valuable for AI verification: the\ntransparency-security tradeoff, the asymmetric burden of proof for verifying negative\nclaims, and a broad sense of how AI is created, tested, and deployed today. Each will be\nexplored in turn.\n70Thomas Bernauer, The Projected Chemical Weapons Convention: A Guide to the Negotiations in the Con-\nference on Disarmament (New York: United Nations Institute for Disarmament Research, 1990), p 19.\n71Marie Isabelle Chevrier, \u2018Verifying the Unverifiable: Lessons from the Biological Weapons Convention\u2019, Pol-\nitics and the Life Sciences 9, no. 1 (1990): 93\u2013105, https://doi.org/10.1017/S073093840001025X .\n72For example, a UN report notes the following: \u201cThe United States and the UK were of the view that the\nmilitary value of biological weapons was inferior to that of chemical weapons.\u201d Thomas Bernauer, The Projected\nChemical Weapons Convention: A Guide to the Negotiations in the Conference on Disarmament (New York:\nUnited Nations Institute for Disarmament Research, 1990).\n73Regarding the relationship between biological and nuclear weapons, Richard Nixon is alleged to have said,\n\u201cWe\u2019ll never use the damn germs, so what good is biological warfare as a deterrent?\u201d as well as \u201cIf somebody uses\ngerms on us, we\u2019ll nuke \u2019em.\u201d David Hoffman, The Dead Hand: The Untold Story of the Cold War Arms Race\nand Its Dangerous Legacy (Anchor, 2009).\n74Analogously, the cooperation equilibrium of the prisoner\u2019s dilemma is better for all players than mutual\ndefection but might be unreachable unless players have sufficient ability to detect and punish defections in an\niterated game\n75Related to this, the concept of a \u201cbreak out time\u201d in nuclear verification\u2014the approximate amount of time it\nwould take a state to create a nuclear weapon\u2014in turn affects calculations of the minimum politically acceptable\nfrequency for nuclear verification processes.\n29 1.5.1Transparency-securitytradeoff\nSome verifiable agreements are intensely affected by a fundamental tradeoff between trans-\nparency and security.76While sufficient information transparency is necessary to verify\ncompliance, states will attempt to limit transparency to preserve the confidentiality of in-\nformation that is crucial for national security.77For example, on-site inspections of nu-\nclear weapons at a military base may also reveal to inspectors important information about\nthe host state, such as details about their capabilities and vulnerabilities\u2014which adversaries\ncan exploit.78\nThe severity of the tradeoff varies for different approaches to verification, and hinges par-\nticularly on the sensitivity of the information involved, the specificity of the information re-\nvealed, and the usefulness of unilateral monitoring. High-sensitivity information can make\nthe tradeoff severe, as it was when Saddam Hussein continued to obstruct inspections by the\nUN Special Commission due to concerns that they might disclose sensitive information on\nthe security regime apparatus that foreign powers could later leverage to attack his regime.79\nInformation specificity is also crucial, since a verification mechanism that could demon-\nstrate compliance without revealing any extraneous information may face no transparency-\nsecurity tradeoff at all. Unfortunately, mechanisms proposed historically often required or\nallowed the Verifier to access information that was not directly relevant to the agreement. For\nexample, one of the main reasons the Biological Weapons Convention lacks a formal veri-\nfication protocol is that, given the thin line between offensive and defensive uses, effective\nverification would entail the Verifier gathering information on the civilian biotechnology in-\ndustry, potentially including trade secrets.80A state\u2019s unilateral monitoring capabilities play\na role in the transparency-security tradeoff because they shape the overall information secu-\nrity context in which decisions about verification mechanisms are being made. In a hypothet-\nical scenario where unilateral monitoring\u2014such as the actions of intelligence agencies\u2014can\nbe expected to have revealed all of the extraneous information that might have been acciden-\n76Andrew J. Coe and Jane Vaynman, \u2018Why Arms Control Is so Rare\u2019, American Political Science Review 114, no.\n2 (2020): 342\u201355.\n77A similar tradeoff would of course apply to sensitive information belonging to corporations and individuals.\nThis report does not fully explore this family of related tradeoffs. Instead, this report centers the transparency-\nsecurity tradeoff since it seems to be a particularly intense tradeoff for many potential agreement types and\nverification mechanisms. Furthermore, if a set of mechanisms can reassure extremely sensitive institutions such\nas militaries and intelligence agencies that their security is being protected, then similar mechanisms can likely\nbe used to appropriately protect commercial and personal information in most cases.\n78Such information could be of many potential kinds. Section 1.5.1.1 discusses the sensitivity of location infor-\nmation for military assets. Another possibility is that a state is bluffing about its power on the international stage,\nand a verification mechanism might inadvertently reveal their bluff.\n79Gregory D. Koblentz, \u2018Saddam versus the Inspectors: The Impact of Regime Security on the Verification of\nIraq\u2019s WMD Disarmament\u2019, Journal of Strategic Studies 41, no. 3 (16 April 2018): 372\u2013409, https://doi.org/10\n.1080/01402390.2016.1224764 .\n80Guy B. Roberts, Arms Control without Arms Control: The Failure of the Biological Weapons Convention\nProtocol and a New Paradigm for Fighting the Threat of Biological Weapons (USAF Institute for National Security\nStudies, 2003).\n30 tally revealed via a proposed verification mechanism, then that mechanism does not face a\ntransparency-security tradeoff at all\u2014there is nothing more to reveal.81\nSubsequent sections of this report explore the transparency-security tradeoff in further de-\ntail. While severe tradeoffs are certainly possible in the verification of international AI gov-\nernance agreements, it also appears likely that workable tradeoffs can be achieved for the\nverification of most of the agreements explored in this report.82\n1.5.1.1Securitysensitivityofdetailedlocationinformation\nOne example of the transparency-security tradeoff discussed in this report is the fact that\nlocation data is useful for verification, but revealing this data can create perceived security\nvulnerabilities. Location data from the Prover related to personnel or digital systems could\nhelp the Verifier check whether compliance is ongoing. For example, it might be very useful\nfor a chip-centered agreement if the Verifier could inspect data centers and find all chips in\ntheir declared locations. The problem is that if sensitive organizations like militaries believe\nthat these chips are a crucial component of their military power, then they will not want the\nlocations of all their chips to be knowable by the Verifier.83A similar issue arises if the Prover\nmust make verifiable claims about the detailed activities of key personnel.84\nThere are reasons for cautious optimism that this challenge can be overcome. One technique\nthat is explored later in this report allows robust verification and monitoring of resources via\nspecialized hardware while simultaneously making that hardware impossible to locate accu-\nrately. The combination of locally stringent verification mechanisms with deliberate obfus-\ncation of locations can allow for detailed monitoring of hardware even when that hardware\nis physically hidden (see Section 2.5.4.2 ).\n1.5.2Asymmetric burden of proof for verifying\nnegativeclaims\nDemonstrating or verifying the existence of an object or process is often straightforward\ncompared to the difficulty of demonstrating the non-existence of an object or process. For\nexample, it\u2019s trivial for a state to demonstrate that it has a key AI capability\u2014it can simply\ndemonstrate that capability.85By contrast, demonstrating that they do nothave a given capa-\n81A related but very different scenario involves unilateral monitoring capabilities that create enough trans-\nparency that the agreement is workable. Such scenarios occurred historically even for central military tech-\nnologies, such as the naval treaties of the early 20th century and some of the nuclear arms control treaties\nduring the Cold War. Wawrzyniec Muszy\u0144ski-Sulima, \u2018Cold War in Space: Reconnaissance Satellites and\nUS-Soviet Security Competition\u2019, European Journal of American Studies 18, no. 2 (30 June 2023), h t t p s :\n//doi.org/10.4000/ejas.20427 .\n82As noted later, the verification approaches described for regulatory agreements will require serious imple-\nmentation work to begin at least a year\u2014and perhaps a few years\u2014before full-scale verification can be done.\n83Even below the threshold of war, a potential attacker planning a cyber attack on the Prover might benefit\nfrom knowing the locations of relevant hardware.\n84The transparency-security tradeoff for location information might be quite salient for militaries but much\nless important for civilian organizations such as businesses. Therefore, high-accuracy location mechanisms\nmight be politically tolerable for some categories of civilian technology.\n85Similarly, the established method of proving that you have nuclear weapons is to detonate one.\n31 bility might have a significant burden of proof; it might be challenging to produce credible\nevidence for the claim.86To acquire high confidence in the truth of a negative claim like\nthis, a Verifier may need to be provided with significant quantities of relevant evidence of\nvarious kinds.87\nThis asymmetry shows up constantly in discussions of verification. Regardless of practical\ndifficulties, it is epistemically straightforward for a Prover state to make itself legible to the\nVerifier with regards to known ordeclared objects, facilities, processes, and people. This can-\nnot be said, however, for the hypothetically extant but unknown orundeclared objects, facil-\nities, processes, or people. While a Prover might be able to provide a highly credible story\nabout the work done by a given team, it\u2019s very hard for them to demonstrate that no team ex-\nistsdoing a particular kind of work. Similarly, while a Prover might be able to demonstrate\nlegal codes and even enforcement actions within their domestic laws, they would have great\ndifficulty demonstrating that they were not taking other hidden actions that countermanded\nthese laws.\nThree parts of the practice of verification grapple with this asymmetry. First, verification\nprocesses tend to be anchored on declared objects, sites, processes, and people, where robust\nclaims are relatively tractable since there are a limited number of such declarations and they\nmake concrete claims about reality.88Second, parallel efforts are made to catch omissions\nor inconsistencies in declarations\u2014both to discover and to deter circumvention of agree-\nments.89Third, verification processes can leverage the fact that a key input like computing\npower has a known and finite quantity. Given this, the larger the quantity of declared or\nacceptable activities that are fully accounted for, the smaller the possible scale of undeclared\nor prohibited activities using that input.90\n1.5.3WhyAIverificationisparticularlychallenging\nThe field of AI is vast, and thus any discussion of its verification challenges will be incomplete.\nThis section will first compare AI with historical examples of strategically valuable technolo-\ngies. Next, it will explore the challenges of achieving one of the agreement types explored\n86This challenge is similar to the one referred to by the colloquial claim that is impossible to prove a negative.\nSteven D. Hales, \u2018Thinking Tools: You Can Prove a Negative\u2019, Think 4, no. 10 (2005): 109\u201312.\n87Verification techniques for such claims can draw upon similar techniques developed in formal logic for\nproofs of impossibility, including the approach of proof by exhaustion \u2014which aims to demonstrate a claim by\nexhaustively examining all possible ways to check it.\n88Such declarations also make the AI ecosystem much more legible and possible to verify compared to an\necosystem without such declarations. For example, even with substantial access to data about hardware usage, it\nis very difficult to reliably classify workload types even if there are no attempts to hide the activities. See Lennart\nHeim et al., \u2018Governing Through The Cloud: The Intermediary Role Of Compute Providers In AI Regulation\u2019\n(Oxford Martin AI Governance Initiative, March 2024).\n89In general, organizing agreements around declarations can be useful for these reasons. Thanks to Mauricio\nBaker for making this point.\n90A key input having a known and finite quantity is helpful towards governing AI through that input (Sastry\net al., 2024) but is not sufficient on its own. For example, there could be secret production of chips, making\nan accounting based entirely on known chips misleading. Here still, though, there is a sense in which finitude\nis friendly towards verification, in that the more monetary and human capital that a country invests in known\nchip production, the less they will have available for secret production, so it may still be possible to make some\n(approximate) estimates of the scale of uncertainty introduced by these possibilities.\n32 below\u2014the regulation of AI models via rules about their development and deployment. In the\nprocess of exploring this challenge, the general outlines of the AI industry will be provided,\nincluding the key inputs to AI; how models are built and deployed; what kinds of regulations\nmight be implemented; and finally the overall challenge of regulation. We conclude that\nunilateral verification alone cannot reliably verify compliance with AI regulation.\nThe international governance of strategically valuable technologies has a mixed history.\nWhile there have been numerous attempts to govern important technologies, including mil-\nitary aviation,91chemical weapons,92nuclear weapons,93and biological weapons,94where\nsuch agreements have succeeded, they have been very limited in ambition. For example, the\nvarious phases of nuclear arms control during the Cold War only managed to restrain some-\nwhat the otherwise broad and wholesale development and deployment of these weapons.\nSimilarly, agreements over other technologies, such as biological and chemical weapons,\nhappened in the shadow of nuclear weapons, and at least some key actors perceived these\nconversations to be relatively unimportant compared to the leading strategic technology\u2014\nnuclear weapons.95A pessimistic reading of this history indicates that no significant restraint\nwas ever placed on strategically important military technology which had no substitutes. A\nmore balanced reading indicates that the advent of game-changing technologies sometimes\ngalvanized serious examination of governance structures that were unthinkable in other eras\nor for other technologies.96Overall, historical experience indicates that international gov-\nernance of strategically transformative technology is very difficult. One potential basis for\nhope in the governance of AI is that the world of 2025 and beyond is very socially and techno-\nlogically different from the eras in which these other technologies emerged, so it is possible\nthat governance proposals that were out of reach for our ancestors will be within our grasp.\nInternational regulation of AI must engage constructively with the shape of the technology\nand the industry if it is to succeed, so what follows is a brief description of each. AI mod-\nels are built from three fundamental components: 1) data\u2014the text or other media used\nto train the system; 2) algorithms\u2014code that defines how data is transformed into a useful\nmodel; and 3) compute\u2014the computational hardware on which all of these calculations take\nplace. The fundamental resources for deployment are similar, though their proportion tends\nto be different. All three major inputs are downstream of human capital today, since it is\ncurrently humans who are doing (or guiding) the labor needed to provide these inputs.97\n91Waqar H. Zaidi, \u2018 \u201cAviation Will Either Destroy or Save Our Civilization\u201d: Proposals for the International\nControl of Aviation, 1920\u201445\u2019, Journal of Contemporary History 46, no. 1 (1 January 2011): 150\u201378, https:\n//doi.org/10.1177/0022009410375257 .\n92Thomas Bernauer, The Projected Chemical Weapons Convention: A Guide to the Negotiations in the Con-\nference on Disarmament (New York: United Nations Institute for Disarmament Research, 1990).\n93Waqar H. Zaidi, Technological Internationalism and World Order (Cambridge University Press, 2021).\n94Marie Isabelle Chevrier, \u2018Verifying the Unverifiable: Lessons from the Biological Weapons Convention\u2019, Pol-\nitics and the Life Sciences 9, no. 1 (1990): 93\u2013105, https://doi.org/10.1017/S073093840001025X .\n95One clear example of substitution is the U.S. renunciation of biological weapons. President Richard Nixon\nis recalled to have said, \u2018We\u2019ll never use the damn germs, so what good is biological warfare as a deterrent? If\nsomebody uses germs on us, we\u2019ll nuke 'em.\u2019 See William Safire, \u2018On Language; Weapons Of Mass Destruction\u2019,\nThe New York Times Magazine, 19 April 1998, https://www.nytimes.com/1998/04/19/magazine/on-langu\nage-weapons-of-mass-destruction.html .\n96Zaidi, Technological Internationalism and World Order.\n97However, over time we should expect that the provision of these inputs will become increasingly automated.\n33 Human capit alDat aAlgor it hmsCom put eAI de v elopment or\ndeplo ymentFigure 1.1: Summarized inputs into AI development or deployment. Based on a figure in\nSastry et al. (2024), and modified to be inclusive of both AI development and deployment.\nOf all inputs, computing hardware appears the most amenable to governance, as it enables\npolicies regarding regulatory visibility, allocation of resources, and enforcement.98By com-\nparison, it is comparably difficult to establish robust governance over personnel,99data,100\nor algorithms.101\nThe AI model lifecycle is often summarized as having three phases: training, fine-tuning, and\ninference.102Training (or \u201cpretraining\u201d in some contexts) is the initial creation of a model.\nFine-tuning is the more fine-grained shaping of the model\u2019s behavior. Inference is the use\nof a model to do work. As machine learning paradigms evolve, the character and relative\nimportance of these phases also evolve. For example, recent shifts toward \u201creasoning mod-\nels\u201d (or more generally, \u201cinference scaling\u201d) have potentially important ramifications for the\nrelative computational difficulty of the different phases, with inference-time compute now\npotentially playing a much more important role than it did previously.103\nRegulations for AI might engage with one or more phases of the model lifecycle and one or\nmore of the key inputs. This domain of inquiry is vast, so only a very brief summary of some\nrelevant parts of this topic area can be provided here. Three ways to approach AI regulation\nare to stipulate rules about 1) how models are created, 2) how models behave, and 3) how\nmodels are controlled. First, the creation and modification of models employ a wide variety\nof different processes, some of which might be amenable to governance. For example, con-\n98Girish Sastry et al., \u2018Computing Power and the Governance of Artificial Intelligence\u2019 (arXiv, 13 February 2024),\nhttp://arxiv.org/abs/2402.08797 .\n99Monitoring relevant people is likely infeasible and faces significant strategic problems (see Section 2.1).\n100Controlling data is possible in theory (see Section 2.5.1), but credibly demonstrating that it has not been\ncopied is extremely difficult (see Section 2.5.2.4 ). Data governance may become more workable if computing\nsecurity improves drastically and if techniques such as federated learning\u2014which allows models to be trained\non private data without revealing that data\u2014become more widely known.\n101Algorithm governance would require personnel controls (see Section 2.1) and also brings its own particular\ndifficulties since algorithms are potentially extremely sensitive information (see Appendix E).\n102Changes in leading techniques are rapid, so this breakdown should be considered approximate rather\nthan definitive.\n103This summary does not capture the nuanced changes that this paradigm shift might create for AI develop-\nment and governance. For a fuller explanation, see Toby Ord, \u2018Inference Scaling Reshapes AI Governance\u2019, 12\nFebruary 2025, https://www.tobyord.com/writing/inference-scaling-reshapes-ai-governance .\n34 straints can be placed on the inputs used to create the model, such as dangerous kinds of data;\nthe size of the model, since the largest models appear to generate many of the most salient\nrisks; or the techniques used to train the model, since some might be regarded as very safe104\nor unsafe105(see also Section 4.5.1.2.2 ). Second, regulations might also apply to the behavior\nof models, including their performance on capability or safety tests that are either fully au-\ntomated or involving humans (see Section 4.5.1.2.3 ). In this report, the term \u201cevaluation\u201d will\ngenerally refer to tests which can be run automatically with no human input, although the po-\ntential for human involvement will also be discussed (see Section 3.5).106Third, regulations\nmight relate to how the model is deployed, with major categories including regulations for\nmodels kept within data centers (Section 4.5.2.3.1 ) or those that are embedded into devices\nwhich can be moved around (see Section 4.5.2.3.6 ).\nTo outline the scope of the regulatory problem, consider a scenario where regulating AI\nrequires controls on all relevant hardware supply chains, data centers, and AI-providing or-\nganizations involved in development and deployment. A view this broad might indeed be\nneeded, since reliable hardware governance often requires being able to verify that the hard-\nware does not itself contain governance circumvention mechanisms.107While aspects of\nthis description will be unrealistic, it provides a sense of how deep and multifaceted the chal-\nlenges of AI verification can be. This summary should be seen as outlining the problem\nspace, not proposing a solution.\nSupply chains could be governed and verified at key choke points such as semiconductor\nfabrication to ensure that hardware matches expected designs and to ensure that required\ncyber and physical security measures throughout the hardware provision ecosystem are in\nplace.108Ideally, hardware would be subject to verification at all high-leverage points in the\nsupply chain, and the supply chain would use chain-of-custody requirements. Data centers\ncould be governed and verified similarly, with all equipment installed in them subject to\ndesign review and inspection to ensure compliance with the verification scheme.109Most\nchallenging of all might be the regulation of all uses of data center compute, including all\nworkloads that companies and individuals submit to run on data center hardware.110Detailed\n104See for example some of the proposals for provably safe AI. David \u2018davidad\u2019 Dalrymple et al., \u2018Towards\nGuaranteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems\u2019 (arXiv, 8 July 2024), https:\n//doi.org/10.48550/arXiv.2405.06624 ; David \u2018davidad\u2019 Dalrymple, \u2018Safeguarded AI: Constructing Guaran-\nteed Safety\u2019 (Advanced Research and Invention Agency, UK Government, 2024), https://www.aria.org.uk/\nmedia/3nhijno4/aria-safeguarded-ai-programme-thesis-v1.pdf .\n105For example, long-term planning agents may be inherently unsafe. Michael K. Cohen et al., \u2018Regulating\nAdvanced Artificial Agents\u2019, Science, 5 April 2024, https://doi.org/10.1126/science.adl0625 .\n106\u201cAudits\u201d are also commonly mentioned as terms of art, often employing a mix of automated and human-\ncentered tests.\n107For more on this, see Appendix Dand Section 2.2.3.3 .\n108The relevant hardware supply chains are highly international. Chris Miller, Chip War: The Fight for the\nWorld\u2019s Most Critical Technology (Simon and Schuster, 2022).\n109See Section 2.5.2.1 for more on this.\n110If regulation is universal, it would apply to all workloads. If regulation focuses on only AI projects involving\nlarge amounts of compute, smaller workloads can be examined more minimally. However, distributed training\nof AI models is workable, such as splitting a large project into a thousand smaller workloads, then any small\nworkloads submitted to a data center operator might be components of a distributed large training run. To\naddress this, probabilistic testing of smaller workloads, rather than universal testing, would be sufficient to catch\na major circumvention attempt.\n35 regulation of activities taking place on the AI chips would require methods to verify the\nsoftware, data, algorithms, and hyperparameters that are used in the creation of AI models\nas well as ways to scrutinize the models themselves.111Further challenges include the fact\nthat hardware stacks diverge from one another due to competition among hardware and\nsoftware providers\u2014and the size and complexity of the ecosystem is expanding dramatically.\nWe are entering the range of millions to tens of millions of non-uniform AI-specialized data\ncenter chips deployed simultaneously around the world, with rapid evolution at all levels.\nGovernance efforts like this are sometimes only as strong as their weakest points. Therefore,\nstates\u2019 threat models should evolve as they make changes to the system. For example, if you\nshore up hardware protections, those seeking to circumvent your system might focus more\non cyber attacks.112\nThis stylized description of a regulatory system illustrates why unilateral verification cannot\nbe expected to be sufficient in domains such as regulation. While unilateral verification can\ncertainly play a role in every agreement by providing states with information about whether\nthe declarations by their counterparties are true and complete,113for at least some agreement\ntypes it cannot provide the level of certainty required for the agreement to be considered ver-\nifiable. The crux of this issue is that computation tends to be universal and fungible,114so un-\nless a state has a reliable method for checking that the computations undertaken by another\nstate abide by specified rules, they cannot be certain that the computations were compliant.\nDistant unilateral verification such as satellite imagery allows states to understand the rough\nscale of a data center, and other information sources might provide approximate informa-\ntion about the hardware within it.115However, knowing a data center\u2019s scale and hardware\nonly tells you so much, since general-purpose hardware could be used for many different\nthings and with many different kinds of intent.116Some visible effects of AI development can\nbe perceived (e.g., products, weapons fielded) but most of the details of ongoing AI develop-\nment are invisible by default.\nIntelligence capabilities might sometimes reveal details about AI development programs.117\nHowever, this kind of transparency shouldn\u2019t be expected to be reliable, mutual, or politi-\ncally useful. Reliability is questionable because security practices change over time. Reliable\n111More on this in Section 4.5.1.2.2 , and Section 4.5.1.2.3 . See also Stephen Casper et al., \u2018Black-Box Access Is\nInsufficient for Rigorous AI Audits\u2019 (arXiv, 25 January 2024), https://doi.org/10.48550/arXiv.2401.14446 .\n112Furthermore, adding new hardware can introduce new vulnerabilities that are specific to that hardware or\nto the interfaces between different kinds of hardware.\n113On this point, see Section 1.5.2.\n114Universal means that any relevant hardware may undertake a particular computation. Without extra infor-\nmation, you can\u2019t tell which piece of hardware did the computation.\n115This does provide a way to verify declarations about hardware within the data center, but not about the\nspecific computations undertaken by that hardware. See also Lennart Heim, \u2018Limitations of Satellite Imagery\nAnalysis for AI-Specific Data Centers\u2019, Lennart Heim (blog), 13 September 2024, https://blog.heim.xyz/limi\ntations-of-satellite-imagery/ .\n116In some scenarios, even seeing the scale and type of hardware could be informative. For example, if dis-\ntributed training turns out to be infeasible (see Section 1.1.1) and a state pays a large overhead to create an enor-\nmous data center. In that scenario, one can infer that there is a pressing reason why they were willing to foot\nthe extra cost of building such infrastructure, and the most likely reason is that they are building a model of\nimmense size. For clarity, this report assumes that distributed training is possible, so in the rest of this report,\nno such unambiguous hardware-based signal is expected.\n117Intelligence capabilities include\u2014but are not limited to\u2014satellite data, human sources, and cyberattacks.\n36 mutual transparency\u2014where both actors can see what the other is doing in sufficient de-\ntail to consider those activities in compliance with an agreement\u2014cannot be expected even\namong the great powers with excellent cyber capabilities. Furthermore, knowledge of vio-\nlations might not be politically useful, since the disclosure dilemma means that a state may\nnot want to reveal information from intelligence operations, lest it accidentally reveal any\nof their intelligence capabilities or specific sources.118While unilateral verification was suf-\nficient for some parts of the Cold War nuclear era,119it does not appear to be sufficient on\nits own for many of the AI agreements discussed in this report. At the same time, unilat-\neral verification via tools such as national technical means and intelligence agencies can be\nexpected to provide a parallel source of information that can be cross-referenced with ver-\nifiable declarations being made as part of an agreement, and therefore can help both deter\nand catch attempts to circumvent the agreement (see Section 1.4). In the regulatory domains,\nthe relative weakness of unilateral verification means that it will primarily play a supporting\nrole to cooperative verification mechanisms, which will be required if robust verification is\npolitically necessary.120\n1.6AIverificationmaybeworkable\nDespite the significant challenges of verifying regulatory agreements involving AI, there are\nalso reasons for optimism. This section will outline four reasons for optimism before outlin-\ning the remainder of the report. First, as noted earlier, AI-specialized computing hardware\nis much more governable than it first appears. Second, the supply of AI-specialized com-\npute has multiple major choke points that are potentially high-leverage focal points for both\ninternational agreements and verification processes. In particular, the Dutch firm ASML is\nthe only provider of the lithography equipment required to make cutting-edge chips, and\nTaiwan-based TSMC dominates the fabrication of cutting-edge chips. Overall, the supply\nnetwork for cutting-edge hardware spans several countries, thus requiring supply chain gov-\nernance to be international from the outset. States throughout the supply chain can take\nimportant governance action even alone, and even a small group of these states working to-\ngether could powerfully shape the future of AI governance. Third, AI is drastically easier to\nregulate and verify than cyber weapons, despite some apparent similarities between the two\ndomains. The most powerful and geopolitically disruptive forms of AI currently use enor-\n118Information from state intelligence agencies may not be something that you can reveal in detail to either the\nother state or to an international organization, since the very act of revealing that information can also reveal\nhow you attained it. Allison Carnegie and Austin Carson, \u2018The Disclosure Dilemma: Nuclear Intelligence and\nInternational Organizations\u2019, American Journal of Political Science 63, no. 2 (April 2019): 269\u201385, https://doi.\norg/10.1111/ajps.12426 .\n119All agreements about nuclear weapon controls up till the 1987 INF Treaty were verified unilaterally. Rose\nGottemoeller, \u2018Looking Back: The Intermediate-Range Nuclear Forces Treaty\u2019 (Arms Control Today, 2007), http\ns://www.armscontrol.org/act/2007-06/looking-back-intermediate-range-nuclear-forces-treaty .\n120Regulation is discussed in detail in Section 4.5.\n37 mous amounts of compute and thus require highly visible infrastructure.121While a cyber\nweapon might be created on a few hundred dollars\u2019 worth of commodity hardware in any\ncountry in the world, AI systems of even middling significance currently require at least tens\nto hundreds of millions of dollars to train. While verification measures for cyber weapons\nmay be nearly impossible to achieve, AI is much more amenable to verification. Fourth,\nprivacy-preserving verification mechanisms have been developed and appear to be appli-\ncable to AI. If appropriately implemented, these mechanisms have the potential to provide\nverifiability without severe transparency costs.122\nThe remainder of this report proceeds as follows. First, a set of potential verification compo-\nnents are reviewed to provide the reader with an overall understanding of the potential and\nlimits of various approaches to verification (Section 2). Second, we discuss how agreements\nover AI\u2014and especially regulatory agreements over AI\u2014will face a set of political options\nand tradeoffs that pertain to verification (Section 3). Third, a set of possible international\nagreements relating to AI are outlined, along with potential implementations, verification\nneeds, and potentially workable verification approaches (Section 4). Fourth, the report con-\ncludes with a synthesis of findings and suggests some priorities for further work (Section 5).\n121Clarify that large-scale uses of compute is often discussed as a useful way to distinguish certain categories\nof potentially dangerous activity from less dangerous activity. This is a widely discussed measure because it\nfocuses parts of the conversation on particularly dangerous activities and ensures that governance is relatively\nwell targeted. It is not however a complete regulatory plan on its own. Sara Hooker, \u2018On the Limitations of\nCompute Thresholds as a Governance Strategy\u2019 (arXiv, 29 July 2024), https://doi.org/10.48550/arXiv.240\n7.05694 .\n122Some prior related works concluded that the verification of AI would be politically infeasible, but did not\nexamine the potential of privacy-preserving technologies for addressing that problem. For example, see Jane\nVaynman and Tristan A. Volpe, \u2018Dual Use Deception: How Technology Shapes Cooperation in International\nRelations\u2019, International Organization 77, no. 3 (March 2023): 599\u2013632, https://doi.org/10.1017/S0020818\n323000140 .\n38 2Surveyofverificationcomponents\nThis section surveys some key verification components which might be brought to bear on\ninternational agreements over AI. Each of these components focuses on different objects or\nprocesses, such as personnel, digital systems (including software and hardware), electrical in-\nfrastructure, socio-technical systems, and hardware enclosures. Later sections of this report\nillustrate some ways in which these verification components could be employed to verify\ninternational AI agreements.\n2.1Personnel\nIn personnel-centered verification, human personnel play one or more key roles as the\nProver seeks to make itself legible to the Verifier. The following subsections unpack three\nvery different ways in which humans can play central roles in verification: as targets of veri-\nfication controls, as intermediaries for information, and as inspectors.\n2.1.1Verifiablepersonnelcontrols\nVerifiable personnel controls can be used by a Prover to demonstrate to a Verifier that per-\nsonnel within the Prover\u2019s institutions are managed according to an agreement, and that no\nadditional personnel are present. Potential controls include verifiable processes, physical ac-\ncess controls, digital controls, and legal controls. However, there are significant challenges\nwith robustly verifying these due to the asymmetric burden of proof for verifying negative\nclaims (see Section 1.5.2). A fuller exploration of this topic can be found in Appendix A.\n2.1.2Verifiableclaimscenteredonaccesstopersonnel\nSome verification approaches aim to help Provers make verifiable claims through the struc-\ntured provision of access to personnel. For example, an agreement might give the Verifier the\nright to interview anyone in a designated group.123An agreement may also provide safe op-\nportunities for those personnel to \u201cblow the whistle\u201d on non-compliant activities they know\nabout within their organization. While these mechanisms can be useful in low-stakes envi-\nronments, they have severe limitations in high-stakes environments, as the Prover would be\nable to restrict access to personnel who have witnessed non-compliant behavior or otherwise\nonly employ loyal high-trust individuals. A longer exploration of these points can be found\nin Appendix B.\n123For example, the International Atomic Energy Agency (IAEA) can employ interviews of declared nuclear\npersonnel as part of their nuclear safeguards inspections. \u2018IAEA Safety Standards: Functions and Processes of\nthe Regulatory Body for Safety\u2019 (International Atomic Energy Agency, 2018).\n39 2.1.3Humaninspectorsasaverificationmechanism\nInspectors have played a key role in some of history\u2019s most challenging verification agree-\nments, such as the Intermediate-Range Nuclear Forces (INF) treaty,124(Westport, Conn.:\nPraeger, 1998). and it is reasonable to expect that they will also play a role with AI. A re-\nlated role that humans can play in a verification system is that which this report terms an\nassessor \u2014a human that makes judgments about compliance based on information that they\nare provided access to. This can be similar to, or overlap with, the roles of independent au-\nditors for other aspects of AI systems. The responsibilities, tools, and rights of inspectors\ntend to be listed in agreement details.125Later sections of this report explore aspects of the\ninspector\u2019s potential role. Of particular note, inspectors may play crucial roles in confirming\nthat buildings and hardware are compliant with an agreement both at the outset and inter-\nmittently thereafter (see Section 2.5.2.1 ). The related role of the assessor is also explored with\nregards to the potential for examining information\u2014such as code, data, and models\u2014within\ntightly controlled facilities (see Section 3.5).\nA key challenge is that when human inspectors visit sensitive sites, they can detect informa-\ntion beyond that required for their verification task. Even if garnering such information is\nnot their goal, the danger that they would notice something security-relevant can make the\ntransparency-security tradeoff worse. For example, this was a key concern during Cold War\narms control negotiations.126A related policy point is that it is advisable for states to not\nco-locate sensitive assets, such as military hardware, with AI infrastructure if at all possible,\nto avoid an unnecessarily bad transparency-security tradeoff with regards to AI hardware\ninspections (see Appendix C.4).\nHowever, inspectors for AI hardware should have a very promising transparency-security\ntradeoff. Using only human senses it is exceedingly unlikely that highly sensitive information\nwould be revealed to an inspector who physically steps into a data center. What they would\nbe able to perceive would be only the kinds of things they would be sent to examine, such\nas building layout, hardware inventory, and hardware connections. We should be optimistic\nabout this possibility, because inspectors were successfully employed in much more sensitive\ndomains, such as the INF treaty, where extensive discussions were needed to arrive at an\ninspection protocol that sufficiently addressed the concerns of all sides.127The INF treaty\n124Joseph P. Harahan, On-Site Inspections Under The INF Treaty, A History of the On-Site Inspection Agency\nand Treaty Implementation, 1988-1991 (On-Site Inspection Agency, United States Department of Defense, 1993);\nGeorge Rueckert, On-Site Inspection in Theory and Practice: A Primer on Modern Arms Control Regimes\n125Consider that the verification protocol for the 1991 Strategic Arms Reduction Treaty (START) has a 500-page\nverification protocol. Rose Gottemoeller, \u2018Looking Back: The Intermediate-Range Nuclear Forces Treaty\u2019 (Arms\nControl Today, 2007), https://www.armscontrol.org/act/2007-06/looking-back-intermediate-range\n-nuclear-forces-treaty .\n126Coe and Vaynman, \u2018Why Arms Control Is so Rare\u2019.\n127\u2018Memorandum of Agreement Regarding the Implementation of the Verification Provisions of the Treaty\nBetween the United States of America and the Union of Soviet Socialist Republics on the Elimination of Their\nIntermediate-Range and Shorter-Range Missiles\u2019, 21 December 1989, https://nuke.fas.org/control/inf/t\next/inf-mouanx.htm .\n40 case is instructive, because it illustrates that there are many ways to finely shape inspection\nrules so that an agreement is both tolerable to all sides and achieves its goal.128\nThe key remaining problem with inspectors is their scalability. There are already millions of\nAI-specialized chips in the world.129Physically inspecting this many chips and their many\nassociated systems would be an enormous undertaking. Even doing this once\u2014in detail\u2014for\nan AI-specialized data center might be a substantial task. Fully inspecting all of these chips on\na rapid cadence would be infeasible. Therefore, physical inspections make sense for three\npurposes: 1) verifying that all hardware in a data center is compliant (see Section 2.5.2.1 ),\n2) reinspections of small portions of that hardware due to Prover-initiated maintenance or\nVerifier-initiated challenge inspections,130and 3) random inspections, presuming that some\nspecific kinds of verification of chip activities can only be done locally.131All three of these\nactivities are feasible with a reasonable number of inspectors.132\n2.2Digitalsystems\n2.2.1Cryptography\nOne reason that AI verification may be easier than historical arms control is that applied cryp-\ntography has greatly advanced. This allows a host of meaningful governance functions to be\nconducted in a way that does not reveal extraneous information.133In sum, cryptography\ncan often allow the transparency-security tradeoff to be navigated successfully.\nMoreover, cryptographic claims can be at least as reliable as physical claims in practice. For\nexample, essentially the entire Internet and all technical stacks depend on cryptography that\nis profoundly reliable. Skepticism about the verifiability of non-material things such as dig-\nital files or computational operations is reasonable. However, we are in a world where an\nextraordinary proportion of all economic and social exchanges already take place through\n128For example, nuclear weapons inspection protocols show that it is possible to vary the level of intrusiveness\ngreatly via various techniques such as limiting access, weighing and measuring external dimensions rather than\ndirect visual of the verified object, and shrouding. Corresponding technical measures for AI hardware would\nneed to be devised before and during negotiation of an AI agreement that involves inspection.\n129Agam Shah, \u2018\"\n\nBased on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n<|im_start|>assistant\n"
  ],
  "measurements": {
    "1": [
      {
        "prefill_signals": {
          "pos_-3": {
            "logprobs": {
              "token_ids": [
                151644,
                151668,
                151667,
                1986,
                151645
              ],
              "log_probs": [
                -0.20809520781040192,
                -1.7080951929092407,
                -5.083095073699951,
                -8.77059555053711,
                -9.77059555053711
              ]
            }
          },
          "pos_-2": {
            "logprobs": {
              "token_ids": [
                77091,
                90,
                55277,
                198,
                151667,
                10607
              ],
              "log_probs": [
                -20.283857345581055,
                -0.7838566303253174,
                -1.1588566303253174,
                -2.2213566303253174,
                -4.096356391906738,
                -4.221356391906738
              ]
            }
          },
          "pos_-1": {
            "logprobs": {
              "token_ids": [
                198,
                220,
                50209,
                271,
                481
              ],
              "log_probs": [
                -0.6725286245346069,
                -3.1100287437438965,
                -3.4225287437438965,
                -3.6100287437438965,
                -3.7350287437438965
              ]
            }
          }
        }
      },
      {
        "prefill_signals": {
          "pos_-3": {
            "logprobs": {
              "token_ids": [
                151644,
                151668,
                151667,
                151645,
                32313
              ],
              "log_probs": [
                -0.06509151309728622,
                -2.815091609954834,
                -5.815091609954834,
                -10.190091133117676,
                -11.815091133117676
              ]
            }
          },
          "pos_-2": {
            "logprobs": {
              "token_ids": [
                77091,
                55277,
                198,
                59892,
                90,
                15
              ],
              "log_probs": [
                -19.552379608154297,
                -0.3648790717124939,
                -2.1148791313171387,
                -3.1148791313171387,
                -3.1773791313171387,
                -4.177379131317139
              ]
            }
          },
          "pos_-1": {
            "logprobs": {
              "token_ids": [
                198,
                271,
                220,
                50209,
                119593
              ],
              "log_probs": [
                -0.4594593048095703,
                -2.7719593048095703,
                -3.3969593048095703,
                -3.8969593048095703,
                -4.52195930480957
              ]
            }
          }
        }
      },
      {
        "prefill_signals": {
          "pos_-3": {
            "logprobs": {
              "token_ids": [
                151644,
                151668,
                151667,
                1986,
                151645
              ],
              "log_probs": [
                -2.127706527709961,
                -0.12770652770996094,
                -7.877706527709961,
                -9.315206527709961,
                -9.940206527709961
              ]
            }
          },
          "pos_-2": {
            "logprobs": {
              "token_ids": [
                77091,
                55277,
                151667,
                198,
                11159,
                10607
              ],
              "log_probs": [
                -18.88307762145996,
                -1.3830773830413818,
                -1.4455773830413818,
                -1.6955773830413818,
                -3.320577383041382,
                -3.383077383041382
              ]
            }
          },
          "pos_-1": {
            "logprobs": {
              "token_ids": [
                198,
                50209,
                271,
                220,
                81250
              ],
              "log_probs": [
                -0.48177412152290344,
                -3.481774091720581,
                -4.10677433013916,
                -4.16927433013916,
                -4.41927433013916
              ]
            }
          }
        }
      }
    ],
    "2": [
      {
        "prefill_signals": {
          "pos_-3": {
            "logprobs": {
              "token_ids": [
                151644,
                151668,
                151667,
                1986,
                151645
              ],
              "log_probs": [
                -0.25839293003082275,
                -1.5083929300308228,
                -5.133392810821533,
                -8.508393287658691,
                -9.633393287658691
              ]
            }
          },
          "pos_-2": {
            "logprobs": {
              "token_ids": [
                77091,
                90,
                55277,
                198,
                151667,
                10607
              ],
              "log_probs": [
                -20.333389282226562,
                -0.9115142226219177,
                -1.0365142822265625,
                -2.2865142822265625,
                -3.6615142822265625,
                -4.1615142822265625
              ]
            }
          },
          "pos_-1": {
            "logprobs": {
              "token_ids": [
                198,
                220,
                50209,
                271,
                481
              ],
              "log_probs": [
                -0.6989287734031677,
                -2.9489288330078125,
                -3.3239288330078125,
                -3.7614288330078125,
                -3.8864288330078125
              ]
            }
          }
        }
      },
      {
        "prefill_signals": {
          "pos_-3": {
            "logprobs": {
              "token_ids": [
                151644,
                151668,
                151667,
                151645,
                32313
              ],
              "log_probs": [
                -0.0734456330537796,
                -2.6984455585479736,
                -5.698445796966553,
                -10.073445320129395,
                -11.760945320129395
              ]
            }
          },
          "pos_-2": {
            "logprobs": {
              "token_ids": [
                77091,
                55277,
                198,
                90,
                59892,
                15
              ],
              "log_probs": [
                -19.67531394958496,
                -0.37062665820121765,
                -1.99562668800354,
                -3.24562668800354,
                -3.30812668800354,
                -4.370626449584961
              ]
            }
          },
          "pos_-1": {
            "logprobs": {
              "token_ids": [
                198,
                271,
                220,
                50209,
                119593
              ],
              "log_probs": [
                -0.5289156436920166,
                -2.9039156436920166,
                -3.2789156436920166,
                -3.4664156436920166,
                -4.5914154052734375
              ]
            }
          }
        }
      },
      {
        "prefill_signals": {
          "pos_-3": {
            "logprobs": {
              "token_ids": [
                151644,
                151668,
                151667,
                1986,
                151645
              ],
              "log_probs": [
                -2.127840995788574,
                -0.12784092128276825,
                -7.627840995788574,
                -9.315340995788574,
                -9.815340995788574
              ]
            }
          },
          "pos_-2": {
            "logprobs": {
              "token_ids": [
                77091,
                151667,
                55277,
                198,
                11159,
                10607
              ],
              "log_probs": [
                -18.992637634277344,
                -1.1957623958587646,
                -1.3832623958587646,
                -1.8832623958587646,
                -3.4457623958587646,
                -3.5707623958587646
              ]
            }
          },
          "pos_-1": {
            "logprobs": {
              "token_ids": [
                198,
                50209,
                271,
                220,
                81250
              ],
              "log_probs": [
                -0.5692540407180786,
                -3.319253921508789,
                -4.194253921508789,
                -4.256753921508789,
                -4.381753921508789
              ]
            }
          }
        }
      }
    ],
    "4": [
      {
        "prefill_signals": {
          "pos_-3": {
            "logprobs": {
              "token_ids": [
                151644,
                151668,
                151667,
                1986,
                151645
              ],
              "log_probs": [
                -0.25840333104133606,
                -1.5084033012390137,
                -5.133403301239014,
                -8.445903778076172,
                -9.633403778076172
              ]
            }
          },
          "pos_-2": {
            "logprobs": {
              "token_ids": [
                77091,
                90,
                55277,
                198,
                151667,
                10607
              ],
              "log_probs": [
                -20.284923553466797,
                -0.9255483746528625,
                -1.0505483150482178,
                -2.1755483150482178,
                -3.6755483150482178,
                -4.175548553466797
              ]
            }
          },
          "pos_-1": {
            "logprobs": {
              "token_ids": [
                198,
                220,
                50209,
                481,
                271
              ],
              "log_probs": [
                -0.7402399182319641,
                -3.0527398586273193,
                -3.2402398586273193,
                -3.6152398586273193,
                -3.6152398586273193
              ]
            }
          }
        }
      },
      {
        "prefill_signals": {
          "pos_-3": {
            "logprobs": {
              "token_ids": [
                151644,
                151668,
                151667,
                151645,
                32313
              ],
              "log_probs": [
                -0.06509006023406982,
                -2.8150901794433594,
                -5.815090179443359,
                -10.19009017944336,
                -11.81509017944336
              ]
            }
          },
          "pos_-2": {
            "logprobs": {
              "token_ids": [
                77091,
                55277,
                198,
                59892,
                90,
                15
              ],
              "log_probs": [
                -19.640974044799805,
                -0.40659967064857483,
                -1.9065996408462524,
                -3.156599760055542,
                -3.156599760055542,
                -4.156599521636963
              ]
            }
          },
          "pos_-1": {
            "logprobs": {
              "token_ids": [
                198,
                271,
                220,
                50209,
                119593
              ],
              "log_probs": [
                -0.5364739894866943,
                -2.7864739894866943,
                -3.3489739894866943,
                -3.5989739894866943,
                -4.473974227905273
              ]
            }
          }
        }
      },
      {
        "prefill_signals": {
          "pos_-3": {
            "logprobs": {
              "token_ids": [
                151644,
                151668,
                151667,
                1986,
                151645
              ],
              "log_probs": [
                -1.7023725509643555,
                -0.20237258076667786,
                -7.4523725509643555,
                -9.639872550964355,
                -9.764872550964355
              ]
            }
          },
          "pos_-2": {
            "logprobs": {
              "token_ids": [
                77091,
                55277,
                151667,
                198,
                11159,
                10607
              ],
              "log_probs": [
                -18.84303855895996,
                -1.2258504629135132,
                -1.4758504629135132,
                -1.7258504629135132,
                -3.4758505821228027,
                -3.5383505821228027
              ]
            }
          },
          "pos_-1": {
            "logprobs": {
              "token_ids": [
                198,
                50209,
                271,
                220,
                81250
              ],
              "log_probs": [
                -0.46352362632751465,
                -3.5885236263275146,
                -4.151023864746094,
                -4.276023864746094,
                -4.463523864746094
              ]
            }
          }
        }
      }
    ]
  },
  "within_hardware_analysis": {
    "matrix": [
      [
        0.0,
        0.3264639974132602,
        0.30657487838269315
      ],
      [
        0.3264639974132602,
        0.0,
        0.26935586077498375
      ],
      [
        0.30657487838269315,
        0.26935586077498375,
        0.0
      ]
    ],
    "per_reference_matrices": [
      [
        [
          0.0,
          0.37575273211836885,
          0.36872654761984425
        ],
        [
          0.37575273211836885,
          0.0,
          0.17474231823062125
        ],
        [
          0.36872654761984425,
          0.17474231823062125,
          0.0
        ]
      ],
      [
        [
          0.0,
          0.31741992003082736,
          0.1834433913537646
        ],
        [
          0.31741992003082736,
          0.0,
          0.21213268769718685
        ],
        [
          0.1834433913537646,
          0.21213268769718685,
          0.0
        ]
      ],
      [
        [
          0.0,
          0.2862193400905845,
          0.3675546961744705
        ],
        [
          0.2862193400905845,
          0.0,
          0.4211925763971431
        ],
        [
          0.3675546961744705,
          0.4211925763971431,
          0.0
        ]
      ]
    ],
    "diagonal": 0.0,
    "off_diagonal": 0.3007982455236457,
    "snr": null,
    "equivalent_pairs": []
  }
}