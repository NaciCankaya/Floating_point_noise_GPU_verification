{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4257d7-4928-4596-9a46-ac8f34eedd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TENSOR PARALLELISM DETECTABILITY EXPERIMENT\n",
      "================================================================================\n",
      "\n",
      "Hostname: 63ddf43f81b3\n",
      "GPUs: 4x NVIDIA A100-SXM4-80GB\n",
      "vLLM: 0.11.2\n",
      "Model: Qwen/Qwen3-14B\n",
      "TP sizes to test: [1, 2, 4]\n",
      "Mode: LOCAL\n",
      "\n",
      "Loading tokenizer...\n",
      "Found 1 PDF(s)\n",
      "  Loading: /workspace/Verification-for-International-AI-Governance.pdf\n",
      "Total source tokens: 120215\n",
      "Prompt structure: 33 prefix + 10000 snippet + 34 suffix = 10067 tokens\n",
      "Created 3 prompts\n",
      "\n",
      "================================================================================\n",
      "TENSOR PARALLEL = 1\n",
      "================================================================================\n",
      "Loading model with TP=1...\n",
      "INFO 11-26 23:45:21 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'max_model_len': 32768, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen3-14B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:45:22 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-26 23:45:22 [model.py:1745] Using max model len 32768\n",
      "INFO 11-26 23:45:30 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-26 23:45:30 [vllm.py:500] Cudagraph is disabled under eager mode\n",
      "WARNING 11-26 23:45:31 [system_utils.py:103] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reasons: CUDA is initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:38 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:39 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.22.0.2:58813 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:39 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:39 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:40 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:40 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:02,  3.07it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:03,  1.77it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:01<00:03,  1.38it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:02<00:03,  1.28it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:03<00:02,  1.25it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:04<00:01,  1.22it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:05<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.28it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:47 [default_loader.py:314] Loading weights took 6.49 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:47 [gpu_model_runner.py:3338] Model loading took 27.5185 GiB memory and 7.395146 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:49 [gpu_worker.py:359] Available KV cache memory: 18.53 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:49 [kv_cache_utils.py:1229] GPU KV cache size: 121,424 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:49 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 3.71x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:49 [core.py:250] init engine (profile, create kv cache, warmup model) took 2.08 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14477)\u001b[0;0m INFO 11-26 23:45:50 [vllm.py:500] Cudagraph is disabled under eager mode\n",
      "INFO 11-26 23:45:51 [llm.py:352] Supported tasks: ['generate']\n",
      "\n",
      "================================================================================\n",
      "REPRODUCIBILITY TEST\n",
      "================================================================================\n",
      "Running same prompt 5 times...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af9d3aea55c46d099a543bb0a1ff6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbfeb779c6824ee785e80e1278ef585c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 1: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca0fdc1ce534dcb96515ecfff7b3f6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df93f70d2fc4db2bf0f6d193aa9ad48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 2: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a29dd58b7cd491e9b40310f487a4a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f106f776fa4918be26327ffec31386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 3: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79a3e1a6f8b14f95bb1487ab77de4b11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3becb4407fc14adfb5a3b63d931fa6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 4: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb5b5c3b95ba4f208fac8a4d08b2697a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba9e3969f6f44cf8fa363fca98320f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Run 5: done\n",
      "\n",
      "âœ“ REPRODUCIBILITY TEST PASSED\n",
      "  All 5 runs identical\n",
      "  Max logprob difference: 0.00e+00\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ba97b9eb6148dcb51ad86ea05deb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afc4967c7b394cc9896fee705060999b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5499286049e54191b79a6a2180d79ccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016aaa6e188547ce987a18efaccb71a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d9242e9c78b4f5fb5b3768b7f4049d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81ff3a907914e7aa99ca468f19bcf90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W1126 23:46:11.064879270 ProcessGroupNCCL.cpp:1524] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TENSOR PARALLEL = 2\n",
      "================================================================================\n",
      "Loading model with TP=2...\n",
      "INFO 11-26 23:46:12 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'max_model_len': 32768, 'tensor_parallel_size': 2, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen3-14B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:46:12 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-26 23:46:12 [model.py:1745] Using max model len 32768\n",
      "INFO 11-26 23:46:12 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-26 23:46:12 [vllm.py:500] Cudagraph is disabled under eager mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=14880)\u001b[0;0m INFO 11-26 23:46:20 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14880)\u001b[0;0m WARNING 11-26 23:46:20 [multiproc_executor.py:869] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:46:28 [parallel_state.py:1208] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:57767 backend=nccl\n",
      "INFO 11-26 23:46:28 [parallel_state.py:1208] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:57767 backend=nccl\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. [Gloo] Rank Expected number of connected peer ranks is : 10\n",
      " is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-26 23:46:29 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-26 23:46:29 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-26 23:46:29 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1\n",
      "INFO 11-26 23:46:30 [parallel_state.py:1394] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-26 23:46:30 [parallel_state.py:1394] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(Worker_TP0 pid=15014)\u001b[0;0m INFO 11-26 23:46:30 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...\n",
      "\u001b[1;36m(Worker_TP1 pid=15015)\u001b[0;0m INFO 11-26 23:46:31 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP1 pid=15015)\u001b[0;0m INFO 11-26 23:46:31 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP0 pid=15014)\u001b[0;0m INFO 11-26 23:46:31 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP0 pid=15014)\u001b[0;0m INFO 11-26 23:46:31 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:01,  3.84it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:01<00:03,  1.53it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:02<00:04,  1.23it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:03<00:03,  1.13it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:04<00:02,  1.09it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:05<00:01,  1.06it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:06<00:00,  1.03it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.11it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:06<00:00,  1.15it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=15014)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=15014)\u001b[0;0m INFO 11-26 23:46:39 [default_loader.py:314] Loading weights took 7.08 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=15014)\u001b[0;0m INFO 11-26 23:46:39 [gpu_model_runner.py:3338] Model loading took 13.8818 GiB memory and 7.995043 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=15014)\u001b[0;0m INFO 11-26 23:46:46 [gpu_worker.py:359] Available KV cache memory: 31.69 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14880)\u001b[0;0m INFO 11-26 23:46:46 [kv_cache_utils.py:1229] GPU KV cache size: 415,360 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14880)\u001b[0;0m INFO 11-26 23:46:46 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 12.68x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14880)\u001b[0;0m INFO 11-26 23:46:46 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 12.68x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14880)\u001b[0;0m INFO 11-26 23:46:46 [core.py:250] init engine (profile, create kv cache, warmup model) took 7.00 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=14880)\u001b[0;0m INFO 11-26 23:46:48 [vllm.py:500] Cudagraph is disabled under eager mode\n",
      "INFO 11-26 23:46:48 [llm.py:352] Supported tasks: ['generate']\n",
      "  ref_0: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5fca30dd6e43989592be3345f182a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c40a7d5367140ca94f6373cf76180c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "  ref_1: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90b6608836d54188b38b45939a698fc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e0623761884117a21f4791cbb54bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "  ref_2: "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7968fc53c74488b8dcb402f48910b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6319377e4cc49ba8c785d59bf3115e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 positions captured\n",
      "\u001b[1;36m(Worker_TP0 pid=15014)\u001b[0;0m INFO 11-26 23:46:53 [multiproc_executor.py:702] Parent process exited, terminating worker\n",
      "\u001b[1;36m(Worker_TP1 pid=15015)\u001b[0;0m INFO 11-26 23:46:53 [multiproc_executor.py:702] Parent process exited, terminating worker\n",
      "\n",
      "================================================================================\n",
      "TENSOR PARALLEL = 4\n",
      "================================================================================\n",
      "Loading model with TP=4...\n",
      "INFO 11-26 23:46:56 [utils.py:253] non-default args: {'trust_remote_code': True, 'download_dir': '/workspace/huggingface_cache', 'dtype': 'bfloat16', 'max_model_len': 32768, 'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.6, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen3-14B'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:46:56 [model.py:631] Resolved architecture: Qwen3ForCausalLM\n",
      "INFO 11-26 23:46:56 [model.py:1745] Using max model len 32768\n",
      "INFO 11-26 23:46:56 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "INFO 11-26 23:46:56 [vllm.py:500] Cudagraph is disabled under eager mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=15183)\u001b[0;0m INFO 11-26 23:47:04 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen3-14B', speculative_config=None, tokenizer='Qwen/Qwen3-14B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir='/workspace/huggingface_cache', load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen3-14B, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15183)\u001b[0;0m WARNING 11-26 23:47:04 [multiproc_executor.py:869] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 23:47:12 [parallel_state.py:1208] world_size=4 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:53357 backend=nccl\n",
      "INFO 11-26 23:47:12 [parallel_state.py:1208] world_size=4 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:53357 backend=nccl\n",
      "INFO 11-26 23:47:12 [parallel_state.py:1208] world_size=4 rank=3 local_rank=3 distributed_init_method=tcp://127.0.0.1:53357 backend=nccl\n",
      "INFO 11-26 23:47:12 [parallel_state.py:1208] world_size=4 rank=2 local_rank=2 distributed_init_method=tcp://127.0.0.1:53357 backend=nccl\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. [Gloo] Rank Expected number of connected peer ranks is : 32\n",
      " is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 11-26 23:47:12 [pynccl.py:111] vLLM is using nccl==2.27.5\n",
      "WARNING 11-26 23:47:13 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-26 23:47:13 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-26 23:47:13 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "WARNING 11-26 23:47:13 [symm_mem.py:67] SymmMemCommunicator: Device capability 8.0 not supported, communicator is not available.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "INFO 11-26 23:47:14 [parallel_state.py:1394] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 11-26 23:47:14 [parallel_state.py:1394] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 11-26 23:47:14 [parallel_state.py:1394] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 11-26 23:47:14 [parallel_state.py:1394] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(Worker_TP0 pid=15316)\u001b[0;0m INFO 11-26 23:47:15 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen3-14B...\n",
      "\u001b[1;36m(Worker_TP0 pid=15316)\u001b[0;0m INFO 11-26 23:47:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP0 pid=15316)\u001b[0;0m INFO 11-26 23:47:15 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP1 pid=15317)\u001b[0;0m INFO 11-26 23:47:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP1 pid=15317)\u001b[0;0m INFO 11-26 23:47:15 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP3 pid=15319)\u001b[0;0m INFO 11-26 23:47:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP3 pid=15319)\u001b[0;0m INFO 11-26 23:47:15 [cuda.py:427] Using FLASH_ATTN backend.\n",
      "\u001b[1;36m(Worker_TP2 pid=15318)\u001b[0;0m INFO 11-26 23:47:15 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']\n",
      "\u001b[1;36m(Worker_TP2 pid=15318)\u001b[0;0m INFO 11-26 23:47:15 [cuda.py:427] Using FLASH_ATTN backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/8 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  12% Completed | 1/8 [00:00<00:01,  6.14it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 2/8 [00:00<00:02,  2.74it/s]\n",
      "Loading safetensors checkpoint shards:  38% Completed | 3/8 [00:01<00:02,  2.14it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 4/8 [00:01<00:02,  1.93it/s]\n",
      "Loading safetensors checkpoint shards:  62% Completed | 5/8 [00:02<00:01,  1.86it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 6/8 [00:03<00:01,  1.78it/s]\n",
      "Loading safetensors checkpoint shards:  88% Completed | 7/8 [00:03<00:00,  1.74it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:04<00:00,  1.89it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 8/8 [00:04<00:00,  1.97it/s]\n",
      "\u001b[1;36m(Worker_TP0 pid=15316)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(Worker_TP0 pid=15316)\u001b[0;0m INFO 11-26 23:47:20 [default_loader.py:314] Loading weights took 4.15 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=15316)\u001b[0;0m INFO 11-26 23:47:20 [gpu_model_runner.py:3338] Model loading took 6.9456 GiB memory and 5.243519 seconds\n",
      "\u001b[1;36m(Worker_TP0 pid=15316)\u001b[0;0m INFO 11-26 23:47:32 [gpu_worker.py:359] Available KV cache memory: 38.63 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15183)\u001b[0;0m INFO 11-26 23:47:32 [kv_cache_utils.py:1229] GPU KV cache size: 1,012,560 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15183)\u001b[0;0m INFO 11-26 23:47:32 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 30.90x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15183)\u001b[0;0m INFO 11-26 23:47:32 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 30.90x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15183)\u001b[0;0m INFO 11-26 23:47:32 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 30.90x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15183)\u001b[0;0m INFO 11-26 23:47:32 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 30.90x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=15183)\u001b[0;0m INFO 11-26 23:47:33 [core.py:250] init engine (profile, create kv cache, warmup model) took 11.43 seconds\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Tensor Parallelism (TP) detectability experiment using vLLM.\n",
    "Compares tensor_parallel=1, 2, 4 on dense model in BF16.\n",
    "Prefill-only, logprobs-only.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.distributed.parallel_state import destroy_model_parallel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import socket\n",
    "import platform\n",
    "import sys\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import PyPDF2\n",
    "import gc\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-14B\"\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "\n",
    "# Mode: False = run locally and save, True = load reference and compare cross-hardware\n",
    "CROSS_HARDWARE_MODE = False\n",
    "REFERENCE_FILE = \"/workspace/tp_reference.json\"\n",
    "\n",
    "TP_SIZES = [1, 2, 4]  # Tensor parallel sizes to compare\n",
    "\n",
    "TOKENS_PER_SLICE = 10000\n",
    "NUM_REFERENCES = 3\n",
    "TOP_K_LOGPROBS = 5\n",
    "\n",
    "EQUIVALENCE_THRESHOLD = 1e-9\n",
    "\n",
    "SYSTEM_PROMPT = \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING\n",
    "# ============================================================================\n",
    "\n",
    "LOG_FILE = None\n",
    "\n",
    "def setup_logging(output_dir='/workspace/experiments'):\n",
    "    global LOG_FILE\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_path = os.path.join(output_dir, f\"tp_experiment_{timestamp}.txt\")\n",
    "    LOG_FILE = open(log_path, 'w')\n",
    "    return log_path\n",
    "\n",
    "def log_print(*args, **kwargs):\n",
    "    print(*args, **kwargs)\n",
    "    if LOG_FILE:\n",
    "        log_kwargs = {k: v for k, v in kwargs.items() if k != 'file'}\n",
    "        print(*args, **log_kwargs, file=LOG_FILE)\n",
    "        LOG_FILE.flush()\n",
    "\n",
    "def close_logging():\n",
    "    global LOG_FILE\n",
    "    if LOG_FILE:\n",
    "        LOG_FILE.close()\n",
    "        LOG_FILE = None\n",
    "\n",
    "# ============================================================================\n",
    "# PDF LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_pdf_text(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        reader = PyPDF2.PdfReader(f)\n",
    "        for page in reader.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += page_text + \" \"\n",
    "    return text.strip()\n",
    "\n",
    "def create_prompts(tokenizer, num_references=NUM_REFERENCES):\n",
    "    pdf_files = sorted(glob.glob(\"/workspace/*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        pdf_files = sorted(glob.glob(\"*.pdf\"))\n",
    "    if not pdf_files:\n",
    "        raise FileNotFoundError(\"No PDF files found\")\n",
    "\n",
    "    log_print(f\"Found {len(pdf_files)} PDF(s)\")\n",
    "    for pdf_path in pdf_files:\n",
    "        log_print(f\"  Loading: {pdf_path}\")\n",
    "\n",
    "    all_text = \"\"\n",
    "    for pdf_path in pdf_files:\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        all_text += text + \" \"\n",
    "\n",
    "    content_tokens = tokenizer.encode(all_text)\n",
    "    log_print(f\"Total source tokens: {len(content_tokens)}\")\n",
    "\n",
    "    if len(content_tokens) < num_references * TOKENS_PER_SLICE:\n",
    "        raise ValueError(f\"Need {num_references * TOKENS_PER_SLICE} tokens but only have {len(content_tokens)}\")\n",
    "\n",
    "    prefix = f\"\"\"<|im_start|>system\n",
    "{SYSTEM_PROMPT}<|im_end|>\n",
    "<|im_start|>user\n",
    "Here is an excerpt from a document:\n",
    "\n",
    "\\\"\"\"\"\n",
    "    \n",
    "    suffix = f\"\"\"\"\n",
    "\n",
    "Based on this excerpt, what type of document do you think this is from, and what is its likely subject matter? Explain your reasoning.<|im_end|>\n",
    "<|im_start|>assistant\n",
    "\"\"\"\n",
    "    \n",
    "    prefix_tokens = tokenizer.encode(prefix)\n",
    "    suffix_tokens = tokenizer.encode(suffix)\n",
    "    \n",
    "    total_len = len(prefix_tokens) + TOKENS_PER_SLICE + len(suffix_tokens)\n",
    "    log_print(f\"Prompt structure: {len(prefix_tokens)} prefix + {TOKENS_PER_SLICE} snippet + {len(suffix_tokens)} suffix = {total_len} tokens\")\n",
    "\n",
    "    prompts = []\n",
    "    prompt_texts = []\n",
    "    for i in range(num_references):\n",
    "        start = i * TOKENS_PER_SLICE\n",
    "        end = start + TOKENS_PER_SLICE\n",
    "        snippet_tokens = content_tokens[start:end]\n",
    "        prompt_tokens = prefix_tokens + snippet_tokens + suffix_tokens\n",
    "        prompts.append(prompt_tokens)\n",
    "        prompt_texts.append(tokenizer.decode(prompt_tokens))\n",
    "    \n",
    "    return prompts, prompt_texts\n",
    "\n",
    "# ============================================================================\n",
    "# SYSTEM INFO\n",
    "# ============================================================================\n",
    "\n",
    "def collect_system_info(tp_size=None):\n",
    "    import transformers\n",
    "    \n",
    "    info = {\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"platform\": platform.platform(),\n",
    "        \"python_version\": sys.version.split()[0],\n",
    "        \"torch_version\": torch.__version__,\n",
    "        \"cuda_version\": torch.version.cuda if torch.cuda.is_available() else \"N/A\",\n",
    "        \"cudnn_version\": str(torch.backends.cudnn.version()) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"transformers_version\": transformers.__version__,\n",
    "        \"numpy_version\": np.__version__,\n",
    "        \"gpu_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"N/A\",\n",
    "        \"gpu_count\": torch.cuda.device_count() if torch.cuda.is_available() else 0,\n",
    "        \"tensor_parallel_size\": tp_size,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        import vllm\n",
    "        info[\"vllm_version\"] = vllm.__version__\n",
    "    except (ImportError, AttributeError):\n",
    "        info[\"vllm_version\"] = \"unknown\"\n",
    "    \n",
    "    return info\n",
    "\n",
    "# ============================================================================\n",
    "# PREFILL LOGPROB EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_prefill_logprobs(output, positions=[-3, -2, -1]):\n",
    "    \"\"\"Extract prompt logprobs at specified positions.\"\"\"\n",
    "    signals = {}\n",
    "    \n",
    "    prompt_logprobs = output.prompt_logprobs\n",
    "    if prompt_logprobs is None:\n",
    "        return signals\n",
    "    \n",
    "    num_positions = len(prompt_logprobs)\n",
    "    \n",
    "    for pos in positions:\n",
    "        actual_idx = pos if pos >= 0 else num_positions + pos\n",
    "        \n",
    "        if actual_idx < 0 or actual_idx >= num_positions:\n",
    "            continue\n",
    "        \n",
    "        pos_label = f\"pos_{pos}\"\n",
    "        token_logprobs = prompt_logprobs[actual_idx]\n",
    "        \n",
    "        if token_logprobs is None:\n",
    "            continue\n",
    "        \n",
    "        token_ids = []\n",
    "        log_probs = []\n",
    "        \n",
    "        for token_id, logprob_obj in token_logprobs.items():\n",
    "            token_ids.append(token_id)\n",
    "            if hasattr(logprob_obj, 'logprob'):\n",
    "                log_probs.append(logprob_obj.logprob)\n",
    "            else:\n",
    "                log_probs.append(float(logprob_obj))\n",
    "        \n",
    "        signals[pos_label] = {\n",
    "            'logprobs': {\n",
    "                'token_ids': token_ids,\n",
    "                'log_probs': log_probs\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return signals\n",
    "\n",
    "def run_prefill(llm, prompt_text):\n",
    "    \"\"\"Run prefill and extract logprobs.\"\"\"\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=1,\n",
    "        prompt_logprobs=TOP_K_LOGPROBS,\n",
    "        logprobs=TOP_K_LOGPROBS,\n",
    "    )\n",
    "    \n",
    "    outputs = llm.generate([prompt_text], sampling_params)\n",
    "    output = outputs[0]\n",
    "    \n",
    "    prefill_signals = extract_prefill_logprobs(output, positions=[-3, -2, -1])\n",
    "    \n",
    "    return {\n",
    "        'prefill_signals': prefill_signals\n",
    "    }\n",
    "\n",
    "def test_reproducibility(llm, prompt_text, num_runs=5):\n",
    "    \"\"\"\n",
    "    Test that the model produces deterministic outputs.\n",
    "    Run the same prompt multiple times and compare logprobs.\n",
    "    \"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"REPRODUCIBILITY TEST\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"Running same prompt {num_runs} times...\")\n",
    "    \n",
    "    results = []\n",
    "    for i in range(num_runs):\n",
    "        result = run_prefill(llm, prompt_text)\n",
    "        results.append(result)\n",
    "        log_print(f\"  Run {i+1}: done\")\n",
    "        # Clear any accumulated cache\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Compare all runs to the first\n",
    "    all_identical = True\n",
    "    max_diffs_seen = []\n",
    "    \n",
    "    for i in range(1, num_runs):\n",
    "        for pos in results[0]['prefill_signals'].keys():\n",
    "            lp0 = results[0]['prefill_signals'][pos]['logprobs']\n",
    "            lpi = results[i]['prefill_signals'][pos]['logprobs']\n",
    "            \n",
    "            # Compare token IDs\n",
    "            if lp0['token_ids'] != lpi['token_ids']:\n",
    "                log_print(f\"  âœ— Run {i+1} {pos}: token_ids differ\")\n",
    "                all_identical = False\n",
    "                continue\n",
    "            \n",
    "            # Compare log_probs values\n",
    "            diffs = [abs(a - b) for a, b in zip(lp0['log_probs'], lpi['log_probs'])]\n",
    "            max_diff = max(diffs) if diffs else 0\n",
    "            max_diffs_seen.append(max_diff)\n",
    "            \n",
    "            if max_diff > 1e-9:\n",
    "                log_print(f\"  âœ— Run {i+1} {pos}: max logprob diff = {max_diff:.2e}\")\n",
    "                all_identical = False\n",
    "    \n",
    "    if all_identical:\n",
    "        overall_max = max(max_diffs_seen) if max_diffs_seen else 0\n",
    "        log_print(f\"\\nâœ“ REPRODUCIBILITY TEST PASSED\")\n",
    "        log_print(f\"  All {num_runs} runs identical\")\n",
    "        log_print(f\"  Max logprob difference: {overall_max:.2e}\")\n",
    "    else:\n",
    "        log_print(f\"\\nâš  REPRODUCIBILITY TEST FAILED\")\n",
    "        log_print(f\"  Non-deterministic outputs detected\")\n",
    "        log_print(f\"  Results may vary between runs even with same TP setting\")\n",
    "    \n",
    "    return all_identical\n",
    "\n",
    "# ============================================================================\n",
    "# DISTANCE METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_logprob_distance_canonical(logprobs1, logprobs2, canonical_ids):\n",
    "    \"\"\"\n",
    "    Compute L2 distance between logprobs for a canonical set of token IDs.\n",
    "    \"\"\"\n",
    "    lp1 = dict(zip(logprobs1['token_ids'], logprobs1['log_probs']))\n",
    "    lp2 = dict(zip(logprobs2['token_ids'], logprobs2['log_probs']))\n",
    "\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "\n",
    "    for tid in canonical_ids:\n",
    "        if tid in lp1 and tid in lp2:\n",
    "            vec1.append(lp1[tid])\n",
    "            vec2.append(lp2[tid])\n",
    "\n",
    "    if len(vec1) == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    return float(np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "\n",
    "def compare_signals_with_canonical_ids(signals1, signals2, canonical_token_ids):\n",
    "    \"\"\"Compare using pre-specified canonical token IDs (top 5).\"\"\"\n",
    "    common_positions = set(signals1.keys()) & set(signals2.keys())\n",
    "\n",
    "    all_dists = []\n",
    "\n",
    "    for pos_label in common_positions:\n",
    "        sig1 = signals1[pos_label]\n",
    "        sig2 = signals2[pos_label]\n",
    "\n",
    "        # Use top 5 for comparison\n",
    "        canonical_ids = canonical_token_ids.get(pos_label, sig1['logprobs']['token_ids'][:5])[:5]\n",
    "        dist = compute_logprob_distance_canonical(\n",
    "            sig1['logprobs'], sig2['logprobs'], canonical_ids\n",
    "        )\n",
    "        all_dists.append(dist)\n",
    "\n",
    "    return {\n",
    "        'logprobs_mean': np.mean(all_dists) if all_dists else 0.0\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_results(measurements, tp_sizes):\n",
    "    \"\"\"Analyze prefill logprob differences across TP sizes (within-hardware).\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"WITHIN-HARDWARE TP COMPARISON (PREFILL)\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    n = len(tp_sizes)\n",
    "    all_matrices = []\n",
    "    \n",
    "    # Use TP=1 (first) as canonical for token IDs\n",
    "    canonical_tp = tp_sizes[0]\n",
    "    \n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        # Get canonical token IDs from first TP size\n",
    "        canonical_signals = measurements[canonical_tp][ref_idx]['prefill_signals']\n",
    "        canonical_token_ids = {}\n",
    "        for pos_label, pos_data in canonical_signals.items():\n",
    "            canonical_token_ids[pos_label] = pos_data['logprobs']['token_ids']\n",
    "        \n",
    "        for i, tp_i in enumerate(tp_sizes):\n",
    "            for j, tp_j in enumerate(tp_sizes):\n",
    "                if i == j:\n",
    "                    matrix[i, j] = 0.0\n",
    "                else:\n",
    "                    sig_i = measurements[tp_i][ref_idx]['prefill_signals']\n",
    "                    sig_j = measurements[tp_j][ref_idx]['prefill_signals']\n",
    "                    \n",
    "                    distances = compare_signals_with_canonical_ids(sig_i, sig_j, canonical_token_ids)\n",
    "                    matrix[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        header = \"       \" + \" \".join(f\"TP={tp:>3}\" for tp in tp_sizes)\n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, tp in enumerate(tp_sizes):\n",
    "            row = f\"TP={tp:<3}\" + \" \".join(f\"  {matrix[i,j]:6.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "    \n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"       \" + \" \".join(f\"TP={tp:>3}\" for tp in tp_sizes)\n",
    "    log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, tp in enumerate(tp_sizes):\n",
    "        row = f\"TP={tp:<3}\" + \" \".join(f\"  {avg_matrix[i,j]:6.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "    \n",
    "    # Off-diagonal stats\n",
    "    off_diag = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag.append(avg_matrix[i, j])\n",
    "    \n",
    "    diagonal = np.mean([avg_matrix[i, i] for i in range(n)])\n",
    "    off_diagonal = np.mean(off_diag)\n",
    "    \n",
    "    log_print(f\"\\nDiagonal (same TP): {diagonal:.2e}\")\n",
    "    log_print(f\"Off-diagonal (different TP): {off_diagonal:.2e}\")\n",
    "    \n",
    "    if diagonal > 0:\n",
    "        snr = off_diagonal / diagonal\n",
    "        log_print(f\"SNR: {snr:.2f}Ã—\")\n",
    "    else:\n",
    "        snr = float('inf') if off_diagonal > 0 else 1.0\n",
    "        log_print(f\"SNR: {snr}\")\n",
    "    \n",
    "    # Check equivalences\n",
    "    equiv_pairs = []\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            if avg_matrix[i, j] < EQUIVALENCE_THRESHOLD:\n",
    "                equiv_pairs.append((tp_sizes[i], tp_sizes[j]))\n",
    "    \n",
    "    if equiv_pairs:\n",
    "        log_print(f\"\\nEquivalent pairs:\")\n",
    "        for p in equiv_pairs:\n",
    "            log_print(f\"  TP={p[0]} â‰ˆ TP={p[1]}\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'diagonal': float(diagonal),\n",
    "        'off_diagonal': float(off_diagonal),\n",
    "        'snr': float(snr) if snr != float('inf') else None,\n",
    "        'equivalent_pairs': equiv_pairs\n",
    "    }\n",
    "\n",
    "\n",
    "def analyze_cross_hardware(ref_measurements, ver_measurements, tp_sizes):\n",
    "    \"\"\"Analyze cross-hardware TP comparison.\"\"\"\n",
    "    log_print(\"\\n\" + \"=\"*80)\n",
    "    log_print(\"CROSS-HARDWARE TP COMPARISON (PREFILL)\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(\"  Rows = reference TP, Cols = verifier TP\")\n",
    "    \n",
    "    n = len(tp_sizes)\n",
    "    all_matrices = []\n",
    "    \n",
    "    # Use reference TP=1 as canonical for token IDs\n",
    "    canonical_tp = str(tp_sizes[0])\n",
    "    \n",
    "    for ref_idx in range(NUM_REFERENCES):\n",
    "        log_print(f\"\\n--- ref_{ref_idx} ---\")\n",
    "        \n",
    "        matrix = np.zeros((n, n))\n",
    "        \n",
    "        # Get canonical token IDs from reference TP=1\n",
    "        canonical_signals = ref_measurements[canonical_tp][ref_idx]['prefill_signals']\n",
    "        canonical_token_ids = {}\n",
    "        for pos_label, pos_data in canonical_signals.items():\n",
    "            canonical_token_ids[pos_label] = pos_data['logprobs']['token_ids']\n",
    "        \n",
    "        for i, tp_ref in enumerate(tp_sizes):\n",
    "            for j, tp_ver in enumerate(tp_sizes):\n",
    "                sig_ref = ref_measurements[str(tp_ref)][ref_idx]['prefill_signals']\n",
    "                sig_ver = ver_measurements[tp_ver][ref_idx]['prefill_signals']\n",
    "                \n",
    "                distances = compare_signals_with_canonical_ids(sig_ref, sig_ver, canonical_token_ids)\n",
    "                matrix[i, j] = distances['logprobs_mean']\n",
    "        \n",
    "        header = \"       \" + \" \".join(f\"v_TP={tp:>2}\" for tp in tp_sizes)\n",
    "        log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "        log_print(header)\n",
    "        for i, tp in enumerate(tp_sizes):\n",
    "            row = f\"r_TP={tp:<2}\" + \" \".join(f\"  {matrix[i,j]:6.2e}\" for j in range(n))\n",
    "            log_print(row)\n",
    "        \n",
    "        all_matrices.append(matrix)\n",
    "    \n",
    "    # Aggregate\n",
    "    avg_matrix = np.mean(all_matrices, axis=0)\n",
    "    \n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"AGGREGATE (average across references):\")\n",
    "    log_print(\"=\"*80)\n",
    "    \n",
    "    header = \"       \" + \" \".join(f\"v_TP={tp:>2}\" for tp in tp_sizes)\n",
    "    log_print(f\"\\nLogprobs (L2 distance):\")\n",
    "    log_print(header)\n",
    "    for i, tp in enumerate(tp_sizes):\n",
    "        row = f\"r_TP={tp:<2}\" + \" \".join(f\"  {avg_matrix[i,j]:6.2e}\" for j in range(n))\n",
    "        log_print(row)\n",
    "    \n",
    "    # SNR calculation\n",
    "    diagonal = np.mean([avg_matrix[i, i] for i in range(n)])\n",
    "    off_diag = []\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                off_diag.append(avg_matrix[i, j])\n",
    "    off_diagonal = np.mean(off_diag)\n",
    "    \n",
    "    log_print(f\"\\n{'='*80}\")\n",
    "    log_print(\"SNR ANALYSIS\")\n",
    "    log_print(\"=\"*80)\n",
    "    log_print(f\"\\nDiagonal (baseline = cross-hardware, same TP):\")\n",
    "    log_print(f\"  Logprobs: {diagonal:.2e}\")\n",
    "    log_print(f\"\\nOff-diagonal (different TP):\")\n",
    "    log_print(f\"  Logprobs - Mean: {off_diagonal:.2e}\", end=\"\")\n",
    "    \n",
    "    if diagonal > 0:\n",
    "        snr = off_diagonal / diagonal\n",
    "        log_print(f\", SNR: {snr:.2f}Ã—\")\n",
    "    else:\n",
    "        snr = float('inf') if off_diagonal > 0 else 1.0\n",
    "        log_print(f\", SNR: {snr}\")\n",
    "    \n",
    "    return {\n",
    "        'matrix': avg_matrix.tolist(),\n",
    "        'per_reference_matrices': [m.tolist() for m in all_matrices],\n",
    "        'diagonal': float(diagonal),\n",
    "        'off_diagonal': float(off_diagonal),\n",
    "        'snr': float(snr) if snr != float('inf') else None\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    log_path = setup_logging()\n",
    "    \n",
    "    log_print(\"=\" * 80)\n",
    "    log_print(\"TENSOR PARALLELISM DETECTABILITY EXPERIMENT\")\n",
    "    log_print(\"=\" * 80)\n",
    "    \n",
    "    system_info = collect_system_info()\n",
    "    log_print(f\"\\nHostname: {system_info['hostname']}\")\n",
    "    log_print(f\"GPUs: {system_info['gpu_count']}x {system_info['gpu_name']}\")\n",
    "    log_print(f\"vLLM: {system_info['vllm_version']}\")\n",
    "    log_print(f\"Model: {MODEL_NAME}\")\n",
    "    log_print(f\"TP sizes to test: {TP_SIZES}\")\n",
    "    log_print(f\"Mode: {'CROSS-HARDWARE' if CROSS_HARDWARE_MODE else 'LOCAL'}\")\n",
    "    \n",
    "    # Load reference file first if in cross-hardware mode (to get prompts)\n",
    "    reference = None\n",
    "    if CROSS_HARDWARE_MODE:\n",
    "        log_print(f\"\\nLoading reference: {REFERENCE_FILE}\")\n",
    "        with open(REFERENCE_FILE, 'r') as f:\n",
    "            content = f.read()\n",
    "            content = content.replace('Infinity', '1e309')\n",
    "            reference = json.loads(content)\n",
    "        prompt_texts = reference['prompt_texts']\n",
    "        log_print(f\"Loaded {len(prompt_texts)} prompts from reference\")\n",
    "    else:\n",
    "        # Use transformers tokenizer (much faster than loading vLLM)\n",
    "        log_print(\"\\nLoading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "        \n",
    "        prompts, prompt_texts = create_prompts(tokenizer, NUM_REFERENCES)\n",
    "        log_print(f\"Created {len(prompts)} prompts\")\n",
    "    \n",
    "    # Run for each TP size\n",
    "    measurements = {}\n",
    "    environments = {}\n",
    "    reproducibility_passed = None\n",
    "    \n",
    "    for tp_size in TP_SIZES:\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(f\"TENSOR PARALLEL = {tp_size}\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        log_print(f\"Loading model with TP={tp_size}...\")\n",
    "        llm = LLM(\n",
    "            model=MODEL_NAME,\n",
    "            download_dir=CACHE_DIR,\n",
    "            dtype=\"bfloat16\",\n",
    "            trust_remote_code=True,\n",
    "            tensor_parallel_size=tp_size,\n",
    "            gpu_memory_utilization=0.6,\n",
    "            max_model_len=32768,\n",
    "            enforce_eager=True,\n",
    "        )\n",
    "        \n",
    "        # Run reproducibility test on first TP size\n",
    "        if reproducibility_passed is None:\n",
    "            reproducibility_passed = test_reproducibility(llm, prompt_texts[0], num_runs=5)\n",
    "        \n",
    "        environments[tp_size] = collect_system_info(tp_size)\n",
    "        measurements[tp_size] = []\n",
    "        \n",
    "        for ref_idx, prompt_text in enumerate(prompt_texts):\n",
    "            log_print(f\"  ref_{ref_idx}: \", end=\"\")\n",
    "            \n",
    "            result = run_prefill(llm, prompt_text)\n",
    "            measurements[tp_size].append(result)\n",
    "            \n",
    "            num_positions = len(result['prefill_signals'])\n",
    "            log_print(f\"{num_positions} positions captured\")\n",
    "        \n",
    "        # Cleanup\n",
    "        del llm\n",
    "        destroy_model_parallel()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Within-hardware analysis\n",
    "    within_analysis = analyze_results(measurements, TP_SIZES)\n",
    "    \n",
    "    output_dir = '/workspace/experiments'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if not CROSS_HARDWARE_MODE:\n",
    "        # Save results for potential cross-hardware comparison later\n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'model': MODEL_NAME,\n",
    "                'dtype': 'bfloat16',\n",
    "                'tp_sizes': TP_SIZES,\n",
    "                'tokens_per_slice': TOKENS_PER_SLICE,\n",
    "                'num_references': NUM_REFERENCES,\n",
    "                'top_k_logprobs': TOP_K_LOGPROBS,\n",
    "                'reproducibility_test_passed': reproducibility_passed,\n",
    "                'environments': {str(k): v for k, v in environments.items()},\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'prompt_texts': prompt_texts,\n",
    "            'measurements': {str(k): v for k, v in measurements.items()},\n",
    "            'within_hardware_analysis': within_analysis\n",
    "        }\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"tp_experiment_{timestamp}.json\")\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        log_print(f\"\\nSaved to: {filepath}\")\n",
    "        log_print(f\"\\nFor cross-hardware comparison:\")\n",
    "        log_print(f\"  1. Copy {filepath} to second machine\")\n",
    "        log_print(f\"  2. Set CROSS_HARDWARE_MODE = True\")\n",
    "        log_print(f\"  3. Set REFERENCE_FILE = '<path to json>'\")\n",
    "        \n",
    "    else:\n",
    "        # Cross-hardware comparison (reference already loaded at start)\n",
    "        log_print(f\"\\n{'='*80}\")\n",
    "        log_print(\"CROSS-HARDWARE COMPARISON\")\n",
    "        log_print(\"=\"*80)\n",
    "        \n",
    "        # Environment comparison\n",
    "        ref_env = reference['metadata']['environments']['1']\n",
    "        ver_env = environments[1]\n",
    "        \n",
    "        log_print(\"\\nEnvironment comparison:\")\n",
    "        log_print(f\"  Reference GPU: {ref_env['gpu_name']}\")\n",
    "        log_print(f\"  Verifier GPU:  {ver_env['gpu_name']}\")\n",
    "        \n",
    "        check_fields = ['vllm_version', 'torch_version', 'cuda_version']\n",
    "        for field in check_fields:\n",
    "            ref_val = ref_env.get(field, 'N/A')\n",
    "            ver_val = ver_env.get(field, 'N/A')\n",
    "            match = \"âœ“\" if ref_val == ver_val else \"âœ—\"\n",
    "            log_print(f\"  {field}: {ref_val} vs {ver_val} {match}\")\n",
    "        \n",
    "        ref_measurements = reference['measurements']\n",
    "        \n",
    "        cross_analysis = analyze_cross_hardware(ref_measurements, measurements, TP_SIZES)\n",
    "        \n",
    "        results = {\n",
    "            'metadata': {\n",
    "                'model': MODEL_NAME,\n",
    "                'dtype': 'bfloat16',\n",
    "                'tp_sizes': TP_SIZES,\n",
    "                'reference_file': REFERENCE_FILE,\n",
    "                'reproducibility_test_passed': reproducibility_passed,\n",
    "                'reference_environments': reference['metadata']['environments'],\n",
    "                'verifier_environments': {str(k): v for k, v in environments.items()},\n",
    "                'timestamp': timestamp\n",
    "            },\n",
    "            'within_hardware_analysis': within_analysis,\n",
    "            'cross_hardware_analysis': cross_analysis\n",
    "        }\n",
    "        \n",
    "        filepath = os.path.join(output_dir, f\"tp_cross_hardware_{timestamp}.json\")\n",
    "        \n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        \n",
    "        log_print(f\"\\nSaved to: {filepath}\")\n",
    "    \n",
    "    close_logging()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b437a62-ea8f-45ab-abee-f5af78ecdad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
