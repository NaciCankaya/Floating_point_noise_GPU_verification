{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7de974a-1231-445b-b980-05b4ccd6896d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BATCH COMPOSITION TEST\n",
      "============================================================\n",
      "System Info:\n",
      "  Hostname: 717db41b5256\n",
      "  Container: 717db41b5256\n",
      "  GPU: NVIDIA A40\n",
      "  PyTorch: 2.8.0+cu128\n",
      "  CUDA: 12.8\n",
      "\n",
      "Environment Variables:\n",
      "  CUDA_MODULE_LOADING=LAZY\n",
      "  CUDA_VERSION=12.8.1\n",
      "  NCCL_VERSION=2.25.1-1\n",
      "  NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
      "  NV_CUDA_CUDART_DEV_VERSION=12.8.90-1\n",
      "  NV_CUDA_CUDART_VERSION=12.8.90-1\n",
      "  NV_CUDA_LIB_VERSION=12.8.1-1\n",
      "  NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-8=12.8.1-1\n",
      "  NV_CUDA_NSIGHT_COMPUTE_VERSION=12.8.1-1\n",
      "  NV_CUDNN_PACKAGE=libcudnn9-cuda-12=9.8.0.87-1\n",
      "  NV_CUDNN_PACKAGE_DEV=libcudnn9-dev-cuda-12=9.8.0.87-1\n",
      "  NV_CUDNN_PACKAGE_NAME=libcudnn9-cuda-12\n",
      "  NV_CUDNN_VERSION=9.8.0.87-1\n",
      "  NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.25.1-1+cuda12.8\n",
      "  NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "  NV_LIBNCCL_DEV_PACKAGE_VERSION=2.25.1-1\n",
      "  NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
      "  NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "  NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
      "\n",
      "Loading Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83faa366f08243fbb7f3fc1742c489d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Layers: 28, Heads: 28, Key dim: 3584\n",
      "\n",
      "Tokenizing and truncating sequences to exactly 100 tokens:\n",
      "  Seq 0: 100 tokens - 'The automated data-processing pipeline ingests raw telemetry...'\n",
      "  Seq 1: 100 tokens - 'Climate modeling techniques have advanced significantly thro...'\n",
      "  Seq 2: 100 tokens - 'Quantum entanglement represents one of the most counterintui...'\n",
      "  Seq 3: 100 tokens - 'The human immune system comprises an intricate network of ce...'\n",
      "  Seq 4: 100 tokens - 'Renaissance architecture flourished throughout Europe during...'\n",
      "  Seq 5: 100 tokens - 'Deep ocean hydrothermal vents create unique ecosystems that ...'\n",
      "  Seq 6: 100 tokens - 'Blockchain technology revolutionizes data integrity through ...'\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: Batch Composition Effect Test\n",
      "============================================================\n",
      "Batch size: 4 (constant)\n",
      "Sequence length: 100 tokens (all sequences)\n",
      "Sequence 0: IDENTICAL across both compositions\n",
      "Positions 1,2,3: DIFFERENT content per composition\n",
      "Repetitions per composition: 5\n",
      "\n",
      "Composition 1:\n",
      "  Position 0: Sequence 0 (always the same)\n",
      "  Position 1: Sequence 1 (climate modeling)\n",
      "  Position 2: Sequence 2 (quantum entanglement)\n",
      "  Position 3: Sequence 3 (immune system)\n",
      "  Rep 0: norm=924.000000, first_val=-0.248047\n",
      "  ✓ All 5 repetitions identical within composition\n",
      "\n",
      "Composition 2:\n",
      "  Position 0: Sequence 0 (always the same)\n",
      "  Position 1: Sequence 4 (Renaissance architecture)\n",
      "  Position 2: Sequence 5 (ocean hydrothermal vents)\n",
      "  Position 3: Sequence 6 (blockchain technology)\n",
      "  Rep 0: norm=924.000000, first_val=-0.248047\n",
      "  ✓ All 5 repetitions identical within composition\n",
      "\n",
      "============================================================\n",
      "CRITICAL ANALYSIS: Does Batch Composition Matter?\n",
      "============================================================\n",
      "Key vector dimension: 512\n",
      "Comp 1 mean norm: 924.000000\n",
      "Comp 2 mean norm: 924.000000\n",
      "\n",
      "L2 distance (Comp 1 vs Comp 2): 0.000000\n",
      "Relative difference: 0.00000000\n",
      "\n",
      "Element-wise differences:\n",
      "  Max |diff|: 0.00000000\n",
      "  Mean |diff|: 0.00000000\n",
      "  Elements with |diff| > 1e-6: 0/512\n",
      "\n",
      "============================================================\n",
      "VERDICT\n",
      "============================================================\n",
      "Test configuration:\n",
      "  - Sequence 0: IDENTICAL in both compositions\n",
      "  - Batch size: 4 (constant)\n",
      "  - Sequence length: 100 tokens (all equal)\n",
      "  - Batch content (pos 1,2,3): DIFFERENT between compositions\n",
      "\n",
      "✓ PERFECT MATCH: Both compositions produce IDENTICAL key vectors\n",
      "  → Batch composition does NOT matter\n",
      "  → Only batch SIZE affects outputs\n",
      "  → Verification needs only: batch size logs\n",
      "  → ✓✓✓ BUBBLE-BASED VERIFICATION IS FEASIBLE\n",
      "\n",
      "✓ Results saved to /mnt/user-data/outputs/batch_composition_test_20251103_192547.json\n",
      "============================================================\n",
      "EXPERIMENT COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Batch Composition Test\n",
    "Critical Question: Does batch CONTENT affect sequence 0's key vectors?\n",
    "- Fixed batch size (4)\n",
    "- Fixed sequence 0 \n",
    "- All sequences truncated to exactly 100 tokens\n",
    "- Different sequences in positions 1,2,3 across runs\n",
    "- Compare sequence 0's key vectors\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BATCH COMPOSITION TEST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"System Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  Container: {CONTAINER_ID}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA: {torch.version.cuda}\")\n",
    "print()\n",
    "\n",
    "# Capture relevant environment variables\n",
    "print(\"Environment Variables:\")\n",
    "env_vars = {}\n",
    "for key in sorted(os.environ.keys()):\n",
    "    if any(x in key.upper() for x in ['CUDA', 'TORCH', 'NCCL', 'CUDNN', 'PYTORCH']):\n",
    "        env_vars[key] = os.environ[key]\n",
    "        print(f\"  {key}={os.environ[key]}\")\n",
    "if not env_vars:\n",
    "    print(\"  (No CUDA/TORCH env vars set)\")\n",
    "print()\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Architecture info\n",
    "num_layers = len(model.model.layers)\n",
    "num_heads = model.config.num_attention_heads\n",
    "head_dim = model.config.hidden_size // num_heads\n",
    "key_vector_dim = num_heads * head_dim\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Layers: {num_layers}, Heads: {num_heads}, Key dim: {key_vector_dim}\\n\")\n",
    "\n",
    "# Seven unique sequences, all >100 tokens before truncation\n",
    "raw_sequences = [\n",
    "    # Sequence 0 - the one we always measure\n",
    "    \"\"\"The automated data-processing pipeline ingests raw telemetry from distributed sensors \n",
    "    across multiple geographic locations. A proprietary algorithm then normalizes the dataset, \n",
    "    filtering for anomalies based on predefined statistical parameters derived from historical \n",
    "    patterns. The resulting output is a clean, structured matrix ready for machine learning model \n",
    "    ingestion and downstream analytical workflows. System efficiency is monitored in real-time \n",
    "    through a comprehensive dashboard, with automated alerts triggered if latency exceeds the \n",
    "    established threshold or if data quality metrics fall below acceptable ranges. Advanced \n",
    "    compression techniques optimize storage utilization across the distributed infrastructure.\n",
    "    Performance metrics are tracked continuously to ensure optimal throughput and minimal latency.\"\"\",\n",
    "    \n",
    "    # Sequences 1-6 - dummy sequences with different content\n",
    "    \"\"\"Climate modeling techniques have advanced significantly through the integration of \n",
    "    high-resolution satellite imagery and ground-based observation networks. Researchers combine \n",
    "    atmospheric physics equations with empirical data to simulate complex weather patterns and \n",
    "    predict long-term climate trends. These models incorporate ocean currents, ice sheet dynamics, \n",
    "    and greenhouse gas concentrations to provide increasingly accurate projections for policymakers \n",
    "    and environmental scientists worldwide. Modern computational infrastructure enables simulations \n",
    "    at unprecedented scales and temporal resolution with remarkable accuracy and scientific validity.\n",
    "    International collaboration facilitates data sharing and model validation across research institutions.\"\"\",\n",
    "    \n",
    "    \"\"\"Quantum entanglement represents one of the most counterintuitive phenomena in modern physics, \n",
    "    where particles become correlated in ways that defy classical explanations. When two particles \n",
    "    are entangled, measuring the state of one instantaneously affects the other regardless of the \n",
    "    distance separating them. This property has profound implications for quantum computing and \n",
    "    cryptography, enabling novel approaches to information processing and secure communication that \n",
    "    are fundamentally impossible with classical systems and traditional computational paradigms. Research \n",
    "    continues to explore practical applications of these quantum mechanical principles. Experimental \n",
    "    verification requires sophisticated detection equipment and precisely controlled laboratory conditions.\"\"\",\n",
    "    \n",
    "    \"\"\"The human immune system comprises an intricate network of cells, tissues, and organs that \n",
    "    work collaboratively to defend against pathogens and foreign substances. White blood cells \n",
    "    patrol the bloodstream and tissues, identifying and neutralizing threats through both innate \n",
    "    and adaptive immune responses. B cells produce antibodies that target specific antigens, while \n",
    "    T cells orchestrate cellular immunity and eliminate infected cells. This sophisticated biological \n",
    "    defense mechanism evolved over millions of years to protect organisms. Memory cells enable rapid \n",
    "    responses to previously encountered pathogens through accelerated antibody production. The lymphatic \n",
    "    system transports immune cells throughout the body to maintain comprehensive surveillance.\"\"\",\n",
    "    \n",
    "    \"\"\"Renaissance architecture flourished throughout Europe during the 14th to 17th centuries, \n",
    "    characterized by symmetry, proportion, and the revival of classical Greco-Roman design principles. \n",
    "    Architects like Brunelleschi and Palladio pioneered innovative structural techniques including \n",
    "    the use of mathematical ratios to create harmonious spatial relationships. Major cathedrals and \n",
    "    palaces from this era demonstrate remarkable engineering achievements, featuring elaborate domes, \n",
    "    precise geometric layouts, and ornamental details that reflected humanist ideals and aesthetic \n",
    "    philosophies of the period. These buildings continue to inspire contemporary architectural design.\n",
    "    Patron families commissioned grand structures to demonstrate wealth and cultural sophistication.\"\"\",\n",
    "    \n",
    "    \"\"\"Deep ocean hydrothermal vents create unique ecosystems that thrive in extreme conditions \n",
    "    without sunlight, relying instead on chemosynthetic bacteria that derive energy from volcanic \n",
    "    minerals. These remarkable habitats support diverse communities of specialized organisms including \n",
    "    tube worms, blind shrimp, and unusual fish species adapted to high pressure and temperature \n",
    "    gradients. Scientists study these environments to understand the origins of life on Earth and \n",
    "    explore possibilities for extraterrestrial biology in similarly extreme conditions on other \n",
    "    planetary bodies. Research expeditions use sophisticated submersibles to document biodiversity.\n",
    "    Chemical analysis reveals novel metabolic pathways that challenge conventional biological theories.\"\"\",\n",
    "    \n",
    "    \"\"\"Blockchain technology revolutionizes data integrity through distributed ledger systems that \n",
    "    maintain cryptographically secure transaction records across decentralized networks. Each block \n",
    "    contains a cryptographic hash of the previous block, creating an immutable chain resistant to \n",
    "    tampering and fraud. Consensus mechanisms like proof-of-work or proof-of-stake ensure network \n",
    "    participants agree on the validity of transactions without requiring a central authority. These \n",
    "    properties enable trustless systems for financial transactions, supply chain tracking, and digital \n",
    "    asset management in various industries. Smart contracts automate business logic on blockchain.\n",
    "    Decentralized applications leverage these capabilities to create transparent and auditable systems.\"\"\",\n",
    "]\n",
    "\n",
    "# Tokenize and truncate to exactly 100 tokens\n",
    "TARGET_LENGTH = 100\n",
    "token_ids_list = []\n",
    "\n",
    "print(\"Tokenizing and truncating sequences to exactly 100 tokens:\")\n",
    "for i, text in enumerate(raw_sequences):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(token_ids) < TARGET_LENGTH:\n",
    "        raise ValueError(f\"Sequence {i} has only {len(token_ids)} tokens, need at least {TARGET_LENGTH}\")\n",
    "    \n",
    "    # Truncate to exactly TARGET_LENGTH\n",
    "    token_ids = token_ids[:TARGET_LENGTH]\n",
    "    token_ids_list.append(token_ids)\n",
    "    \n",
    "    # Decode back to text for display\n",
    "    text_truncated = tokenizer.decode(token_ids, skip_special_tokens=True)\n",
    "    print(f\"  Seq {i}: {len(token_ids)} tokens - '{text_truncated[:60]}...'\")\n",
    "\n",
    "print()\n",
    "\n",
    "def collect_key_vector(model, token_ids_batch, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Forward pass with batch of pre-tokenized sequences.\n",
    "    All sequences have exactly the same length (no padding needed).\n",
    "    Returns key vector for sequence 0.\n",
    "    \"\"\"\n",
    "    # Convert token IDs to tensor (batch_size, seq_len)\n",
    "    input_ids = torch.tensor(token_ids_batch, dtype=torch.long).to(device)\n",
    "    \n",
    "    # All sequences same length, so attention mask is all ones\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            use_cache=True,\n",
    "            return_dict=True\n",
    "        )\n",
    "    \n",
    "    # Extract key vectors from last layer's KV cache\n",
    "    past_kv = outputs.past_key_values\n",
    "    last_layer_kv = past_kv[-1]  # Last layer\n",
    "    key_cache = last_layer_kv[0]  # Keys (batch, num_heads, seq_len, head_dim)\n",
    "    \n",
    "    # Get sequence 0's keys at the last token position (position 99, 0-indexed)\n",
    "    seq_0_keys = key_cache[0, :, -1, :]  # (num_heads, head_dim)\n",
    "    key_vector = seq_0_keys.reshape(-1).cpu().clone()  # Flatten to 1D\n",
    "    \n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return key_vector\n",
    "\n",
    "# Define 2 different batch compositions (batch size 4, different content)\n",
    "# Both include sequence 0, but different sequences in positions 1,2,3\n",
    "compositions = [\n",
    "    [token_ids_list[0], token_ids_list[1], token_ids_list[2], token_ids_list[3]],  # seq0 + seq1,2,3\n",
    "    [token_ids_list[0], token_ids_list[4], token_ids_list[5], token_ids_list[6]],  # seq0 + seq4,5,6\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT: Batch Composition Effect Test\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Batch size: 4 (constant)\")\n",
    "print(f\"Sequence length: {TARGET_LENGTH} tokens (all sequences)\")\n",
    "print(f\"Sequence 0: IDENTICAL across both compositions\")\n",
    "print(f\"Positions 1,2,3: DIFFERENT content per composition\")\n",
    "print(f\"Repetitions per composition: 5\")\n",
    "print()\n",
    "\n",
    "num_reps = 5\n",
    "results = {}\n",
    "\n",
    "for comp_idx, composition in enumerate(compositions, 1):\n",
    "    print(f\"Composition {comp_idx}:\")\n",
    "    print(f\"  Position 0: Sequence 0 (always the same)\")\n",
    "    if comp_idx == 1:\n",
    "        print(f\"  Position 1: Sequence 1 (climate modeling)\")\n",
    "        print(f\"  Position 2: Sequence 2 (quantum entanglement)\")\n",
    "        print(f\"  Position 3: Sequence 3 (immune system)\")\n",
    "    else:\n",
    "        print(f\"  Position 1: Sequence 4 (Renaissance architecture)\")\n",
    "        print(f\"  Position 2: Sequence 5 (ocean hydrothermal vents)\")\n",
    "        print(f\"  Position 3: Sequence 6 (blockchain technology)\")\n",
    "    \n",
    "    runs = []\n",
    "    for rep in range(num_reps):\n",
    "        key_vec = collect_key_vector(model, composition, device=\"cuda\")\n",
    "        runs.append(key_vec)\n",
    "        if rep == 0:\n",
    "            print(f\"  Rep 0: norm={torch.norm(key_vec).item():.6f}, first_val={key_vec[0].item():.6f}\")\n",
    "    \n",
    "    results[f\"composition_{comp_idx}\"] = torch.stack(runs)\n",
    "    \n",
    "    # Check repeatability within composition\n",
    "    first_rep = runs[0]\n",
    "    all_identical = all(torch.equal(first_rep, runs[i]) for i in range(1, num_reps))\n",
    "    if all_identical:\n",
    "        print(f\"  ✓ All {num_reps} repetitions identical within composition\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Repetitions vary within composition (unexpected!)\")\n",
    "        for i in range(1, num_reps):\n",
    "            if not torch.equal(first_rep, runs[i]):\n",
    "                l2 = torch.norm(first_rep - runs[i]).item()\n",
    "                print(f\"    Rep 0 vs {i}: L2={l2:.6f}\")\n",
    "    print()\n",
    "\n",
    "# Compare across compositions\n",
    "print(\"=\"*60)\n",
    "print(\"CRITICAL ANALYSIS: Does Batch Composition Matter?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comp1_mean = results[\"composition_1\"].mean(dim=0)\n",
    "comp2_mean = results[\"composition_2\"].mean(dim=0)\n",
    "\n",
    "print(f\"Key vector dimension: {comp1_mean.shape[0]}\")\n",
    "print(f\"Comp 1 mean norm: {torch.norm(comp1_mean).item():.6f}\")\n",
    "print(f\"Comp 2 mean norm: {torch.norm(comp2_mean).item():.6f}\")\n",
    "print()\n",
    "\n",
    "l2_distance = torch.norm(comp1_mean - comp2_mean).item()\n",
    "\n",
    "print(f\"L2 distance (Comp 1 vs Comp 2): {l2_distance:.6f}\")\n",
    "\n",
    "if torch.norm(comp1_mean) > 0:\n",
    "    rel_diff = l2_distance / torch.norm(comp1_mean).item()\n",
    "    print(f\"Relative difference: {rel_diff:.8f}\")\n",
    "print()\n",
    "\n",
    "# Element-wise analysis\n",
    "diff = (comp1_mean - comp2_mean).abs()\n",
    "print(\"Element-wise differences:\")\n",
    "print(f\"  Max |diff|: {diff.max().item():.8f}\")\n",
    "print(f\"  Mean |diff|: {diff.mean().item():.8f}\")\n",
    "print(f\"  Elements with |diff| > 1e-6: {(diff > 1e-6).sum().item()}/{diff.shape[0]}\")\n",
    "print()\n",
    "\n",
    "# Check if compositions are exactly identical\n",
    "exact_match = torch.equal(comp1_mean, comp2_mean)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test configuration:\")\n",
    "print(f\"  - Sequence 0: IDENTICAL in both compositions\")\n",
    "print(f\"  - Batch size: {len(compositions[0])} (constant)\")\n",
    "print(f\"  - Sequence length: {TARGET_LENGTH} tokens (all equal)\")\n",
    "print(f\"  - Batch content (pos 1,2,3): DIFFERENT between compositions\")\n",
    "print()\n",
    "\n",
    "if exact_match:\n",
    "    print(\"✓ PERFECT MATCH: Both compositions produce IDENTICAL key vectors\")\n",
    "    print(\"  → Batch composition does NOT matter\")\n",
    "    print(\"  → Only batch SIZE affects outputs\")\n",
    "    print(\"  → Verification needs only: batch size logs\")\n",
    "    print(\"  → ✓✓✓ BUBBLE-BASED VERIFICATION IS FEASIBLE\")\n",
    "elif l2_distance < 1e-6:\n",
    "    print(\"✓ NEGLIGIBLE DIFFERENCE: Compositions produce nearly identical results\")\n",
    "    print(f\"  L2 distance: {l2_distance:.8f} (effectively zero)\")\n",
    "    print(\"  → Batch composition effects are below noise floor\")\n",
    "    print(\"  → Verification viable with batch size alone\")\n",
    "    print(\"  → ✓✓ BUBBLE-BASED VERIFICATION IS FEASIBLE\")\n",
    "elif l2_distance < 0.01:\n",
    "    print(\"⚠ SMALL DIFFERENCE: Detectable but small composition effect\")\n",
    "    print(f\"  L2 distance: {l2_distance:.6f}\")\n",
    "    print(\"  → Batch composition might have minor effects\")\n",
    "    print(\"  → Need to test decode phase to confirm\")\n",
    "    print(\"  → Verification may require batch composition logs\")\n",
    "else:\n",
    "    print(\"✗ SIGNIFICANT DIFFERENCE: Batch composition DOES matter\")\n",
    "    print(f\"  L2 distance: {l2_distance:.6f}\")\n",
    "    print(\"  → Sequence 0's outputs depend on batch neighbors\")\n",
    "    print(\"  → Verification REQUIRES full batch composition logs\")\n",
    "    print(\"  → ✗✗ BUBBLE-BASED VERIFICATION NEEDS HEAVY LOGGING\")\n",
    "\n",
    "# Save results\n",
    "output = {\n",
    "    \"experiment\": \"batch_composition_prefill_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"pytorch\": torch.__version__,\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"environment\": env_vars,\n",
    "    \"config\": {\n",
    "        \"batch_size\": len(compositions[0]),\n",
    "        \"sequence_length\": TARGET_LENGTH,\n",
    "        \"num_compositions\": len(compositions),\n",
    "        \"repetitions_per_composition\": num_reps,\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"operation\": \"single_forward_pass_prefill\",\n",
    "        \"extraction_method\": \"last_layer_keys_last_token_position\"\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"exact_match\": exact_match,\n",
    "        \"l2_distance\": l2_distance,\n",
    "        \"relative_difference\": rel_diff if torch.norm(comp1_mean) > 0 else 0,\n",
    "        \"max_element_diff\": diff.max().item(),\n",
    "        \"mean_element_diff\": diff.mean().item(),\n",
    "    },\n",
    "    \"conclusion\": (\n",
    "        \"composition_irrelevant\" if exact_match else\n",
    "        \"negligible_effect\" if l2_distance < 1e-6 else\n",
    "        \"small_effect\" if l2_distance < 0.01 else\n",
    "        \"significant_effect\"\n",
    "    )\n",
    "}\n",
    "\n",
    "output_file = f\"batch_composition_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "output_path = f\"/mnt/user-data/outputs/{output_file}\"\n",
    "\n",
    "os.makedirs(\"/mnt/user-data/outputs\", exist_ok=True)\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847dadb0-1faf-4915-b4d9-6d6dc837e843",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
