{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d31f4e6-5f35-44c5-ade3-b74a3ed42e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "BATCH SIZE CONTROL TEST - DECODE\n",
      "============================================================\n",
      "System Info:\n",
      "  Hostname: d07c0e0bfd13\n",
      "  Container: d07c0e0bfd13\n",
      "  GPU: NVIDIA A40\n",
      "  PyTorch: 2.8.0+cu128\n",
      "  CUDA: 12.8\n",
      "\n",
      "Environment Variables:\n",
      "  CUDA_MODULE_LOADING=LAZY\n",
      "  CUDA_VERSION=12.8.1\n",
      "  NCCL_VERSION=2.25.1-1\n",
      "  NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
      "  NV_CUDA_CUDART_DEV_VERSION=12.8.90-1\n",
      "  NV_CUDA_CUDART_VERSION=12.8.90-1\n",
      "  NV_CUDA_LIB_VERSION=12.8.1-1\n",
      "  NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-8=12.8.1-1\n",
      "  NV_CUDA_NSIGHT_COMPUTE_VERSION=12.8.1-1\n",
      "  NV_CUDNN_PACKAGE=libcudnn9-cuda-12=9.8.0.87-1\n",
      "  NV_CUDNN_PACKAGE_DEV=libcudnn9-dev-cuda-12=9.8.0.87-1\n",
      "  NV_CUDNN_PACKAGE_NAME=libcudnn9-cuda-12\n",
      "  NV_CUDNN_VERSION=9.8.0.87-1\n",
      "  NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.25.1-1+cuda12.8\n",
      "  NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "  NV_LIBNCCL_DEV_PACKAGE_VERSION=2.25.1-1\n",
      "  NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
      "  NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "  NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
      "\n",
      "Loading Qwen/Qwen2.5-7B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25a61ae855b64eff8e4a48387d5a1b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Layers: 28, Heads: 28, Key dim: 3584\n",
      "\n",
      "Tokenizing:\n",
      "  Seq 0: 100 tokens\n",
      "  Seq 1: 100 tokens\n",
      "  Seq 2: 100 tokens\n",
      "  Seq 3: 100 tokens\n",
      "\n",
      "============================================================\n",
      "EXPERIMENT: Batch Size Control - DECODE\n",
      "============================================================\n",
      "Repetitions: 5 per batch size\n",
      "EXPECTED: bs1 ≠ bs4 (otherwise no signal for verification)\n",
      "\n",
      "Batch size 1:\n",
      "  Generated: ' acceptable levels.\n",
      "\n",
      "How would you modify this system to inc...'\n",
      "  Key: norm=924.000000, first=0.028809\n",
      "  ✓ Reproducibility within batch size\n",
      "\n",
      "Batch size 4:\n",
      "  Generated: ' acceptable levels.\n",
      "\n",
      "How would you modify this system to inc...'\n",
      "  Key: norm=924.000000, first=0.026367\n",
      "  ✓ Reproducibility within batch size\n",
      "\n",
      "============================================================\n",
      "ANALYSIS: Does Batch Size Matter?\n",
      "============================================================\n",
      "L2 distance (bs1 vs bs4): 0.330078\n",
      "Relative difference: 0.00035723\n",
      "\n",
      "Max element diff: 0.25000000\n",
      "Elements with |diff| > 1e-6: 392/512\n",
      "\n",
      "============================================================\n",
      "VERDICT\n",
      "============================================================\n",
      "Generated tokens: SAME\n",
      "\n",
      "✓ STRONG SIGNAL: Batch size creates clear differences\n",
      "  L2 distance: 0.3301\n",
      "  → Verification CAN detect hidden batch capacity\n",
      "  → ✓✓✓ BATCH SIZE IS A VIABLE FORENSIC SIGNAL\n",
      "\n",
      "✓ Results saved to /mnt/user-data/outputs/batch_size_control_decode_20251103_202752.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Batch Size Control Test - DECODE Phase\n",
    "Control experiment: Does batch SIZE affect sequence 0's key vectors during decode?\n",
    "- Same sequence 0 across all runs\n",
    "- Different batch sizes: 1 vs 4\n",
    "- Generate 30 tokens autoregressively\n",
    "- Compare sequence 0's key vectors at final decode step\n",
    "\n",
    "This SHOULD show a difference (otherwise verification is impossible)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"BATCH SIZE CONTROL TEST - DECODE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"System Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  Container: {CONTAINER_ID}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA: {torch.version.cuda}\")\n",
    "print()\n",
    "\n",
    "# Capture relevant environment variables\n",
    "print(\"Environment Variables:\")\n",
    "env_vars = {}\n",
    "for key in sorted(os.environ.keys()):\n",
    "    if any(x in key.upper() for x in ['CUDA', 'TORCH', 'NCCL', 'CUDNN', 'PYTORCH']):\n",
    "        env_vars[key] = os.environ[key]\n",
    "        print(f\"  {key}={os.environ[key]}\")\n",
    "if not env_vars:\n",
    "    print(\"  (No CUDA/TORCH env vars set)\")\n",
    "print()\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "num_layers = len(model.model.layers)\n",
    "num_heads = model.config.num_attention_heads\n",
    "head_dim = model.config.hidden_size // num_heads\n",
    "key_vector_dim = num_heads * head_dim\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Layers: {num_layers}, Heads: {num_heads}, Key dim: {key_vector_dim}\\n\")\n",
    "\n",
    "# Use same sequences\n",
    "raw_sequences = [\n",
    "    \"\"\"The automated data-processing pipeline ingests raw telemetry from distributed sensors \n",
    "    across multiple geographic locations. A proprietary algorithm then normalizes the dataset, \n",
    "    filtering for anomalies based on predefined statistical parameters derived from historical \n",
    "    patterns. The resulting output is a clean, structured matrix ready for machine learning model \n",
    "    ingestion and downstream analytical workflows. System efficiency is monitored in real-time \n",
    "    through a comprehensive dashboard, with automated alerts triggered if latency exceeds the \n",
    "    established threshold or if data quality metrics fall below acceptable ranges. Advanced \n",
    "    compression techniques optimize storage utilization across the distributed infrastructure.\n",
    "    Performance metrics are tracked continuously to ensure optimal throughput and minimal latency.\"\"\",\n",
    "    \n",
    "    \"\"\"Climate modeling techniques have advanced significantly through the integration of \n",
    "    high-resolution satellite imagery and ground-based observation networks. Researchers combine \n",
    "    atmospheric physics equations with empirical data to simulate complex weather patterns and \n",
    "    predict long-term climate trends. These models incorporate ocean currents, ice sheet dynamics, \n",
    "    and greenhouse gas concentrations to provide increasingly accurate projections for policymakers \n",
    "    and environmental scientists worldwide. Modern computational infrastructure enables simulations \n",
    "    at unprecedented scales and temporal resolution with remarkable accuracy and scientific validity.\n",
    "    International collaboration facilitates data sharing and model validation across research institutions.\"\"\",\n",
    "    \n",
    "    \"\"\"Quantum entanglement represents one of the most counterintuitive phenomena in modern physics, \n",
    "    where particles become correlated in ways that defy classical explanations. When two particles \n",
    "    are entangled, measuring the state of one instantaneously affects the other regardless of the \n",
    "    distance separating them. This property has profound implications for quantum computing and \n",
    "    cryptography, enabling novel approaches to information processing and secure communication that \n",
    "    are fundamentally impossible with classical systems and traditional computational paradigms. Research \n",
    "    continues to explore practical applications of these quantum mechanical principles. Experimental \n",
    "    verification requires sophisticated detection equipment and precisely controlled laboratory conditions.\"\"\",\n",
    "    \n",
    "    \"\"\"The human immune system comprises an intricate network of cells, tissues, and organs that \n",
    "    work collaboratively to defend against pathogens and foreign substances. White blood cells \n",
    "    patrol the bloodstream and tissues, identifying and neutralizing threats through both innate \n",
    "    and adaptive immune responses. B cells produce antibodies that target specific antigens, while \n",
    "    T cells orchestrate cellular immunity and eliminate infected cells. This sophisticated biological \n",
    "    defense mechanism evolved over millions of years to protect organisms. Memory cells enable rapid \n",
    "    responses to previously encountered pathogens through accelerated antibody production. The lymphatic \n",
    "    system transports immune cells throughout the body to maintain comprehensive surveillance.\"\"\",\n",
    "]\n",
    "\n",
    "TARGET_LENGTH = 100\n",
    "token_ids_list = []\n",
    "\n",
    "print(\"Tokenizing:\")\n",
    "for i, text in enumerate(raw_sequences):\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(token_ids) < TARGET_LENGTH:\n",
    "        raise ValueError(f\"Sequence {i} has only {len(token_ids)} tokens\")\n",
    "    token_ids = token_ids[:TARGET_LENGTH]\n",
    "    token_ids_list.append(token_ids)\n",
    "    print(f\"  Seq {i}: {len(token_ids)} tokens\")\n",
    "\n",
    "print()\n",
    "\n",
    "def collect_key_vector_decode(model, tokenizer, token_ids_batch, max_new_tokens=30, device=\"cuda\"):\n",
    "    input_ids = torch.tensor(token_ids_batch, dtype=torch.long).to(device)\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            return_dict_in_generate=True,\n",
    "            use_cache=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    if not hasattr(outputs, 'past_key_values') or outputs.past_key_values is None:\n",
    "        raise RuntimeError(\"No KV cache returned\")\n",
    "    \n",
    "    past_kv = outputs.past_key_values\n",
    "    key_cache = past_kv[-1][0]  # Last layer keys\n",
    "    seq_0_keys = key_cache[0, :, -1, :]\n",
    "    key_vector = seq_0_keys.reshape(-1).cpu().clone()\n",
    "    \n",
    "    seq_0_generated = outputs.sequences[0, input_ids.shape[1]:].cpu().tolist()\n",
    "    \n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return key_vector, seq_0_generated\n",
    "\n",
    "batch_configs = {\n",
    "    \"bs1\": [token_ids_list[0]],\n",
    "    \"bs4\": [token_ids_list[0], token_ids_list[1], token_ids_list[2], token_ids_list[3]],\n",
    "}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT: Batch Size Control - DECODE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Repetitions: 5 per batch size\")\n",
    "print(f\"EXPECTED: bs1 ≠ bs4 (otherwise no signal for verification)\")\n",
    "print()\n",
    "\n",
    "num_reps = 5\n",
    "results = {}\n",
    "generated_texts = {}\n",
    "\n",
    "for bs_name, batch in batch_configs.items():\n",
    "    bs = len(batch)\n",
    "    print(f\"Batch size {bs}:\")\n",
    "    \n",
    "    runs = []\n",
    "    gen_tokens_list = []\n",
    "    \n",
    "    for rep in range(num_reps):\n",
    "        key_vec, gen_tokens = collect_key_vector_decode(\n",
    "            model, tokenizer, batch, max_new_tokens=30, device=\"cuda\"\n",
    "        )\n",
    "        runs.append(key_vec)\n",
    "        gen_tokens_list.append(gen_tokens)\n",
    "        \n",
    "        if rep == 0:\n",
    "            gen_text = tokenizer.decode(gen_tokens, skip_special_tokens=True)\n",
    "            print(f\"  Generated: '{gen_text[:60]}...'\")\n",
    "            print(f\"  Key: norm={torch.norm(key_vec).item():.6f}, first={key_vec[0].item():.6f}\")\n",
    "    \n",
    "    results[bs_name] = torch.stack(runs)\n",
    "    generated_texts[bs_name] = gen_tokens_list\n",
    "    \n",
    "    # Check reproducibility\n",
    "    first = runs[0]\n",
    "    all_identical = all(torch.equal(first, runs[i]) for i in range(1, num_reps))\n",
    "    print(f\"  {'✓' if all_identical else '⚠'} Reproducibility within batch size\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ANALYSIS: Does Batch Size Matter?\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bs1_mean = results[\"bs1\"].mean(dim=0)\n",
    "bs4_mean = results[\"bs4\"].mean(dim=0)\n",
    "\n",
    "l2_distance = torch.norm(bs1_mean - bs4_mean).item()\n",
    "rel_diff = l2_distance / torch.norm(bs1_mean).item() if torch.norm(bs1_mean) > 0 else 0\n",
    "\n",
    "print(f\"L2 distance (bs1 vs bs4): {l2_distance:.6f}\")\n",
    "print(f\"Relative difference: {rel_diff:.8f}\")\n",
    "print()\n",
    "\n",
    "diff = (bs1_mean - bs4_mean).abs()\n",
    "print(f\"Max element diff: {diff.max().item():.8f}\")\n",
    "print(f\"Elements with |diff| > 1e-6: {(diff > 1e-6).sum().item()}/{diff.shape[0]}\")\n",
    "print()\n",
    "\n",
    "# Check generated tokens\n",
    "bs1_tokens = generated_texts[\"bs1\"][0]\n",
    "bs4_tokens = generated_texts[\"bs4\"][0]\n",
    "tokens_match = bs1_tokens == bs4_tokens\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"VERDICT\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Generated tokens: {'SAME' if tokens_match else 'DIFFERENT'}\")\n",
    "print()\n",
    "\n",
    "if l2_distance > 0.1:\n",
    "    print(\"✓ STRONG SIGNAL: Batch size creates clear differences\")\n",
    "    print(f\"  L2 distance: {l2_distance:.4f}\")\n",
    "    print(\"  → Verification CAN detect hidden batch capacity\")\n",
    "    print(\"  → ✓✓✓ BATCH SIZE IS A VIABLE FORENSIC SIGNAL\")\n",
    "elif l2_distance > 0.001:\n",
    "    print(\"⚠ WEAK SIGNAL: Small but detectable batch size effect\")\n",
    "    print(f\"  L2 distance: {l2_distance:.6f}\")\n",
    "    print(\"  → May be sufficient for verification\")\n",
    "elif l2_distance > 0:\n",
    "    print(\"⚠ VERY WEAK: Minimal batch size effect\")\n",
    "    print(f\"  L2 distance: {l2_distance:.8f}\")\n",
    "    print(\"  → Signal might be too weak for practical verification\")\n",
    "else:\n",
    "    print(\"✗ NO SIGNAL: Batch size has no effect\")\n",
    "    print(\"  → Cannot detect hidden capacity via batch size\")\n",
    "    print(\"  → ✗✗ VERIFICATION SCHEME FAILS\")\n",
    "\n",
    "output = {\n",
    "    \"experiment\": \"batch_size_control_decode_test\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"pytorch\": torch.__version__,\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"environment\": env_vars,\n",
    "    \"config\": {\n",
    "        \"batch_sizes\": [1, 4],\n",
    "        \"prefill_length\": TARGET_LENGTH,\n",
    "        \"decode_length\": 30,\n",
    "        \"repetitions\": num_reps,\n",
    "        \"dtype\": \"bfloat16\",\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"l2_distance_bs1_vs_bs4\": l2_distance,\n",
    "        \"relative_difference\": rel_diff,\n",
    "        \"tokens_match\": tokens_match,\n",
    "        \"max_element_diff\": diff.max().item(),\n",
    "    },\n",
    "    \"conclusion\": (\n",
    "        \"strong_signal\" if l2_distance > 0.1 else\n",
    "        \"weak_signal\" if l2_distance > 0.001 else\n",
    "        \"very_weak\" if l2_distance > 0 else\n",
    "        \"no_signal\"\n",
    "    )\n",
    "}\n",
    "\n",
    "output_file = f\"batch_size_control_decode_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "output_path = f\"/mnt/user-data/outputs/{output_file}\"\n",
    "\n",
    "os.makedirs(\"/mnt/user-data/outputs\", exist_ok=True)\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42e820f-83d5-4607-b0fb-a4fa35145e14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
