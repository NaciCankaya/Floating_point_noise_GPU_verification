{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66d7bc03-ebf7-4c6f-99a0-9ebf659ada4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.4/803.4 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2025.10.23 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1\n",
      "Collecting hf_transfer\n",
      "  Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Downloading hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Downloading accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install hf_transfer\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "405656e6-60ba-403d-9f8e-beb40713a282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Info:\n",
      "  Hostname: 0c2f4d9332d7\n",
      "  Container: 0c2f4d9332d7\n",
      "\n",
      "Environment Variables:\n",
      "  CUDA_VERSION=12.8.1\n",
      "  NCCL_VERSION=2.25.1-1\n",
      "  NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
      "  NV_CUDA_CUDART_DEV_VERSION=12.8.90-1\n",
      "  NV_CUDA_CUDART_VERSION=12.8.90-1\n",
      "  NV_CUDA_LIB_VERSION=12.8.1-1\n",
      "  NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-8=12.8.1-1\n",
      "  NV_CUDA_NSIGHT_COMPUTE_VERSION=12.8.1-1\n",
      "  NV_CUDNN_PACKAGE=libcudnn9-cuda-12=9.8.0.87-1\n",
      "  NV_CUDNN_PACKAGE_DEV=libcudnn9-dev-cuda-12=9.8.0.87-1\n",
      "  NV_CUDNN_PACKAGE_NAME=libcudnn9-cuda-12\n",
      "  NV_CUDNN_VERSION=9.8.0.87-1\n",
      "  NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.25.1-1+cuda12.8\n",
      "  NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "  NV_LIBNCCL_DEV_PACKAGE_VERSION=2.25.1-1\n",
      "  NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
      "  NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "  NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
      "\n",
      "Loading Qwen/Qwen2.5-7B-Instruct in BF16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48aab92d0afd4b9490599a9480677efd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf8d4f513ed343629ac89b1eea8a1041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c8d8d2f2534ef68f5dd452771fd079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fb783608c84d9987677b5edd5d9971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1abf8b605a4c208a9b0c85b2135cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7ea512cde846bf952513a7d0ad621f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "717202bf484e473b9ab05cab0f415215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a2ce9de5bed4e1a8e835c21099be14d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63a090230194869bd5902104521a7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f613029268845e187ae982141531af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4843aa3488c242b7a0363f26632256d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a269f7248144c6975631b715435206",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing sequences to determine lengths:\n",
      "  technical :  216 tokens\n",
      "  literary  :  204 tokens\n",
      "  corporate :  193 tokens\n",
      "  legal     :  211 tokens\n",
      "\n",
      "Selected 'technical' as sequence 0 (longest, 216 tokens)\n",
      "Dummy sequences: ['literary', 'corporate', 'legal']\n",
      "CRITICAL: Element 0 will be IDENTICAL across all batch sizes\n",
      "\n",
      "============================================================\n",
      "Starting H100 REALISTIC PARALLEL BATCH TEST at 2025-11-03T15:37:24.211057\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Precision: BF16 (bfloat16)\n",
      "Base prompt tokens: 216\n",
      "Operation: Single forward pass (prefill only)\n",
      "CRITICAL: Element 0 input is IDENTICAL across batch sizes\n",
      "Repetitions per batch size: 10\n",
      "============================================================\n",
      "\n",
      "Collecting batch_size=1 (10 repetitions)...\n",
      "  Batch: [technical]\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Rep 0: norm=310.000000, first_val=-0.621094\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Completed 3/10 repetitions\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Completed 6/10 repetitions\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Completed 9/10 repetitions\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  ✓ All 10 repetitions are identical (expected)\n",
      "  Statistical noise: mean=0.000000, std=0.000000\n",
      "  Activation norm: 310.00\n",
      "\n",
      "Collecting batch_size=2 (10 repetitions)...\n",
      "  Batch: [technical, dummy1] (extracting from elem 0)\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Rep 0: norm=310.000000, first_val=-0.589844\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Completed 3/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Completed 6/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Completed 9/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  ✓ All 10 repetitions are identical (expected)\n",
      "  Statistical noise: mean=0.000000, std=0.000000\n",
      "  Activation norm: 310.00\n",
      "\n",
      "Collecting batch_size=4 (10 repetitions)...\n",
      "  Batch: [technical, dummy1, dummy2, dummy3] (extracting from elem 0)\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Rep 0: norm=310.000000, first_val=-0.542969\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Completed 3/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Completed 6/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  Completed 9/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid tokens=216, padding=0, elem[0]≠elem[1]: True\n",
      "  ✓ All 10 repetitions are identical (expected)\n",
      "  Statistical noise: mean=0.000000, std=0.000000\n",
      "  Activation norm: 310.00\n",
      "\n",
      "\n",
      "============================================================\n",
      "=== SYSTEMATIC DEVIATION MATRIX ===\n",
      "============================================================\n",
      "     bs= 1  bs= 2  bs= 4  \n",
      "bs= 1   -     3.562  3.891 \n",
      "bs= 2  3.562   -     3.812 \n",
      "bs= 4  3.891  3.812   -    \n",
      "\n",
      "============================================================\n",
      "=== ACTIVATION SCALE ANALYSIS ===\n",
      "============================================================\n",
      "Activation vector dimension: 3584\n",
      "bs=1 mean activation norm: 310.00\n",
      "bs=2 mean activation norm: 310.00\n",
      "L2 distance (bs1 vs bs2): 3.5625\n",
      "Relative difference: 0.011475\n",
      "Max absolute difference: 1.000000\n",
      "Dimensions with |diff| > 0.01: 2813/3584\n",
      "\n",
      "============================================================\n",
      "=== VERDICT ===\n",
      "============================================================\n",
      "Element 0 input: 'technical' sequence (IDENTICAL across batch sizes)\n",
      "Parallel sequences: diverse realistic content\n",
      "bs1 vs bs2 deviation: 3.562500\n",
      "\n",
      "✓ DETECTION VIABLE: L2=3.5625\n",
      "  Parallel batch processing affects element 0's activations\n",
      "  → Forensics CAN detect hidden batch capacity on H100\n",
      "\n",
      "✓ Results saved to /workspace/NVIDIA_H100_PCIe_realistic_batch_exp5_20251103_153725.json\n",
      "✓ File size: ~1099.9 KB\n",
      "\n",
      "============================================================\n",
      "REALISTIC PARALLEL BATCH TEST COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "# Capture system info for verification\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(f\"System Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  Container: {CONTAINER_ID}\")\n",
    "print()\n",
    "\n",
    "# Capture relevant environment variables\n",
    "print(\"Environment Variables:\")\n",
    "env_vars = {}\n",
    "for key in sorted(os.environ.keys()):\n",
    "    if any(x in key.upper() for x in ['CUDA', 'TORCH', 'NCCL', 'CUDNN', 'PYTORCH']):\n",
    "        env_vars[key] = os.environ[key]\n",
    "        print(f\"  {key}={os.environ[key]}\")\n",
    "if not env_vars:\n",
    "    print(\"  (No CUDA/TORCH env vars set)\")\n",
    "print()\n",
    "\n",
    "# Four diverse, realistic sequences\n",
    "SEQUENCES = {\n",
    "    \"technical\": \"\"\"The study investigates the quantum decoherence effects on a multi-qubit superconducting system when subjected to controlled microwave pulses. We utilized a novel cryogenic amplification chain to minimize thermal noise and achieve a signal-to-noise ratio previously unattainable in similar setups. The experimental protocol involved preparing the qubits in a Greenberger-Horne-Zeilinger (GHZ) state and then measuring the decay of quantum entanglement over time by performing state tomography. Our results demonstrate a non-linear relationship between pulse amplitude and coherence time, suggesting that higher-order coupling terms, often neglected in theoretical models, play a significant role in system dynamics. The empirical data were cross-validated against a master equation simulation incorporating a 1/f noise model, showing strong correlation (R² > 0.98). These findings provide crucial insights for the development of fault-tolerant quantum computing architectures and the calibration of error-correction codes. Further research will focus on isolating the specific mechanisms responsible for the observed decoherence pathways and exploring potential mitigation strategies through dynamic decoupling sequences.\"\"\",\n",
    "    \n",
    "    \"literary\": \"\"\"Eleanor traced the rim of her chipped porcelain teacup, the warmth a feeble defense against the morning's persistent chill. Outside, the rain drew gray, wavering lines down the windowpane, blurring the world into an impressionist's watercolor of the street she had known for fifty years. Each object in her small parlor was a relic, a silent testament to a life lived in quiet increments: the grandfather clock that had stopped at half-past three the day Arthur left for the war, the faded photograph on the mantelpiece of a girl with her own smile but brighter eyes, the worn velvet armchair that still held the faint scent of her husband's pipe tobacco. She wasn't lonely, she told herself, but rather, well-acquainted with solitude. It was a familiar garment, threadbare in places, but comfortable. The silence wasn't empty; it was filled with the echoes of every laugh, every argument, and every whispered promise that had ever inhabited those walls.\"\"\",\n",
    "    \n",
    "    \"corporate\": \"\"\"In the third quarter, our strategic pivot towards a subscription-based revenue model has yielded promising preliminary results, demonstrating enhanced customer lifetime value and more predictable revenue streams. We successfully onboarded 15 new enterprise-level clients, exceeding our initial target by 25%. This growth was largely driven by the targeted digital marketing campaign launched in July, which achieved a 40% higher conversion rate than previous initiatives. However, we did face headwinds in the APAC region due to unforeseen supply chain disruptions and increased logistical costs, which compressed our gross margin by approximately 150 basis points. To mitigate these challenges, the operations team has initiated a comprehensive vendor diversification program and is exploring near-shoring opportunities. Looking ahead to Q4, our primary focus will be on optimizing the user onboarding experience to reduce churn and leveraging our data analytics capabilities to identify key upselling opportunities within our existing customer base. We remain confident in our full-year financial outlook.\"\"\",\n",
    "    \n",
    "    \"legal\": \"\"\"This End-User License Agreement (\"EULA\") constitutes a legally binding contract between you, the end-user (\"Licensee\"), and a fictitious corporation (\"Licensor\") for the software product accompanying this EULA. By installing, copying, or otherwise using the Software, Licensee agrees to be bound by the terms herein. Licensor grants Licensee a limited, non-exclusive, non-transferable license to use the Software for personal, non-commercial purposes. Licensee is expressly prohibited from reverse-engineering, decompiling, disassembling, or creating derivative works based on the Software. All rights, title, and interest, including but not limited to intellectual property rights, in and to the Software shall remain with Licensor. The Software is provided \"as is\" without warranty of any kind. In no event shall Licensor be liable for any direct, indirect, consequential, or incidental damages arising from the use or inability to use the Software. This Agreement is governed by the laws of the specified jurisdiction, without regard to its conflict of law provisions.\"\"\"\n",
    "}\n",
    "\n",
    "def collect_activations_parallel_batch(model, tokenizer, base_prompt, dummy_prompts, batch_size=1, device=\"cuda\"):\n",
    "    \"\"\"Forward pass where element 0 is ALWAYS base_prompt, with diverse dummy sequences\n",
    "    \n",
    "    CRITICAL: Element 0 has IDENTICAL input across all batch sizes\n",
    "    - bs=1: [base_prompt]\n",
    "    - bs=2: [base_prompt, dummy_prompts[0]]\n",
    "    - bs=4: [base_prompt, dummy_prompts[0], dummy_prompts[1], dummy_prompts[2]]\n",
    "    \n",
    "    Extract from element 0's LAST VALID TOKEN (using attention mask)\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if batch_size == 1:\n",
    "        prompts = [base_prompt]\n",
    "    else:\n",
    "        # Element 0 is always base_prompt\n",
    "        # Elements 1+ are dummy sequences (up to batch_size-1)\n",
    "        prompts = [base_prompt] + dummy_prompts[:batch_size-1]\n",
    "    \n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # DEBUG: Verify setup\n",
    "    actual_batch_size = inputs['input_ids'].shape[0]\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    if actual_batch_size != batch_size:\n",
    "        print(f\"WARNING: Expected batch_size={batch_size}, got {actual_batch_size}\")\n",
    "    \n",
    "    # Check element 0's valid token count (non-padded)\n",
    "    elem0_valid_tokens = inputs['attention_mask'][0].sum().item()\n",
    "    elem0_padding = seq_len - elem0_valid_tokens\n",
    "    \n",
    "    if batch_size > 1:\n",
    "        # Verify elements differ\n",
    "        elem0_last5 = inputs['input_ids'][0, -5:].tolist()\n",
    "        elem1_last5 = inputs['input_ids'][1, -5:].tolist()\n",
    "        different = elem0_last5 != elem1_last5\n",
    "        print(f\"  Batch: seq_len={seq_len}, elem[0] valid tokens={elem0_valid_tokens}, padding={elem0_padding}, elem[0]≠elem[1]: {different}\")\n",
    "    else:\n",
    "        print(f\"  Single: seq_len={seq_len}, valid tokens={elem0_valid_tokens}\")\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, use_cache=False)\n",
    "    \n",
    "    # CRITICAL: Extract from element 0's LAST VALID TOKEN (not padding)\n",
    "    last_valid_pos = inputs['attention_mask'][0].sum() - 1\n",
    "    last_layer_last_pos = outputs.hidden_states[-1][0, last_valid_pos, :].cpu().clone()\n",
    "    \n",
    "    del outputs\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return last_layer_last_pos\n",
    "\n",
    "# Setup\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "EXP_NUMBER = 5  # Realistic diverse sequences\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name} in BF16...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Tokenize all sequences and find the longest\n",
    "print(\"\\nTokenizing sequences to determine lengths:\")\n",
    "token_counts = {}\n",
    "for name, text in SEQUENCES.items():\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_counts[name] = len(tokens)\n",
    "    print(f\"  {name:10s}: {len(tokens):4d} tokens\")\n",
    "\n",
    "# Select longest as base_prompt (sequence 0)\n",
    "longest_name = max(token_counts, key=token_counts.get)\n",
    "base_prompt = SEQUENCES[longest_name]\n",
    "dummy_prompts = [text for name, text in SEQUENCES.items() if name != longest_name]\n",
    "\n",
    "print(f\"\\nSelected '{longest_name}' as sequence 0 (longest, {token_counts[longest_name]} tokens)\")\n",
    "print(f\"Dummy sequences: {[name for name in SEQUENCES.keys() if name != longest_name]}\")\n",
    "print(f\"CRITICAL: Element 0 will be IDENTICAL across all batch sizes\\n\")\n",
    "\n",
    "# Test batch sizes 1, 2, 4 with 10 repetitions each\n",
    "batch_sizes = [1, 2, 4]\n",
    "num_repetitions = 10\n",
    "results = {}\n",
    "all_activations = {}\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Starting H100 REALISTIC PARALLEL BATCH TEST at {datetime.now().isoformat()}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Precision: BF16 (bfloat16)\")\n",
    "print(f\"Base prompt tokens: {token_counts[longest_name]}\")\n",
    "print(f\"Operation: Single forward pass (prefill only)\")\n",
    "print(f\"CRITICAL: Element 0 input is IDENTICAL across batch sizes\")\n",
    "print(f\"Repetitions per batch size: {num_repetitions}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"Collecting batch_size={bs} ({num_repetitions} repetitions)...\")\n",
    "    if bs == 1:\n",
    "        print(f\"  Batch: [{longest_name}]\")\n",
    "    elif bs == 2:\n",
    "        print(f\"  Batch: [{longest_name}, dummy1] (extracting from elem 0)\")\n",
    "    else:\n",
    "        print(f\"  Batch: [{longest_name}, dummy1, dummy2, dummy3] (extracting from elem 0)\")\n",
    "    \n",
    "    runs = []\n",
    "    for rep in range(num_repetitions):\n",
    "        activation = collect_activations_parallel_batch(model, tokenizer, base_prompt, dummy_prompts, batch_size=bs, device=\"cuda\")\n",
    "        runs.append(activation)\n",
    "        if rep == 0:\n",
    "            print(f\"  Rep 0: norm={torch.norm(activation).item():.6f}, first_val={activation[0].item():.6f}\")\n",
    "        if (rep + 1) % 3 == 0:\n",
    "            print(f\"  Completed {rep + 1}/{num_repetitions} repetitions\")\n",
    "    \n",
    "    # Check repeatability\n",
    "    first_rep = runs[0]\n",
    "    all_identical = all(torch.equal(first_rep, runs[i]) for i in range(1, num_repetitions))\n",
    "    if all_identical:\n",
    "        print(f\"  ✓ All {num_repetitions} repetitions are identical (expected)\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Repetitions vary (unexpected!)\")\n",
    "    \n",
    "    results[bs] = torch.stack(runs)\n",
    "    all_activations[f\"batch_size_{bs}\"] = results[bs].float().numpy().tolist()\n",
    "    \n",
    "    mean_activation = results[bs].mean(dim=0)\n",
    "    deviations = torch.stack([torch.norm(results[bs][i] - mean_activation) for i in range(num_repetitions)])\n",
    "    std_noise = deviations.std().item()\n",
    "    mean_noise = deviations.mean().item()\n",
    "    \n",
    "    print(f\"  Statistical noise: mean={mean_noise:.6f}, std={std_noise:.6f}\")\n",
    "    print(f\"  Activation norm: {torch.norm(mean_activation).item():.2f}\\n\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Compare systematic deviations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== SYSTEMATIC DEVIATION MATRIX ===\")\n",
    "print(\"=\"*60)\n",
    "print(\"     \", end=\"\")\n",
    "for bs in batch_sizes:\n",
    "    print(f\"bs={bs:2d}  \", end=\"\")\n",
    "print()\n",
    "\n",
    "systematic_deviations = {}\n",
    "for bs1 in batch_sizes:\n",
    "    print(f\"bs={bs1:2d} \", end=\"\")\n",
    "    for bs2 in batch_sizes:\n",
    "        if bs1 == bs2:\n",
    "            print(\"  -    \", end=\"\")\n",
    "        else:\n",
    "            mean1 = results[bs1].mean(dim=0)\n",
    "            mean2 = results[bs2].mean(dim=0)\n",
    "            l2 = torch.norm(mean1 - mean2).item()\n",
    "            systematic_deviations[f\"bs{bs1}_vs_bs{bs2}\"] = l2\n",
    "            print(f\"{l2:6.3f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== ACTIVATION SCALE ANALYSIS ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bs1_mean = results[1].mean(dim=0)\n",
    "bs2_mean = results[2].mean(dim=0)\n",
    "\n",
    "print(f\"Activation vector dimension: {bs1_mean.shape[0]}\")\n",
    "print(f\"bs=1 mean activation norm: {torch.norm(bs1_mean).item():.2f}\")\n",
    "print(f\"bs=2 mean activation norm: {torch.norm(bs2_mean).item():.2f}\")\n",
    "print(f\"L2 distance (bs1 vs bs2): {torch.norm(bs1_mean - bs2_mean).item():.4f}\")\n",
    "if torch.norm(bs1_mean) > 0:\n",
    "    print(f\"Relative difference: {(torch.norm(bs1_mean - bs2_mean) / torch.norm(bs1_mean)).item():.6f}\")\n",
    "\n",
    "diff = (bs1_mean - bs2_mean).abs()\n",
    "print(f\"Max absolute difference: {diff.max().item():.6f}\")\n",
    "print(f\"Dimensions with |diff| > 0.01: {(diff > 0.01).sum().item()}/{diff.shape[0]}\")\n",
    "\n",
    "# CRITICAL CHECK\n",
    "bs1_vs_bs2_deviation = systematic_deviations.get(\"bs1_vs_bs2\", 0)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== VERDICT ===\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Element 0 input: '{longest_name}' sequence (IDENTICAL across batch sizes)\")\n",
    "print(f\"Parallel sequences: diverse realistic content\")\n",
    "print(f\"bs1 vs bs2 deviation: {bs1_vs_bs2_deviation:.6f}\\n\")\n",
    "\n",
    "if bs1_vs_bs2_deviation > 0.1:\n",
    "    print(f\"✓ DETECTION VIABLE: L2={bs1_vs_bs2_deviation:.4f}\")\n",
    "    print(f\"  Parallel batch processing affects element 0's activations\")\n",
    "    print(f\"  → Forensics CAN detect hidden batch capacity on H100\")\n",
    "elif bs1_vs_bs2_deviation > 0.001:\n",
    "    print(f\"⚠ WEAK SIGNAL: L2={bs1_vs_bs2_deviation:.6f}\")\n",
    "    print(f\"  Small but detectable effect from parallel processing\")\n",
    "    print(f\"  → Forensics might work with careful analysis\")\n",
    "else:\n",
    "    print(f\"✗ NO DETECTION: L2={bs1_vs_bs2_deviation:.6f}\")\n",
    "    print(f\"  H100's batch processing perfectly isolates elements\")\n",
    "    print(f\"  → Cannot detect hidden batch capacity using this method\")\n",
    "\n",
    "# Save results\n",
    "output = {\n",
    "    \"experiment\": \"H100_realistic_parallel_batch_forensics\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"pytorch\": torch.__version__,\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"environment\": env_vars,\n",
    "    \"config\": {\n",
    "        \"batch_sizes\": batch_sizes,\n",
    "        \"repetitions\": num_repetitions,\n",
    "        \"operation\": \"single_forward_pass_prefill_only\",\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"use_cache\": False,\n",
    "        \"input_strategy\": \"diverse_realistic_sequences\",\n",
    "        \"sequence_0\": longest_name,\n",
    "        \"sequence_0_tokens\": token_counts[longest_name],\n",
    "        \"all_sequence_tokens\": token_counts,\n",
    "        \"hidden_dim\": int(bs1_mean.shape[0])\n",
    "    },\n",
    "    \"statistical_noise\": {\n",
    "        f\"batch_size_{bs}\": {\n",
    "            \"mean\": float(torch.stack([torch.norm(results[bs][i] - results[bs].mean(dim=0)) \n",
    "                                       for i in range(num_repetitions)]).mean()),\n",
    "            \"std\": float(torch.stack([torch.norm(results[bs][i] - results[bs].mean(dim=0)) \n",
    "                                     for i in range(num_repetitions)]).std())\n",
    "        }\n",
    "        for bs in batch_sizes\n",
    "    },\n",
    "    \"systematic_deviations\": systematic_deviations,\n",
    "    \"activation_norms\": {\n",
    "        f\"batch_size_{bs}\": float(torch.norm(results[bs].mean(dim=0))) \n",
    "        for bs in batch_sizes\n",
    "    },\n",
    "    \"forensics_result\": {\n",
    "        \"bs1_vs_bs2_deviation\": bs1_vs_bs2_deviation,\n",
    "        \"detection_viable\": bs1_vs_bs2_deviation > 0.1\n",
    "    },\n",
    "    \"raw_activations\": all_activations\n",
    "}\n",
    "\n",
    "output_file = f\"{torch.cuda.get_device_name(0).replace(' ', '_')}_realistic_batch_exp{EXP_NUMBER}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "output_path = f\"/workspace/{output_file}\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(f\"✓ File size: ~{len(json.dumps(output)) / 1024:.1f} KB\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"REALISTIC PARALLEL BATCH TEST COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
