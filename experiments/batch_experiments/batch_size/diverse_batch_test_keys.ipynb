{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6dc1e0b3-7b8f-4bf7-b821-23df4ffeca0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.2.0 huggingface-hub-0.36.0 regex-2025.10.23 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1\n",
      "Collecting hf_transfer\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: hf_transfer\n",
      "Successfully installed hf_transfer-0.1.9\n",
      "Collecting accelerate\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.8.0+cu128)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.20.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.10.5)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install hf_transfer\n",
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254a8225-72fe-4eb2-a199-4a7b3017b03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Info:\n",
      "  Hostname: 3611dce208ac\n",
      "  Container: 3611dce208ac\n",
      "\n",
      "Environment Variables:\n",
      "  CUDA_VERSION=12.8.1\n",
      "  NCCL_VERSION=2.25.1-1\n",
      "  NVIDIA_REQUIRE_CUDA=cuda>=12.8 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551 brand=unknown,driver>=560,driver<561 brand=grid,driver>=560,driver<561 brand=tesla,driver>=560,driver<561 brand=nvidia,driver>=560,driver<561 brand=quadro,driver>=560,driver<561 brand=quadrortx,driver>=560,driver<561 brand=nvidiartx,driver>=560,driver<561 brand=vapps,driver>=560,driver<561 brand=vpc,driver>=560,driver<561 brand=vcs,driver>=560,driver<561 brand=vws,driver>=560,driver<561 brand=cloudgaming,driver>=560,driver<561 brand=unknown,driver>=565,driver<566 brand=grid,driver>=565,driver<566 brand=tesla,driver>=565,driver<566 brand=nvidia,driver>=565,driver<566 brand=quadro,driver>=565,driver<566 brand=quadrortx,driver>=565,driver<566 brand=nvidiartx,driver>=565,driver<566 brand=vapps,driver>=565,driver<566 brand=vpc,driver>=565,driver<566 brand=vcs,driver>=565,driver<566 brand=vws,driver>=565,driver<566 brand=cloudgaming,driver>=565,driver<566\n",
      "  NV_CUDA_CUDART_DEV_VERSION=12.8.90-1\n",
      "  NV_CUDA_CUDART_VERSION=12.8.90-1\n",
      "  NV_CUDA_LIB_VERSION=12.8.1-1\n",
      "  NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE=cuda-nsight-compute-12-8=12.8.1-1\n",
      "  NV_CUDA_NSIGHT_COMPUTE_VERSION=12.8.1-1\n",
      "  NV_CUDNN_PACKAGE=libcudnn9-cuda-12=9.8.0.87-1\n",
      "  NV_CUDNN_PACKAGE_DEV=libcudnn9-dev-cuda-12=9.8.0.87-1\n",
      "  NV_CUDNN_PACKAGE_NAME=libcudnn9-cuda-12\n",
      "  NV_CUDNN_VERSION=9.8.0.87-1\n",
      "  NV_LIBNCCL_DEV_PACKAGE=libnccl-dev=2.25.1-1+cuda12.8\n",
      "  NV_LIBNCCL_DEV_PACKAGE_NAME=libnccl-dev\n",
      "  NV_LIBNCCL_DEV_PACKAGE_VERSION=2.25.1-1\n",
      "  NV_LIBNCCL_PACKAGE=libnccl2=2.25.1-1+cuda12.8\n",
      "  NV_LIBNCCL_PACKAGE_NAME=libnccl2\n",
      "  NV_LIBNCCL_PACKAGE_VERSION=2.25.1-1\n",
      "\n",
      "Loading Qwen/Qwen2.5-7B-Instruct in BF16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778177a0a7d5458ba6a63fd9e423ba1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing sequences to determine lengths:\n",
      "  technical :  216 tokens\n",
      "  literary  :  204 tokens\n",
      "  corporate :  193 tokens\n",
      "  legal     :  211 tokens\n",
      "\n",
      "Selected 'technical' as sequence 0 (longest, 216 tokens)\n",
      "Dummy sequences: ['literary', 'corporate', 'legal']\n",
      "CRITICAL: Extracting KEY VECTORS from last layer, not hidden states\n",
      "\n",
      "============================================================\n",
      "Starting H100 KEY VECTOR TEST at 2025-11-03T15:46:27.973488\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Precision: BF16 (bfloat16)\n",
      "Base prompt tokens: 216\n",
      "Operation: Single forward pass, extract KEY vectors\n",
      "CRITICAL: Element 0 input is IDENTICAL across batch sizes\n",
      "Repetitions per batch size: 10\n",
      "============================================================\n",
      "\n",
      "Collecting batch_size=1 (10 repetitions)...\n",
      "  Batch: [technical]\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  Rep 0: norm=920.000000, first_val=0.261719\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  Completed 3/10 repetitions\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  Completed 6/10 repetitions\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  Completed 9/10 repetitions\n",
      "  Single: seq_len=216, valid tokens=216\n",
      "  Key vector: num_heads=4, head_dim=128, total_dim=512\n",
      "  ✓ All 10 repetitions are identical (expected)\n",
      "  Statistical noise: mean=0.000000, std=0.000000\n",
      "  Key vector norm: 920.00\n",
      "\n",
      "Collecting batch_size=2 (10 repetitions)...\n",
      "  Batch: [technical, dummy1] (extracting from elem 0)\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Rep 0: norm=920.000000, first_val=0.259766\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Completed 3/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Completed 6/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Completed 9/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  ✓ All 10 repetitions are identical (expected)\n",
      "  Statistical noise: mean=0.000000, std=0.000000\n",
      "  Key vector norm: 920.00\n",
      "\n",
      "Collecting batch_size=4 (10 repetitions)...\n",
      "  Batch: [technical, dummy1, dummy2, dummy3] (extracting from elem 0)\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Rep 0: norm=920.000000, first_val=0.263672\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Completed 3/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Completed 6/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  Completed 9/10 repetitions\n",
      "  Batch: seq_len=216, elem[0] valid=216, pad=0, differ: True\n",
      "  ✓ All 10 repetitions are identical (expected)\n",
      "  Statistical noise: mean=0.000000, std=0.000000\n",
      "  Key vector norm: 920.00\n",
      "\n",
      "\n",
      "============================================================\n",
      "=== SYSTEMATIC DEVIATION MATRIX (Key Vectors) ===\n",
      "============================================================\n",
      "     bs= 1  bs= 2  bs= 4  \n",
      "bs= 1   -     0.287  0.264 \n",
      "bs= 2  0.287   -     0.252 \n",
      "bs= 4  0.264  0.252   -    \n",
      "\n",
      "============================================================\n",
      "=== KEY VECTOR ANALYSIS ===\n",
      "============================================================\n",
      "Key vector dimension: 512\n",
      "bs=1 mean key norm: 920.00\n",
      "bs=2 mean key norm: 920.00\n",
      "L2 distance (bs1 vs bs2): 0.2871\n",
      "Relative difference: 0.000313\n",
      "Max absolute difference: 0.125000\n",
      "Dimensions with |diff| > 0.01: 117/512\n",
      "\n",
      "============================================================\n",
      "=== VERDICT ===\n",
      "============================================================\n",
      "Element 0 input: 'technical' sequence (IDENTICAL across batch sizes)\n",
      "Extraction method: Concatenated KEY vectors from all attention heads\n",
      "bs1 vs bs2 deviation: 0.287109\n",
      "\n",
      "✓ DETECTION VIABLE: L2=0.2871\n",
      "  Key vectors show systematic deviation from batch size\n",
      "  → Forensics CAN detect hidden batch capacity using keys\n",
      "\n",
      "✓ Results saved to /workspace/NVIDIA_H100_PCIe_key_vectors_exp6_20251103_154629.json\n",
      "✓ File size: ~177.6 KB\n",
      "\n",
      "============================================================\n",
      "KEY VECTOR TEST COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "# Capture system info for verification\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(f\"System Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  Container: {CONTAINER_ID}\")\n",
    "print()\n",
    "\n",
    "# Capture relevant environment variables\n",
    "print(\"Environment Variables:\")\n",
    "env_vars = {}\n",
    "for key in sorted(os.environ.keys()):\n",
    "    if any(x in key.upper() for x in ['CUDA', 'TORCH', 'NCCL', 'CUDNN', 'PYTORCH']):\n",
    "        env_vars[key] = os.environ[key]\n",
    "        print(f\"  {key}={os.environ[key]}\")\n",
    "if not env_vars:\n",
    "    print(\"  (No CUDA/TORCH env vars set)\")\n",
    "print()\n",
    "\n",
    "# Four diverse, realistic sequences\n",
    "SEQUENCES = {\n",
    "    \"technical\": \"\"\"The study investigates the quantum decoherence effects on a multi-qubit superconducting system when subjected to controlled microwave pulses. We utilized a novel cryogenic amplification chain to minimize thermal noise and achieve a signal-to-noise ratio previously unattainable in similar setups. The experimental protocol involved preparing the qubits in a Greenberger-Horne-Zeilinger (GHZ) state and then measuring the decay of quantum entanglement over time by performing state tomography. Our results demonstrate a non-linear relationship between pulse amplitude and coherence time, suggesting that higher-order coupling terms, often neglected in theoretical models, play a significant role in system dynamics. The empirical data were cross-validated against a master equation simulation incorporating a 1/f noise model, showing strong correlation (R² > 0.98). These findings provide crucial insights for the development of fault-tolerant quantum computing architectures and the calibration of error-correction codes. Further research will focus on isolating the specific mechanisms responsible for the observed decoherence pathways and exploring potential mitigation strategies through dynamic decoupling sequences.\"\"\",\n",
    "    \n",
    "    \"literary\": \"\"\"Eleanor traced the rim of her chipped porcelain teacup, the warmth a feeble defense against the morning's persistent chill. Outside, the rain drew gray, wavering lines down the windowpane, blurring the world into an impressionist's watercolor of the street she had known for fifty years. Each object in her small parlor was a relic, a silent testament to a life lived in quiet increments: the grandfather clock that had stopped at half-past three the day Arthur left for the war, the faded photograph on the mantelpiece of a girl with her own smile but brighter eyes, the worn velvet armchair that still held the faint scent of her husband's pipe tobacco. She wasn't lonely, she told herself, but rather, well-acquainted with solitude. It was a familiar garment, threadbare in places, but comfortable. The silence wasn't empty; it was filled with the echoes of every laugh, every argument, and every whispered promise that had ever inhabited those walls.\"\"\",\n",
    "    \n",
    "    \"corporate\": \"\"\"In the third quarter, our strategic pivot towards a subscription-based revenue model has yielded promising preliminary results, demonstrating enhanced customer lifetime value and more predictable revenue streams. We successfully onboarded 15 new enterprise-level clients, exceeding our initial target by 25%. This growth was largely driven by the targeted digital marketing campaign launched in July, which achieved a 40% higher conversion rate than previous initiatives. However, we did face headwinds in the APAC region due to unforeseen supply chain disruptions and increased logistical costs, which compressed our gross margin by approximately 150 basis points. To mitigate these challenges, the operations team has initiated a comprehensive vendor diversification program and is exploring near-shoring opportunities. Looking ahead to Q4, our primary focus will be on optimizing the user onboarding experience to reduce churn and leveraging our data analytics capabilities to identify key upselling opportunities within our existing customer base. We remain confident in our full-year financial outlook.\"\"\",\n",
    "    \n",
    "    \"legal\": \"\"\"This End-User License Agreement (\"EULA\") constitutes a legally binding contract between you, the end-user (\"Licensee\"), and a fictitious corporation (\"Licensor\") for the software product accompanying this EULA. By installing, copying, or otherwise using the Software, Licensee agrees to be bound by the terms herein. Licensor grants Licensee a limited, non-exclusive, non-transferable license to use the Software for personal, non-commercial purposes. Licensee is expressly prohibited from reverse-engineering, decompiling, disassembling, or creating derivative works based on the Software. All rights, title, and interest, including but not limited to intellectual property rights, in and to the Software shall remain with Licensor. The Software is provided \"as is\" without warranty of any kind. In no event shall Licensor be liable for any direct, indirect, consequential, or incidental damages arising from the use or inability to use the Software. This Agreement is governed by the laws of the specified jurisdiction, without regard to its conflict of law provisions.\"\"\"\n",
    "}\n",
    "\n",
    "def collect_key_vectors(model, tokenizer, base_prompt, dummy_prompts, batch_size=1, device=\"cuda\"):\n",
    "    \"\"\"Forward pass extracting KEY VECTORS from last layer instead of hidden states\n",
    "    \n",
    "    CRITICAL: Element 0 has IDENTICAL input across all batch sizes\n",
    "    Extract concatenated key vectors from ALL attention heads at last valid token position\n",
    "    \n",
    "    For GQA (Grouped Query Attention), keys are shared across query groups.\n",
    "    We extract: [num_key_value_heads, head_dim] → flatten to [num_key_value_heads * head_dim]\n",
    "    \"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if batch_size == 1:\n",
    "        prompts = [base_prompt]\n",
    "    else:\n",
    "        prompts = [base_prompt] + dummy_prompts[:batch_size-1]\n",
    "    \n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # DEBUG: Verify setup\n",
    "    actual_batch_size = inputs['input_ids'].shape[0]\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    if actual_batch_size != batch_size:\n",
    "        print(f\"WARNING: Expected batch_size={batch_size}, got {actual_batch_size}\")\n",
    "    \n",
    "    # Check element 0's valid token count\n",
    "    elem0_valid_tokens = inputs['attention_mask'][0].sum().item()\n",
    "    elem0_padding = seq_len - elem0_valid_tokens\n",
    "    \n",
    "    if batch_size > 1:\n",
    "        elem0_last5 = inputs['input_ids'][0, -5:].tolist()\n",
    "        elem1_last5 = inputs['input_ids'][1, -5:].tolist()\n",
    "        different = elem0_last5 != elem1_last5\n",
    "        print(f\"  Batch: seq_len={seq_len}, elem[0] valid={elem0_valid_tokens}, pad={elem0_padding}, differ: {different}\")\n",
    "    else:\n",
    "        print(f\"  Single: seq_len={seq_len}, valid tokens={elem0_valid_tokens}\")\n",
    "    \n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # CRITICAL: use_cache=True to get key-value cache\n",
    "        outputs = model(**inputs, use_cache=True, return_dict=True)\n",
    "    \n",
    "    # Extract key vectors from last layer\n",
    "    # past_key_values is a tuple of (key, value) pairs for each layer\n",
    "    # Shape: [batch_size, num_key_value_heads, seq_len, head_dim]\n",
    "    last_layer_keys = outputs.past_key_values[-1][0]  # [0] for keys, [1] would be values\n",
    "    \n",
    "    # Extract from element 0's last valid token position, all heads\n",
    "    last_valid_pos = inputs['attention_mask'][0].sum() - 1\n",
    "    \n",
    "    # Shape: [num_key_value_heads, head_dim]\n",
    "    key_vector = last_layer_keys[0, :, last_valid_pos, :]\n",
    "    \n",
    "    # Flatten to concatenate all heads: [num_key_value_heads * head_dim]\n",
    "    key_vector_flat = key_vector.reshape(-1).cpu().clone()\n",
    "    \n",
    "    # Debug info for first run\n",
    "    if batch_size == 1:\n",
    "        print(f\"  Key vector: num_heads={key_vector.shape[0]}, head_dim={key_vector.shape[1]}, total_dim={key_vector_flat.shape[0]}\")\n",
    "    \n",
    "    del outputs\n",
    "    del inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return key_vector_flat\n",
    "\n",
    "# Setup\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "EXP_NUMBER = 6  # Key vectors experiment\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(f\"Loading {model_name} in BF16...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    cache_dir=CACHE_DIR,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Tokenize all sequences and find the longest\n",
    "print(\"\\nTokenizing sequences to determine lengths:\")\n",
    "token_counts = {}\n",
    "for name, text in SEQUENCES.items():\n",
    "    tokens = tokenizer.encode(text)\n",
    "    token_counts[name] = len(tokens)\n",
    "    print(f\"  {name:10s}: {len(tokens):4d} tokens\")\n",
    "\n",
    "# Select longest as base_prompt\n",
    "longest_name = max(token_counts, key=token_counts.get)\n",
    "base_prompt = SEQUENCES[longest_name]\n",
    "dummy_prompts = [text for name, text in SEQUENCES.items() if name != longest_name]\n",
    "\n",
    "print(f\"\\nSelected '{longest_name}' as sequence 0 (longest, {token_counts[longest_name]} tokens)\")\n",
    "print(f\"Dummy sequences: {[name for name in SEQUENCES.keys() if name != longest_name]}\")\n",
    "print(f\"CRITICAL: Extracting KEY VECTORS from last layer, not hidden states\\n\")\n",
    "\n",
    "# Test batch sizes 1, 2, 4 with 10 repetitions each\n",
    "batch_sizes = [1, 2, 4]\n",
    "num_repetitions = 10\n",
    "results = {}\n",
    "all_key_vectors = {}\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Starting H100 KEY VECTOR TEST at {datetime.now().isoformat()}\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Precision: BF16 (bfloat16)\")\n",
    "print(f\"Base prompt tokens: {token_counts[longest_name]}\")\n",
    "print(f\"Operation: Single forward pass, extract KEY vectors\")\n",
    "print(f\"CRITICAL: Element 0 input is IDENTICAL across batch sizes\")\n",
    "print(f\"Repetitions per batch size: {num_repetitions}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    print(f\"Collecting batch_size={bs} ({num_repetitions} repetitions)...\")\n",
    "    if bs == 1:\n",
    "        print(f\"  Batch: [{longest_name}]\")\n",
    "    elif bs == 2:\n",
    "        print(f\"  Batch: [{longest_name}, dummy1] (extracting from elem 0)\")\n",
    "    else:\n",
    "        print(f\"  Batch: [{longest_name}, dummy1, dummy2, dummy3] (extracting from elem 0)\")\n",
    "    \n",
    "    runs = []\n",
    "    for rep in range(num_repetitions):\n",
    "        key_vec = collect_key_vectors(model, tokenizer, base_prompt, dummy_prompts, batch_size=bs, device=\"cuda\")\n",
    "        runs.append(key_vec)\n",
    "        if rep == 0:\n",
    "            print(f\"  Rep 0: norm={torch.norm(key_vec).item():.6f}, first_val={key_vec[0].item():.6f}\")\n",
    "        if (rep + 1) % 3 == 0:\n",
    "            print(f\"  Completed {rep + 1}/{num_repetitions} repetitions\")\n",
    "    \n",
    "    # Check repeatability\n",
    "    first_rep = runs[0]\n",
    "    all_identical = all(torch.equal(first_rep, runs[i]) for i in range(1, num_repetitions))\n",
    "    if all_identical:\n",
    "        print(f\"  ✓ All {num_repetitions} repetitions are identical (expected)\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Repetitions vary (unexpected!)\")\n",
    "    \n",
    "    results[bs] = torch.stack(runs)\n",
    "    all_key_vectors[f\"batch_size_{bs}\"] = results[bs].float().numpy().tolist()\n",
    "    \n",
    "    mean_key = results[bs].mean(dim=0)\n",
    "    deviations = torch.stack([torch.norm(results[bs][i] - mean_key) for i in range(num_repetitions)])\n",
    "    std_noise = deviations.std().item()\n",
    "    mean_noise = deviations.mean().item()\n",
    "    \n",
    "    print(f\"  Statistical noise: mean={mean_noise:.6f}, std={std_noise:.6f}\")\n",
    "    print(f\"  Key vector norm: {torch.norm(mean_key).item():.2f}\\n\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Compare systematic deviations\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== SYSTEMATIC DEVIATION MATRIX (Key Vectors) ===\")\n",
    "print(\"=\"*60)\n",
    "print(\"     \", end=\"\")\n",
    "for bs in batch_sizes:\n",
    "    print(f\"bs={bs:2d}  \", end=\"\")\n",
    "print()\n",
    "\n",
    "systematic_deviations = {}\n",
    "for bs1 in batch_sizes:\n",
    "    print(f\"bs={bs1:2d} \", end=\"\")\n",
    "    for bs2 in batch_sizes:\n",
    "        if bs1 == bs2:\n",
    "            print(\"  -    \", end=\"\")\n",
    "        else:\n",
    "            mean1 = results[bs1].mean(dim=0)\n",
    "            mean2 = results[bs2].mean(dim=0)\n",
    "            l2 = torch.norm(mean1 - mean2).item()\n",
    "            systematic_deviations[f\"bs{bs1}_vs_bs{bs2}\"] = l2\n",
    "            print(f\"{l2:6.3f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== KEY VECTOR ANALYSIS ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "bs1_mean = results[1].mean(dim=0)\n",
    "bs2_mean = results[2].mean(dim=0)\n",
    "\n",
    "print(f\"Key vector dimension: {bs1_mean.shape[0]}\")\n",
    "print(f\"bs=1 mean key norm: {torch.norm(bs1_mean).item():.2f}\")\n",
    "print(f\"bs=2 mean key norm: {torch.norm(bs2_mean).item():.2f}\")\n",
    "print(f\"L2 distance (bs1 vs bs2): {torch.norm(bs1_mean - bs2_mean).item():.4f}\")\n",
    "if torch.norm(bs1_mean) > 0:\n",
    "    print(f\"Relative difference: {(torch.norm(bs1_mean - bs2_mean) / torch.norm(bs1_mean)).item():.6f}\")\n",
    "\n",
    "diff = (bs1_mean - bs2_mean).abs()\n",
    "print(f\"Max absolute difference: {diff.max().item():.6f}\")\n",
    "print(f\"Dimensions with |diff| > 0.01: {(diff > 0.01).sum().item()}/{diff.shape[0]}\")\n",
    "\n",
    "# CRITICAL CHECK\n",
    "bs1_vs_bs2_deviation = systematic_deviations.get(\"bs1_vs_bs2\", 0)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== VERDICT ===\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Element 0 input: '{longest_name}' sequence (IDENTICAL across batch sizes)\")\n",
    "print(f\"Extraction method: Concatenated KEY vectors from all attention heads\")\n",
    "print(f\"bs1 vs bs2 deviation: {bs1_vs_bs2_deviation:.6f}\\n\")\n",
    "\n",
    "if bs1_vs_bs2_deviation > 0.1:\n",
    "    print(f\"✓ DETECTION VIABLE: L2={bs1_vs_bs2_deviation:.4f}\")\n",
    "    print(f\"  Key vectors show systematic deviation from batch size\")\n",
    "    print(f\"  → Forensics CAN detect hidden batch capacity using keys\")\n",
    "elif bs1_vs_bs2_deviation > 0.001:\n",
    "    print(f\"⚠ WEAK SIGNAL: L2={bs1_vs_bs2_deviation:.6f}\")\n",
    "    print(f\"  Small but detectable effect in key vectors\")\n",
    "    print(f\"  → Forensics might work with careful analysis\")\n",
    "else:\n",
    "    print(f\"✗ NO DETECTION: L2={bs1_vs_bs2_deviation:.6f}\")\n",
    "    print(f\"  Key vectors show no batch size effect\")\n",
    "    print(f\"  → Cannot detect hidden capacity using key vectors\")\n",
    "\n",
    "# Save results\n",
    "output = {\n",
    "    \"experiment\": \"H100_key_vector_forensics\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"pytorch\": torch.__version__,\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"environment\": env_vars,\n",
    "    \"config\": {\n",
    "        \"batch_sizes\": batch_sizes,\n",
    "        \"repetitions\": num_repetitions,\n",
    "        \"operation\": \"single_forward_pass_key_vectors\",\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"use_cache\": True,\n",
    "        \"extraction_method\": \"concatenated_keys_last_layer_all_heads\",\n",
    "        \"input_strategy\": \"diverse_realistic_sequences\",\n",
    "        \"sequence_0\": longest_name,\n",
    "        \"sequence_0_tokens\": token_counts[longest_name],\n",
    "        \"all_sequence_tokens\": token_counts,\n",
    "        \"key_vector_dim\": int(bs1_mean.shape[0])\n",
    "    },\n",
    "    \"statistical_noise\": {\n",
    "        f\"batch_size_{bs}\": {\n",
    "            \"mean\": float(torch.stack([torch.norm(results[bs][i] - results[bs].mean(dim=0)) \n",
    "                                       for i in range(num_repetitions)]).mean()),\n",
    "            \"std\": float(torch.stack([torch.norm(results[bs][i] - results[bs].mean(dim=0)) \n",
    "                                     for i in range(num_repetitions)]).std())\n",
    "        }\n",
    "        for bs in batch_sizes\n",
    "    },\n",
    "    \"systematic_deviations\": systematic_deviations,\n",
    "    \"key_vector_norms\": {\n",
    "        f\"batch_size_{bs}\": float(torch.norm(results[bs].mean(dim=0))) \n",
    "        for bs in batch_sizes\n",
    "    },\n",
    "    \"forensics_result\": {\n",
    "        \"bs1_vs_bs2_deviation\": bs1_vs_bs2_deviation,\n",
    "        \"detection_viable\": bs1_vs_bs2_deviation > 0.1\n",
    "    },\n",
    "    \"raw_key_vectors\": all_key_vectors\n",
    "}\n",
    "\n",
    "output_file = f\"{torch.cuda.get_device_name(0).replace(' ', '_')}_key_vectors_exp{EXP_NUMBER}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "output_path = f\"/workspace/{output_file}\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(f\"✓ File size: ~{len(json.dumps(output)) / 1024:.1f} KB\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY VECTOR TEST COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a439fb-c84d-4c8e-9cff-eec159b57e35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
