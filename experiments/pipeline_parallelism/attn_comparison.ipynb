{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74326c60-2459-41b3-8956-b50145ce025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CHECKING FLASHATTENTION AVAILABILITY\n",
      "============================================================\n",
      "âœ“ FlashAttention installed: version 2.8.3\n",
      "  FlashAttention 2 API: âœ“ Available\n",
      "\n",
      "âœ“ FlashAttention available - proceeding with test\n",
      "\n",
      "============================================================\n",
      "CLEANING GPU MEMORY\n",
      "============================================================\n",
      "GPU memory allocated: 0.00 GB\n",
      "\n",
      "System Info:\n",
      "  Hostname: 15bebaaf0ce0\n",
      "  GPU: NVIDIA H100 PCIe\n",
      "  PyTorch: 2.8.0+cu128\n",
      "  CUDA: 12.8\n",
      "\n",
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cf8ec808034e50956f4240843ad32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737771d521734a85a8f9124ff8ff92a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8ff7e42c5f94109bf0e995365dd2347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca90c3b257094b7cb507b6427cb9d235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  Warning: dummytext.txt not found, using default prompt\n",
      "Test prompt: 55 tokens\n",
      "\n",
      "============================================================\n",
      "FLASHATTENTION MULTI-LAYER FORENSICS\n",
      "Model: Qwen/Qwen2.5-7B-Instruct\n",
      "Precision: BF16 (bfloat16)\n",
      "Prompt tokens: 55\n",
      "Strategy: Extract activations from 8 layers [1,4,7,10,14,18,22,28]\n",
      "Purpose: Fine-grained error propagation tracking\n",
      "Repetitions per condition: 5\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TESTING: eager\n",
      "============================================================\n",
      "\n",
      "Loading model with eager attention...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "002e669c207b4e5e8cc6712b1c4d7970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d41a3de0170468daabeba2e422d49d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bb7a2c7fde4549a78f63d3e0d9ccc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef336efef1834cab98023f616422333e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c6e6d588314ec3b4c6c28ad9669dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d556b4539f4e12ac59c2854a09397e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import sys\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: CHECK FLASHATTENTION AVAILABILITY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING FLASHATTENTION AVAILABILITY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    import flash_attn\n",
    "    FLASH_ATTN_VERSION = flash_attn.__version__ if hasattr(flash_attn, '__version__') else \"unknown\"\n",
    "    print(f\"âœ“ FlashAttention installed: version {FLASH_ATTN_VERSION}\")\n",
    "    \n",
    "    has_fa2 = hasattr(flash_attn, 'flash_attn_func')\n",
    "    print(f\"  FlashAttention 2 API: {'âœ“ Available' if has_fa2 else 'âœ— Not found'}\")\n",
    "    \n",
    "    if not has_fa2:\n",
    "        print(\"\\nâœ— ERROR: FlashAttention 2 API not found\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(\"\\nâœ“ FlashAttention available - proceeding with test\\n\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(\"âœ— FlashAttention NOT installed\")\n",
    "    print(f\"   Import error: {e}\")\n",
    "    print(\"\\nInstall with: pip install flash-attn --no-build-isolation\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: SYSTEM INFO AND MEMORY CLEANUP\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLEANING GPU MEMORY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    mem_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"GPU memory allocated: {mem_before:.2f} GB\")\n",
    "    if mem_before > 10:\n",
    "        print(f\"âš  WARNING: {mem_before:.1f} GB already allocated - consider restarting Python\")\n",
    "print()\n",
    "\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(f\"System Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA: {torch.version.cuda}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def verify_attention_implementation(model):\n",
    "    \"\"\"Check what attention implementation the model is using via config\"\"\"\n",
    "    config_impl = getattr(model.config, '_attn_implementation', 'not set')\n",
    "    print(f\"  Model config._attn_implementation: {config_impl}\")\n",
    "    print(f\"  Model config.attn_implementation: {getattr(model.config, 'attn_implementation', 'not set')}\")\n",
    "    \n",
    "    # Check first attention layer class (just for info)\n",
    "    first_layer = model.model.layers[0]\n",
    "    attn_class_name = first_layer.self_attn.__class__.__name__\n",
    "    print(f\"  Actual attention class: {attn_class_name}\")\n",
    "    \n",
    "    # In transformers 4.57.1+, there's only one Qwen2Attention class\n",
    "    # that dispatches internally based on config._attn_implementation\n",
    "    is_using_flash = (config_impl == \"flash_attention_2\")\n",
    "    print(f\"  Using FlashAttention: {is_using_flash}\")\n",
    "    \n",
    "    return attn_class_name, is_using_flash\n",
    "\n",
    "def load_model_with_attention(attn_impl, cache_dir):\n",
    "    \"\"\"Load model with specified attention implementation\"\"\"\n",
    "    print(f\"\\nLoading model with {attn_impl} attention...\")\n",
    "    \n",
    "    # Clear GPU before loading\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=cache_dir,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=attn_impl\n",
    "    )\n",
    "    \n",
    "    # Verify what we got\n",
    "    attn_class, is_using_flash = verify_attention_implementation(model)\n",
    "    \n",
    "    mem_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"  GPU memory after load: {mem_after:.2f} GB\")\n",
    "    \n",
    "    return model, attn_class, is_using_flash\n",
    "\n",
    "def collect_activations_multilayer(model, tokenizer, prompt, device=\"cuda\"):\n",
    "    \"\"\"Extract hidden states from multiple layers to check error propagation\"\"\"\n",
    "    # Ensure all prior ops are complete and GPU is clean\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "    seq_len = inputs['input_ids'].shape[1]\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True, use_cache=False)\n",
    "    \n",
    "    last_valid_pos = inputs['attention_mask'][0].sum() - 1\n",
    "    \n",
    "    # Extract from multiple layers\n",
    "    # outputs.hidden_states[0] is embedding layer\n",
    "    # outputs.hidden_states[1] is after first transformer layer\n",
    "    # outputs.hidden_states[-1] is after last transformer layer\n",
    "    num_layers = len(outputs.hidden_states) - 1  # -1 because first is embedding\n",
    "    \n",
    "    # Sample: layers across the network for fine-grained divergence tracking\n",
    "    # Dense sampling in early layers (1-4) to catch initial divergence\n",
    "    layer_indices = [1, 2, 3, 4, 7, 10, 14, 18, 22, num_layers]\n",
    "    \n",
    "    # Extract immediately and move to CPU to minimize GPU memory usage\n",
    "    activations = {}\n",
    "    for idx in layer_indices:\n",
    "        # Clone and move to CPU immediately, then delete reference\n",
    "        layer_activation = outputs.hidden_states[idx][0, last_valid_pos, :].cpu().clone()\n",
    "        activations[f\"layer_{idx}\"] = layer_activation\n",
    "    \n",
    "    # Aggressive cleanup - delete everything from GPU immediately\n",
    "    del outputs.hidden_states  # Delete the tuple of hidden states first\n",
    "    del outputs\n",
    "    del inputs\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    return activations, seq_len, layer_indices\n",
    "\n",
    "def unload_model(model):\n",
    "    \"\"\"Completely remove model from memory\"\"\"\n",
    "    print(f\"  Unloading model...\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    mem_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"  GPU memory after unload: {mem_after:.2f} GB\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: SETUP\n",
    "# ============================================================================\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load prompt from file\n",
    "prompt_file = \"dummytext.txt\"\n",
    "try:\n",
    "    with open(prompt_file, 'r', encoding='utf-8') as f:\n",
    "        prompt = f.read().strip()\n",
    "    print(f\"âœ“ Loaded prompt from {prompt_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âš  Warning: {prompt_file} not found, using default prompt\")\n",
    "    prompt = \"\"\"The automated data-processing pipeline ingests raw telemetry from distributed sensors across multiple geographic locations. A proprietary algorithm then normalizes the dataset, filtering for anomalies based on predefined statistical parameters derived from historical patterns. The resulting output is a clean, structured matrix ready for machine learning model ingestion.\"\"\"\n",
    "\n",
    "prompt_tokens = len(tokenizer.encode(prompt))\n",
    "print(f\"Test prompt: {prompt_tokens} tokens\")\n",
    "print()\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('/workspace/experiments', exist_ok=True)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: RUN EXPERIMENTS\n",
    "# ============================================================================\n",
    "\n",
    "num_repetitions = 5\n",
    "results = {}  # Will store {condition: {layer_name: tensor}}\n",
    "all_activations = {}\n",
    "attention_info = {}\n",
    "layer_indices = None\n",
    "\n",
    "conditions = [\n",
    "    (\"eager\", \"eager\"),\n",
    "    (\"flash_attention_2\", \"flash_attention_2\")\n",
    "]\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"FLASHATTENTION MULTI-LAYER FORENSICS\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Precision: BF16 (bfloat16)\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Strategy: Extract activations from 8 layers [1,4,7,10,14,18,22,28]\")\n",
    "print(f\"Purpose: Fine-grained error propagation tracking\")\n",
    "print(f\"Repetitions per condition: {num_repetitions}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for condition_name, attn_impl in conditions:\n",
    "    print(f\"=\"*60)\n",
    "    print(f\"TESTING: {condition_name}\")\n",
    "    print(f\"=\"*60)\n",
    "    \n",
    "    # Load model fresh for this condition\n",
    "    model, attn_class, is_using_flash = load_model_with_attention(attn_impl, CACHE_DIR)\n",
    "    \n",
    "    mem_after_load = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"  GPU memory after model load: {mem_after_load:.2f} GB\")\n",
    "    \n",
    "    attention_info[condition_name] = {\n",
    "        \"requested\": attn_impl,\n",
    "        \"actual_class\": attn_class,\n",
    "        \"is_using_flash\": is_using_flash,\n",
    "        \"config_attn_implementation\": model.config._attn_implementation\n",
    "    }\n",
    "    \n",
    "    # Collect activations\n",
    "    print(f\"\\nCollecting multi-layer activations ({num_repetitions} repetitions)...\")\n",
    "    \n",
    "    # Initialize storage for this condition\n",
    "    results[condition_name] = {}\n",
    "    \n",
    "    for rep in range(num_repetitions):\n",
    "        mem_before = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        \n",
    "        activations, seq_len, layer_idx_list = collect_activations_multilayer(\n",
    "            model, tokenizer, prompt, device=\"cuda\"\n",
    "        )\n",
    "        \n",
    "        mem_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "        \n",
    "        # First time through, set up layer indices and storage\n",
    "        if layer_indices is None:\n",
    "            layer_indices = layer_idx_list\n",
    "            print(f\"  Extracting from layers: {layer_indices}\")\n",
    "        \n",
    "        # Initialize storage on first rep of each condition\n",
    "        if rep == 0:\n",
    "            for layer_name in activations.keys():\n",
    "                results[condition_name][layer_name] = []\n",
    "        \n",
    "        # Store activations for each layer\n",
    "        for layer_name, activation in activations.items():\n",
    "            results[condition_name][layer_name].append(activation)\n",
    "        \n",
    "        if rep == 0:\n",
    "            print(f\"  Rep 0 norms: {', '.join([f'{k}={torch.norm(v).item():.2f}' for k, v in activations.items()])}\")\n",
    "            print(f\"  Rep 0 memory: before={mem_before:.2f}GB, after={mem_after:.2f}GB, delta={mem_after-mem_before:.3f}GB\")\n",
    "        if (rep + 1) % 3 == 0:\n",
    "            mem_current = torch.cuda.memory_allocated(0) / 1024**3\n",
    "            print(f\"  Completed {rep + 1}/{num_repetitions} repetitions (GPU: {mem_current:.2f} GB)\")\n",
    "        \n",
    "        # Aggressive memory cleanup after each repetition\n",
    "        del activations\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Stack repetitions into tensors\n",
    "    for layer_name in results[condition_name].keys():\n",
    "        results[condition_name][layer_name] = torch.stack(results[condition_name][layer_name])\n",
    "    \n",
    "    # Aggressive cleanup after stacking\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Check repeatability for last layer\n",
    "    last_layer_name = f\"layer_{layer_indices[-1]}\"\n",
    "    first_rep = results[condition_name][last_layer_name][0]\n",
    "    all_identical = all(\n",
    "        torch.equal(first_rep, results[condition_name][last_layer_name][i]) \n",
    "        for i in range(1, num_repetitions)\n",
    "    )\n",
    "    print(f\"  Repeatability (last layer): {'âœ“ All identical' if all_identical else 'âš  Varies'}\")\n",
    "    \n",
    "    # Convert to numpy/lists for JSON BEFORE next condition\n",
    "    # This frees the GPU tensors\n",
    "    condition_activations = {}\n",
    "    for layer_name, tensor in results[condition_name].items():\n",
    "        condition_activations[layer_name] = tensor.float().cpu().numpy().tolist()\n",
    "        # Delete the GPU tensor immediately after conversion\n",
    "        del tensor\n",
    "    \n",
    "    all_activations[condition_name] = condition_activations\n",
    "    \n",
    "    # Clear the GPU tensors from results to save memory\n",
    "    # (we'll need them for analysis so keep only CPU copies)\n",
    "    results_cpu = {\n",
    "        layer_name: tensor.cpu().float() \n",
    "        for layer_name, tensor in results[condition_name].items()\n",
    "    }\n",
    "    results[condition_name] = results_cpu\n",
    "    \n",
    "    # Final cleanup before unloading model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print()\n",
    "    unload_model(model)\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: LAYER-BY-LAYER ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"=== ATTENTION IMPLEMENTATION VERIFICATION ===\")\n",
    "print(\"=\"*60)\n",
    "for condition_name, info in attention_info.items():\n",
    "    print(f\"\\n{condition_name}:\")\n",
    "    print(f\"  Requested: {info['requested']}\")\n",
    "    print(f\"  Config reports: {info['config_attn_implementation']}\")\n",
    "    print(f\"  Actual class: {info['actual_class']}\")\n",
    "    print(f\"  Using FlashAttention: {info['is_using_flash']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== LAYER-BY-LAYER DEVIATION ANALYSIS ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "layer_deviations = {}\n",
    "layer_names = [f\"layer_{idx}\" for idx in layer_indices]\n",
    "\n",
    "print(f\"\\nComparing: eager vs flash_attention_2\")\n",
    "print(f\"Layers analyzed: {layer_indices}\\n\")\n",
    "\n",
    "for layer_name, layer_idx in zip(layer_names, layer_indices):\n",
    "    eager_mean = results[\"eager\"][layer_name].mean(dim=0)\n",
    "    flash_mean = results[\"flash_attention_2\"][layer_name].mean(dim=0)\n",
    "    \n",
    "    l2_distance = torch.norm(eager_mean - flash_mean).item()\n",
    "    relative_diff = (l2_distance / torch.norm(eager_mean)).item() if torch.norm(eager_mean) > 0 else 0\n",
    "    \n",
    "    diff = (eager_mean - flash_mean).abs()\n",
    "    max_diff = diff.max().item()\n",
    "    dims_affected = (diff > 0.01).sum().item()\n",
    "    dims_total = diff.shape[0]\n",
    "    \n",
    "    layer_deviations[layer_name] = {\n",
    "        \"layer_index\": layer_idx,\n",
    "        \"l2_distance\": l2_distance,\n",
    "        \"relative_difference\": relative_diff,\n",
    "        \"max_absolute_diff\": max_diff,\n",
    "        \"dims_affected\": dims_affected,\n",
    "        \"dims_total\": dims_total,\n",
    "        \"eager_norm\": float(torch.norm(eager_mean)),\n",
    "        \"flash_norm\": float(torch.norm(flash_mean))\n",
    "    }\n",
    "    \n",
    "    print(f\"Layer {layer_idx}:\")\n",
    "    print(f\"  L2 distance: {l2_distance:.6f}\")\n",
    "    print(f\"  Relative diff: {relative_diff:.6f} ({relative_diff*100:.3f}%)\")\n",
    "    print(f\"  Max |diff|: {max_diff:.6f}\")\n",
    "    print(f\"  Dims with |diff| > 0.01: {dims_affected}/{dims_total}\")\n",
    "    print(f\"  Eager norm: {torch.norm(eager_mean).item():.2f}\")\n",
    "    print(f\"  Flash norm: {torch.norm(flash_mean).item():.2f}\")\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: ERROR PROPAGATION ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"=== ERROR PROPAGATION DIAGNOSIS ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if both actually used different implementations\n",
    "eager_config = attention_info[\"eager\"][\"config_attn_implementation\"]\n",
    "flash_config = attention_info[\"flash_attention_2\"][\"config_attn_implementation\"]\n",
    "\n",
    "if eager_config == \"eager\" and flash_config == \"flash_attention_2\":\n",
    "    print(f\"âœ“ Confirmed: Comparing eager vs FlashAttention 2\\n\")\n",
    "    \n",
    "    # Analyze progression\n",
    "    l2_distances = [layer_deviations[f\"layer_{idx}\"][\"l2_distance\"] for idx in layer_indices]\n",
    "    relative_diffs = [layer_deviations[f\"layer_{idx}\"][\"relative_difference\"] for idx in layer_indices]\n",
    "    \n",
    "    print(f\"L2 distance progression:\")\n",
    "    for idx, l2 in zip(layer_indices, l2_distances):\n",
    "        print(f\"  Layer {idx}: {l2:.6f}\")\n",
    "    \n",
    "    print(f\"\\nRelative difference progression:\")\n",
    "    for idx, rel in zip(layer_indices, relative_diffs):\n",
    "        print(f\"  Layer {idx}: {rel:.6f} ({rel*100:.3f}%)\")\n",
    "    \n",
    "    # Check if error is growing\n",
    "    is_growing = all(l2_distances[i] <= l2_distances[i+1] for i in range(len(l2_distances)-1))\n",
    "    growth_rate = l2_distances[-1] / l2_distances[0] if l2_distances[0] > 0 else float('inf')\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"DIAGNOSIS:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if is_growing:\n",
    "        print(f\"âœ“ ERROR PROPAGATION CONFIRMED\")\n",
    "        print(f\"  Deviation grows across layers: {is_growing}\")\n",
    "        print(f\"  Growth factor (firstâ†’last): {growth_rate:.2f}x\")\n",
    "        print(f\"  First layer L2: {l2_distances[0]:.6f}\")\n",
    "        print(f\"  Last layer L2: {l2_distances[-1]:.6f}\")\n",
    "        \n",
    "        if l2_distances[0] < 1.0:\n",
    "            print(f\"\\nâœ“ LEGITIMATE: Small initial deviation suggests genuine\")\n",
    "            print(f\"  algorithmic difference accumulating through layers\")\n",
    "        else:\n",
    "            print(f\"\\nâš  SUSPICIOUS: Large deviation even in first layer\")\n",
    "            print(f\"  May indicate models differ in more than just attention\")\n",
    "    else:\n",
    "        print(f\"âš  UNEXPECTED PATTERN\")\n",
    "        print(f\"  Deviation does NOT grow monotonically\")\n",
    "        print(f\"  This is unusual for error propagation\")\n",
    "        print(f\"  Possible issues:\")\n",
    "        print(f\"    - Models not actually using different implementations\")\n",
    "        print(f\"    - Normalization layers resetting accumulated error\")\n",
    "        print(f\"    - Other systematic differences beyond attention\")\n",
    "    \n",
    "    # Final assessment\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"VERIFICATION VIABILITY:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    last_l2 = l2_distances[-1]\n",
    "    if last_l2 > 50:\n",
    "        print(f\"ðŸ“Š EXCELLENT SIGNAL: L2={last_l2:.1f}\")\n",
    "        print(f\"  This deviation is easily detectable for forensics\")\n",
    "    elif last_l2 > 10:\n",
    "        print(f\"âœ“ STRONG SIGNAL: L2={last_l2:.1f}\")\n",
    "        print(f\"  Clearly detectable for forensics applications\")\n",
    "    elif last_l2 > 1:\n",
    "        print(f\"âœ“ DETECTABLE: L2={last_l2:.1f}\")\n",
    "        print(f\"  Forensics feasible with careful measurement\")\n",
    "    elif last_l2 > 0.1:\n",
    "        print(f\"âš  WEAK SIGNAL: L2={last_l2:.3f}\")\n",
    "        print(f\"  May require multiple samples\")\n",
    "    else:\n",
    "        print(f\"âœ— POOR SIGNAL: L2={last_l2:.3f}\")\n",
    "        print(f\"  Below practical detection threshold\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âš  WARNING: Config mismatch detected\")\n",
    "    print(f\"  Eager config: {eager_config}\")\n",
    "    print(f\"  Flash config: {flash_config}\")\n",
    "    print(f\"  Results may not reflect true implementation differences\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output = {\n",
    "    \"experiment\": \"flashattention_multilayer_forensics\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"gpu\": torch.cuda.get_device_name(0),\n",
    "        \"pytorch\": torch.__version__,\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"flash_attention\": {\n",
    "        \"version\": FLASH_ATTN_VERSION\n",
    "    },\n",
    "    \"attention_implementations\": attention_info,\n",
    "    \"config\": {\n",
    "        \"strategy\": \"reload_model_extract_multiple_layers\",\n",
    "        \"layers_extracted\": layer_indices,\n",
    "        \"conditions_tested\": [c[0] for c in conditions],\n",
    "        \"repetitions\": num_repetitions,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"hidden_dim\": dims_total\n",
    "    },\n",
    "    \"layer_by_layer_deviations\": layer_deviations,\n",
    "    \"error_propagation_analysis\": {\n",
    "        \"is_growing\": is_growing,\n",
    "        \"growth_rate\": growth_rate,\n",
    "        \"l2_progression\": l2_distances,\n",
    "        \"relative_diff_progression\": relative_diffs\n",
    "    },\n",
    "    \"raw_activations\": all_activations\n",
    "}\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0).replace(' ', '_')\n",
    "output_file = f\"{gpu_name}_flashattn_multilayer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "output_path = f\"/workspace/experiments/{output_file}\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Results saved to {output_path}\")\n",
    "print(f\"âœ“ File size: ~{len(json.dumps(output)) / 1024:.1f} KB\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36368837-7ce3-4266-ae47-33f621ce6530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
