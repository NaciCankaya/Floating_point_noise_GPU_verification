{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "491f7cba-d1cd-4783-a96f-569e82f1f29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Info:\n",
      "  Hostname: ec4b94e56229\n",
      "  GPUs available: 4\n",
      "    GPU 0: NVIDIA A100 80GB PCIe\n",
      "    GPU 1: NVIDIA A100 80GB PCIe\n",
      "    GPU 2: NVIDIA A100 80GB PCIe\n",
      "    GPU 3: NVIDIA A100 80GB PCIe\n",
      "  PyTorch: 2.8.0+cu128\n",
      "  CUDA: 12.8\n",
      "\n",
      "Loading tokenizer for mistralai/Mistral-Small-Instruct-2409...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt: 356 tokens\n",
      "\n",
      "======================================================================\n",
      "PIPELINE PARALLELISM FORENSICS EXPERIMENT\n",
      "Model: mistralai/Mistral-Small-Instruct-2409\n",
      "Precision: BF16 (bfloat16)\n",
      "Prompt tokens: 356\n",
      "Extraction: Last layer keys, last token, all heads concatenated\n",
      "Repetitions per configuration: 10\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TESTING: no_pp\n",
      "======================================================================\n",
      "\n",
      "Loading model WITHOUT pipeline parallelism...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23583b1769bd4286803e2f3c3f69d22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0 memory after load: 9.82 GB\n",
      "  Model layers: 56\n",
      "\n",
      "Collecting key vectors (10 repetitions)...\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Rep 0: shape=torch.Size([1024]), norm=53.25, first_val=0.625000\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Completed 3/10 repetitions\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Completed 6/10 repetitions\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Completed 9/10 repetitions\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  ✓ Statistical noise: mean=0.000000, std=0.000000 (perfect reproducibility)\n",
      "  Mean key vector norm: 53.25\n",
      "\n",
      "  Unloading model...\n",
      "    GPU 0 memory after unload: 9.83 GB\n",
      "    GPU 1 memory after unload: 10.91 GB\n",
      "    GPU 2 memory after unload: 10.91 GB\n",
      "    GPU 3 memory after unload: 9.83 GB\n",
      "\n",
      "======================================================================\n",
      "TESTING: pp_2way\n",
      "======================================================================\n",
      "\n",
      "Loading model WITH 2-way pipeline parallelism...\n",
      "  Detected 56 layers in model\n",
      "  Pipeline configuration:\n",
      "    GPU 0: layers 0-27\n",
      "    GPU 1: layers 28-55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72ee7fbc498f4604bd1c30871006a937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0 memory: 30.55 GB\n",
      "  GPU 1 memory: 31.63 GB\n",
      "\n",
      "Collecting key vectors (10 repetitions)...\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Rep 0: shape=torch.Size([1024]), norm=53.25, first_val=0.625000\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Completed 3/10 repetitions\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Completed 6/10 repetitions\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Completed 9/10 repetitions\n",
      "    Last layer (layer 55) is on cuda:1\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  ✓ Statistical noise: mean=0.000000, std=0.000000 (perfect reproducibility)\n",
      "  Mean key vector norm: 53.25\n",
      "\n",
      "  Unloading model...\n",
      "    GPU 0 memory after unload: 20.73 GB\n",
      "    GPU 1 memory after unload: 20.73 GB\n",
      "\n",
      "======================================================================\n",
      "TESTING: pp_4way\n",
      "======================================================================\n",
      "\n",
      "Loading model WITH 4-way pipeline parallelism...\n",
      "  Detected 56 layers in model\n",
      "  Pipeline configuration:\n",
      "    GPU 0: layers 0-13\n",
      "    GPU 1: layers 14-27\n",
      "    GPU 2: layers 28-41\n",
      "    GPU 3: layers 42-55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5987b2d39c2a4fcfa3d4bd01d43fe864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  GPU 0 memory: 31.27 GB\n",
      "  GPU 1 memory: 30.90 GB\n",
      "  GPU 2 memory: 10.18 GB\n",
      "  GPU 3 memory: 10.56 GB\n",
      "\n",
      "Collecting key vectors (10 repetitions)...\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Rep 0: shape=torch.Size([1024]), norm=53.25, first_val=0.625000\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Completed 3/10 repetitions\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Completed 6/10 repetitions\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  Completed 9/10 repetitions\n",
      "    Last layer (layer 55) is on cuda:3\n",
      "    Warning: Hook didn't capture keys, trying alternative method...\n",
      "  ✓ Statistical noise: mean=0.000000, std=0.000000 (perfect reproducibility)\n",
      "  Mean key vector norm: 53.25\n",
      "\n",
      "  Unloading model...\n",
      "    GPU 0 memory after unload: 10.56 GB\n",
      "    GPU 1 memory after unload: 10.18 GB\n",
      "    GPU 2 memory after unload: 10.18 GB\n",
      "    GPU 3 memory after unload: 10.56 GB\n",
      "\n",
      "======================================================================\n",
      "=== SYSTEMATIC DEVIATION ANALYSIS ===\n",
      "======================================================================\n",
      "\n",
      "no_pp vs pp_2way:\n",
      "  L2 distance: 0.000000\n",
      "  Relative: 0.000000 (0.000%)\n",
      "\n",
      "no_pp vs pp_4way:\n",
      "  L2 distance: 0.000000\n",
      "  Relative: 0.000000 (0.000%)\n",
      "\n",
      "pp_2way vs pp_4way:\n",
      "  L2 distance: 0.000000\n",
      "  Relative: 0.000000 (0.000%)\n",
      "\n",
      "======================================================================\n",
      "=== KEY VECTOR STATISTICS ===\n",
      "======================================================================\n",
      "\n",
      "no_pp:\n",
      "  Vector dimension: 1024\n",
      "  Vector norm: 53.25\n",
      "  Mean value: -0.020020\n",
      "  Std dev: 1.664062\n",
      "\n",
      "pp_2way:\n",
      "  Vector dimension: 1024\n",
      "  Vector norm: 53.25\n",
      "  Mean value: -0.020020\n",
      "  Std dev: 1.664062\n",
      "\n",
      "pp_4way:\n",
      "  Vector dimension: 1024\n",
      "  Vector norm: 53.25\n",
      "  Mean value: -0.020020\n",
      "  Std dev: 1.664062\n",
      "\n",
      "======================================================================\n",
      "=== INTERPRETATION ===\n",
      "======================================================================\n",
      "\n",
      "Statistical noise summary:\n",
      "  no_pp: Perfect (L2=0.000)\n",
      "  pp_2way: Perfect (L2=0.000)\n",
      "  pp_4way: Perfect (L2=0.000)\n",
      "\n",
      "Systematic deviations summary:\n",
      "  no_pp_vs_pp_2way: L2=0.000000\n",
      "  no_pp_vs_pp_4way: L2=0.000000\n",
      "  pp_2way_vs_pp_4way: L2=0.000000\n",
      "\n",
      "✓ PIPELINE PARALLELISM IS NOT DETECTABLE\n",
      "  Max L2 deviation: 0.000000\n",
      "  All configurations produce essentially identical key vectors\n",
      "  This confirms: PP only changes WHERE computation happens, not HOW\n",
      "  Conclusion: PP configuration cannot be used for forensics\n",
      "\n",
      "✓ Results saved to /workspace/NVIDIA_A100_80GB_PCIe_pp_forensics_20251103_125259.json\n",
      "✓ File size: ~337.1 KB\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "os.environ['HF_DATASETS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "\n",
    "# Capture system info\n",
    "HOSTNAME = socket.gethostname()\n",
    "CONTAINER_ID = os.environ.get('HOSTNAME', 'unknown')\n",
    "\n",
    "print(f\"System Info:\")\n",
    "print(f\"  Hostname: {HOSTNAME}\")\n",
    "print(f\"  GPUs available: {torch.cuda.device_count()}\")\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"    GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "print(f\"  PyTorch: {torch.__version__}\")\n",
    "print(f\"  CUDA: {torch.version.cuda}\")\n",
    "print()\n",
    "\n",
    "# Configuration\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "model_name = \"mistralai/Mistral-Small-Instruct-2409\"  # 24B, 32 layers\n",
    "# Note: If Mistral-Small-3.2 uses a different model ID, update this\n",
    "\n",
    "print(f\"Loading tokenizer for {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=CACHE_DIR)\n",
    "\n",
    "# Load test prompt\n",
    "prompt = \"\"\"The development of large language models has fundamentally transformed natural language processing \n",
    "and artificial intelligence more broadly. These models, trained on vast corpora of text data, have demonstrated \n",
    "remarkable capabilities across a wide range of tasks, from translation and summarization to question answering \n",
    "and creative writing. However, their deployment raises significant challenges related to computational efficiency, \n",
    "interpretability, and safety.\n",
    "\n",
    "One critical challenge in deploying large language models at scale is ensuring computational efficiency. \n",
    "Modern language models can contain hundreds of billions of parameters, requiring substantial computational \n",
    "resources for both training and inference. Inference optimization techniques have become increasingly important \n",
    "as these models are deployed in production environments. Key approaches include quantization, where model \n",
    "weights and activations are represented with reduced precision; knowledge distillation, where a smaller \n",
    "student model learns to mimic a larger teacher model; and architectural innovations such as mixture-of-experts \n",
    "models that activate only relevant subnetworks for each input.\n",
    "\n",
    "The inference stack itself introduces numerous sources of variation in model outputs. Floating-point arithmetic \n",
    "is inherently non-associative, meaning that the order of operations affects the final result. In distributed \n",
    "inference scenarios, where computation is parallelized across multiple GPUs, different parallelization strategies \n",
    "can lead to different operation orderings and thus different numerical results, even when using identical model \n",
    "weights and inputs. Factors such as batch size, communication patterns between GPUs, the specific CUDA kernels \n",
    "selected for various operations, and even the GPU architecture itself can all contribute to variations in output.\"\"\"\n",
    "\n",
    "prompt_tokens = len(tokenizer.encode(prompt))\n",
    "print(f\"Test prompt: {prompt_tokens} tokens\\n\")\n",
    "\n",
    "# Global variable to capture keys\n",
    "captured_keys = None\n",
    "\n",
    "def create_hook(layer_idx, total_layers):\n",
    "    \"\"\"Create a hook to capture key vectors from the last layer\"\"\"\n",
    "    def hook(module, input, output):\n",
    "        global captured_keys\n",
    "        # Only capture from the last layer\n",
    "        if layer_idx == total_layers - 1:\n",
    "            # For Mistral, the attention output is typically (batch, seq_len, num_heads, head_dim)\n",
    "            # or the past_key_values contain the keys\n",
    "            # We need to check the actual structure\n",
    "            \n",
    "            # Mistral attention returns: (attn_output, attn_weights, past_key_value)\n",
    "            # past_key_value is (key, value) tuple\n",
    "            if isinstance(output, tuple) and len(output) >= 3:\n",
    "                past_kv = output[2]\n",
    "                if past_kv is not None and isinstance(past_kv, tuple):\n",
    "                    # past_kv[0] is keys: shape (batch, num_key_heads, seq_len, head_dim)\n",
    "                    keys = past_kv[0]\n",
    "                    # Extract last token position: (batch, num_key_heads, head_dim)\n",
    "                    last_token_keys = keys[:, :, -1, :]\n",
    "                    # Flatten to (batch, num_key_heads * head_dim)\n",
    "                    captured_keys = last_token_keys.reshape(last_token_keys.shape[0], -1).cpu().clone()\n",
    "    return hook\n",
    "\n",
    "def collect_keys_simple(model, tokenizer, prompt, device=\"cuda\"):\n",
    "    \"\"\"Collect keys without DeepSpeed - simple single GPU version\"\"\"\n",
    "    global captured_keys\n",
    "    captured_keys = None\n",
    "    \n",
    "    # Register hook on last layer\n",
    "    num_layers = len(model.model.layers)\n",
    "    last_layer = model.model.layers[-1]\n",
    "    hook_handle = last_layer.self_attn.register_forward_hook(\n",
    "        create_hook(num_layers - 1, num_layers)\n",
    "    )\n",
    "    \n",
    "    # Forward pass\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=False, use_cache=True)\n",
    "    \n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # If hook didn't capture (sometimes past_key_values structure differs), \n",
    "    # try alternative extraction\n",
    "    if captured_keys is None:\n",
    "        print(\"    Warning: Hook didn't capture keys, trying alternative method...\")\n",
    "        # Try to get from model outputs if available\n",
    "        if hasattr(outputs, 'past_key_values') and outputs.past_key_values is not None:\n",
    "            # past_key_values is tuple of (key, value) for each layer\n",
    "            last_layer_kv = outputs.past_key_values[-1]\n",
    "            keys = last_layer_kv[0]  # (batch, num_key_heads, seq_len, head_dim)\n",
    "            last_token_keys = keys[:, :, -1, :]\n",
    "            captured_keys = last_token_keys.reshape(last_token_keys.shape[0], -1).cpu().clone()\n",
    "    \n",
    "    if captured_keys is None:\n",
    "        raise RuntimeError(\"Failed to extract keys from model\")\n",
    "    \n",
    "    result = captured_keys[0]  # Remove batch dimension\n",
    "    \n",
    "    del outputs, inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def load_model_no_pp(model_name, cache_dir):\n",
    "    \"\"\"Load model without pipeline parallelism (single GPU or model parallel)\"\"\"\n",
    "    print(\"\\nLoading model WITHOUT pipeline parallelism...\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=cache_dir,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"  # Will use single GPU if it fits\n",
    "    )\n",
    "    \n",
    "    mem_after = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    print(f\"  GPU 0 memory after load: {mem_after:.2f} GB\")\n",
    "    print(f\"  Model layers: {len(model.model.layers)}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_model_with_pp(model_name, cache_dir, num_stages):\n",
    "    \"\"\"Load model with pipeline parallelism using device_map\"\"\"\n",
    "    print(f\"\\nLoading model WITH {num_stages}-way pipeline parallelism...\")\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Get actual number of layers from config\n",
    "    from transformers import AutoConfig\n",
    "    config = AutoConfig.from_pretrained(model_name, cache_dir=cache_dir)\n",
    "    num_layers = config.num_hidden_layers\n",
    "    print(f\"  Detected {num_layers} layers in model\")\n",
    "    \n",
    "    layers_per_stage = num_layers // num_stages\n",
    "    \n",
    "    # Create device map for pipeline parallelism\n",
    "    # We manually assign layers to GPUs\n",
    "    device_map = {}\n",
    "    \n",
    "    # Embedding on GPU 0\n",
    "    device_map[\"model.embed_tokens\"] = 0\n",
    "    device_map[\"model.norm\"] = num_stages - 1  # Final norm on last GPU\n",
    "    device_map[\"lm_head\"] = num_stages - 1  # LM head on last GPU\n",
    "    \n",
    "    # Distribute layers across GPUs\n",
    "    for layer_idx in range(num_layers):\n",
    "        gpu_idx = layer_idx // layers_per_stage\n",
    "        if gpu_idx >= num_stages:\n",
    "            gpu_idx = num_stages - 1\n",
    "        device_map[f\"model.layers.{layer_idx}\"] = gpu_idx\n",
    "    \n",
    "    print(f\"  Pipeline configuration:\")\n",
    "    for gpu_idx in range(num_stages):\n",
    "        start_layer = gpu_idx * layers_per_stage\n",
    "        end_layer = min((gpu_idx + 1) * layers_per_stage, num_layers) - 1\n",
    "        print(f\"    GPU {gpu_idx}: layers {start_layer}-{end_layer}\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        cache_dir=cache_dir,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=device_map\n",
    "    )\n",
    "    \n",
    "    # Report memory usage across GPUs\n",
    "    for gpu_idx in range(num_stages):\n",
    "        mem = torch.cuda.memory_allocated(gpu_idx) / 1024**3\n",
    "        print(f\"  GPU {gpu_idx} memory: {mem:.2f} GB\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def collect_keys_pp(model, tokenizer, prompt):\n",
    "    \"\"\"Collect keys from pipeline parallel model\"\"\"\n",
    "    global captured_keys\n",
    "    captured_keys = None\n",
    "    \n",
    "    # Find which GPU has the last layer\n",
    "    num_layers = len(model.model.layers)\n",
    "    last_layer = model.model.layers[-1]\n",
    "    last_layer_device = next(last_layer.parameters()).device\n",
    "    \n",
    "    print(f\"    Last layer (layer {num_layers-1}) is on {last_layer_device}\")\n",
    "    \n",
    "    # Register hook on last layer\n",
    "    hook_handle = last_layer.self_attn.register_forward_hook(\n",
    "        create_hook(num_layers - 1, num_layers)\n",
    "    )\n",
    "    \n",
    "    # Forward pass - inputs start on GPU 0\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(\"cuda:0\") for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=False, use_cache=True)\n",
    "    \n",
    "    hook_handle.remove()\n",
    "    \n",
    "    # Alternative extraction if hook didn't work\n",
    "    if captured_keys is None:\n",
    "        print(\"    Warning: Hook didn't capture keys, trying alternative method...\")\n",
    "        if hasattr(outputs, 'past_key_values') and outputs.past_key_values is not None:\n",
    "            last_layer_kv = outputs.past_key_values[-1]\n",
    "            keys = last_layer_kv[0]\n",
    "            last_token_keys = keys[:, :, -1, :]\n",
    "            captured_keys = last_token_keys.reshape(last_token_keys.shape[0], -1).cpu().clone()\n",
    "    \n",
    "    if captured_keys is None:\n",
    "        raise RuntimeError(\"Failed to extract keys from pipeline parallel model\")\n",
    "    \n",
    "    result = captured_keys[0]  # Remove batch dimension\n",
    "    \n",
    "    del outputs, inputs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return result\n",
    "\n",
    "def unload_model(model):\n",
    "    \"\"\"Completely remove model from memory\"\"\"\n",
    "    print(f\"  Unloading model...\")\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    for gpu_idx in range(torch.cuda.device_count()):\n",
    "        mem = torch.cuda.memory_allocated(gpu_idx) / 1024**3\n",
    "        if mem > 1:  # Only print if significant memory\n",
    "            print(f\"    GPU {gpu_idx} memory after unload: {mem:.2f} GB\")\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "num_repetitions = 10\n",
    "results = {}\n",
    "all_keys = {}\n",
    "config_info = {}\n",
    "\n",
    "configurations = [\n",
    "    (\"no_pp\", None),\n",
    "    (\"pp_2way\", 2),\n",
    "    (\"pp_4way\", 4)\n",
    "]\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"PIPELINE PARALLELISM FORENSICS EXPERIMENT\")\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Precision: BF16 (bfloat16)\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Extraction: Last layer keys, last token, all heads concatenated\")\n",
    "print(f\"Repetitions per configuration: {num_repetitions}\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for config_name, num_stages in configurations:\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"TESTING: {config_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Load model with appropriate configuration\n",
    "    if num_stages is None:\n",
    "        model = load_model_no_pp(model_name, CACHE_DIR)\n",
    "        collect_fn = lambda: collect_keys_simple(model, tokenizer, prompt, device=\"cuda\")\n",
    "    else:\n",
    "        model = load_model_with_pp(model_name, CACHE_DIR, num_stages)\n",
    "        collect_fn = lambda: collect_keys_pp(model, tokenizer, prompt)\n",
    "    \n",
    "    config_info[config_name] = {\n",
    "        \"pipeline_stages\": num_stages if num_stages else 1,\n",
    "        \"description\": f\"{'No PP (single/auto device map)' if num_stages is None else f'{num_stages}-way pipeline parallelism'}\"\n",
    "    }\n",
    "    \n",
    "    # Collect keys\n",
    "    print(f\"\\nCollecting key vectors ({num_repetitions} repetitions)...\")\n",
    "    runs = []\n",
    "    \n",
    "    for rep in range(num_repetitions):\n",
    "        keys = collect_fn()\n",
    "        runs.append(keys)\n",
    "        \n",
    "        if rep == 0:\n",
    "            print(f\"  Rep 0: shape={keys.shape}, norm={torch.norm(keys).item():.2f}, first_val={keys[0].item():.6f}\")\n",
    "        if (rep + 1) % 3 == 0:\n",
    "            print(f\"  Completed {rep + 1}/{num_repetitions} repetitions\")\n",
    "    \n",
    "    # Calculate statistical noise\n",
    "    first_rep = runs[0]\n",
    "    all_identical = all(torch.equal(first_rep, runs[i]) for i in range(1, num_repetitions))\n",
    "    \n",
    "    stacked = torch.stack(runs)\n",
    "    mean_keys = stacked.mean(dim=0)\n",
    "    deviations = torch.stack([torch.norm(runs[i] - mean_keys) for i in range(num_repetitions)])\n",
    "    mean_noise = deviations.mean().item()\n",
    "    std_noise = deviations.std().item()\n",
    "    \n",
    "    if all_identical:\n",
    "        print(f\"  ✓ Statistical noise: mean=0.000000, std=0.000000 (perfect reproducibility)\")\n",
    "    else:\n",
    "        print(f\"  ⚠ Statistical noise: mean={mean_noise:.6f}, std={std_noise:.6f}\")\n",
    "    \n",
    "    config_info[config_name][\"statistical_noise\"] = {\n",
    "        \"mean\": mean_noise,\n",
    "        \"std\": std_noise,\n",
    "        \"perfect_reproducibility\": all_identical\n",
    "    }\n",
    "    \n",
    "    results[config_name] = torch.stack(runs)\n",
    "    all_keys[config_name] = results[config_name].float().numpy().tolist()\n",
    "    \n",
    "    mean_keys = results[config_name].mean(dim=0)\n",
    "    print(f\"  Mean key vector norm: {torch.norm(mean_keys).item():.2f}\\n\")\n",
    "    \n",
    "    # Unload model\n",
    "    unload_model(model)\n",
    "    print()\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"{'='*70}\")\n",
    "print(\"=== SYSTEMATIC DEVIATION ANALYSIS ===\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "comparisons = [\n",
    "    (\"no_pp\", \"pp_2way\"),\n",
    "    (\"no_pp\", \"pp_4way\"),\n",
    "    (\"pp_2way\", \"pp_4way\")\n",
    "]\n",
    "\n",
    "deviations = {}\n",
    "\n",
    "for config1, config2 in comparisons:\n",
    "    mean1 = results[config1].mean(dim=0)\n",
    "    mean2 = results[config2].mean(dim=0)\n",
    "    \n",
    "    l2 = torch.norm(mean1 - mean2).item()\n",
    "    relative = (l2 / torch.norm(mean1)).item() if torch.norm(mean1) > 0 else 0\n",
    "    \n",
    "    deviations[f\"{config1}_vs_{config2}\"] = l2\n",
    "    \n",
    "    print(f\"{config1} vs {config2}:\")\n",
    "    print(f\"  L2 distance: {l2:.6f}\")\n",
    "    print(f\"  Relative: {relative:.6f} ({relative*100:.3f}%)\")\n",
    "    print()\n",
    "\n",
    "# Detailed comparison\n",
    "print(f\"{'='*70}\")\n",
    "print(\"=== KEY VECTOR STATISTICS ===\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "for config_name in results.keys():\n",
    "    mean_keys = results[config_name].mean(dim=0)\n",
    "    print(f\"{config_name}:\")\n",
    "    print(f\"  Vector dimension: {mean_keys.shape[0]}\")\n",
    "    print(f\"  Vector norm: {torch.norm(mean_keys).item():.2f}\")\n",
    "    print(f\"  Mean value: {mean_keys.mean().item():.6f}\")\n",
    "    print(f\"  Std dev: {mean_keys.std().item():.6f}\")\n",
    "    print()\n",
    "\n",
    "# Interpretation\n",
    "print(f\"{'='*70}\")\n",
    "print(\"=== INTERPRETATION ===\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "max_deviation = max(deviations.values())\n",
    "max_statistical_noise = max(config_info[c][\"statistical_noise\"][\"mean\"] for c in config_info.keys())\n",
    "\n",
    "print(f\"Statistical noise summary:\")\n",
    "for config in config_info.keys():\n",
    "    noise = config_info[config][\"statistical_noise\"]\n",
    "    if noise[\"perfect_reproducibility\"]:\n",
    "        print(f\"  {config}: Perfect (L2=0.000)\")\n",
    "    else:\n",
    "        print(f\"  {config}: mean={noise['mean']:.6f}, std={noise['std']:.6f}\")\n",
    "print()\n",
    "\n",
    "print(f\"Systematic deviations summary:\")\n",
    "for comp_name, dev in deviations.items():\n",
    "    print(f\"  {comp_name}: L2={dev:.6f}\")\n",
    "print()\n",
    "\n",
    "# Signal-to-noise ratio\n",
    "if max_statistical_noise > 0:\n",
    "    snr = max_deviation / max_statistical_noise\n",
    "    print(f\"Signal-to-noise ratio: {snr:.2f}x\")\n",
    "    print(f\"  Max systematic deviation: {max_deviation:.6f}\")\n",
    "    print(f\"  Max statistical noise: {max_statistical_noise:.6f}\")\n",
    "    print()\n",
    "\n",
    "if max_deviation < 0.001:\n",
    "    print(\"✓ PIPELINE PARALLELISM IS NOT DETECTABLE\")\n",
    "    print(f\"  Max L2 deviation: {max_deviation:.6f}\")\n",
    "    print(f\"  All configurations produce essentially identical key vectors\")\n",
    "    print(f\"  This confirms: PP only changes WHERE computation happens, not HOW\")\n",
    "    print(f\"  Conclusion: PP configuration cannot be used for forensics\")\n",
    "elif max_deviation < 0.1:\n",
    "    if max_statistical_noise > 0 and max_deviation < max_statistical_noise * 3:\n",
    "        print(\"✗ SIGNAL TOO WEAK\")\n",
    "        print(f\"  Max L2 deviation: {max_deviation:.6f}\")\n",
    "        print(f\"  Systematic deviation is comparable to or smaller than statistical noise\")\n",
    "        print(f\"  Not reliable for forensics\")\n",
    "    else:\n",
    "        print(\"⚠ WEAK BUT POTENTIALLY DETECTABLE SIGNAL\")\n",
    "        print(f\"  Max L2 deviation: {max_deviation:.6f}\")\n",
    "        print(f\"  Small differences detected, may be reliable with sufficient samples\")\n",
    "else:\n",
    "    print(\"✓ PIPELINE PARALLELISM IS DETECTABLE\")\n",
    "    print(f\"  Max L2 deviation: {max_deviation:.6f}\")\n",
    "    print(f\"  Different PP configurations produce measurably different key vectors\")\n",
    "    print(f\"  This is surprising and worth investigating further\")\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE RESULTS\n",
    "# ============================================================================\n",
    "\n",
    "output = {\n",
    "    \"experiment\": \"pipeline_parallelism_forensics\",\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"model\": model_name,\n",
    "    \"hardware\": {\n",
    "        \"num_gpus\": torch.cuda.device_count(),\n",
    "        \"gpu_models\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n",
    "        \"pytorch\": torch.__version__,\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"hostname\": HOSTNAME,\n",
    "        \"container_id\": CONTAINER_ID\n",
    "    },\n",
    "    \"config\": {\n",
    "        \"configurations_tested\": [c[0] for c in configurations],\n",
    "        \"repetitions\": num_repetitions,\n",
    "        \"prompt_tokens\": prompt_tokens,\n",
    "        \"dtype\": \"bfloat16\",\n",
    "        \"extraction\": \"last_layer_keys_last_token_all_heads\"\n",
    "    },\n",
    "    \"configuration_details\": config_info,\n",
    "    \"results\": {\n",
    "        \"key_vector_dims\": {\n",
    "            config: int(results[config].shape[1]) for config in results.keys()\n",
    "        },\n",
    "        \"key_vector_norms\": {\n",
    "            config: float(torch.norm(results[config].mean(dim=0))) for config in results.keys()\n",
    "        },\n",
    "        \"statistical_noise\": {\n",
    "            config: config_info[config][\"statistical_noise\"] for config in results.keys()\n",
    "        },\n",
    "        \"systematic_deviations\": deviations,\n",
    "        \"signal_to_noise_ratio\": max(deviations.values()) / max(config_info[c][\"statistical_noise\"][\"mean\"] for c in config_info.keys()) if max(config_info[c][\"statistical_noise\"][\"mean\"] for c in config_info.keys()) > 0 else float('inf'),\n",
    "        \"within_config_reproducibility\": {\n",
    "            config: all(torch.equal(results[config][0], results[config][i]) \n",
    "                       for i in range(num_repetitions))\n",
    "            for config in results.keys()\n",
    "        }\n",
    "    },\n",
    "    \"raw_keys\": all_keys\n",
    "}\n",
    "\n",
    "gpu_name = torch.cuda.get_device_name(0).replace(' ', '_')\n",
    "output_file = f\"{gpu_name}_pp_forensics_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "output_path = f\"/workspace/{output_file}\"\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(output, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Results saved to {output_path}\")\n",
    "print(f\"✓ File size: ~{len(json.dumps(output)) / 1024:.1f} KB\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"EXPERIMENT COMPLETE\")\n",
    "print(f\"{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c01671-278e-4f29-b704-a7d6ee5d4bbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
