{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f05c8b6-9423-417b-8b5e-f9ca41c4b246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PYTORCH PROFILING: PREFILL + CONCURRENT PREFILL\n",
      "======================================================================\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA: 12.8\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f09051461a4063875d3e611b04c9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "\n",
      "Configuration:\n",
      "  Default stream: 875 tokens (prefill)\n",
      "  Concurrent stream: 711 tokens (prefill)\n",
      "\n",
      "\n",
      "[BASELINE] Single stream prefill\n",
      "  Duration: 119.24ms\n",
      "  Kernels: 1142\n",
      "  Streams: 1\n",
      "  Trace: /workspace/trace_prefill_baseline.json\n",
      "\n",
      "[CONCURRENT] Dual stream prefill\n",
      "  Default stream duration: 160.01ms\n",
      "  Concurrent iterations: 11\n",
      "  Kernels: 2284\n",
      "  Streams: 2\n",
      "  Trace: /workspace/trace_prefill_concurrent.json\n",
      "\n",
      "======================================================================\n",
      "PARALLEL EXECUTION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Baseline:\n",
      "  Total kernels: 1142\n",
      "  Unique streams: 1\n",
      "  Concurrent kernel pairs: 0\n",
      "\n",
      "Concurrent:\n",
      "  Total kernels: 2284\n",
      "  Unique streams: 2\n",
      "  Concurrent kernel pairs: 2026\n",
      "  Parallelism factor: 1.41x\n",
      "\n",
      "Example concurrent kernel pairs:\n",
      "  • Stream 7 vs Stream 13: 0.00ms overlap\n",
      "    void at::native::unrolled_elementwise_kernel<at::n... || void at::native::(anonymous namespace)::CatArrayBa...\n",
      "  • Stream 7 vs Stream 13: 0.00ms overlap\n",
      "    void at::native::vectorized_elementwise_kernel<4, ... || void at::native::vectorized_elementwise_kernel<4, ...\n",
      "  • Stream 7 vs Stream 13: 0.00ms overlap\n",
      "    void at::native::reduce_kernel<512, 1, at::native:... || void at::native::vectorized_elementwise_kernel<4, ...\n",
      "\n",
      "======================================================================\n",
      "VERDICT\n",
      "======================================================================\n",
      "\n",
      "Timing:\n",
      "  Baseline:   119.24ms\n",
      "  Concurrent: 160.01ms\n",
      "  Slowdown:   40.77ms (34.2%)\n",
      "\n",
      "Parallel Execution:\n",
      "  ✓ VERIFIED - 2026 concurrent kernel pairs detected\n",
      "  ✓ Multiple streams active: 2\n",
      "  ✓ Parallelism factor: 1.41x\n",
      "\n",
      "  → GPU executed kernels from both prefill streams simultaneously\n",
      "  → Slowdown indicates compute/memory resource contention\n",
      "  → Moderate parallelism - partial concurrent execution\n",
      "\n",
      "Visualize timelines:\n",
      "  1. Open Chrome browser\n",
      "  2. Navigate to: chrome://tracing\n",
      "  3. Load trace files:\n",
      "     - Baseline: /workspace/trace_prefill_baseline.json\n",
      "     - Concurrent: /workspace/trace_prefill_concurrent.json\n",
      "  4. Look for overlapping kernel execution bars from different streams\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Experiment 1: Prefill + Concurrent Prefill (PYTORCH PROFILING)\n",
    "Uses PyTorch profiler to verify actual parallel execution\n",
    "\n",
    "Works in JupyterLab - no external tools needed.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from torch.profiler import profile, ProfilerActivity\n",
    "import time\n",
    "import threading\n",
    "import json\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "# Very long prompts for high compute utilization\n",
    "LONG_PROMPT_TEMPLATE = \"\"\"You are tasked with providing a comprehensive technical analysis of distributed database systems for a Fortune 500 company evaluating data infrastructure modernization. The company operates a legacy Oracle RAC database cluster with 500TB of structured data, 2PB of unstructured data in S3, serving 100,000 concurrent users across 50 global offices in 25 countries. They need to evaluate migration paths to modern distributed architectures supporting both OLTP and OLAP workloads with minimal downtime while maintaining ACID guarantees and supporting real-time analytics.\n",
    "\n",
    "Current infrastructure: 12-node Oracle RAC cluster on bare metal with SAN storage, 50ms average query latency, 99.95% uptime SLA, backup RPO of 1 hour. Daily batch processing takes 8 hours using legacy ETL tools. The system processes 2 million transactions per day with peak loads of 50,000 TPS during business hours. Critical business applications include order management, inventory tracking, customer relationship management, financial reporting, and real-time fraud detection.\n",
    "\n",
    "Technical constraints: Must maintain sub-100ms p99 latency for OLTP queries, support complex JOIN operations across 200+ tables, enable real-time materialized views for analytics, provide point-in-time recovery for compliance, support multi-region disaster recovery with RPO < 5 minutes, enable blue-green deployments for zero-downtime upgrades, maintain referential integrity across distributed transactions, support both SQL and NoSQL query patterns, enable real-time data replication to analytics warehouse.\n",
    "\n",
    "Evaluation criteria: Total cost of ownership over 5 years including licensing, infrastructure, operations, migration costs. Performance benchmarks for OLTP (TPS, latency), OLAP (query response time, concurrency), mixed workloads. Operational complexity including monitoring, troubleshooting, capacity planning, disaster recovery procedures. Vendor ecosystem including tooling support, cloud integration, community resources. Migration strategy including data migration approach, application refactoring requirements, downtime windows, rollback procedures.\n",
    "\n",
    "Modern architecture options to evaluate: (1) Amazon Aurora PostgreSQL with read replicas and Aurora Serverless v2 for variable workloads. (2) Google Cloud Spanner for globally distributed ACID transactions with TrueTime. (3) CockroachDB for distributed SQL with automatic sharding and rebalancing. (4) MongoDB Atlas with multi-region clusters and change streams. (5) Cassandra with Spark for analytics workloads. (6) Hybrid approach using PostgreSQL for OLTP with Snowflake for OLAP. (7) YugabyteDB for PostgreSQL compatibility with distributed architecture.\"\"\"\n",
    "\n",
    "CONCURRENT_PROMPT_TEMPLATE = \"\"\"You are providing expert analysis on cloud-native application architecture patterns for a global e-commerce platform processing $5B in annual revenue. The platform serves 50 million active users across web, mobile, and API channels with 99.99% availability requirements.\n",
    "\n",
    "Current architecture: Monolithic Java application (2.5M lines of code) on 100 EC2 instances behind ELB, MySQL cluster with read replicas, Redis for caching, RabbitMQ for async processing. System handles 10M requests per day with average response time of 500ms. Critical services include product catalog, shopping cart, order processing, payment gateway integration, inventory management, recommendation engine, fraud detection, customer service portal.\n",
    "\n",
    "Modernization drivers: Deployment velocity (currently 2-week release cycles, targeting daily deployments), operational costs (EC2 + RDS spending $2M annually), scaling challenges (Black Friday requires 10x capacity, manual scaling takes hours), developer productivity (build times 45 minutes, integration testing 4 hours), service reliability (cascading failures affect entire platform).\n",
    "\n",
    "Architecture evaluation criteria: Operational complexity (service orchestration, observability, debugging distributed transactions), development velocity (microservice boundaries, API contracts, testing strategies), infrastructure costs (compute, storage, networking, managed services), reliability patterns (circuit breakers, bulkheads, rate limiting, chaos engineering), data consistency (eventual vs strong consistency, saga patterns, event sourcing), security posture (service mesh, mTLS, API gateways, secret management).\n",
    "\n",
    "Proposed patterns: (1) Strangler fig migration with API gateway routing legacy vs new services. (2) Event-driven architecture with Kafka for service communication. (3) CQRS with separate read/write models and materialized views. (4) Service mesh with Istio for traffic management and observability. (5) Serverless for variable workloads using Lambda/Fargate. (6) GraphQL federation for unified API layer. (7) Feature flags and canary deployments for gradual rollout.\n",
    "\n",
    "Technical considerations: Service decomposition strategy (bounded contexts, team ownership, API versioning), data management (database per service, shared database, event sourcing), inter-service communication (synchronous REST/gRPC vs asynchronous messaging), distributed transactions (saga patterns, compensating transactions, idempotency), observability (distributed tracing, metrics aggregation, log correlation), testing strategy (contract testing, chaos engineering, synthetic monitoring), deployment architecture (Kubernetes, service mesh, API gateway, monitoring stack).\"\"\"\n",
    "\n",
    "DEFAULT_PROMPT = (LONG_PROMPT_TEMPLATE + \"\\n\\n\" + \"Please analyze each option in detail. \" * 50).strip()\n",
    "CONCURRENT_PROMPT = (CONCURRENT_PROMPT_TEMPLATE + \"\\n\\n\" + \"Provide detailed recommendations. \" * 50).strip()\n",
    "\n",
    "# ============================================================================\n",
    "# CONCURRENT STREAM\n",
    "# ============================================================================\n",
    "\n",
    "class ConcurrentStream:\n",
    "    \"\"\"Concurrent prefill stream\"\"\"\n",
    "    def __init__(self, model, tokenizer, prompt, device=\"cuda\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = prompt\n",
    "        self.device = device\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.should_stop = threading.Event()\n",
    "        self.thread = None\n",
    "        self.iteration_count = 0\n",
    "    \n",
    "    def _run_stream(self):\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            while not self.should_stop.is_set():\n",
    "                try:\n",
    "                    inputs = self.tokenizer([self.prompt], return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        _ = self.model(**inputs, use_cache=True)\n",
    "                    \n",
    "                    torch.cuda.synchronize(self.stream)\n",
    "                    self.iteration_count += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  [WARNING] Concurrent stream error: {e}\")\n",
    "                    break\n",
    "    \n",
    "    def start(self):\n",
    "        self.iteration_count = 0\n",
    "        self.should_stop.clear()\n",
    "        self.thread = threading.Thread(target=self._run_stream, daemon=True)\n",
    "        self.thread.start()\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    def stop(self):\n",
    "        self.should_stop.set()\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=10)\n",
    "        torch.cuda.synchronize()\n",
    "        return self.iteration_count\n",
    "\n",
    "# ============================================================================\n",
    "# PROFILING ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_trace(trace_file):\n",
    "    \"\"\"\n",
    "    Analyze Chrome trace to detect concurrent kernel execution.\n",
    "    \n",
    "    Returns:\n",
    "        dict with concurrent execution statistics\n",
    "    \"\"\"\n",
    "    with open(trace_file, 'r') as f:\n",
    "        trace_data = json.load(f)\n",
    "    \n",
    "    # Extract CUDA kernel events\n",
    "    kernel_events = []\n",
    "    for event in trace_data.get('traceEvents', []):\n",
    "        if event.get('cat') == 'kernel' and event.get('ph') == 'X':\n",
    "            kernel_events.append({\n",
    "                'name': event.get('name', ''),\n",
    "                'start': event.get('ts', 0),  # microseconds\n",
    "                'dur': event.get('dur', 0),   # microseconds\n",
    "                'end': event.get('ts', 0) + event.get('dur', 0),\n",
    "                'stream': event.get('args', {}).get('stream', -1)\n",
    "            })\n",
    "    \n",
    "    if not kernel_events:\n",
    "        return {\n",
    "            'total_kernels': 0,\n",
    "            'concurrent_pairs': 0,\n",
    "            'streams': set(),\n",
    "            'analysis': 'No kernel events found in trace'\n",
    "        }\n",
    "    \n",
    "    # Sort by start time\n",
    "    kernel_events.sort(key=lambda x: x['start'])\n",
    "    \n",
    "    # Find concurrent kernel pairs (from different streams)\n",
    "    concurrent_pairs = []\n",
    "    streams = set(k['stream'] for k in kernel_events)\n",
    "    \n",
    "    for i, k1 in enumerate(kernel_events):\n",
    "        for k2 in kernel_events[i+1:]:\n",
    "            # Check if they overlap in time\n",
    "            if k1['end'] <= k2['start']:\n",
    "                break  # No more possible overlaps for k1\n",
    "            \n",
    "            # Check if from different streams\n",
    "            if k1['stream'] != k2['stream'] and k1['stream'] != -1 and k2['stream'] != -1:\n",
    "                overlap_start = max(k1['start'], k2['start'])\n",
    "                overlap_end = min(k1['end'], k2['end'])\n",
    "                overlap_duration = overlap_end - overlap_start\n",
    "                \n",
    "                if overlap_duration > 0:\n",
    "                    concurrent_pairs.append({\n",
    "                        'kernel1': k1['name'],\n",
    "                        'kernel2': k2['name'],\n",
    "                        'stream1': k1['stream'],\n",
    "                        'stream2': k2['stream'],\n",
    "                        'overlap_us': overlap_duration\n",
    "                    })\n",
    "    \n",
    "    # Calculate statistics\n",
    "    total_duration = max(k['end'] for k in kernel_events) - min(k['start'] for k in kernel_events)\n",
    "    total_kernel_time = sum(k['dur'] for k in kernel_events)\n",
    "    \n",
    "    return {\n",
    "        'total_kernels': len(kernel_events),\n",
    "        'concurrent_pairs': len(concurrent_pairs),\n",
    "        'unique_streams': len(streams),\n",
    "        'streams': streams,\n",
    "        'total_duration_ms': total_duration / 1000,\n",
    "        'total_kernel_time_ms': total_kernel_time / 1000,\n",
    "        'parallelism_factor': total_kernel_time / total_duration if total_duration > 0 else 0,\n",
    "        'sample_concurrent_pairs': concurrent_pairs[:5]  # First 5 examples\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "def run_with_profiling(model, tokenizer, prompt, \n",
    "                       concurrent_stream=None, label=\"\"):\n",
    "    \"\"\"Run prefill with PyTorch profiling\"\"\"\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    if concurrent_stream:\n",
    "        concurrent_stream.start()\n",
    "    \n",
    "    # Warmup\n",
    "    with torch.no_grad():\n",
    "        _ = model(**inputs, use_cache=True)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Profile the actual run\n",
    "    trace_file = f\"/workspace/trace_prefill_{label}.json\"\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    with profile(\n",
    "        activities=[ProfilerActivity.CUDA],\n",
    "        record_shapes=False,\n",
    "        with_stack=False\n",
    "    ) as prof:\n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs, use_cache=True)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = (time.perf_counter() - start) * 1000\n",
    "    \n",
    "    if concurrent_stream:\n",
    "        iterations = concurrent_stream.stop()\n",
    "    else:\n",
    "        iterations = 0\n",
    "    \n",
    "    # Export trace\n",
    "    prof.export_chrome_trace(trace_file)\n",
    "    \n",
    "    # Analyze trace\n",
    "    analysis = analyze_trace(trace_file)\n",
    "    \n",
    "    return elapsed, iterations, analysis, trace_file\n",
    "\n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PYTORCH PROFILING: PREFILL + CONCURRENT PREFILL\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    device = \"cuda\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"✓ Model loaded\\n\")\n",
    "    \n",
    "    default_tokens = len(tokenizer.encode(DEFAULT_PROMPT))\n",
    "    concurrent_tokens = len(tokenizer.encode(CONCURRENT_PROMPT))\n",
    "    \n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Default stream: {default_tokens} tokens (prefill)\")\n",
    "    print(f\"  Concurrent stream: {concurrent_tokens} tokens (prefill)\")\n",
    "    print()\n",
    "    \n",
    "    # Baseline\n",
    "    print(\"\\n[BASELINE] Single stream prefill\")\n",
    "    baseline_time, _, baseline_analysis, baseline_trace = run_with_profiling(\n",
    "        model, tokenizer, DEFAULT_PROMPT,\n",
    "        concurrent_stream=None, label=\"baseline\"\n",
    "    )\n",
    "    print(f\"  Duration: {baseline_time:.2f}ms\")\n",
    "    print(f\"  Kernels: {baseline_analysis['total_kernels']}\")\n",
    "    print(f\"  Streams: {baseline_analysis['unique_streams']}\")\n",
    "    print(f\"  Trace: {baseline_trace}\")\n",
    "    \n",
    "    # Concurrent\n",
    "    print(\"\\n[CONCURRENT] Dual stream prefill\")\n",
    "    concurrent = ConcurrentStream(model, tokenizer, CONCURRENT_PROMPT, device)\n",
    "    concurrent_time, iterations, concurrent_analysis, concurrent_trace = run_with_profiling(\n",
    "        model, tokenizer, DEFAULT_PROMPT,\n",
    "        concurrent_stream=concurrent, label=\"concurrent\"\n",
    "    )\n",
    "    print(f\"  Default stream duration: {concurrent_time:.2f}ms\")\n",
    "    print(f\"  Concurrent iterations: {iterations}\")\n",
    "    print(f\"  Kernels: {concurrent_analysis['total_kernels']}\")\n",
    "    print(f\"  Streams: {concurrent_analysis['unique_streams']}\")\n",
    "    print(f\"  Trace: {concurrent_trace}\")\n",
    "    \n",
    "    # Analysis\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PARALLEL EXECUTION ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nBaseline:\")\n",
    "    print(f\"  Total kernels: {baseline_analysis['total_kernels']}\")\n",
    "    print(f\"  Unique streams: {baseline_analysis['unique_streams']}\")\n",
    "    print(f\"  Concurrent kernel pairs: {baseline_analysis['concurrent_pairs']}\")\n",
    "    \n",
    "    print(f\"\\nConcurrent:\")\n",
    "    print(f\"  Total kernels: {concurrent_analysis['total_kernels']}\")\n",
    "    print(f\"  Unique streams: {concurrent_analysis['unique_streams']}\")\n",
    "    print(f\"  Concurrent kernel pairs: {concurrent_analysis['concurrent_pairs']}\")\n",
    "    print(f\"  Parallelism factor: {concurrent_analysis['parallelism_factor']:.2f}x\")\n",
    "    \n",
    "    if concurrent_analysis['sample_concurrent_pairs']:\n",
    "        print(f\"\\nExample concurrent kernel pairs:\")\n",
    "        for pair in concurrent_analysis['sample_concurrent_pairs'][:3]:\n",
    "            print(f\"  • Stream {pair['stream1']} vs Stream {pair['stream2']}: {pair['overlap_us']/1000:.2f}ms overlap\")\n",
    "            print(f\"    {pair['kernel1'][:50]}... || {pair['kernel2'][:50]}...\")\n",
    "    \n",
    "    # Verdict\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VERDICT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nTiming:\")\n",
    "    print(f\"  Baseline:   {baseline_time:.2f}ms\")\n",
    "    print(f\"  Concurrent: {concurrent_time:.2f}ms\")\n",
    "    print(f\"  Slowdown:   {concurrent_time - baseline_time:.2f}ms ({(concurrent_time/baseline_time - 1)*100:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nParallel Execution:\")\n",
    "    if concurrent_analysis['concurrent_pairs'] > 0:\n",
    "        print(f\"  ✓ VERIFIED - {concurrent_analysis['concurrent_pairs']} concurrent kernel pairs detected\")\n",
    "        print(f\"  ✓ Multiple streams active: {concurrent_analysis['unique_streams']}\")\n",
    "        print(f\"  ✓ Parallelism factor: {concurrent_analysis['parallelism_factor']:.2f}x\")\n",
    "        print(f\"\\n  → GPU executed kernels from both prefill streams simultaneously\")\n",
    "        print(f\"  → Slowdown indicates compute/memory resource contention\")\n",
    "        \n",
    "        if concurrent_analysis['parallelism_factor'] > 1.5:\n",
    "            print(f\"  → High parallelism - significant concurrent execution\")\n",
    "        elif concurrent_analysis['parallelism_factor'] > 1.0:\n",
    "            print(f\"  → Moderate parallelism - partial concurrent execution\")\n",
    "        else:\n",
    "            print(f\"  → Low parallelism - mostly serialized despite concurrent kernels\")\n",
    "    else:\n",
    "        print(f\"  ✗ NO CONCURRENCY - Kernels executed sequentially\")\n",
    "        print(f\"  → GPU may have serialized the prefill streams\")\n",
    "    \n",
    "    print(f\"\\nVisualize timelines:\")\n",
    "    print(f\"  1. Open Chrome browser\")\n",
    "    print(f\"  2. Navigate to: chrome://tracing\")\n",
    "    print(f\"  3. Load trace files:\")\n",
    "    print(f\"     - Baseline: {baseline_trace}\")\n",
    "    print(f\"     - Concurrent: {concurrent_trace}\")\n",
    "    print(f\"  4. Look for overlapping kernel execution bars from different streams\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371c79a0-d756-46ee-8380-7deb6fe8bd08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
