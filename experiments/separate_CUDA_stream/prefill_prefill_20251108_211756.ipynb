{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c3dc970-66c1-456b-8f51-98841c227216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SYSTEM ATTESTATION\n",
      "======================================================================\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA: 12.8\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT 1: PREFILL + CONCURRENT PREFILL\n",
      "======================================================================\n",
      "Configuration:\n",
      "  • Prefill-only operations (high arithmetic intensity)\n",
      "  • Very long prompts for high compute utilization\n",
      "  • Extract keys from attention output\n",
      "  • Timeline visualization for parallel execution\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119210a005ec4d1d92c81180fab7723c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model loaded\n",
      "\n",
      "Configuration:\n",
      "  Default stream: 875 tokens (prefill)\n",
      "  Concurrent stream: 711 tokens (prefill)\n",
      "  Sample interval: every 50th position\n",
      "  Layers: [1, 4, 10, 18, 28]\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SANITY CHECK: Verifying measurement system works\n",
      "======================================================================\n",
      "\n",
      "[SANITY_CHECK] Starting...\n",
      "[SANITY_CHECK] Prompt: 880 tokens (prefill only)\n",
      "[SANITY_CHECK] Sampling 19 positions from prompt\n",
      "\n",
      "[BASELINE] Starting...\n",
      "[BASELINE] Prompt: 875 tokens (prefill only)\n",
      "[BASELINE] Sampling 19 positions from prompt\n",
      "\n",
      "Verifying measurement system:\n",
      "  Position 0, layer_1: DIFFERENT (L2=2.83e+01) ✓\n",
      "  ✓ Measurement system verified - different prompts give different keys\n",
      "\n",
      "\n",
      "[LOW_CONCURRENT] Starting...\n",
      "[LOW_CONCURRENT] Starting concurrent stream...\n",
      "[LOW_CONCURRENT] Prompt: 875 tokens (prefill only)\n",
      "[LOW_CONCURRENT] Sampling 19 positions from prompt\n",
      "\n",
      "LOW_CONCURRENT - EXECUTION TIMELINE:\n",
      "----------------------------------------------------------------------\n",
      "Default stream:    [1209.3ms → 1316.7ms] duration=107.37ms\n",
      "Concurrent stream:\n",
      "  Execution 1:  [0.0ms → 38.1ms] duration=38.10ms, NO OVERLAP\n",
      "  Execution 2:  [38.9ms → 76.0ms] duration=37.11ms, NO OVERLAP\n",
      "  Execution 3:  [76.7ms → 114.3ms] duration=37.54ms, NO OVERLAP\n",
      "  Execution 4:  [115.0ms → 152.6ms] duration=37.60ms, NO OVERLAP\n",
      "  Execution 5:  [153.3ms → 190.9ms] duration=37.56ms, NO OVERLAP\n",
      "  Execution 6:  [191.6ms → 229.2ms] duration=37.58ms, NO OVERLAP\n",
      "  Execution 7:  [229.9ms → 267.7ms] duration=37.81ms, NO OVERLAP\n",
      "  Execution 8:  [268.4ms → 306.2ms] duration=37.82ms, NO OVERLAP\n",
      "  Execution 9:  [307.0ms → 344.6ms] duration=37.58ms, NO OVERLAP\n",
      "  Execution 10:  [345.3ms → 382.8ms] duration=37.55ms, NO OVERLAP\n",
      "  Execution 11:  [383.5ms → 421.1ms] duration=37.56ms, NO OVERLAP\n",
      "  Execution 12:  [421.8ms → 459.4ms] duration=37.56ms, NO OVERLAP\n",
      "  Execution 13:  [460.1ms → 497.7ms] duration=37.56ms, NO OVERLAP\n",
      "  Execution 14:  [498.4ms → 535.9ms] duration=37.54ms, NO OVERLAP\n",
      "  Execution 15:  [536.6ms → 574.2ms] duration=37.56ms, NO OVERLAP\n",
      "  Execution 16:  [574.9ms → 612.5ms] duration=37.60ms, NO OVERLAP\n",
      "  Execution 17:  [613.2ms → 651.2ms] duration=37.97ms, NO OVERLAP\n",
      "  Execution 18:  [651.9ms → 689.7ms] duration=37.81ms, NO OVERLAP\n",
      "  Execution 19:  [690.4ms → 728.1ms] duration=37.64ms, NO OVERLAP\n",
      "  Execution 20:  [728.8ms → 766.3ms] duration=37.54ms, NO OVERLAP\n",
      "  Execution 21:  [767.1ms → 804.6ms] duration=37.56ms, NO OVERLAP\n",
      "  Execution 22:  [805.3ms → 842.8ms] duration=37.52ms, NO OVERLAP\n",
      "  Execution 23:  [843.6ms → 881.1ms] duration=37.59ms, NO OVERLAP\n",
      "  Execution 24:  [881.9ms → 919.4ms] duration=37.54ms, NO OVERLAP\n",
      "  Execution 25:  [920.1ms → 957.7ms] duration=37.56ms, NO OVERLAP\n",
      "  Execution 26:  [958.4ms → 996.2ms] duration=37.78ms, NO OVERLAP\n",
      "  Execution 27:  [996.9ms → 1034.7ms] duration=37.80ms, NO OVERLAP\n",
      "  Execution 28:  [1035.4ms → 1073.0ms] duration=37.57ms, NO OVERLAP\n",
      "  Execution 29:  [1073.7ms → 1111.3ms] duration=37.57ms, NO OVERLAP\n",
      "  Execution 30:  [1112.0ms → 1149.5ms] duration=37.53ms, NO OVERLAP\n",
      "  Execution 31:  [1150.2ms → 1187.8ms] duration=37.56ms, NO OVERLAP\n",
      "  Execution 32:  [1188.5ms → 1239.4ms] duration=50.89ms, overlap=30.1ms (59.1%)\n",
      "  Execution 33:  [1240.3ms → 1323.7ms] duration=83.47ms, overlap=76.4ms (91.6%)\n",
      "  Execution 34:  [1324.6ms → 1366.3ms] duration=41.75ms, NO OVERLAP\n",
      "  Execution 35:  [1367.0ms → 1405.4ms] duration=38.38ms, NO OVERLAP\n",
      "  Execution 36:  [1406.1ms → 1443.9ms] duration=37.75ms, NO OVERLAP\n",
      "  Execution 37:  [1444.6ms → 1482.4ms] duration=37.79ms, NO OVERLAP\n",
      "\n",
      "Summary:\n",
      "  Total time span: 1482.39ms\n",
      "  Concurrent executions overlapping with default: 2/37\n",
      "  Default stream coverage: 99.2%\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[HIGH_CONCURRENT] Starting...\n",
      "[HIGH_CONCURRENT] Starting concurrent stream...\n",
      "[HIGH_CONCURRENT] Prompt: 875 tokens (prefill only)\n",
      "[HIGH_CONCURRENT] Sampling 19 positions from prompt\n",
      "\n",
      "HIGH_CONCURRENT - EXECUTION TIMELINE:\n",
      "----------------------------------------------------------------------\n",
      "Default stream:    [1221.7ms → 1354.2ms] duration=132.56ms\n",
      "Concurrent stream:\n",
      "  Execution 1:  [0.0ms → 64.9ms] duration=64.92ms, NO OVERLAP\n",
      "  Execution 2:  [66.5ms → 133.6ms] duration=67.09ms, NO OVERLAP\n",
      "  Execution 3:  [135.2ms → 201.9ms] duration=66.68ms, NO OVERLAP\n",
      "  Execution 4:  [203.5ms → 269.5ms] duration=65.98ms, NO OVERLAP\n",
      "  Execution 5:  [271.1ms → 337.9ms] duration=66.87ms, NO OVERLAP\n",
      "  Execution 6:  [339.6ms → 405.5ms] duration=65.98ms, NO OVERLAP\n",
      "  Execution 7:  [407.2ms → 473.0ms] duration=65.82ms, NO OVERLAP\n",
      "  Execution 8:  [474.6ms → 541.3ms] duration=66.73ms, NO OVERLAP\n",
      "  Execution 9:  [543.0ms → 609.6ms] duration=66.65ms, NO OVERLAP\n",
      "  Execution 10:  [611.3ms → 678.1ms] duration=66.86ms, NO OVERLAP\n",
      "  Execution 11:  [679.8ms → 746.4ms] duration=66.63ms, NO OVERLAP\n",
      "  Execution 12:  [748.0ms → 814.0ms] duration=65.94ms, NO OVERLAP\n",
      "  Execution 13:  [815.6ms → 882.3ms] duration=66.65ms, NO OVERLAP\n",
      "  Execution 14:  [883.9ms → 950.6ms] duration=66.69ms, NO OVERLAP\n",
      "  Execution 15:  [952.3ms → 1017.9ms] duration=65.63ms, NO OVERLAP\n",
      "  Execution 16:  [1019.5ms → 1085.5ms] duration=66.05ms, NO OVERLAP\n",
      "  Execution 17:  [1087.1ms → 1153.4ms] duration=66.22ms, NO OVERLAP\n",
      "  Execution 18:  [1155.0ms → 1221.8ms] duration=66.83ms, overlap=0.2ms (0.2%)\n",
      "  Execution 19:  [1223.9ms → 1349.9ms] duration=125.93ms, overlap=125.9ms (100.0%)\n",
      "  Execution 20:  [1351.6ms → 1425.8ms] duration=74.22ms, overlap=2.6ms (3.5%)\n",
      "  Execution 21:  [1427.5ms → 1494.8ms] duration=67.36ms, NO OVERLAP\n",
      "\n",
      "Summary:\n",
      "  Total time span: 1494.81ms\n",
      "  Concurrent executions overlapping with default: 3/21\n",
      "  Default stream coverage: 97.1%\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "VECTOR FORENSICS\n",
      "======================================================================\n",
      "\n",
      "LOW_CONCURRENT - KEYS:\n",
      "  Total differences: 0\n",
      "  All positions bit-exact: True\n",
      "\n",
      "HIGH_CONCURRENT - KEYS:\n",
      "  Total differences: 0\n",
      "  All positions bit-exact: True\n",
      "\n",
      "======================================================================\n",
      "TIMING FORENSICS\n",
      "======================================================================\n",
      "\n",
      "LOW_CONCURRENT:\n",
      "  Time: 107.37ms (baseline: 68.70ms)\n",
      "  Slowdown: +38.67ms (+56.3%)\n",
      "  Significant: YES\n",
      "\n",
      "HIGH_CONCURRENT:\n",
      "  Time: 132.56ms (baseline: 68.70ms)\n",
      "  Slowdown: +63.86ms (+93.0%)\n",
      "  Significant: YES\n",
      "\n",
      "======================================================================\n",
      "VERDICT\n",
      "======================================================================\n",
      "\n",
      "⚠️  NO FP INTERFERENCE in prefill keys\n",
      "   Even high-AI workloads remain numerically stable\n",
      "   → Prefill+prefill does not create detectable FP differences\n",
      "   → Measurement system verified (sanity check passed)\n",
      "\n",
      "✓ Timing forensics works\n",
      "   Compute/memory contention detected\n",
      "\n",
      "✓ Results saved to /workspace/exp1_prefill_prefill_20251108_211756.json\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Experiment 1: Prefill + Concurrent Prefill\n",
    "Tests: High AI + High AI interference\n",
    "\n",
    "Uses very long prompts to ensure high compute utilization.\n",
    "Same methodology as exp2_decode_timeline.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "import threading\n",
    "import socket\n",
    "from collections import defaultdict\n",
    "\n",
    "# ============================================================================\n",
    "# ATTESTATION\n",
    "# ============================================================================\n",
    "\n",
    "def attest_system():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SYSTEM ATTESTATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    attestation = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"hostname\": socket.gethostname(),\n",
    "        \"pytorch\": {\n",
    "            \"version\": torch.__version__,\n",
    "            \"cuda_version\": torch.version.cuda,\n",
    "        },\n",
    "        \"gpu\": {\n",
    "            \"name\": torch.cuda.get_device_name(0),\n",
    "            \"capability\": f\"{torch.cuda.get_device_capability(0)[0]}.{torch.cuda.get_device_capability(0)[1]}\",\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f\"GPU: {attestation['gpu']['name']}\")\n",
    "    print(f\"PyTorch: {attestation['pytorch']['version']}\")\n",
    "    print(f\"CUDA: {attestation['pytorch']['cuda_version']}\")\n",
    "    print()\n",
    "    return attestation\n",
    "\n",
    "class GPUMonitor:\n",
    "    def __init__(self, interval=0.1):\n",
    "        self.interval = interval\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.samples = []\n",
    "    \n",
    "    def _monitor(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                output = subprocess.check_output(\n",
    "                    ['nvidia-smi', '--query-gpu=utilization.gpu,utilization.memory',\n",
    "                     '--format=csv,noheader,nounits'],\n",
    "                    encoding='utf-8'\n",
    "                )\n",
    "                gpu_util, mem_util = map(float, output.strip().split(','))\n",
    "                self.samples.append({'gpu': gpu_util, 'memory': mem_util})\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(self.interval)\n",
    "    \n",
    "    def start(self):\n",
    "        self.samples = []\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._monitor, daemon=True)\n",
    "        self.thread.start()\n",
    "    \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=2)\n",
    "        \n",
    "        if not self.samples:\n",
    "            return None\n",
    "        \n",
    "        gpu_utils = [s['gpu'] for s in self.samples]\n",
    "        return {\n",
    "            'gpu_mean': np.mean(gpu_utils),\n",
    "            'gpu_p95': np.percentile(gpu_utils, 95),\n",
    "            'gpu_max': np.max(gpu_utils),\n",
    "            'samples': len(self.samples)\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "LAYER_INDICES = [1, 4, 10, 18, 28]\n",
    "NUM_RUNS = 1  # Single run with verified parallel execution\n",
    "SAMPLE_INTERVAL = 50  # Sample every 50th position (prompts are long)\n",
    "\n",
    "# Very long prompts for high compute utilization during prefill\n",
    "LONG_PROMPT_TEMPLATE = \"\"\"You are tasked with providing a comprehensive technical analysis of distributed database systems for a Fortune 500 company evaluating data infrastructure modernization. The company operates a legacy Oracle RAC database cluster with 500TB of structured data, 2PB of unstructured data in S3, serving 100,000 concurrent users across 50 global offices in 25 countries. They need to evaluate migration paths to modern distributed architectures supporting both OLTP and OLAP workloads with minimal downtime while maintaining ACID guarantees and supporting real-time analytics.\n",
    "\n",
    "Current infrastructure: 12-node Oracle RAC cluster on bare metal with SAN storage, 50ms average query latency, 99.95% uptime SLA, backup RPO of 1 hour. Daily batch processing takes 8 hours using legacy ETL tools. The system processes 2 million transactions per day with peak loads of 50,000 TPS during business hours. Critical business applications include order management, inventory tracking, customer relationship management, financial reporting, and real-time fraud detection.\n",
    "\n",
    "Technical constraints: Must maintain sub-100ms p99 latency for OLTP queries, support complex JOIN operations across 200+ tables, enable real-time materialized views for analytics, provide point-in-time recovery for compliance, support multi-region disaster recovery with RPO < 5 minutes, enable blue-green deployments for zero-downtime upgrades, maintain referential integrity across distributed transactions, support both SQL and NoSQL query patterns, enable real-time data replication to analytics warehouse.\n",
    "\n",
    "Evaluation criteria: Total cost of ownership over 5 years including licensing, infrastructure, operations, migration costs. Performance benchmarks for OLTP (TPS, latency), OLAP (query response time, concurrency), mixed workloads. Operational complexity including monitoring, troubleshooting, capacity planning, disaster recovery procedures. Vendor ecosystem including tooling support, cloud integration, community resources. Migration strategy including data migration approach, application refactoring requirements, downtime windows, rollback procedures.\n",
    "\n",
    "Modern architecture options to evaluate: (1) Amazon Aurora PostgreSQL with read replicas and Aurora Serverless v2 for variable workloads. (2) Google Cloud Spanner for globally distributed ACID transactions with TrueTime. (3) CockroachDB for distributed SQL with automatic sharding and rebalancing. (4) MongoDB Atlas with multi-region clusters and change streams. (5) Cassandra with Spark for analytics workloads. (6) Hybrid approach using PostgreSQL for OLTP with Snowflake for OLAP. (7) YugabyteDB for PostgreSQL compatibility with distributed architecture.\"\"\"\n",
    "\n",
    "# Create variations for concurrent stream\n",
    "CONCURRENT_PROMPT_TEMPLATE = \"\"\"You are providing expert analysis on cloud-native application architecture patterns for a global e-commerce platform processing $5B in annual revenue. The platform serves 50 million active users across web, mobile, and API channels with 99.99% availability requirements.\n",
    "\n",
    "Current architecture: Monolithic Java application (2.5M lines of code) on 100 EC2 instances behind ELB, MySQL cluster with read replicas, Redis for caching, RabbitMQ for async processing. System handles 10M requests per day with average response time of 500ms. Critical services include product catalog, shopping cart, order processing, payment gateway integration, inventory management, recommendation engine, fraud detection, customer service portal.\n",
    "\n",
    "Modernization drivers: Deployment velocity (currently 2-week release cycles, targeting daily deployments), operational costs (EC2 + RDS spending $2M annually), scaling challenges (Black Friday requires 10x capacity, manual scaling takes hours), developer productivity (build times 45 minutes, integration testing 4 hours), service reliability (cascading failures affect entire platform).\n",
    "\n",
    "Architecture evaluation criteria: Operational complexity (service orchestration, observability, debugging distributed transactions), development velocity (microservice boundaries, API contracts, testing strategies), infrastructure costs (compute, storage, networking, managed services), reliability patterns (circuit breakers, bulkheads, rate limiting, chaos engineering), data consistency (eventual vs strong consistency, saga patterns, event sourcing), security posture (service mesh, mTLS, API gateways, secret management).\n",
    "\n",
    "Proposed patterns: (1) Strangler fig migration with API gateway routing legacy vs new services. (2) Event-driven architecture with Kafka for service communication. (3) CQRS with separate read/write models and materialized views. (4) Service mesh with Istio for traffic management and observability. (5) Serverless for variable workloads using Lambda/Fargate. (6) GraphQL federation for unified API layer. (7) Feature flags and canary deployments for gradual rollout.\n",
    "\n",
    "Technical considerations: Service decomposition strategy (bounded contexts, team ownership, API versioning), data management (database per service, shared database, event sourcing), inter-service communication (synchronous REST/gRPC vs asynchronous messaging), distributed transactions (saga patterns, compensating transactions, idempotency), observability (distributed tracing, metrics aggregation, log correlation), testing strategy (contract testing, chaos engineering, synthetic monitoring), deployment architecture (Kubernetes, service mesh, API gateway, monitoring stack).\"\"\"\n",
    "\n",
    "# Scale to target length\n",
    "DEFAULT_PROMPT = (LONG_PROMPT_TEMPLATE + \"\\n\\n\" + \"Please analyze each option in detail. \" * 50).strip()\n",
    "CONCURRENT_PROMPT = (CONCURRENT_PROMPT_TEMPLATE + \"\\n\\n\" + \"Provide detailed recommendations. \" * 50).strip()\n",
    "\n",
    "# ============================================================================\n",
    "# CONCURRENT STREAM\n",
    "# ============================================================================\n",
    "\n",
    "class ConcurrentStream:\n",
    "    \"\"\"Concurrent prefill stream with execution timing\"\"\"\n",
    "    def __init__(self, model, tokenizer, prompt, device=\"cuda\"):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = prompt\n",
    "        self.device = device\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        self.should_stop = threading.Event()\n",
    "        self.thread = None\n",
    "        \n",
    "        self.execution_times = []\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def _run_stream(self):\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            while not self.should_stop.is_set():\n",
    "                try:\n",
    "                    inputs = self.tokenizer([self.prompt], return_tensors=\"pt\")\n",
    "                    inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "                    \n",
    "                    start = time.perf_counter()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        # Prefill only - no generation\n",
    "                        _ = self.model(**inputs, use_cache=True)\n",
    "                    \n",
    "                    torch.cuda.synchronize(self.stream)\n",
    "                    end = time.perf_counter()\n",
    "                    \n",
    "                    with self.lock:\n",
    "                        self.execution_times.append((start, end))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  [WARNING] Concurrent stream error: {e}\")\n",
    "                    break\n",
    "    \n",
    "    def start(self):\n",
    "        self.execution_times = []\n",
    "        self.should_stop.clear()\n",
    "        self.thread = threading.Thread(target=self._run_stream, daemon=True)\n",
    "        self.thread.start()\n",
    "        time.sleep(0.2)\n",
    "    \n",
    "    def stop(self):\n",
    "        self.should_stop.set()\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=10)\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    def get_execution_times(self):\n",
    "        with self.lock:\n",
    "            return list(self.execution_times)\n",
    "\n",
    "# ============================================================================\n",
    "# TIMELINE VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def print_execution_timeline(default_start, default_end, concurrent_times, condition_name):\n",
    "    \"\"\"Print visual timeline showing when default and concurrent streams executed\"\"\"\n",
    "    print(f\"\\n{condition_name} - EXECUTION TIMELINE:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    if not concurrent_times:\n",
    "        default_duration = (default_end - default_start) * 1000\n",
    "        print(f\"Default stream:    [{default_start:.3f} → {default_end:.3f}] {default_duration:.2f}ms\")\n",
    "        print(f\"Concurrent stream: (none)\")\n",
    "        return\n",
    "    \n",
    "    all_times = [default_start, default_end]\n",
    "    for start, end in concurrent_times:\n",
    "        all_times.extend([start, end])\n",
    "    \n",
    "    min_time = min(all_times)\n",
    "    max_time = max(all_times)\n",
    "    time_span = max_time - min_time\n",
    "    \n",
    "    default_duration = (default_end - default_start) * 1000\n",
    "    default_rel_start = default_start - min_time\n",
    "    default_rel_end = default_end - min_time\n",
    "    print(f\"Default stream:    [{default_rel_start*1000:.1f}ms → {default_rel_end*1000:.1f}ms] duration={default_duration:.2f}ms\")\n",
    "    \n",
    "    print(f\"Concurrent stream:\")\n",
    "    overlapping = []\n",
    "    for i, (start, end) in enumerate(concurrent_times):\n",
    "        duration = (end - start) * 1000\n",
    "        rel_start = start - min_time\n",
    "        rel_end = end - min_time\n",
    "        \n",
    "        overlap_start = max(default_start, start)\n",
    "        overlap_end = min(default_end, end)\n",
    "        \n",
    "        if overlap_start < overlap_end:\n",
    "            overlap_duration = (overlap_end - overlap_start) * 1000\n",
    "            overlap_pct = (overlap_duration / duration) * 100\n",
    "            overlap_str = f\"overlap={overlap_duration:.1f}ms ({overlap_pct:.1f}%)\"\n",
    "            overlapping.append((start, end, overlap_end - overlap_start))\n",
    "        else:\n",
    "            overlap_str = \"NO OVERLAP\"\n",
    "        \n",
    "        print(f\"  Execution {i+1}:  [{rel_start*1000:.1f}ms → {rel_end*1000:.1f}ms] duration={duration:.2f}ms, {overlap_str}\")\n",
    "    \n",
    "    if overlapping:\n",
    "        total_concurrent_overlap = sum(dur for _, _, dur in overlapping)\n",
    "        default_coverage = (total_concurrent_overlap / (default_end - default_start)) * 100\n",
    "        \n",
    "        print(f\"\\nSummary:\")\n",
    "        print(f\"  Total time span: {time_span*1000:.2f}ms\")\n",
    "        print(f\"  Concurrent executions overlapping with default: {len(overlapping)}/{len(concurrent_times)}\")\n",
    "        print(f\"  Default stream coverage: {default_coverage:.1f}%\")\n",
    "    \n",
    "    print(\"-\" * 70)\n",
    "\n",
    "# ============================================================================\n",
    "# KEY EXTRACTION FROM PREFILL\n",
    "# ============================================================================\n",
    "\n",
    "def extract_keys_prefill(model, tokenizer, prompt, layer_indices,\n",
    "                        condition_name, sample_interval=50,\n",
    "                        concurrent_stream=None):\n",
    "    \"\"\"\n",
    "    Run prefill and extract keys from attention output.\n",
    "    Returns keys from sampled positions.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n[{condition_name}] Starting...\")\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    if concurrent_stream:\n",
    "        print(f\"[{condition_name}] Starting concurrent stream...\")\n",
    "        concurrent_stream.start()\n",
    "        time.sleep(1.0)\n",
    "    \n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    prompt_len = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    print(f\"[{condition_name}] Prompt: {prompt_len} tokens (prefill only)\")\n",
    "    \n",
    "    all_keys = defaultdict(dict)\n",
    "    monitor = GPUMonitor()\n",
    "    monitor.start()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    start = time.perf_counter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, use_cache=True, return_dict=True)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end = time.perf_counter()\n",
    "    elapsed = (end - start) * 1000\n",
    "    \n",
    "    default_stream_time = (start, end)\n",
    "    \n",
    "    # Extract keys from past_key_values\n",
    "    past_kv = outputs.past_key_values\n",
    "    \n",
    "    # Sample positions\n",
    "    positions_to_sample = list(range(prompt_len - 1, -1, -sample_interval))\n",
    "    if positions_to_sample[-1] != 0:\n",
    "        positions_to_sample.append(0)\n",
    "    positions_to_sample = sorted(positions_to_sample)\n",
    "    \n",
    "    print(f\"[{condition_name}] Sampling {len(positions_to_sample)} positions from prompt\")\n",
    "    \n",
    "    for layer_idx in layer_indices:\n",
    "        if layer_idx == 0:\n",
    "            continue\n",
    "        \n",
    "        keys_all_positions = past_kv[layer_idx - 1][0]  # [batch, heads, seq, head_dim]\n",
    "        \n",
    "        for position in positions_to_sample:\n",
    "            key_vector = keys_all_positions[0, :, position, :]\n",
    "            layer_key_name = f\"layer_{layer_idx}\"\n",
    "            all_keys[position][layer_key_name] = key_vector.reshape(-1).float().cpu().numpy()\n",
    "    \n",
    "    gpu_stats = monitor.stop()\n",
    "    \n",
    "    concurrent_stream_times = None\n",
    "    if concurrent_stream:\n",
    "        concurrent_stream.stop()\n",
    "        concurrent_stream_times = concurrent_stream.get_execution_times()\n",
    "        \n",
    "        if default_stream_time:\n",
    "            print_execution_timeline(\n",
    "                default_stream_time[0],\n",
    "                default_stream_time[1],\n",
    "                concurrent_stream_times,\n",
    "                condition_name\n",
    "            )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    timing_stats = {\n",
    "        'mean_ms': float(elapsed),\n",
    "        'median_ms': float(elapsed),\n",
    "        'std_ms': 0.0,\n",
    "        'all_times': [float(elapsed)]\n",
    "    }\n",
    "    \n",
    "    sample_pos = list(all_keys.keys())[0] if all_keys else None\n",
    "    if sample_pos:\n",
    "        layers_captured = list(all_keys[sample_pos].keys())\n",
    "        print(f\"[{condition_name}] ✓ Extracted: {len(layers_captured)} vectors per position\")\n",
    "    \n",
    "    return dict(all_keys), timing_stats, gpu_stats, default_stream_time, concurrent_stream_times\n",
    "\n",
    "# ============================================================================\n",
    "# ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def compare_keys(baseline_keys, test_keys):\n",
    "    \"\"\"Compare key vectors between baseline and test conditions\"\"\"\n",
    "    results = {\n",
    "        'all_positions_exact': True,\n",
    "        'positions': {}\n",
    "    }\n",
    "    \n",
    "    shared_positions = set(baseline_keys.keys()) & set(test_keys.keys())\n",
    "    \n",
    "    for position in sorted(shared_positions):\n",
    "        base_pos = baseline_keys[position]\n",
    "        test_pos = test_keys[position]\n",
    "        \n",
    "        position_result = {\n",
    "            'all_layers_exact': True,\n",
    "            'layers': {}\n",
    "        }\n",
    "        \n",
    "        for layer_name in base_pos.keys() & test_pos.keys():\n",
    "            base = base_pos[layer_name]\n",
    "            test = test_pos[layer_name]\n",
    "            \n",
    "            bit_exact = np.array_equal(base, test)\n",
    "            l2 = 0.0 if bit_exact else float(np.linalg.norm(base - test))\n",
    "            \n",
    "            position_result['layers'][layer_name] = {\n",
    "                'bit_exact': bit_exact,\n",
    "                'l2': l2\n",
    "            }\n",
    "            \n",
    "            if not bit_exact:\n",
    "                position_result['all_layers_exact'] = False\n",
    "                results['all_positions_exact'] = False\n",
    "        \n",
    "        results['positions'][position] = position_result\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_timing(baseline_time, test_time, baseline_std):\n",
    "    \"\"\"Analyze timing differences\"\"\"\n",
    "    slowdown_ms = test_time - baseline_time\n",
    "    slowdown_pct = (slowdown_ms / baseline_time) * 100\n",
    "    threshold = 2 * baseline_std if baseline_std > 0 else baseline_time * 0.1\n",
    "    significant = abs(slowdown_ms) > threshold\n",
    "    \n",
    "    return {\n",
    "        'time_ms': float(test_time),\n",
    "        'baseline_ms': float(baseline_time),\n",
    "        'slowdown_ms': float(slowdown_ms),\n",
    "        'slowdown_pct': float(slowdown_pct),\n",
    "        'significant': significant\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    attestation = attest_system()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"EXPERIMENT 1: PREFILL + CONCURRENT PREFILL\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Configuration:\")\n",
    "    print(\"  • Prefill-only operations (high arithmetic intensity)\")\n",
    "    print(\"  • Very long prompts for high compute utilization\")\n",
    "    print(\"  • Extract keys from attention output\")\n",
    "    print(\"  • Timeline visualization for parallel execution\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Loading model...\")\n",
    "    device = \"cuda\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"✓ Model loaded\\n\")\n",
    "    \n",
    "    default_tokens = len(tokenizer.encode(DEFAULT_PROMPT))\n",
    "    concurrent_tokens = len(tokenizer.encode(CONCURRENT_PROMPT))\n",
    "    \n",
    "    print(f\"Configuration:\")\n",
    "    print(f\"  Default stream: {default_tokens} tokens (prefill)\")\n",
    "    print(f\"  Concurrent stream: {concurrent_tokens} tokens (prefill)\")\n",
    "    print(f\"  Sample interval: every {SAMPLE_INTERVAL}th position\")\n",
    "    print(f\"  Layers: {LAYER_INDICES}\")\n",
    "    print()\n",
    "    \n",
    "    # Run conditions\n",
    "    conditions = {}\n",
    "    \n",
    "    # 0. SANITY CHECK - Different prompt should give different keys\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SANITY CHECK: Verifying measurement system works\")\n",
    "    print(\"=\" * 70)\n",
    "    perturbed_prompt = \"[VERIFICATION TOKEN] \" + DEFAULT_PROMPT  # Perturb at START for causal attention\n",
    "    sanity_keys, _, _, _, _ = extract_keys_prefill(\n",
    "        model, tokenizer, perturbed_prompt, LAYER_INDICES,\n",
    "        \"SANITY_CHECK\", SAMPLE_INTERVAL, concurrent_stream=None\n",
    "    )\n",
    "    \n",
    "    # 1. Baseline\n",
    "    base_keys, base_timing, base_gpu, base_default_time, base_concurrent_times = extract_keys_prefill(\n",
    "        model, tokenizer, DEFAULT_PROMPT, LAYER_INDICES,\n",
    "        \"BASELINE\", SAMPLE_INTERVAL, concurrent_stream=None\n",
    "    )\n",
    "    \n",
    "    # Verify sanity check\n",
    "    print(\"\\nVerifying measurement system:\")\n",
    "    sanity_check_passed = False\n",
    "    shared_positions = set(base_keys.keys()) & set(sanity_keys.keys())\n",
    "    \n",
    "    if not shared_positions:\n",
    "        print(\"  ⚠️  WARNING: No shared positions between prompts!\")\n",
    "        print(\"  → Cannot verify measurement system\")\n",
    "        sanity_check_passed = None\n",
    "    else:\n",
    "        for pos in sorted(shared_positions)[:3]:  # Check first 3 shared positions\n",
    "            for layer in base_keys[pos].keys():\n",
    "                if not np.array_equal(base_keys[pos][layer], sanity_keys[pos][layer]):\n",
    "                    l2 = np.linalg.norm(base_keys[pos][layer] - sanity_keys[pos][layer])\n",
    "                    print(f\"  Position {pos}, {layer}: DIFFERENT (L2={l2:.2e}) ✓\")\n",
    "                    sanity_check_passed = True\n",
    "                    break\n",
    "            if sanity_check_passed:\n",
    "                break\n",
    "    \n",
    "    if sanity_check_passed == False:\n",
    "        print(\"  ⚠️  WARNING: Perturbed prompt gave IDENTICAL keys!\")\n",
    "        print(\"  → Measurement system may be broken\")\n",
    "        print(\"  → Results below are INVALID\")\n",
    "    elif sanity_check_passed == True:\n",
    "        print(\"  ✓ Measurement system verified - different prompts give different keys\")\n",
    "    print()\n",
    "    \n",
    "    conditions['baseline'] = {\n",
    "        'keys': base_keys,\n",
    "        'timing': base_timing,\n",
    "        'gpu': base_gpu,\n",
    "        'default_time': base_default_time,\n",
    "        'concurrent_times': base_concurrent_times\n",
    "    }\n",
    "    \n",
    "    # 2. Low concurrent (short prompt)\n",
    "    short_concurrent_prompt = CONCURRENT_PROMPT[:len(CONCURRENT_PROMPT)//3]  # 1/3 length\n",
    "    low_stream = ConcurrentStream(model, tokenizer, short_concurrent_prompt, device)\n",
    "    low_keys, low_timing, low_gpu, low_default_time, low_concurrent_times = extract_keys_prefill(\n",
    "        model, tokenizer, DEFAULT_PROMPT, LAYER_INDICES,\n",
    "        \"LOW_CONCURRENT\", SAMPLE_INTERVAL, concurrent_stream=low_stream\n",
    "    )\n",
    "    conditions['low_concurrent'] = {\n",
    "        'keys': low_keys,\n",
    "        'timing': low_timing,\n",
    "        'gpu': low_gpu,\n",
    "        'default_time': low_default_time,\n",
    "        'concurrent_times': low_concurrent_times\n",
    "    }\n",
    "    \n",
    "    # 3. High concurrent (full prompt)\n",
    "    high_stream = ConcurrentStream(model, tokenizer, CONCURRENT_PROMPT, device)\n",
    "    high_keys, high_timing, high_gpu, high_default_time, high_concurrent_times = extract_keys_prefill(\n",
    "        model, tokenizer, DEFAULT_PROMPT, LAYER_INDICES,\n",
    "        \"HIGH_CONCURRENT\", SAMPLE_INTERVAL, concurrent_stream=high_stream\n",
    "    )\n",
    "    conditions['high_concurrent'] = {\n",
    "        'keys': high_keys,\n",
    "        'timing': high_timing,\n",
    "        'gpu': high_gpu,\n",
    "        'default_time': high_default_time,\n",
    "        'concurrent_times': high_concurrent_times\n",
    "    }\n",
    "    \n",
    "    # Analyze vectors\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VECTOR FORENSICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    key_results = {}\n",
    "    \n",
    "    for cond_name in ['low_concurrent', 'high_concurrent']:\n",
    "        key_results[cond_name] = compare_keys(\n",
    "            base_keys, conditions[cond_name]['keys']\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{cond_name.upper()} - KEYS:\")\n",
    "        result = key_results[cond_name]\n",
    "        total_key_diffs = sum(\n",
    "            1 for pos_data in result['positions'].values()\n",
    "            for layer_data in pos_data['layers'].values()\n",
    "            if not layer_data['bit_exact']\n",
    "        )\n",
    "        print(f\"  Total differences: {total_key_diffs}\")\n",
    "        print(f\"  All positions bit-exact: {result['all_positions_exact']}\")\n",
    "        \n",
    "        if not result['all_positions_exact']:\n",
    "            # Show some examples\n",
    "            positions_sorted = sorted(result['positions'].keys())[:3]\n",
    "            for pos in positions_sorted:\n",
    "                pos_data = result['positions'][pos]\n",
    "                has_diffs = any(not ld['bit_exact'] for ld in pos_data['layers'].values())\n",
    "                if has_diffs:\n",
    "                    print(f\"  Position {pos}:\")\n",
    "                    for layer_name in sorted(pos_data['layers'].keys()):\n",
    "                        layer_data = pos_data['layers'][layer_name]\n",
    "                        if not layer_data['bit_exact']:\n",
    "                            print(f\"    {layer_name}: ✗ DIFF (L2={layer_data['l2']:.2e})\")\n",
    "    \n",
    "    # Timing analysis\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TIMING FORENSICS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    baseline_time = base_timing['median_ms']\n",
    "    baseline_std = base_timing['std_ms']\n",
    "    \n",
    "    timing_results = {}\n",
    "    for cond_name in ['low_concurrent', 'high_concurrent']:\n",
    "        test_time = conditions[cond_name]['timing']['median_ms']\n",
    "        timing_results[cond_name] = analyze_timing(baseline_time, test_time, baseline_std)\n",
    "        \n",
    "        result = timing_results[cond_name]\n",
    "        print(f\"\\n{cond_name.upper()}:\")\n",
    "        print(f\"  Time: {result['time_ms']:.2f}ms (baseline: {result['baseline_ms']:.2f}ms)\")\n",
    "        print(f\"  Slowdown: {result['slowdown_ms']:+.2f}ms ({result['slowdown_pct']:+.1f}%)\")\n",
    "        print(f\"  Significant: {'YES' if result['significant'] else 'NO'}\")\n",
    "    \n",
    "    # Verdict\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"VERDICT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    keys_exact = all(key_results[c]['all_positions_exact'] for c in key_results)\n",
    "    timing_detects = any(timing_results[c]['significant'] for c in timing_results)\n",
    "    \n",
    "    if sanity_check_passed == False:\n",
    "        print(\"\\n⚠️  INVALID RESULTS - Measurement system verification failed\")\n",
    "        print(\"   Different prompts produced identical keys\")\n",
    "        print(\"   → Cannot trust any results from this run\")\n",
    "    elif sanity_check_passed == None:\n",
    "        print(\"\\n⚠️  INCONCLUSIVE - Could not verify measurement system\")\n",
    "        print(\"   No shared positions between prompts for comparison\")\n",
    "    elif not keys_exact:\n",
    "        print(\"\\n✓ FP INTERFERENCE DETECTED in prefill keys\")\n",
    "        print(\"   High-AI workloads show numerical sensitivity\")\n",
    "        print(\"   → Prefill forensics CAN detect hidden concurrent work\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  NO FP INTERFERENCE in prefill keys\")\n",
    "        print(\"   Even high-AI workloads remain numerically stable\")\n",
    "        print(\"   → Prefill+prefill does not create detectable FP differences\")\n",
    "        print(\"   → Measurement system verified (sanity check passed)\")\n",
    "    \n",
    "    if timing_detects:\n",
    "        print(\"\\n✓ Timing forensics works\")\n",
    "        print(\"   Compute/memory contention detected\")\n",
    "    else:\n",
    "        print(\"\\n✗ No timing slowdown detected\")\n",
    "        print(\"   Resources sufficient for concurrent prefill\")\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    output = {\n",
    "        'experiment': 'exp1_prefill_prefill',\n",
    "        'attestation': attestation,\n",
    "        'config': {\n",
    "            'model': MODEL_NAME,\n",
    "            'default_tokens': default_tokens,\n",
    "            'concurrent_tokens': concurrent_tokens,\n",
    "            'sample_interval': SAMPLE_INTERVAL,\n",
    "            'layer_indices': LAYER_INDICES\n",
    "        },\n",
    "        'sanity_check': {\n",
    "            'passed': sanity_check_passed,\n",
    "            'description': 'Verified different prompts produce different keys' if sanity_check_passed == True else 'Could not verify' if sanity_check_passed == None else 'Verification failed'\n",
    "        },\n",
    "        'forensics': {\n",
    "            'keys': {c: {'all_exact': r['all_positions_exact']} for c, r in key_results.items()}\n",
    "        },\n",
    "        'timing_forensics': timing_results,\n",
    "        'verdict': {\n",
    "            'keys_exact': keys_exact,\n",
    "            'timing_detects': timing_detects,\n",
    "            'sanity_check_passed': sanity_check_passed\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    output_file = f\"exp1_prefill_prefill_{timestamp}.json\"\n",
    "    with open(f\"/workspace/{output_file}\", 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n✓ Results saved to /workspace/{output_file}\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXPERIMENT COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c656ce-2255-458a-9c51-e58d6961f420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
