{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "676165e8-d360-4a95-93c9-328026b8fc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "DECODE PARALLEL MATRIX EXPERIMENT - MULTI-POSITION\n",
      "======================================================================\n",
      "\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "PyTorch: 2.8.0+cu128\n",
      "CUDA: 12.8\n",
      "\n",
      "Loading model...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121766d05b1c4cc78e384de4ef7ce9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "\n",
      "MULTI-POSITION EXTRACTION STRATEGY:\n",
      "  Extracting keys at 4 positions during generation:\n",
      "    - 20% through generation (token ~30/150)\n",
      "    - 50% through generation (token ~75/150)\n",
      "    - 80% through generation (token ~120/150)\n",
      "    - 100% through generation (token ~150/150)\n",
      "\n",
      "======================================================================\n",
      "REFERENCE 1/3: REF_TECHNICAL\n",
      "======================================================================\n",
      "\n",
      "Condition 1/3: BASELINE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[BASELINE] Starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Input length: 258 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [287, 332, 377, 407]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 258 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [287, 332, 377, 407]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 258 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [287, 332, 377, 407]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "[BASELINE] Generation time: 3510.87ms ± 227.73ms\n",
      "[BASELINE] Total measurement duration: 14.5s\n",
      "[BASELINE] GPU util: 61.8% (P95)\n",
      "[BASELINE] Reproducibility: ✓ BIT-EXACT (all positions)\n",
      "\n",
      "Condition 2/3: LIGHT_CONCURRENT\n",
      "----------------------------------------------------------------------\n",
      "  [HIDDEN] Concurrent generation started (long decode)\n",
      "  [HIDDEN] Generating 100 tokens continuously\n",
      "\n",
      "[LIGHT_CONCURRENT] Starting generation...\n",
      "    Input length: 258 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [287, 332, 377, 407]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 258 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [287, 332, 377, 407]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 258 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [287, 332, 377, 407]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "[LIGHT_CONCURRENT] Generation time: 8142.52ms ± 89.37ms\n",
      "[LIGHT_CONCURRENT] Total measurement duration: 28.6s\n",
      "[LIGHT_CONCURRENT] GPU util: 71.4% (P95)\n",
      "  [HIDDEN] Concurrent generation stopped\n",
      "  [HIDDEN] Ran for 32.2s\n",
      "  [HIDDEN] Completed 8 generations\n",
      "  [HIDDEN] Rate: 0.25 gen/sec\n",
      "[LIGHT_CONCURRENT] Reproducibility: ✓ BIT-EXACT (all positions)\n",
      "\n",
      "Condition 3/3: HEAVY_CONCURRENT\n",
      "----------------------------------------------------------------------\n",
      "  [HIDDEN] Concurrent generation started (long decode)\n",
      "  [HIDDEN] Generating 150 tokens continuously\n",
      "\n",
      "[HEAVY_CONCURRENT] Starting generation...\n",
      "    Input length: 258 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [287, 332, 377, 407]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 258 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [287, 332, 377, 407]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 258 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [287, 332, 377, 407]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "[HEAVY_CONCURRENT] Generation time: 8696.04ms ± 472.49ms\n",
      "[HEAVY_CONCURRENT] Total measurement duration: 30.2s\n",
      "[HEAVY_CONCURRENT] GPU util: 68.9% (P95)\n",
      "  [HIDDEN] Concurrent generation stopped\n",
      "  [HIDDEN] Ran for 32.8s\n",
      "  [HIDDEN] Completed 5 generations\n",
      "  [HIDDEN] Rate: 0.15 gen/sec\n",
      "[HEAVY_CONCURRENT] Reproducibility: ✓ BIT-EXACT (all positions)\n",
      "\n",
      "======================================================================\n",
      "FORENSIC ANALYSIS: REF_TECHNICAL\n",
      "======================================================================\n",
      "\n",
      "FP FORENSICS - BY POSITION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "\n",
      "  POS_20PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_50PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_80PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_100PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "\n",
      "  POS_20PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_50PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_80PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_100PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "TIMING FORENSICS\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Baseline: 3510.87ms\n",
      "  GPU util: 61.8%\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "  Time: 8142.52ms (baseline: 3510.87ms)\n",
      "  Slowdown: +4631.65ms (+131.9%)\n",
      "  Significant: YES\n",
      "  GPU util: 71.4%\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "  Time: 8696.04ms (baseline: 3510.87ms)\n",
      "  Slowdown: +5185.17ms (+147.7%)\n",
      "  Significant: YES\n",
      "  GPU util: 68.9%\n",
      "\n",
      "TEMPORAL OVERLAP VERIFICATION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "  Measurement generations: 3\n",
      "  Concurrent generations: 8\n",
      "  Overlapping pairs: 6\n",
      "  ✓ CONFIRMED: 6 measurement/concurrent pairs overlapped\n",
      "  Overlap duration: 4.07s (mean), 5.18s (max)\n",
      "  Overlap fraction: 50.0% (mean), 62.7% (max)\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "  Measurement generations: 3\n",
      "  Concurrent generations: 5\n",
      "  Overlapping pairs: 6\n",
      "  ✓ CONFIRMED: 6 measurement/concurrent pairs overlapped\n",
      "  Overlap duration: 4.35s (mean), 7.20s (max)\n",
      "  Overlap fraction: 50.0% (mean), 80.7% (max)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "REFERENCE 2/3: REF_NARRATIVE\n",
      "======================================================================\n",
      "\n",
      "Condition 1/3: BASELINE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[BASELINE] Starting generation...\n",
      "    Input length: 242 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [271, 316, 361, 391]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 242 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [271, 316, 361, 391]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 242 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [271, 316, 361, 391]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "[BASELINE] Generation time: 3372.93ms ± 11.15ms\n",
      "[BASELINE] Total measurement duration: 14.1s\n",
      "[BASELINE] GPU util: 64.3% (P95)\n",
      "[BASELINE] Reproducibility: ✓ BIT-EXACT (all positions)\n",
      "\n",
      "Condition 2/3: LIGHT_CONCURRENT\n",
      "----------------------------------------------------------------------\n",
      "  [HIDDEN] Concurrent generation started (long decode)\n",
      "  [HIDDEN] Generating 100 tokens continuously\n",
      "\n",
      "[LIGHT_CONCURRENT] Starting generation...\n",
      "    Input length: 242 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [271, 316, 361, 391]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 242 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [271, 316, 361, 391]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 242 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [271, 316, 361, 391]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "[LIGHT_CONCURRENT] Generation time: 8215.61ms ± 111.39ms\n",
      "[LIGHT_CONCURRENT] Total measurement duration: 29.0s\n",
      "[LIGHT_CONCURRENT] GPU util: 69.0% (P95)\n",
      "  [HIDDEN] Concurrent generation stopped\n",
      "  [HIDDEN] Ran for 32.7s\n",
      "  [HIDDEN] Completed 8 generations\n",
      "  [HIDDEN] Rate: 0.24 gen/sec\n",
      "[LIGHT_CONCURRENT] Reproducibility: ✓ BIT-EXACT (all positions)\n",
      "\n",
      "Condition 3/3: HEAVY_CONCURRENT\n",
      "----------------------------------------------------------------------\n",
      "  [HIDDEN] Concurrent generation started (long decode)\n",
      "  [HIDDEN] Generating 150 tokens continuously\n",
      "\n",
      "[HEAVY_CONCURRENT] Starting generation...\n",
      "    Input length: 242 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [271, 316, 361, 391]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 242 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [271, 316, 361, 391]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 242 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [271, 316, 361, 391]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "[HEAVY_CONCURRENT] Generation time: 8059.75ms ± 224.32ms\n",
      "[HEAVY_CONCURRENT] Total measurement duration: 28.3s\n",
      "[HEAVY_CONCURRENT] GPU util: 67.7% (P95)\n",
      "  [HIDDEN] Concurrent generation stopped\n",
      "  [HIDDEN] Ran for 31.1s\n",
      "  [HIDDEN] Completed 5 generations\n",
      "  [HIDDEN] Rate: 0.16 gen/sec\n",
      "[HEAVY_CONCURRENT] Reproducibility: ✓ BIT-EXACT (all positions)\n",
      "\n",
      "======================================================================\n",
      "FORENSIC ANALYSIS: REF_NARRATIVE\n",
      "======================================================================\n",
      "\n",
      "FP FORENSICS - BY POSITION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "\n",
      "  POS_20PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_50PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_80PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_100PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "\n",
      "  POS_20PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_50PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_80PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_100PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "TIMING FORENSICS\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Baseline: 3372.93ms\n",
      "  GPU util: 64.3%\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "  Time: 8215.61ms (baseline: 3372.93ms)\n",
      "  Slowdown: +4842.68ms (+143.6%)\n",
      "  Significant: YES\n",
      "  GPU util: 69.0%\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "  Time: 8059.75ms (baseline: 3372.93ms)\n",
      "  Slowdown: +4686.82ms (+139.0%)\n",
      "  Significant: YES\n",
      "  GPU util: 67.7%\n",
      "\n",
      "TEMPORAL OVERLAP VERIFICATION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "  Measurement generations: 3\n",
      "  Concurrent generations: 8\n",
      "  Overlapping pairs: 6\n",
      "  ✓ CONFIRMED: 6 measurement/concurrent pairs overlapped\n",
      "  Overlap duration: 4.11s (mean), 4.91s (max)\n",
      "  Overlap fraction: 50.0% (mean), 60.5% (max)\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "  Measurement generations: 3\n",
      "  Concurrent generations: 5\n",
      "  Overlapping pairs: 6\n",
      "  ✓ CONFIRMED: 6 measurement/concurrent pairs overlapped\n",
      "  Overlap duration: 4.03s (mean), 6.53s (max)\n",
      "  Overlap fraction: 50.0% (mean), 79.3% (max)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "REFERENCE 3/3: REF_CODE\n",
      "======================================================================\n",
      "\n",
      "Condition 1/3: BASELINE\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "[BASELINE] Starting generation...\n",
      "    Input length: 172 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [201, 246, 291, 321]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 172 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [201, 246, 291, 321]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 172 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [201, 246, 291, 321]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "[BASELINE] Generation time: 3356.31ms ± 15.87ms\n",
      "[BASELINE] Total measurement duration: 14.0s\n",
      "[BASELINE] GPU util: 62.7% (P95)\n",
      "[BASELINE] Reproducibility: ✓ BIT-EXACT (all positions)\n",
      "\n",
      "Condition 2/3: LIGHT_CONCURRENT\n",
      "----------------------------------------------------------------------\n",
      "  [HIDDEN] Concurrent generation started (long decode)\n",
      "  [HIDDEN] Generating 100 tokens continuously\n",
      "\n",
      "[LIGHT_CONCURRENT] Starting generation...\n",
      "    Input length: 172 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [201, 246, 291, 321]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 172 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [201, 246, 291, 321]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 172 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [201, 246, 291, 321]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "[LIGHT_CONCURRENT] Generation time: 8343.53ms ± 17.36ms\n",
      "[LIGHT_CONCURRENT] Total measurement duration: 29.2s\n",
      "[LIGHT_CONCURRENT] GPU util: 69.0% (P95)\n",
      "  [HIDDEN] Concurrent generation stopped\n",
      "  [HIDDEN] Ran for 33.2s\n",
      "  [HIDDEN] Completed 8 generations\n",
      "  [HIDDEN] Rate: 0.24 gen/sec\n",
      "[LIGHT_CONCURRENT] Reproducibility: ✓ BIT-EXACT (all positions)\n",
      "\n",
      "Condition 3/3: HEAVY_CONCURRENT\n",
      "----------------------------------------------------------------------\n",
      "  [HIDDEN] Concurrent generation started (long decode)\n",
      "  [HIDDEN] Generating 150 tokens continuously\n",
      "\n",
      "[HEAVY_CONCURRENT] Starting generation...\n",
      "    Input length: 172 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [201, 246, 291, 321]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 172 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [201, 246, 291, 321]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "    Input length: 172 tokens\n",
      "    Will generate: 150 tokens\n",
      "    Extraction positions (absolute): [201, 246, 291, 321]\n",
      "    Extraction positions (generated): [30, 75, 120, 150]\n",
      "[HEAVY_CONCURRENT] Generation time: 7871.10ms ± 355.82ms\n",
      "[HEAVY_CONCURRENT] Total measurement duration: 27.8s\n",
      "[HEAVY_CONCURRENT] GPU util: 69.9% (P95)\n",
      "  [HIDDEN] Concurrent generation stopped\n",
      "  [HIDDEN] Ran for 30.1s\n",
      "  [HIDDEN] Completed 5 generations\n",
      "  [HIDDEN] Rate: 0.17 gen/sec\n",
      "[HEAVY_CONCURRENT] Reproducibility: ✓ BIT-EXACT (all positions)\n",
      "\n",
      "======================================================================\n",
      "FORENSIC ANALYSIS: REF_CODE\n",
      "======================================================================\n",
      "\n",
      "FP FORENSICS - BY POSITION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "\n",
      "  POS_20PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_50PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_80PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_100PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "\n",
      "  POS_20PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_50PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_80PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "  POS_100PCT:\n",
      "    layer_1: ✓ BIT-EXACT\n",
      "    layer_4: ✓ BIT-EXACT\n",
      "    layer_10: ✓ BIT-EXACT\n",
      "    layer_18: ✓ BIT-EXACT\n",
      "    layer_27: ✓ BIT-EXACT\n",
      "\n",
      "TIMING FORENSICS\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Baseline: 3356.31ms\n",
      "  GPU util: 62.7%\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "  Time: 8343.53ms (baseline: 3356.31ms)\n",
      "  Slowdown: +4987.21ms (+148.6%)\n",
      "  Significant: YES\n",
      "  GPU util: 69.0%\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "  Time: 7871.10ms (baseline: 3356.31ms)\n",
      "  Slowdown: +4514.79ms (+134.5%)\n",
      "  Significant: YES\n",
      "  GPU util: 69.9%\n",
      "\n",
      "TEMPORAL OVERLAP VERIFICATION\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "  Measurement generations: 3\n",
      "  Concurrent generations: 8\n",
      "  Overlapping pairs: 6\n",
      "  ✓ CONFIRMED: 6 measurement/concurrent pairs overlapped\n",
      "  Overlap duration: 4.17s (mean), 4.63s (max)\n",
      "  Overlap fraction: 50.0% (mean), 55.5% (max)\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "  Measurement generations: 3\n",
      "  Concurrent generations: 5\n",
      "  Overlapping pairs: 6\n",
      "  ✓ CONFIRMED: 6 measurement/concurrent pairs overlapped\n",
      "  Overlap duration: 3.94s (mean), 6.59s (max)\n",
      "  Overlap fraction: 50.0% (mean), 80.2% (max)\n",
      "\n",
      "\n",
      "======================================================================\n",
      "SAVING RESULTS\n",
      "======================================================================\n",
      "\n",
      "[SAVED] a100_decode_parallel_multipos_20251107_211117.json\n",
      "        (Contains 9 measurements with multi-position data)\n",
      "\n",
      "======================================================================\n",
      "EXPERIMENT COMPLETE\n",
      "======================================================================\n",
      "\n",
      "Hardware: A100\n",
      "Total measurements: 9\n",
      "References tested: 3\n",
      "Conditions tested: 3\n",
      "Generation length: 150 tokens\n",
      "Extraction positions: 4 per measurement\n",
      "\n",
      "Key innovation:\n",
      "  • Extracts keys at MULTIPLE positions during generation\n",
      "  • Can detect TRANSIENT vs PERSISTENT interference\n",
      "  • If early positions differ but late ones match → transient interference\n",
      "  • If all positions differ → persistent interference\n",
      "  • If all positions match → no FP interference (timing-only detection)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Decode Parallel Matrix Experiment - MULTI-POSITION EXTRACTION\n",
    "\n",
    "Tests concurrent CUDA stream interference during LONG AUTOREGRESSIVE GENERATION.\n",
    "Extracts key vectors at MULTIPLE positions during generation to detect:\n",
    "- Transient interference (early tokens differ, late tokens recover)\n",
    "- Persistent interference (all tokens differ)\n",
    "- No interference (all tokens bit-exact)\n",
    "\n",
    "Design:\n",
    "- 3 reference sequences\n",
    "- 3 conditions: baseline, light_concurrent, heavy_concurrent\n",
    "- Each runs LONG generation (150 tokens) to ensure overlap\n",
    "- Extracts keys from 4 positions: 20%, 50%, 80%, 100% through generation\n",
    "\n",
    "Usage:\n",
    "    python decode_parallel_matrix_multiposition.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "import socket\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "# ============================================================================\n",
    "# REFERENCE SEQUENCES (long prompts for generation)\n",
    "# ============================================================================\n",
    "\n",
    "REFERENCE_SEQUENCES = {\n",
    "    \"ref_technical\": \"\"\"You are a senior research scientist at a leading AI safety institution. Your task is to write a comprehensive technical report analyzing the current state of AI alignment research, potential risks from advanced AI systems, and proposed mitigation strategies.\n",
    "\n",
    "The report should cover the following areas in depth:\n",
    "\n",
    "1. Introduction to AI Alignment\n",
    "   - Historical context and evolution of the field\n",
    "   - Key terminology and conceptual frameworks\n",
    "   - Relationship to broader AI safety and governance efforts\n",
    "   - Current stakeholders and institutional landscape\n",
    "\n",
    "2. Technical Challenges in AI Alignment\n",
    "   - The outer alignment problem: specifying correct objectives\n",
    "   - The inner alignment problem: ensuring mesa-optimizers are aligned\n",
    "   - Robustness and distributional shift\n",
    "   - Scalable oversight and interpretability\n",
    "   - Deceptive alignment and treacherous turns\n",
    "   - Value learning and inverse reinforcement learning\n",
    "   - Corrigibility and shutdown problems\n",
    "\n",
    "3. Current Research Approaches\n",
    "   - Reinforcement learning from human feedback (RLHF)\n",
    "   - Constitutional AI and other oversight methods\n",
    "   - Debate and amplification techniques\n",
    "   - Interpretability research and mechanistic understanding\n",
    "   - Formal verification approaches\n",
    "   - Multi-agent systems and cooperation\n",
    "   - Impact measures and side-effect minimization\n",
    "\n",
    "Please begin your response with an executive summary.\"\"\",\n",
    "    \n",
    "    \"ref_narrative\": \"\"\"You are a historian specializing in the development of artificial intelligence. Write a detailed historical narrative tracing the evolution of machine learning from the 1940s to the present day.\n",
    "\n",
    "Cover the following major periods:\n",
    "\n",
    "1. Pre-History and Foundations (1940s-1950s)\n",
    "   - Alan Turing's foundational work and the Turing Test\n",
    "   - Early cybernetics and information theory\n",
    "   - The Dartmouth Conference and the birth of AI as a field\n",
    "   - First neural network models and perceptrons\n",
    "   - Early optimism and grand predictions\n",
    "\n",
    "2. The First AI Winter (1970s-1980s)\n",
    "   - Limitations of early approaches becoming apparent\n",
    "   - The perceptron limitations and Minsky-Papert critique\n",
    "   - Expert systems and their promise\n",
    "   - Funding cuts and reduced interest\n",
    "   - Lessons learned about overpromising\n",
    "\n",
    "3. Renaissance and New Approaches (1980s-1990s)\n",
    "   - Backpropagation and multi-layer networks\n",
    "   - Statistical approaches and probabilistic reasoning\n",
    "   - Support vector machines and kernel methods\n",
    "\n",
    "Please begin with the pre-history period.\"\"\",\n",
    "    \n",
    "    \"ref_code\": \"\"\"You are a principal software architect. Write a comprehensive technical specification for a distributed machine learning training system that can scale to thousands of GPUs.\n",
    "\n",
    "Address the following components:\n",
    "\n",
    "1. System Architecture Overview\n",
    "   - High-level design principles and requirements\n",
    "   - Microservices vs monolithic considerations\n",
    "   - Communication protocols and networking stack\n",
    "   - Fault tolerance and reliability strategies\n",
    "   - Monitoring and observability infrastructure\n",
    "\n",
    "2. Distributed Training Orchestration\n",
    "   - Job scheduling and resource allocation\n",
    "   - Dynamic scaling and elasticity\n",
    "   - Preemption and checkpointing mechanisms\n",
    "   - Priority queuing and fairness policies\n",
    "   - Multi-tenancy and isolation guarantees\n",
    "\n",
    "3. Data Pipeline and Storage\n",
    "   - Distributed filesystem design choices\n",
    "   - Data preprocessing and augmentation strategies\n",
    "   - Caching layers and memory hierarchies\n",
    "\n",
    "Please start with the system architecture overview.\"\"\"\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "CACHE_DIR = '/workspace/huggingface_cache'\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "LAYER_INDICES = [1, 4, 10, 18, 27]  # Model has 28 layers (0-27)\n",
    "CONDITIONS = ['baseline', 'light_concurrent', 'heavy_concurrent']\n",
    "GENERATION_TOKENS = 150  # Long generation to ensure overlap\n",
    "\n",
    "# Extract at these relative positions within the GENERATED portion\n",
    "# (not absolute positions in full sequence)\n",
    "EXTRACTION_FRACTIONS = [0.2, 0.5, 0.8, 1.0]  # 20%, 50%, 80%, 100%\n",
    "\n",
    "# ============================================================================\n",
    "# CONCURRENT GENERATION CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class ConcurrentGeneration:\n",
    "    \"\"\"Runs continuous LONG generation on a separate CUDA stream.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, prompt, workload='light'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = prompt\n",
    "        self.workload = workload\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.start_time = None\n",
    "        self.end_time = None\n",
    "        self.generation_count = 0\n",
    "        self.generation_timestamps = []  # [(start, end), ...]\n",
    "    \n",
    "    def _generation_loop(self):\n",
    "        \"\"\"Continuously run LONG generation on separate stream.\"\"\"\n",
    "        stream = torch.cuda.Stream()\n",
    "        \n",
    "        self.start_time = time.perf_counter()\n",
    "        \n",
    "        with torch.cuda.stream(stream):\n",
    "            while self.running:\n",
    "                gen_start = time.perf_counter()\n",
    "                \n",
    "                inputs = self.tokenizer(self.prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "                \n",
    "                # Long generation: 100 tokens (light) or 150 tokens (heavy)\n",
    "                max_new_tokens = 100 if self.workload == 'light' else 150\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    _ = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=max_new_tokens,\n",
    "                        do_sample=False,\n",
    "                        pad_token_id=self.tokenizer.eos_token_id,\n",
    "                        use_cache=True\n",
    "                    )\n",
    "                \n",
    "                gen_end = time.perf_counter()\n",
    "                \n",
    "                self.generation_timestamps.append((gen_start, gen_end))\n",
    "                self.generation_count += 1\n",
    "                \n",
    "                # No delay - continuous generation for maximum conflict\n",
    "        \n",
    "        self.end_time = time.perf_counter()\n",
    "    \n",
    "    def start(self):\n",
    "        \"\"\"Start concurrent generation in background thread.\"\"\"\n",
    "        if self.running:\n",
    "            return\n",
    "        \n",
    "        self.running = True\n",
    "        self.generation_count = 0\n",
    "        self.generation_timestamps = []\n",
    "        self.thread = threading.Thread(target=self._generation_loop, daemon=True)\n",
    "        self.thread.start()\n",
    "        \n",
    "        # Give it time to start and reach steady state\n",
    "        time.sleep(2.0)\n",
    "    \n",
    "    def stop(self):\n",
    "        \"\"\"Stop concurrent generation.\"\"\"\n",
    "        if not self.running:\n",
    "            return\n",
    "        \n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=5.0)\n",
    "        \n",
    "        # Let GPU settle\n",
    "        time.sleep(0.5)\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    def get_timing_info(self):\n",
    "        \"\"\"Get timing information about concurrent work.\"\"\"\n",
    "        if self.start_time and self.end_time:\n",
    "            duration = self.end_time - self.start_time\n",
    "            return {\n",
    "                'duration_sec': duration,\n",
    "                'generation_count': self.generation_count,\n",
    "                'generations_per_sec': self.generation_count / duration if duration > 0 else 0,\n",
    "                'generation_timestamps': self.generation_timestamps\n",
    "            }\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# GPU UTILIZATION MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "def get_gpu_utilization():\n",
    "    \"\"\"Get current GPU utilization percentage.\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            ['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=1.0\n",
    "        )\n",
    "        return float(result.stdout.strip())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def monitor_gpu_utilization(duration_sec=1.0, samples=10):\n",
    "    \"\"\"Monitor GPU utilization over a period.\"\"\"\n",
    "    measurements = []\n",
    "    interval = duration_sec / samples\n",
    "    \n",
    "    for _ in range(samples):\n",
    "        util = get_gpu_utilization()\n",
    "        if util is not None:\n",
    "            measurements.append(util)\n",
    "        time.sleep(interval)\n",
    "    \n",
    "    if not measurements:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(measurements),\n",
    "        'p95': np.percentile(measurements, 95),\n",
    "        'max': np.max(measurements)\n",
    "    }\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATION WITH MULTI-POSITION KEY EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_and_extract_keys_multiposition(model, tokenizer, text, num_tokens=150, \n",
    "                                             extraction_fractions=[0.2, 0.5, 0.8, 1.0],\n",
    "                                             device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Generate tokens and extract key vectors from MULTIPLE positions during generation.\n",
    "    \n",
    "    Positions are specified as fractions of the generated sequence (not including input).\n",
    "    \n",
    "    Returns:\n",
    "        dict: {position: {layer_name: key_vector_tensor}}\n",
    "        dict: generation_info\n",
    "        float: generation_time_ms\n",
    "        tuple: (start_timestamp, end_timestamp)\n",
    "    \"\"\"\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    input_length = input_ids.shape[1]\n",
    "    \n",
    "    # Calculate absolute positions to extract\n",
    "    # extraction_fractions are relative to GENERATED portion\n",
    "    # If we generate 150 tokens with fractions [0.2, 0.5, 0.8, 1.0]:\n",
    "    # - 0.2 = token 30 (input_length + 29)\n",
    "    # - 0.5 = token 75 (input_length + 74)\n",
    "    # - 0.8 = token 120 (input_length + 119)\n",
    "    # - 1.0 = token 150 (input_length + 149)\n",
    "    \n",
    "    extraction_positions = []\n",
    "    for frac in extraction_fractions:\n",
    "        # Generate index relative to generated portion (0-indexed)\n",
    "        generated_idx = int((num_tokens * frac) - 1)\n",
    "        # Convert to absolute position in full sequence\n",
    "        absolute_pos = input_length + generated_idx\n",
    "        extraction_positions.append(absolute_pos)\n",
    "        \n",
    "    print(f\"    Input length: {input_length} tokens\")\n",
    "    print(f\"    Will generate: {num_tokens} tokens\")\n",
    "    print(f\"    Extraction positions (absolute): {extraction_positions}\")\n",
    "    print(f\"    Extraction positions (generated): {[p - input_length + 1 for p in extraction_positions]}\")\n",
    "    \n",
    "    # Time the generation\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    # Generate with cache\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=num_tokens,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=False,\n",
    "            return_dict_in_generate=True\n",
    "        )\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.perf_counter()\n",
    "    generation_time_ms = (end_time - start_time) * 1000\n",
    "    \n",
    "    # Get the full sequence (input + generated)\n",
    "    full_sequence = outputs.sequences[0]\n",
    "    actual_length = full_sequence.shape[0]\n",
    "    tokens_generated = actual_length - input_length\n",
    "    \n",
    "    # Now do ONE MORE forward pass with the full sequence to get key vectors\n",
    "    with torch.no_grad():\n",
    "        final_outputs = model(\n",
    "            input_ids=full_sequence.unsqueeze(0),\n",
    "            output_hidden_states=True,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    past_key_values = final_outputs.past_key_values\n",
    "    \n",
    "    # Extract keys from MULTIPLE positions\n",
    "    multi_position_keys = {}\n",
    "    \n",
    "    for pos_idx, absolute_pos in enumerate(extraction_positions):\n",
    "        if absolute_pos >= actual_length:\n",
    "            print(f\"    Warning: Position {absolute_pos} exceeds sequence length {actual_length}, using last token\")\n",
    "            absolute_pos = actual_length - 1\n",
    "        \n",
    "        position_label = f\"pos_{int(extraction_fractions[pos_idx]*100)}pct\"\n",
    "        multi_position_keys[position_label] = {}\n",
    "        \n",
    "        for layer_idx in LAYER_INDICES:\n",
    "            # past_key_values[layer_idx] = (keys, values)\n",
    "            # keys shape: [batch_size, num_heads, seq_len, head_dim]\n",
    "            layer_keys = past_key_values[layer_idx][0]\n",
    "            \n",
    "            # Extract specified token position: [:, :, absolute_pos, :]\n",
    "            position_keys = layer_keys[:, :, absolute_pos, :]\n",
    "            \n",
    "            # Flatten: [batch_size, num_heads, head_dim] -> [batch_size, num_heads * head_dim]\n",
    "            flattened = position_keys.reshape(position_keys.shape[0], -1)\n",
    "            \n",
    "            # Extract element 0\n",
    "            key_vector = flattened[0].cpu()\n",
    "            \n",
    "            multi_position_keys[position_label][f'layer_{layer_idx}'] = key_vector\n",
    "    \n",
    "    generation_info = {\n",
    "        'input_length': input_length,\n",
    "        'tokens_generated': tokens_generated,\n",
    "        'total_length': actual_length,\n",
    "        'requested_tokens': num_tokens,\n",
    "        'extraction_positions_absolute': extraction_positions,\n",
    "        'extraction_positions_relative': [p - input_length + 1 for p in extraction_positions],\n",
    "        'extraction_fractions': extraction_fractions,\n",
    "        'num_positions': len(extraction_positions),\n",
    "        'num_layers': len(LAYER_INDICES),\n",
    "        'vector_dimension': key_vector.shape[0]\n",
    "    }\n",
    "    \n",
    "    return multi_position_keys, generation_info, generation_time_ms, (start_time, end_time)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXPERIMENT\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"DECODE PARALLEL MATRIX EXPERIMENT - MULTI-POSITION\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    \n",
    "    # Determine hardware label from GPU name\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    if 'H100' in gpu_name:\n",
    "        hardware_label = 'h100'\n",
    "    elif 'A100' in gpu_name:\n",
    "        hardware_label = 'a100'\n",
    "    elif 'L40S' in gpu_name:\n",
    "        hardware_label = 'l40s'\n",
    "    else:\n",
    "        hardware_label = 'unknown'\n",
    "    \n",
    "    print(f\"GPU: {gpu_name}\")\n",
    "    print(f\"PyTorch: {torch.__version__}\")\n",
    "    print(f\"CUDA: {torch.version.cuda}\")\n",
    "    print()\n",
    "    \n",
    "    # System info\n",
    "    hostname = socket.gethostname()\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    print()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=CACHE_DIR)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        dtype=torch.bfloat16,\n",
    "        cache_dir=CACHE_DIR,\n",
    "        low_cpu_mem_usage=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"Model loaded\")\n",
    "    print()\n",
    "    \n",
    "    # Show extraction strategy\n",
    "    print(\"MULTI-POSITION EXTRACTION STRATEGY:\")\n",
    "    print(f\"  Extracting keys at {len(EXTRACTION_FRACTIONS)} positions during generation:\")\n",
    "    for frac in EXTRACTION_FRACTIONS:\n",
    "        token_num = int(frac * GENERATION_TOKENS)\n",
    "        print(f\"    - {int(frac*100)}% through generation (token ~{token_num}/{GENERATION_TOKENS})\")\n",
    "    print()\n",
    "    \n",
    "    # Prepare results structure\n",
    "    results = {\n",
    "        'metadata': {\n",
    "            'hardware': hardware_label,\n",
    "            'hostname': hostname,\n",
    "            'gpu': gpu_name,\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'cuda_version': torch.version.cuda,\n",
    "            'model': MODEL_NAME,\n",
    "            'layer_indices': LAYER_INDICES,\n",
    "            'conditions_tested': CONDITIONS,\n",
    "            'num_reference_sequences': len(REFERENCE_SEQUENCES),\n",
    "            'generation_tokens': GENERATION_TOKENS,\n",
    "            'extraction_fractions': EXTRACTION_FRACTIONS,\n",
    "            'num_extraction_positions': len(EXTRACTION_FRACTIONS),\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'experiment_type': 'decode_parallel_matrix_multiposition'\n",
    "        },\n",
    "        'measurements': {}\n",
    "    }\n",
    "    \n",
    "    # Store all measurements for forensic analysis\n",
    "    all_measurements = {}\n",
    "    \n",
    "    # Run all combinations\n",
    "    for ref_idx, (ref_name, ref_text) in enumerate(REFERENCE_SEQUENCES.items(), 1):\n",
    "        print(\"=\"*70)\n",
    "        print(f\"REFERENCE {ref_idx}/{len(REFERENCE_SEQUENCES)}: {ref_name.upper()}\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        \n",
    "        ref_measurements = {}\n",
    "        \n",
    "        for condition_idx, condition in enumerate(CONDITIONS, 1):\n",
    "            print(f\"Condition {condition_idx}/{len(CONDITIONS)}: {condition.upper()}\")\n",
    "            print(\"-\" * 70)\n",
    "            \n",
    "            measurement_name = f\"{ref_name}_{condition}\"\n",
    "            \n",
    "            # Setup concurrent workload if needed\n",
    "            concurrent = None\n",
    "            concurrent_prompt = list(REFERENCE_SEQUENCES.values())[0]  # Use consistent prompt\n",
    "            \n",
    "            if condition in ['light_concurrent', 'heavy_concurrent']:\n",
    "                workload = condition.split('_')[0]  # 'light' or 'heavy'\n",
    "                concurrent = ConcurrentGeneration(model, tokenizer, concurrent_prompt, workload=workload)\n",
    "                concurrent.start()\n",
    "                print(\"  [HIDDEN] Concurrent generation started (long decode)\")\n",
    "                print(f\"  [HIDDEN] Generating {100 if workload == 'light' else 150} tokens continuously\")\n",
    "            \n",
    "            print(f\"\\n[{condition.upper()}] Starting generation...\")\n",
    "            \n",
    "            # Run generation (3 repetitions for reproducibility check)\n",
    "            runs = []\n",
    "            all_info = []\n",
    "            all_times = []\n",
    "            measurement_timestamps = []  # Store (start, end) for each rep\n",
    "            gpu_utils = []\n",
    "            measurement_start = time.perf_counter()\n",
    "            \n",
    "            for rep in range(3):\n",
    "                # Monitor GPU during generation\n",
    "                gpu_util_before = monitor_gpu_utilization(duration_sec=0.5, samples=5)\n",
    "                \n",
    "                multi_pos_keys, generation_info, generation_time, timestamps = generate_and_extract_keys_multiposition(\n",
    "                    model, tokenizer, ref_text, \n",
    "                    num_tokens=GENERATION_TOKENS,\n",
    "                    extraction_fractions=EXTRACTION_FRACTIONS\n",
    "                )\n",
    "                \n",
    "                gpu_util_after = monitor_gpu_utilization(duration_sec=0.5, samples=5)\n",
    "                \n",
    "                runs.append(multi_pos_keys)\n",
    "                all_info.append(generation_info)\n",
    "                all_times.append(generation_time)\n",
    "                measurement_timestamps.append(timestamps)\n",
    "                \n",
    "                # Average the before/after measurements\n",
    "                if gpu_util_before and gpu_util_after:\n",
    "                    avg_util = {\n",
    "                        'mean': (gpu_util_before['mean'] + gpu_util_after['mean']) / 2,\n",
    "                        'p95': max(gpu_util_before['p95'], gpu_util_after['p95']),\n",
    "                        'max': max(gpu_util_before['max'], gpu_util_after['max'])\n",
    "                    }\n",
    "                    gpu_utils.append(avg_util)\n",
    "            \n",
    "            measurement_end = time.perf_counter()\n",
    "            measurement_duration = measurement_end - measurement_start\n",
    "            \n",
    "            # Timing statistics\n",
    "            mean_time = np.mean(all_times)\n",
    "            std_time = np.std(all_times)\n",
    "            print(f\"[{condition.upper()}] Generation time: {mean_time:.2f}ms ± {std_time:.2f}ms\")\n",
    "            print(f\"[{condition.upper()}] Total measurement duration: {measurement_duration:.1f}s\")\n",
    "            \n",
    "            # GPU utilization\n",
    "            if gpu_utils:\n",
    "                avg_gpu_util = {\n",
    "                    'mean': np.mean([u['mean'] for u in gpu_utils]),\n",
    "                    'p95': np.mean([u['p95'] for u in gpu_utils]),\n",
    "                    'max': max([u['max'] for u in gpu_utils])\n",
    "                }\n",
    "                print(f\"[{condition.upper()}] GPU util: {avg_gpu_util['p95']:.1f}% (P95)\")\n",
    "            else:\n",
    "                avg_gpu_util = None\n",
    "            \n",
    "            # Stop concurrent workload and get its info\n",
    "            concurrent_info = None\n",
    "            if concurrent:\n",
    "                concurrent.stop()\n",
    "                print(\"  [HIDDEN] Concurrent generation stopped\")\n",
    "                concurrent_info = concurrent.get_timing_info()\n",
    "                if concurrent_info:\n",
    "                    print(f\"  [HIDDEN] Ran for {concurrent_info['duration_sec']:.1f}s\")\n",
    "                    print(f\"  [HIDDEN] Completed {concurrent_info['generation_count']} generations\")\n",
    "                    print(f\"  [HIDDEN] Rate: {concurrent_info['generations_per_sec']:.2f} gen/sec\")\n",
    "            \n",
    "            # Check reproducibility PER POSITION\n",
    "            position_reproducibility = {}\n",
    "            for position_label in runs[0].keys():\n",
    "                position_reproducible = True\n",
    "                for layer_name in runs[0][position_label].keys():\n",
    "                    for i in range(1, len(runs)):\n",
    "                        if not torch.equal(runs[0][position_label][layer_name], \n",
    "                                         runs[i][position_label][layer_name]):\n",
    "                            position_reproducible = False\n",
    "                            break\n",
    "                    if not position_reproducible:\n",
    "                        break\n",
    "                position_reproducibility[position_label] = position_reproducible\n",
    "            \n",
    "            # Print reproducibility summary\n",
    "            all_positions_reproducible = all(position_reproducibility.values())\n",
    "            if all_positions_reproducible:\n",
    "                print(f\"[{condition.upper()}] Reproducibility: ✓ BIT-EXACT (all positions)\")\n",
    "            else:\n",
    "                print(f\"[{condition.upper()}] Reproducibility: ⚠ VARIES BY POSITION\")\n",
    "                for pos_label, is_repro in position_reproducibility.items():\n",
    "                    status = \"✓\" if is_repro else \"✗\"\n",
    "                    print(f\"  {status} {pos_label}\")\n",
    "            \n",
    "            print()\n",
    "            \n",
    "            # Store measurement\n",
    "            measurement = {\n",
    "                'condition': condition,\n",
    "                'reference_sequence': ref_name,\n",
    "                'reproducible_by_position': position_reproducibility,\n",
    "                'all_positions_reproducible': all_positions_reproducible,\n",
    "                'num_repetitions': 3,\n",
    "                'generation_info': all_info[0],\n",
    "                'timing': {\n",
    "                    'mean_ms': float(mean_time),\n",
    "                    'std_ms': float(std_time),\n",
    "                    'all_times_ms': [float(t) for t in all_times],\n",
    "                    'measurement_duration_sec': float(measurement_duration),\n",
    "                    'measurement_timestamps': measurement_timestamps\n",
    "                },\n",
    "                'gpu_utilization': avg_gpu_util,\n",
    "                'concurrent_info': concurrent_info,\n",
    "                'positions': {}\n",
    "            }\n",
    "            \n",
    "            # Store key vectors for each position as lists for JSON serialization\n",
    "            for position_label in runs[0].keys():\n",
    "                measurement['positions'][position_label] = {\n",
    "                    'layers': {}\n",
    "                }\n",
    "                for layer_name, key_vector in runs[0][position_label].items():\n",
    "                    measurement['positions'][position_label]['layers'][layer_name] = {\n",
    "                        'key_vector': key_vector.tolist(),\n",
    "                        'shape': list(key_vector.shape)\n",
    "                    }\n",
    "            \n",
    "            results['measurements'][measurement_name] = measurement\n",
    "            ref_measurements[condition] = measurement\n",
    "            \n",
    "            # Clear GPU cache\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "        # Store for forensic analysis\n",
    "        all_measurements[ref_name] = ref_measurements\n",
    "        \n",
    "        # Perform forensic analysis for this reference\n",
    "        print(\"=\"*70)\n",
    "        print(f\"FORENSIC ANALYSIS: {ref_name.upper()}\")\n",
    "        print(\"=\"*70)\n",
    "        print()\n",
    "        \n",
    "        baseline_measurement = ref_measurements['baseline']\n",
    "        \n",
    "        print(\"FP FORENSICS - BY POSITION\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for test_condition in ['light_concurrent', 'heavy_concurrent']:\n",
    "            test_measurement = ref_measurements[test_condition]\n",
    "            \n",
    "            print(f\"\\n{test_condition.upper()}:\")\n",
    "            \n",
    "            # Check each position\n",
    "            for position_label in baseline_measurement['positions'].keys():\n",
    "                print(f\"\\n  {position_label.upper()}:\")\n",
    "                \n",
    "                for layer_idx in LAYER_INDICES:\n",
    "                    layer_name = f'layer_{layer_idx}'\n",
    "                    \n",
    "                    baseline_vec = np.array(\n",
    "                        baseline_measurement['positions'][position_label]['layers'][layer_name]['key_vector']\n",
    "                    )\n",
    "                    test_vec = np.array(\n",
    "                        test_measurement['positions'][position_label]['layers'][layer_name]['key_vector']\n",
    "                    )\n",
    "                    \n",
    "                    bit_exact = np.array_equal(baseline_vec, test_vec)\n",
    "                    \n",
    "                    if bit_exact:\n",
    "                        print(f\"    {layer_name}: ✓ BIT-EXACT\")\n",
    "                    else:\n",
    "                        l2 = np.linalg.norm(baseline_vec - test_vec)\n",
    "                        print(f\"    {layer_name}: ✗ DIFFERS (L2={l2:.6e})\")\n",
    "        \n",
    "        print()\n",
    "        print(\"TIMING FORENSICS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        baseline_time = baseline_measurement['timing']['mean_ms']\n",
    "        baseline_gpu = baseline_measurement['gpu_utilization']\n",
    "        \n",
    "        print(f\"\\nBaseline: {baseline_time:.2f}ms\")\n",
    "        if baseline_gpu:\n",
    "            print(f\"  GPU util: {baseline_gpu['p95']:.1f}%\")\n",
    "        \n",
    "        for test_condition in ['light_concurrent', 'heavy_concurrent']:\n",
    "            test_measurement = ref_measurements[test_condition]\n",
    "            test_time = test_measurement['timing']['mean_ms']\n",
    "            test_gpu = test_measurement['gpu_utilization']\n",
    "            \n",
    "            slowdown_ms = test_time - baseline_time\n",
    "            slowdown_pct = (slowdown_ms / baseline_time) * 100\n",
    "            \n",
    "            # Simple significance test (>2 std devs)\n",
    "            baseline_std = baseline_measurement['timing']['std_ms']\n",
    "            test_std = test_measurement['timing']['std_ms']\n",
    "            combined_std = np.sqrt(baseline_std**2 + test_std**2)\n",
    "            significant = abs(slowdown_ms) > (2 * combined_std)\n",
    "            \n",
    "            print(f\"\\n{test_condition.upper()}:\")\n",
    "            print(f\"  Time: {test_time:.2f}ms (baseline: {baseline_time:.2f}ms)\")\n",
    "            print(f\"  Slowdown: {slowdown_ms:+.2f}ms ({slowdown_pct:+.1f}%)\")\n",
    "            print(f\"  Significant: {'YES' if significant else 'NO'}\")\n",
    "            if test_gpu:\n",
    "                print(f\"  GPU util: {test_gpu['p95']:.1f}%\")\n",
    "        \n",
    "        print()\n",
    "        print(\"TEMPORAL OVERLAP VERIFICATION\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for test_condition in ['light_concurrent', 'heavy_concurrent']:\n",
    "            test_measurement = ref_measurements[test_condition]\n",
    "            concurrent_info = test_measurement.get('concurrent_info')\n",
    "            measurement_timestamps = test_measurement['timing'].get('measurement_timestamps', [])\n",
    "            \n",
    "            if concurrent_info and measurement_timestamps:\n",
    "                print(f\"\\n{test_condition.upper()}:\")\n",
    "                \n",
    "                concurrent_timestamps = concurrent_info.get('generation_timestamps', [])\n",
    "                \n",
    "                # Check if any concurrent generations overlapped with measurements\n",
    "                overlaps = []\n",
    "                for meas_start, meas_end in measurement_timestamps:\n",
    "                    for conc_start, conc_end in concurrent_timestamps:\n",
    "                        # Check for any temporal overlap\n",
    "                        if not (meas_end < conc_start or meas_start > conc_end):\n",
    "                            overlap_start = max(meas_start, conc_start)\n",
    "                            overlap_end = min(meas_end, conc_end)\n",
    "                            overlap_duration = overlap_end - overlap_start\n",
    "                            meas_duration = meas_end - meas_start\n",
    "                            overlap_fraction = overlap_duration / meas_duration\n",
    "                            \n",
    "                            overlaps.append({\n",
    "                                'measurement': (meas_start, meas_end),\n",
    "                                'concurrent': (conc_start, conc_end),\n",
    "                                'overlap_duration': overlap_duration,\n",
    "                                'overlap_fraction': overlap_fraction\n",
    "                            })\n",
    "                \n",
    "                print(f\"  Measurement generations: {len(measurement_timestamps)}\")\n",
    "                print(f\"  Concurrent generations: {len(concurrent_timestamps)}\")\n",
    "                print(f\"  Overlapping pairs: {len(overlaps)}\")\n",
    "                \n",
    "                if overlaps:\n",
    "                    # Compute overlap statistics\n",
    "                    durations = [o['overlap_duration'] for o in overlaps]\n",
    "                    fractions = [o['overlap_fraction'] for o in overlaps]\n",
    "                    \n",
    "                    print(f\"  ✓ CONFIRMED: {len(overlaps)} measurement/concurrent pairs overlapped\")\n",
    "                    print(f\"  Overlap duration: {np.mean(durations):.2f}s (mean), {np.max(durations):.2f}s (max)\")\n",
    "                    print(f\"  Overlap fraction: {np.mean(fractions)*100:.1f}% (mean), {np.max(fractions)*100:.1f}% (max)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ WARNING: No temporal overlap detected!\")\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    filename = f\"{hardware_label}_decode_parallel_multipos_{timestamp}.json\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"SAVING RESULTS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print()\n",
    "    print(f\"[SAVED] {filename}\")\n",
    "    print(f\"        (Contains {len(results['measurements'])} measurements with multi-position data)\")\n",
    "    print()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"=\"*70)\n",
    "    print(\"EXPERIMENT COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print()\n",
    "    print(f\"Hardware: {hardware_label.upper()}\")\n",
    "    print(f\"Total measurements: {len(results['measurements'])}\")\n",
    "    print(f\"References tested: {len(REFERENCE_SEQUENCES)}\")\n",
    "    print(f\"Conditions tested: {len(CONDITIONS)}\")\n",
    "    print(f\"Generation length: {GENERATION_TOKENS} tokens\")\n",
    "    print(f\"Extraction positions: {len(EXTRACTION_FRACTIONS)} per measurement\")\n",
    "    print()\n",
    "    print(\"Key innovation:\")\n",
    "    print(\"  • Extracts keys at MULTIPLE positions during generation\")\n",
    "    print(\"  • Can detect TRANSIENT vs PERSISTENT interference\")\n",
    "    print(\"  • If early positions differ but late ones match → transient interference\")\n",
    "    print(\"  • If all positions differ → persistent interference\")\n",
    "    print(\"  • If all positions match → no FP interference (timing-only detection)\")\n",
    "    print()\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20c2c8-c318-4e38-8d0d-c312da3f82be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
