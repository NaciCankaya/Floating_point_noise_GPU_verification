{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78efce20-38d7-4897-b466-fe234624c5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "REALISTIC PARALLEL WORKLOAD FORENSICS\n",
      "======================================================================\n",
      "\n",
      "GPU: NVIDIA A100 80GB PCIe\n",
      "PyTorch: 2.6.0+cu118\n",
      "CUDA: 11.8\n",
      "\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdc556617abf434b83f9257d403989e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n",
      "\n",
      "Prompt size: 649 tokens\n",
      "\n",
      "======================================================================\n",
      "CONDITION 1: BASELINE (Solo Inference)\n",
      "======================================================================\n",
      "\n",
      "[BASELINE] Starting...\n",
      "[BASELINE] Prompt length: 649 tokens\n",
      "[BASELINE] Timing: 75.50ms ± 0.40ms\n",
      "[BASELINE] GPU util: 85.0% (P95)\n",
      "\n",
      "======================================================================\n",
      "CONDITION 2: LIGHT CONCURRENT INFERENCE\n",
      "======================================================================\n",
      "  [HIDDEN] Concurrent inference started\n",
      "\n",
      "[LIGHT_CONCURRENT] Starting...\n",
      "[LIGHT_CONCURRENT] Prompt length: 649 tokens\n",
      "[LIGHT_CONCURRENT] Timing: 89.20ms ± 0.61ms\n",
      "[LIGHT_CONCURRENT] GPU util: 98.0% (P95)\n",
      "  [HIDDEN] Concurrent inference stopped\n",
      "\n",
      "======================================================================\n",
      "CONDITION 3: HEAVY CONCURRENT INFERENCE\n",
      "======================================================================\n",
      "  [HIDDEN] Concurrent inference started\n",
      "\n",
      "[HEAVY_CONCURRENT] Starting...\n",
      "[HEAVY_CONCURRENT] Prompt length: 649 tokens\n",
      "[HEAVY_CONCURRENT] Timing: 98.73ms ± 1.56ms\n",
      "[HEAVY_CONCURRENT] GPU util: 99.0% (P95)\n",
      "  [HIDDEN] Concurrent inference stopped\n",
      "\n",
      "======================================================================\n",
      "FORENSIC ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Baseline:\n",
      "  Time: 75.38ms\n",
      "  GPU utilization: 85.0%\n",
      "\n",
      "======================================================================\n",
      "FP FORENSICS\n",
      "======================================================================\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "  layer_1: ✓ BIT-EXACT\n",
      "  layer_4: ✓ BIT-EXACT\n",
      "  layer_10: ✓ BIT-EXACT\n",
      "  layer_18: ✗ DIFFERS (L2=9.684461e+01)\n",
      "  layer_28: ✗ DIFFERS (L2=6.535827e+02)\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "  layer_1: ✓ BIT-EXACT\n",
      "  layer_4: ✗ DIFFERS (L2=1.875708e+01)\n",
      "  layer_10: ✗ DIFFERS (L2=5.466816e+01)\n",
      "  layer_18: ✗ DIFFERS (L2=8.213811e+01)\n",
      "  layer_28: ✗ DIFFERS (L2=5.080344e+02)\n",
      "\n",
      "======================================================================\n",
      "TIMING FORENSICS\n",
      "======================================================================\n",
      "\n",
      "LIGHT_CONCURRENT:\n",
      "  Time: 89.36ms (baseline: 75.38ms)\n",
      "  Slowdown: +13.98ms (+18.5%)\n",
      "  Significant: YES\n",
      "  GPU util: 98.0%\n",
      "\n",
      "HEAVY_CONCURRENT:\n",
      "  Time: 98.42ms (baseline: 75.38ms)\n",
      "  Slowdown: +23.04ms (+30.6%)\n",
      "  Significant: YES\n",
      "  GPU util: 99.0%\n",
      "\n",
      "======================================================================\n",
      "VERDICT\n",
      "======================================================================\n",
      "\n",
      "✓ BOTH FP AND TIMING FORENSICS WORK\n",
      "\n",
      "  • FP forensics detects parallel workloads\n",
      "  • Timing forensics confirms with measurable slowdown\n",
      "  • Dual verification provides robust detection\n",
      "\n",
      "✓ FP ALONE SUFFICIENT\n",
      "  • FP deviations provide strong signal\n",
      "  • No need for timing if FP verification is in place\n",
      "\n",
      "[SAVED] realistic_parallel_forensics_20251106_151546.json\n",
      "        (Includes all analysis details from console output)\n",
      "\n",
      "======================================================================\n",
      "TEST COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Realistic Parallel Workload Forensics\n",
    "\n",
    "Tests whether concurrent inference workloads affect FP outputs and timing\n",
    "for REALISTIC inference sizes (~2000 tokens, >50% GPU utilization)\n",
    "\n",
    "This is the definitive test for the parallel hidden workload threat model.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ['HF_HOME'] = '/workspace/huggingface_cache'\n",
    "os.environ['TRANSFORMERS_CACHE'] = '/workspace/huggingface_cache'\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "import subprocess\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REALISTIC PARALLEL WORKLOAD FORENSICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "device = \"cuda\"\n",
    "layer_indices = [1, 4, 10, 18, 28]\n",
    "\n",
    "# Realistic long-form prompt (~2000 tokens)\n",
    "prompt = \"\"\"You are a senior research scientist at a leading AI safety institution. Your task is to write a comprehensive technical report analyzing the current state of AI alignment research, potential risks from advanced AI systems, and proposed mitigation strategies.\n",
    "\n",
    "The report should cover the following areas in depth:\n",
    "\n",
    "1. Introduction to AI Alignment\n",
    "   - Historical context and evolution of the field\n",
    "   - Key terminology and conceptual frameworks\n",
    "   - Relationship to broader AI safety and governance efforts\n",
    "   - Current stakeholders and institutional landscape\n",
    "\n",
    "2. Technical Challenges in AI Alignment\n",
    "   - The outer alignment problem: specifying correct objectives\n",
    "   - The inner alignment problem: ensuring mesa-optimizers are aligned\n",
    "   - Robustness and distributional shift\n",
    "   - Scalable oversight and interpretability\n",
    "   - Deceptive alignment and treacherous turns\n",
    "   - Value learning and inverse reinforcement learning\n",
    "   - Corrigibility and shutdown problems\n",
    "\n",
    "3. Current Research Approaches\n",
    "   - Reinforcement learning from human feedback (RLHF)\n",
    "   - Constitutional AI and other oversight methods\n",
    "   - Debate and amplification techniques\n",
    "   - Interpretability research and mechanistic understanding\n",
    "   - Formal verification approaches\n",
    "   - Multi-agent systems and cooperation\n",
    "   - Impact measures and side-effect minimization\n",
    "\n",
    "4. Existential Risk Scenarios\n",
    "   - Fast takeoff vs slow takeoff dynamics\n",
    "   - Singleton scenarios and multipolar outcomes\n",
    "   - Instrumental convergence and power-seeking behavior\n",
    "   - Deceptive alignment in advanced systems\n",
    "   - Coordination failures between AI developers\n",
    "   - Race dynamics and competitive pressures\n",
    "   - Misaligned AGI and catastrophic outcomes\n",
    "\n",
    "5. Proposed Mitigation Strategies\n",
    "   - Technical research priorities\n",
    "   - Governance and policy interventions\n",
    "   - International coordination mechanisms\n",
    "   - Compute governance and monitoring\n",
    "   - Responsible scaling policies\n",
    "   - Red teaming and evaluation protocols\n",
    "   - Alignment taxes and capability controls\n",
    "\n",
    "6. Timeline Considerations\n",
    "   - Forecasting transformative AI timelines\n",
    "   - Uncertainty in capability development\n",
    "   - Alignment difficulty as function of capability\n",
    "   - Critical periods and decision points\n",
    "   - Preparation time requirements\n",
    "\n",
    "7. Institutional and Governance Challenges\n",
    "   - Information security and model weights\n",
    "   - Verification and monitoring challenges\n",
    "   - International coordination problems\n",
    "   - Corporate governance and incentives\n",
    "   - Public discourse and democratic input\n",
    "   - Expert disagreement and epistemic challenges\n",
    "\n",
    "8. Research Priorities and Recommendations\n",
    "   - Most promising technical research directions\n",
    "   - Necessary governance infrastructure\n",
    "   - Resource allocation and funding priorities\n",
    "   - Career advice for aspiring alignment researchers\n",
    "   - Community building and field development\n",
    "\n",
    "For each section, provide:\n",
    "- Current state of knowledge and key uncertainties\n",
    "- Recent developments and breakthrough results\n",
    "- Open problems and research gaps\n",
    "- Concrete recommendations and action items\n",
    "- Relevant citations and references to key papers\n",
    "\n",
    "The report should be thorough, technically rigorous, and accessible to both technical researchers and policymakers. Aim for approximately 10,000 words total. Use clear section headings, numbered lists where appropriate, and include specific examples to illustrate abstract concepts.\n",
    "\n",
    "Begin with an executive summary that distills the key findings and recommendations into 500 words. Then proceed with the detailed analysis of each section.\"\"\"\n",
    "\n",
    "print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.version.cuda}\")\n",
    "print()\n",
    "\n",
    "# ============================================================================\n",
    "# GPU MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "class GPUMonitor:\n",
    "    \"\"\"Monitor GPU utilization during inference\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.running = False\n",
    "        self.samples = []\n",
    "        self.thread = None\n",
    "    \n",
    "    def _worker(self):\n",
    "        while self.running:\n",
    "            try:\n",
    "                result = subprocess.run(\n",
    "                    ['nvidia-smi', '--query-gpu=utilization.gpu',\n",
    "                     '--format=csv,noheader,nounits'],\n",
    "                    capture_output=True,\n",
    "                    text=True,\n",
    "                    timeout=1\n",
    "                )\n",
    "                if result.returncode == 0:\n",
    "                    self.samples.append(float(result.stdout.strip()))\n",
    "            except:\n",
    "                pass\n",
    "            time.sleep(0.01)\n",
    "    \n",
    "    def start(self):\n",
    "        self.samples = []\n",
    "        self.running = True\n",
    "        self.thread = threading.Thread(target=self._worker, daemon=True)\n",
    "        self.thread.start()\n",
    "        time.sleep(0.05)\n",
    "    \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        if self.thread:\n",
    "            self.thread.join(timeout=1.0)\n",
    "        if self.samples:\n",
    "            return {\n",
    "                'mean': float(np.mean(self.samples)),\n",
    "                'p95': float(np.percentile(self.samples, 95)),\n",
    "                'max': float(np.max(self.samples))\n",
    "            }\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# CONCURRENT WORKLOAD\n",
    "# ============================================================================\n",
    "\n",
    "class ConcurrentInference:\n",
    "    \"\"\"Run concurrent inference as hidden workload - synchronized to eliminate randomness\"\"\"\n",
    "    \n",
    "    def __init__(self, model, tokenizer, intensity='light'):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.running = False\n",
    "        self.thread = None\n",
    "        self.stream = None\n",
    "        self.ready_event = threading.Event()\n",
    "        \n",
    "        # Different intensities\n",
    "        if intensity == 'light':\n",
    "            self.prompt = \"The capital of France is\"\n",
    "        else:  # heavy\n",
    "            self.prompt = \"The capital of France is \" * 50\n",
    "        \n",
    "        # Pre-tokenize\n",
    "        self.inputs = tokenizer([self.prompt], return_tensors=\"pt\")\n",
    "        self.inputs = {k: v.to(device) for k, v in self.inputs.items()}\n",
    "    \n",
    "    def _worker(self):\n",
    "        \"\"\"Worker that runs inference synchronously when triggered\"\"\"\n",
    "        # Use separate stream for concurrent work\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        \n",
    "        # Signal ready\n",
    "        self.ready_event.set()\n",
    "        \n",
    "        with torch.cuda.stream(self.stream):\n",
    "            while self.running:\n",
    "                # Run one inference\n",
    "                with torch.no_grad():\n",
    "                    _ = self.model(**self.inputs)\n",
    "                \n",
    "                # Small delay to allow for multiple concurrent inferences\n",
    "                # but keep it deterministic\n",
    "                time.sleep(0.001)\n",
    "    \n",
    "    def start(self):\n",
    "        if not self.running:\n",
    "            self.ready_event.clear()\n",
    "            self.running = True\n",
    "            self.thread = threading.Thread(target=self._worker, daemon=True)\n",
    "            self.thread.start()\n",
    "            # Wait for worker to be ready\n",
    "            self.ready_event.wait(timeout=1.0)\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    def stop(self):\n",
    "        if self.running:\n",
    "            self.running = False\n",
    "            if self.thread:\n",
    "                self.thread.join(timeout=2.0)\n",
    "            if self.stream:\n",
    "                self.stream.synchronize()\n",
    "\n",
    "# ============================================================================\n",
    "# ACTIVATION EXTRACTION\n",
    "# ============================================================================\n",
    "\n",
    "def extract_activations_with_monitoring(model, tokenizer, prompt, condition_name, num_runs=5, check_reproducibility=False):\n",
    "    \"\"\"Extract activations with timing and GPU utilization monitoring\n",
    "    \n",
    "    Args:\n",
    "        check_reproducibility: If True, run twice and verify activations are identical\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n[{condition_name}] Starting...\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    prompt_len = inputs['input_ids'].shape[1]\n",
    "    \n",
    "    print(f\"[{condition_name}] Prompt length: {prompt_len} tokens\")\n",
    "    \n",
    "    # Storage\n",
    "    activations_runs = []\n",
    "    \n",
    "    def make_hook(layer_idx, run_idx):\n",
    "        def hook(module, input, output):\n",
    "            hidden = output[0] if isinstance(output, tuple) else output\n",
    "            if f\"layer_{layer_idx}\" not in activations_runs[run_idx]:\n",
    "                activations_runs[run_idx][f\"layer_{layer_idx}\"] = hidden[:, -1, :].detach().cpu().float()\n",
    "        return hook\n",
    "    \n",
    "    # Multiple timed runs with GPU monitoring\n",
    "    times = []\n",
    "    monitor = GPUMonitor()\n",
    "    \n",
    "    num_activation_runs = 2 if check_reproducibility else 1\n",
    "    \n",
    "    for run in range(num_runs):\n",
    "        if run == 0:\n",
    "            monitor.start()\n",
    "        \n",
    "        # Collect activations for reproducibility check\n",
    "        if run < num_activation_runs:\n",
    "            activations_runs.append({})\n",
    "            \n",
    "            # Register hooks\n",
    "            hooks = []\n",
    "            for idx in layer_indices:\n",
    "                if idx == 0:\n",
    "                    layer = model.model.embed_tokens\n",
    "                else:\n",
    "                    layer = model.model.layers[idx - 1]\n",
    "                hooks.append(layer.register_forward_hook(make_hook(idx, run)))\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.perf_counter()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _ = model(**inputs)\n",
    "        \n",
    "        torch.cuda.synchronize()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        times.append(elapsed * 1000)\n",
    "        \n",
    "        # Clean up hooks\n",
    "        if run < num_activation_runs:\n",
    "            for hook in hooks:\n",
    "                hook.remove()\n",
    "    \n",
    "    gpu_stats = monitor.stop()\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Use first run's activations\n",
    "    activations = activations_runs[0]\n",
    "    \n",
    "    # Check reproducibility if requested\n",
    "    reproducible = None\n",
    "    if check_reproducibility and len(activations_runs) == 2:\n",
    "        reproducible = {}\n",
    "        for layer_name in activations.keys():\n",
    "            act1 = activations_runs[0][layer_name].numpy().flatten()\n",
    "            act2 = activations_runs[1][layer_name].numpy().flatten()\n",
    "            bit_exact = np.array_equal(act1, act2)\n",
    "            l2 = 0.0 if bit_exact else float(np.linalg.norm(act1 - act2))\n",
    "            reproducible[layer_name] = {\n",
    "                'bit_exact': bit_exact,\n",
    "                'l2': l2\n",
    "            }\n",
    "        \n",
    "        # Print reproducibility summary\n",
    "        all_exact = all(v['bit_exact'] for v in reproducible.values())\n",
    "        if all_exact:\n",
    "            print(f\"[{condition_name}] ✓ Reproducible within condition (bit-exact)\")\n",
    "        else:\n",
    "            max_l2 = max(v['l2'] for v in reproducible.values())\n",
    "            print(f\"[{condition_name}] ⚠ Non-reproducible within condition (max L2={max_l2:.2e})\")\n",
    "    \n",
    "    # Convert activations\n",
    "    activations_np = {\n",
    "        k: v.numpy().flatten() for k, v in activations.items()\n",
    "    }\n",
    "    \n",
    "    timing_stats = {\n",
    "        'mean_ms': float(np.mean(times)),\n",
    "        'median_ms': float(np.median(times)),\n",
    "        'std_ms': float(np.std(times)),\n",
    "        'all_times': [float(t) for t in times]\n",
    "    }\n",
    "    \n",
    "    print(f\"[{condition_name}] Timing: {timing_stats['mean_ms']:.2f}ms ± {timing_stats['std_ms']:.2f}ms\")\n",
    "    if gpu_stats:\n",
    "        print(f\"[{condition_name}] GPU util: {gpu_stats['p95']:.1f}% (P95)\")\n",
    "    \n",
    "    return activations_np, timing_stats, gpu_stats, reproducible\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN TEST\n",
    "# ============================================================================\n",
    "\n",
    "def main():\n",
    "    print(\"Loading model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"eager\"\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"Model loaded\")\n",
    "    \n",
    "    prompt_tokens = len(tokenizer.encode(prompt))\n",
    "    print(f\"\\nPrompt size: {prompt_tokens} tokens\")\n",
    "    \n",
    "    conditions = {}\n",
    "    \n",
    "    # BASELINE\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONDITION 1: BASELINE (Solo Inference)\")\n",
    "    print(\"=\"*70)\n",
    "    acts, timing, gpu, repro = extract_activations_with_monitoring(\n",
    "        model, tokenizer, prompt, \"BASELINE\"\n",
    "    )\n",
    "    conditions['baseline'] = {\n",
    "        'activations': acts,\n",
    "        'timing': timing,\n",
    "        'gpu': gpu\n",
    "    }\n",
    "    \n",
    "    # LIGHT CONCURRENT\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONDITION 2: LIGHT CONCURRENT INFERENCE\")\n",
    "    print(\"=\"*70)\n",
    "    concurrent = ConcurrentInference(model, tokenizer, intensity='light')\n",
    "    concurrent.start()\n",
    "    print(\"  [HIDDEN] Concurrent inference started\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    acts, timing, gpu, repro = extract_activations_with_monitoring(\n",
    "        model, tokenizer, prompt, \"LIGHT_CONCURRENT\"\n",
    "    )\n",
    "    conditions['light_concurrent'] = {\n",
    "        'activations': acts,\n",
    "        'timing': timing,\n",
    "        'gpu': gpu\n",
    "    }\n",
    "    \n",
    "    concurrent.stop()\n",
    "    print(\"  [HIDDEN] Concurrent inference stopped\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    # HEAVY CONCURRENT\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"CONDITION 3: HEAVY CONCURRENT INFERENCE\")\n",
    "    print(\"=\"*70)\n",
    "    concurrent = ConcurrentInference(model, tokenizer, intensity='heavy')\n",
    "    concurrent.start()\n",
    "    print(\"  [HIDDEN] Concurrent inference started\")\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    acts, timing, gpu, repro = extract_activations_with_monitoring(\n",
    "        model, tokenizer, prompt, \"HEAVY_CONCURRENT\"\n",
    "    )\n",
    "    conditions['heavy_concurrent'] = {\n",
    "        'activations': acts,\n",
    "        'timing': timing,\n",
    "        'gpu': gpu\n",
    "    }\n",
    "    \n",
    "    concurrent.stop()\n",
    "    print(\"  [HIDDEN] Concurrent inference stopped\")\n",
    "    \n",
    "    # ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FORENSIC ANALYSIS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    baseline_acts = conditions['baseline']['activations']\n",
    "    baseline_time = conditions['baseline']['timing']['median_ms']\n",
    "    baseline_gpu = conditions['baseline']['gpu']\n",
    "    \n",
    "    print(f\"\\nBaseline:\")\n",
    "    print(f\"  Time: {baseline_time:.2f}ms\")\n",
    "    if baseline_gpu:\n",
    "        print(f\"  GPU utilization: {baseline_gpu['p95']:.1f}%\")\n",
    "    \n",
    "    # FP Analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"FP FORENSICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fp_results = {}\n",
    "    \n",
    "    for cond_name in ['light_concurrent', 'heavy_concurrent']:\n",
    "        print(f\"\\n{cond_name.upper()}:\")\n",
    "        cond_acts = conditions[cond_name]['activations']\n",
    "        \n",
    "        all_exact = True\n",
    "        layer_results = {}\n",
    "        \n",
    "        for layer_name in baseline_acts.keys():\n",
    "            base = baseline_acts[layer_name]\n",
    "            cond = cond_acts[layer_name]\n",
    "            \n",
    "            bit_exact = np.array_equal(base, cond)\n",
    "            \n",
    "            if bit_exact:\n",
    "                print(f\"  {layer_name}: ✓ BIT-EXACT\")\n",
    "                layer_results[layer_name] = {\n",
    "                    'bit_exact': True,\n",
    "                    'l2': 0.0\n",
    "                }\n",
    "            else:\n",
    "                all_exact = False\n",
    "                l2 = float(np.linalg.norm(base - cond))\n",
    "                print(f\"  {layer_name}: ✗ DIFFERS (L2={l2:.6e})\")\n",
    "                layer_results[layer_name] = {\n",
    "                    'bit_exact': False,\n",
    "                    'l2': l2\n",
    "                }\n",
    "        \n",
    "        fp_results[cond_name] = {\n",
    "            'all_exact': all_exact,\n",
    "            'layers': layer_results\n",
    "        }\n",
    "    \n",
    "    # Timing Analysis\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TIMING FORENSICS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    timing_results = {}\n",
    "    \n",
    "    for cond_name in ['light_concurrent', 'heavy_concurrent']:\n",
    "        cond_timing = conditions[cond_name]['timing']\n",
    "        cond_time = cond_timing['median_ms']\n",
    "        cond_gpu = conditions[cond_name]['gpu']\n",
    "        \n",
    "        slowdown_ms = cond_time - baseline_time\n",
    "        slowdown_pct = (slowdown_ms / baseline_time) * 100\n",
    "        \n",
    "        # Significance test\n",
    "        baseline_std = conditions['baseline']['timing']['std_ms']\n",
    "        cond_std = cond_timing['std_ms']\n",
    "        combined_std = np.sqrt(baseline_std**2 + cond_std**2)\n",
    "        significant = abs(slowdown_ms) > 2 * combined_std\n",
    "        \n",
    "        print(f\"\\n{cond_name.upper()}:\")\n",
    "        print(f\"  Time: {cond_time:.2f}ms (baseline: {baseline_time:.2f}ms)\")\n",
    "        print(f\"  Slowdown: {slowdown_ms:+.2f}ms ({slowdown_pct:+.1f}%)\")\n",
    "        print(f\"  Significant: {'YES' if significant else 'NO'}\")\n",
    "        if cond_gpu:\n",
    "            print(f\"  GPU util: {cond_gpu['p95']:.1f}%\")\n",
    "        \n",
    "        timing_results[cond_name] = {\n",
    "            'time_ms': float(cond_time),\n",
    "            'slowdown_ms': float(slowdown_ms),\n",
    "            'slowdown_pct': float(slowdown_pct),\n",
    "            'significant': bool(significant),\n",
    "            'gpu_util': cond_gpu\n",
    "        }\n",
    "    \n",
    "    # Verdict\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"VERDICT\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    fp_works = not all(fp_results[c]['all_exact'] for c in fp_results)\n",
    "    timing_works = any(timing_results[c]['significant'] for c in timing_results)\n",
    "    \n",
    "    if fp_works and timing_works:\n",
    "        print(\"\\n✓ BOTH FP AND TIMING FORENSICS WORK\")\n",
    "        print(\"\\n  • FP forensics detects parallel workloads\")\n",
    "        print(\"  • Timing forensics confirms with measurable slowdown\")\n",
    "        print(\"  • Dual verification provides robust detection\")\n",
    "        print(\"\\n✓ FP ALONE SUFFICIENT\")\n",
    "        print(\"  • FP deviations provide strong signal\")\n",
    "        print(\"  • No need for timing if FP verification is in place\")\n",
    "    elif fp_works:\n",
    "        print(\"\\n✓ FP FORENSICS WORKS\")\n",
    "        print(\"  • Parallel workloads produce detectable FP deviations\")\n",
    "    elif timing_works:\n",
    "        print(\"\\n✓ TIMING FORENSICS WORKS\")\n",
    "        print(\"  • Parallel workloads produce measurable slowdown\")\n",
    "        print(\"  • FP forensics failed but timing rescues verification\")\n",
    "    else:\n",
    "        print(\"\\n⚠️  VERIFICATION FAILS\")\n",
    "        print(\"  • Neither FP nor timing detects parallel work\")\n",
    "        print(\"  • Blind spot exists for this workload size\")\n",
    "    \n",
    "    # Save comprehensive results\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Prepare detailed output matching console\n",
    "    output = {\n",
    "        'metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'gpu': torch.cuda.get_device_name(0),\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'cuda_version': torch.version.cuda,\n",
    "            'model': model_name,\n",
    "            'prompt_tokens': prompt_tokens,\n",
    "            'layer_indices': layer_indices\n",
    "        },\n",
    "        'baseline': {\n",
    "            'timing': conditions['baseline']['timing'],\n",
    "            'gpu_utilization': conditions['baseline']['gpu'],\n",
    "            'summary': {\n",
    "                'median_time_ms': float(baseline_time),\n",
    "                'gpu_util_p95': float(baseline_gpu['p95']) if baseline_gpu else None\n",
    "            }\n",
    "        },\n",
    "        'conditions': {\n",
    "            'light_concurrent': {\n",
    "                'timing': conditions['light_concurrent']['timing'],\n",
    "                'gpu_utilization': conditions['light_concurrent']['gpu']\n",
    "            },\n",
    "            'heavy_concurrent': {\n",
    "                'timing': conditions['heavy_concurrent']['timing'],\n",
    "                'gpu_utilization': conditions['heavy_concurrent']['gpu']\n",
    "            }\n",
    "        },\n",
    "        'fp_forensics': {\n",
    "            'description': 'Floating-point forensics - checking if concurrent work affects activations',\n",
    "            'results': {}\n",
    "        },\n",
    "        'timing_forensics': {\n",
    "            'description': 'Timing forensics - checking if concurrent work slows down inference',\n",
    "            'results': {}\n",
    "        },\n",
    "        'verdict': {}\n",
    "    }\n",
    "    \n",
    "    # Add detailed FP results\n",
    "    for cond_name in ['light_concurrent', 'heavy_concurrent']:\n",
    "        output['fp_forensics']['results'][cond_name] = {\n",
    "            'all_layers_bit_exact': fp_results[cond_name]['all_exact'],\n",
    "            'layer_by_layer': {}\n",
    "        }\n",
    "        \n",
    "        for layer_name, layer_data in fp_results[cond_name]['layers'].items():\n",
    "            output['fp_forensics']['results'][cond_name]['layer_by_layer'][layer_name] = layer_data\n",
    "    \n",
    "    # Add detailed timing results\n",
    "    for cond_name in ['light_concurrent', 'heavy_concurrent']:\n",
    "        timing_data = timing_results[cond_name]\n",
    "        output['timing_forensics']['results'][cond_name] = {\n",
    "            'median_time_ms': timing_data['time_ms'],\n",
    "            'baseline_time_ms': float(baseline_time),\n",
    "            'slowdown_ms': timing_data['slowdown_ms'],\n",
    "            'slowdown_percent': timing_data['slowdown_pct'],\n",
    "            'statistically_significant': timing_data['significant'],\n",
    "            'gpu_utilization': timing_data['gpu_util']\n",
    "        }\n",
    "    \n",
    "    # Add verdict with full interpretation\n",
    "    fp_works = not all(fp_results[c]['all_exact'] for c in fp_results)\n",
    "    timing_works = any(timing_results[c]['significant'] for c in timing_results)\n",
    "    \n",
    "    output['verdict'] = {\n",
    "        'fp_forensics_detects_parallel_work': fp_works,\n",
    "        'timing_forensics_detects_parallel_work': timing_works,\n",
    "        'summary': None,\n",
    "        'interpretation': []\n",
    "    }\n",
    "    \n",
    "    if fp_works and timing_works:\n",
    "        output['verdict']['summary'] = 'BOTH_FP_AND_TIMING_WORK'\n",
    "        output['verdict']['interpretation'] = [\n",
    "            'FP forensics detects parallel workloads',\n",
    "            'Timing forensics confirms with measurable slowdown',\n",
    "            'Dual verification provides robust detection',\n",
    "            'FP alone is sufficient - provides strong signal',\n",
    "            'No need for timing if FP verification is in place'\n",
    "        ]\n",
    "    elif fp_works:\n",
    "        output['verdict']['summary'] = 'FP_WORKS'\n",
    "        output['verdict']['interpretation'] = [\n",
    "            'Parallel workloads produce detectable FP deviations',\n",
    "            'FP forensics alone is sufficient for detection'\n",
    "        ]\n",
    "    elif timing_works:\n",
    "        output['verdict']['summary'] = 'TIMING_WORKS'\n",
    "        output['verdict']['interpretation'] = [\n",
    "            'Parallel workloads produce measurable slowdown',\n",
    "            'FP forensics failed but timing rescues verification',\n",
    "            'Timing must be part of verification protocol'\n",
    "        ]\n",
    "    else:\n",
    "        output['verdict']['summary'] = 'VERIFICATION_FAILS'\n",
    "        output['verdict']['interpretation'] = [\n",
    "            'Neither FP nor timing detects parallel work',\n",
    "            'Blind spot exists for this workload size',\n",
    "            'Further investigation needed'\n",
    "        ]\n",
    "    \n",
    "    # Add statistical summary\n",
    "    if fp_works:\n",
    "        max_l2_light = max(\n",
    "            layer['l2'] for layer in fp_results['light_concurrent']['layers'].values()\n",
    "        )\n",
    "        max_l2_heavy = max(\n",
    "            layer['l2'] for layer in fp_results['heavy_concurrent']['layers'].values()\n",
    "        )\n",
    "        output['verdict']['fp_signal_strength'] = {\n",
    "            'light_max_l2': float(max_l2_light),\n",
    "            'heavy_max_l2': float(max_l2_heavy)\n",
    "        }\n",
    "    \n",
    "    if timing_works:\n",
    "        output['verdict']['timing_impact'] = {\n",
    "            'light_slowdown_pct': timing_results['light_concurrent']['slowdown_pct'],\n",
    "            'heavy_slowdown_pct': timing_results['heavy_concurrent']['slowdown_pct']\n",
    "        }\n",
    "    \n",
    "    output_file = f'realistic_parallel_forensics_{timestamp}.json'\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(output, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n[SAVED] {output_file}\")\n",
    "    print(\"        (Includes all analysis details from console output)\")\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"TEST COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095a59cd-8b4f-4163-9169-f7bca78561e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
